### **Part I: Research Philosophy and Strategy**
 
- **Neel Nanda's Research Process Framework**

	- 1.1. How I Think About My Research Process: Explore, Understand, Distill

		- Stage 1: Ideation - Choosing a Problem
		- Stage 2: Exploration - Gaining Surface Area
		- Stage 3: Understanding - Testing Hypotheses
		- Stage 4: Distillation - Compressing, Refining, and Communicating
	- 1.2. Key Mindsets for Research: Truth-Seeking, Prioritization, Moving Fast

		- Truth-Seeking and Resisting Bias
		- Prioritization and Goal-Setting
		- Moving Fast and Acting Under Uncertainty
	- 1.3. Understanding and Cultivating Research Taste

		- Decomposing Taste: Intuition, Conceptual Frameworks, and Strategic Picture
		- Methods for Cultivating Taste: Leveraging Mentors, Papers, and Reflection
- **Jacob Steinhardt on Research as a Stochastic Decision Process**

	- 2.1. Prioritizing by Information Rate vs. Naive Strategies
	- 2.2. De-risking, Front-loading Information, and Practical Patterns
	- 2.3. Research as a Branching Search Tree
- **Advice on Writing Machine Learning Papers**

	- 3.1. The Essence of a Paper: Crafting a Cohesive Narrative
	- 3.2. Providing Rigorous Supporting Evidence and Avoiding Pitfalls
	- 3.3. Iterative Writing Process: Compress then Expand
	- 3.4. Detailed Paper Structure: Abstract, Introduction, Main Body, Figures, etc.

---

### **Part II: Foundations of Mechanistic Interpretability**

- **Core Concepts and Terminology**

	- 1.1. A Comprehensive Mechanistic Interpretability Explainer & Glossary (Neel Nanda)

		- General Concepts: Features, Circuits, Decomposability
		- Representations: Linear Representation Hypothesis, Privileged Basis
		- Superposition: Bottleneck vs. Neuron Superposition, Polysemanticity
		- Transformer-Specific Concepts and Techniques
- **Surveys of the Field: Key Papers and Open Problems**

	- 2.1. An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers (Neel Nanda)

		- Foundational Work (e.g., A Mathematical Framework for Transformer Circuits)
		- Superposition & Sparse Autoencoders (SAEs)
		- Activation Patching & Causal Interventions
		- Narrow Circuits (e.g., Indirect Object Identification)
	- 2.2. A Primer on the Inner Workings of Transformer-Based Language Models (Javier Ferrando et al.)

		- Transformer Components and the Residual Stream Perspective
		- Techniques for Behavior Localization (Attribution, Causal Interventions)
		- Techniques for Information Decoding (Probing, SAEs)
		- Discovered Inner Behaviors and Known Circuits
	- 2.3. Open Problems in Mechanistic Interpretability (Lee Sharkey et al.)

		- Challenges in Methods: Reverse Engineering, Decomposition, and Validation
		- Challenges in Applications: Monitoring, Control, Prediction, and Microscope AI
		- Socio-technical and Governance Challenges

---

### **Part III: Tooling & Hands-On Tutorials**

- **TransformerLens: A Library for Mechanistic Interpretability**

	- 1.1. Introduction and Getting Started
	- 1.2. Key Features: Hooks, Activation Caching, Model Loading
	- 1.3. ARENA Tutorials with TransformerLens:

		- 1.3.1. Building a Transformer from Scratch
		- 1.3.2. Introduction to Mech Interp & Finding Induction Heads
		- 1.3.3. Indirect Object Identification (IOI) Circuit Analysis
		- 1.3.4. Toy Models of Superposition & Sparse Autoencoders (SAEs)
- **NNsight: A Library for Transparent Science on Black-Box AI**

	- 2.1. Introduction and Getting Started with Remote Execution (NDIF)
	- 2.2. Core Concepts: The Intervention Graph
	- 2.3. Key Features: Getting/Setting Activations, Gradients, Cross-Prompt Interventions, Multi-Token Generation
	- 2.4. ARENA Tutorial with NNsight: Function Vectors & Model Steering

# [How I Think About My Research Process: Explore, Understand, Distill](https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand)

by [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1?from=post_header)26th Apr 2025*This is the first post in a sequence about how I think about and break down my research process. Post 2 is coming soon.*

*Thanks to Oli Clive-Griffin, Paul Bogdan, Shivam Raval and especially to Jemima Jones for help and feedback, and to my co-author Gemini 2.5 Pro - putting 200K tokens of past blog posts and a long voice memo in the context window is OP.*

## Introduction

Research, especially in a young and rapidly evolving field like mechanistic interpretability (mech interp), can often feel messy, confusing, and intimidating. Where do you even start? How do you know if you're making progress? When do you double down, and when do you pivot?

These are far from settled questions, but I’ve supervised 20+ papers by now, and have developed my own mental model of the research process that I find helpful. This isn't *the* definitive way to do research (and I’d love to hear other people’s perspectives!) but it's a way that has worked for me and others.

My goal here is to demystify the process by breaking it down into stages and offering some practical advice on common pitfalls and productive mindsets for each stage. I’ve also tried to be concrete about what the various facets of ‘being a good researcher’ actually mean, like ‘research taste’ ([see post 3](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research)). I’ve written this post for a mech interp audience, but hopefully it is useful for any empirical science with short feedback loops, and possibly even beyond that.

This guide focuses more on the *strategic* (high-level direction, when to give up or pivot, etc) and *tactical* (what to do next, how to prioritise, etc) aspects of research – the "how to think about it" rather than just the "how to do it." Some of skills (coding, reading papers, understanding ML/mech interp concepts) are vital for how to do it, but not in scope here (I recommend the [ARENA curriculum](https://arena-chapter1-transformer-interp.streamlit.app/) and [my paper reading list](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite) if you need to skill up).

How to get started? Strategic and tactical thinking are hard skills, and it is rare to be any good at them when starting out at research (or ever tbh). The best way to learn them is by trying things, making predictions, seeing what you get right or wrong (i.e., getting feedback from reality), and iterating. Mentorship can substantially speed up this process by providing ["supervised data" to learn from](https://colah.github.io/notes/taste/), but either way you ultimately learn by doing.

I’ve erred towards making this post comprehensive, which may make it somewhat overwhelming. You do *not*need to try to remember everything in here! Instead think of it more as a guide for the high level things to keep in mind, and a source of advice for what to do at each stage. And, obviously, this is massively flavoured by my own subjective experience and may not generalise to you - I’d love to hear what other researchers think.

**A cautionary note:** Research is hard. Expect frustration, dead ends, and failed hypotheses. Imposter syndrome is common. Focus on the process and what you're learning. Take breaks, the total change to productive time is typically positive. Find sustainable ways to work. [Your standards are likely too high](https://www.neelnanda.io/blog/35-standards).

## The key stages

I see research as breaking down into a few stages:

1. **Ideation - Choose a problem/domain to focus on**
2. **Exploration - Gain Surface area**
	1. **North star**: Gain information
3. **Understanding - Test Hypotheses**
	1. **North star**: Convince *yourself*of a key hypothesis
4. **Distillation - Compress, Refine, Communicate**
	1. **North star**: Compress your research findings into concise, rigorous truth that you can communicate to the world

### Ideation (Stage 1): Choose a problem

- This can vary from a long, high-effort exploration across areas looking for a promising angle, to just being handed a problem by a mentor.
	- Replicating and extending an existing paper can be a good starting point, especially if you don’t have an existing mentor.
- This stage is crucial, but if you have a mentor (or other high quality source of suggestions, like someone else’s research agenda) it can be quick to just lean on them.
- It's important to understand how your work fits into the existing literature: what is already known about the problem and what remains open.
	- Where possible, for your first project or two, lean on a mentor for guidance and just read a few key papers. Building deep knowledge of a literature takes time, and is easier once you have some hands-on experience.
	- Google/OpenAI Deep Research is invaluable for literature reviews, especially in unfamiliar domains.
- Doing this well yourself and choosing a good problem often requires "**research taste**", and is the most commonly discussed aspect, but [is **just one facet of what research taste means**](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research) - research taste also covers the following:
	- Exploration: **Noticing** **when an anomaly is interesting** and should be investigated, vs boring and to be ignored
	- Understanding: **Designing great experiments** that precisely distinguish hypotheses. This often stems from having a deep enough conceptual understanding to intuit *why*a hypothesis is true
	- Distillation: Having the taste to **identify the most interesting and defensible narrative**, and what to deprioritise.
	- On a broader level, I see research taste as being about an intuitive understanding of what good research looks like, to both guide high level strategy and tactical decisions in practice, informed by a deep understanding of the domain, familiarity with what good and bad research looks like, and the high level strategic picture of which problems actually matter.

### Exploration (Stage 2): Gain surface area

- *Examples:*[*My research streams*](https://www.youtube.com/watch?v=m8tzXelUTLo&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T)*, and*[*my Othello research process write-up*](https://www.alignmentforum.org/s/nhGNHyJHbrofpPbRG/p/TAz44Lb9n9yf52pv8)
- At the start, your understanding of the problem is often vague. Naively, it’s easy to think of research as being about testing specific hypotheses, but in practice you often start out not even knowing the right questions to ask, or the most promising directions. The exploration stage is about moving past this.
	- E.g. starting with “what changes in an LLM during chat fine-tuning?” or even “I’m sure there’s something interesting about how chat models behave, let’s mess around and find out”
- **Your north star is just to gain information** - do exploratory experiments, visualise data, follow your curiosity, prioritise moving fast.
- Junior researchers often get stuck in the early stages of a project and don’t know what to do next. In my opinion this is because **they** **think they are in the understanding stage, but are actually in the exploration stage**.
	- That is, they think they ought to have a clear goal, and hypothesis, and obvious next step, and feel bad when they don’t. But this is totally fine and normal!
	- The solution is to have a toolkit of standard ways to gain surface area, brainstorm experiments that might teach something interesting, and be comfortable exploring a bunch and hoping something interesting happens.
- Not having a clear goal/next step doesn’t mean that you don’t need to prioritise! **Prioritise for information gain**.
	- Try to do a lot of experiments (and don’t be a perfectionist about finding the ‘best’ experiments!), visualise things in many different ways, ensure you’re always learning.
	- Frequently ask yourself “**am I getting enough information per unit time**?” If you haven’t learned anything recently, shake it up.
	- Having fast feedback loops and powerful, flexible tooling is absolutely crucial here.
- Note: **In the long-term exploration should feel like play** - be fascinated by a problem, follow your curiosity, try to understand it deeply, zooming out when you get bored, etc (though it's still worth checking in on whether you're in a rabbit hole). But this isn't something you should worry about at first, as it needs well calibrated intuitions, which take time.
- Note: often most of the work in the exploration was about **discovering the right kinds of questions to be asking**, e.g. that where information was stored is an important and interesting question, crystallising that into a precise hypothesis is often easy after that.
	- This both means ‘identify the right questions to ask’, but also gain a **deeper understanding and intuition of the domain** so you can design experiments that make sense, and **build a more gears-level model** of why a certain question may or may not be true.
- A key practical tip is to **keep a highlights doc** of particularly interesting results, this makes it easier to spot connections

### Understanding (Stage 3): Test Hypotheses

- This stage begins when you understand the problem domain enough to **have some specific hypotheses that you think are interesting** - hypotheses you can write down, and have some idea of what evidence you could find to show if they’re true or false.
	- E.g. “do chat models store summarised information about the user prompt in the <end\_of\_turn> special token?”
- Your north star is to **gain evidence for and against these hypotheses**
	- Here the prioritisation is a mix of goal-directed and exploratory - you often need to briefly dip back into explore mode as you realise your hypothesis was ill-posed, your experiment didn’t make sense, you get weird and anomalous results, etc.
	- This stage is much closer to what people imagine when thinking about research.
	- Frequently ask yourself “**what am I learning and is it relevant?**”
- The mark of a good researcher is a deep commitment to **skepticism of your results**.
	- You’ll have hypotheses that are wrong, experiments that are inconclusive, beautiful methods that lose to dumb baselines, etc. This is totally fine and normal, and a part of the natural process of science, but emotionally can be pretty hard to accept.
	- This *sounds*obvious, but in practice this requires constant active effort, and if you are not actively doing this you’ll inevitably fall into traps. Always seek alternative explanations, seek and implement strong baselines, check for bugs, etc.
- A surprisingly deep and nuanced skill is **designing good experiments**. I think of this as one facet of “[research taste](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research)”
	- A great experiment elegantly, and conclusively distinguishes between several plausible hypotheses, validates non-trivial predictions made by one hypothesis, and is tractable to implement in practice.
		- This is an ideal rarely reached in practice but helpful to have in mind
	- My internal experience when generating good experiments is often that I try to simulate the world where hypothesis X is true, think through what this would mean and all the various implications of this, and notice if any can be turned into good experiments.
	- When reading papers, pay attention to the key experiments that their core claims hinge upon and ask yourself what made it important and how you might've thought of that experiment.

### Distillation (Stage 4): Compress, Refine, Communicate

- This stage begins when you have **enough evidence for*****you*****to be fairly convinced that your hypotheses are true/false**
- The north star here is to **distill your research findings**into**concise, rigorous truth**that you can**communicate to the world**
	- **Compress**your work into some concrete, well-scoped claims - something you could list in a few bullet points. Compress it as far as you can without losing the message. Readers will not take away more than a few claims.
		- How would you explain your work to a peer? How would you write a lightning talk?
	- **Refine**your evidence into a rigorous case for each key claim, enough to be persuasive to a skeptical observer
		- This is persuasive in the sense of “actually provide strong evidence”, not just writing well enough that people don’t notice flaws! This means sanity checks, statistical robustness, and strong baselines.
		- Note that this is a higher bar than convincing yourself, both since you’re aiming for a more skeptical observer and you need to make all the key evidence you’ve seen legible to an outsider.
		- You should spend a lot of time on red-teaming here - what could you be missing? What alternative hypotheses could explain your observations? What experiments could distinguish between them? Etc
	- **Communicate**these with a clear and concise write-up - make clear what your points are, what evidence you provide, and its limitations. Write to inform, not persuade - if you are clear (a high bar), and your results are interesting, people will likely appreciate your work.
		- The form of write-up doesn’t really matter - Arxiv paper, blog post, peer-reviewed paper, etc. It doesn’t need to be polished, it just needs to present the evidence clearly, and to have strong enough evidence to meaningfully inform someone’s opinion
- **People often under-rate this stage** and think doing the write-up is wasting time better spent on research, and can be left to the last minute. I think it’s actually a great use of time, at least for the first draft! I typically recommend my scholars make a start on distillation a month before conference deadlines.
	- Writing things up forces you to clarify your understanding to yourself. You also often notice holes and missing experiments. A common anecdote is that people didn’t really understand their project until they wrote it up.
	- If you don’t communicate your research well, it’s very hard to have an impact with it! (or to get recognition and career capital)
- Conversely, **people often over-rate this stage**and default to writing a paper with the main goal of getting accepted to a conference. This has obvious advantages, but can also lead to warped thinking if you’re thinking about it from the start.
	- E.g. choosing questions that look good rather than being important, or focusing on forms of evidence that reviewers will like or understand, rather than ruthlessly focusing on actually establishing what’s true.
- Sometimes you’ll discover that actually things are way messier than thought. It’s important to acknowledge this, rather than denying inconvenient truths! **Your ultimate goal is to find truth, not to produce an exciting paper**. You may need to go back to understanding or even exploration - this is totally fine and normal, and does not mean you’ve screwed anything up.

*Next up:*[*Post 2 of the sequence*](https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking)*, on key research mindsets*

# [My Research Process: Key Mindsets - Truth-Seeking, Prioritisation, Moving Fast](https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking)

by [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1?from=post_header)27th Apr 2025*This is post 2 of a sequence on my framework for doing and thinking about research.*[*Start here*](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/hjMy4ZxS5ogA9cTYK)*.*

Before I get into what exactly to do at each stage of the research process, it’s worth reflecting on the key mindsets that are crucial throughout the process, and how they should manifest at each stage.

I think the most important mindsets are:

- ***Truth-seeking***: By default, many research insights will be false - finding truth is hard. It’s not enough to just know this, **you must put in active effort to be skeptical and resist bias**, lest you risk your research being worthless.
- ***Prioritisation***: You have finite time, and a *lot*of possible actions. **Your project will live or die according to whether you pick good ones.**
- ***Moving fast***: You have finite time and a lot to do. This doesn’t just mean “push yourself to go faster” - **there’s a lot of ways to eliminate inefficiency without sacrificing quality**.
	- In particular, you must **learn to act without knowing the “correct” next step**, and avoid analysis paralysis.

**Warning**: It is extremely hard to be anywhere near perfect on one of these mindsets, let alone all three. I’m trying to describe an ideal worth aiming towards, but you should be realistic about the amount of mistakes you will make - I certainly am nowhere near the ideal on any of these! **Please interpret this post as a list of ideals to aim for, not something to beat yourself up about failing to meet.**

## Truth Seeking

Our ultimate goal in doing research is to uncover the truth about what’s really going on in the domain of interest. The truth exists, whether I like it or not, and being a good researcher is about understanding it regardless.

- This *sounds* pretty obvious. Who doesn't like truth? It’s easy to see this section, dismiss it as obvious and move on. But in practice this is extremely hard to achieve.
	- We have [many biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases) that cut against finding truth
	- Insufficient skepticism doesn't *feel* like insufficient skepticism from the inside. It just feels like doing research.
- This means that **you must be putting in constant active effort into ensuring your results are robust**. This **must be integrated into part of your research process** - if you’re not, then there’s a good chance your results are BS.
	- “[Just try harder to be skeptical](https://www.neelnanda.io/blog/mini-blog-post-6-stop-pressing-the-try-harder-button)” is empirically a fairly ineffective strategy
	- One of the most common reasons I dismiss a paper is because I see a simple and boring explanation for the author’s observations, and they didn’t test for it - this often renders the results basically worthless.
		- I’d estimate that at least 50% of papers are basically useless due to insufficient skepticism

**What does putting in active effort actually mean**?

This takes different forms for the different stages:

- For exploration, the key failure mode is **not being creative enough when thinking about hypotheses**, getting attached to one or two ideas, and missing out on what’s actually going on.
	- Resist the urge to move on to the understanding stage the moment you have a plausible hypothesis - are there any unexplained anomalies? Could you do more experiments to gain more surface area first? What other hypotheses could explain your results? Etc
	- The standard hypothesis testing framework can be misleading here, because it has an implicit frame of being able to list all the hypotheses. But actually, **most of your probability mass should normally be on “something I haven’t thought of yet”**
		- You should regularly zoom out and look for alternative hypotheses for your observations. Asking another researcher, especially a mentor is a great source of perspective, asking LLMs is very cheap and can be effective.
		- That said, I still often find it helpful to think in a Bayesian way when doing research - if I have two hypotheses, how likely was some piece of evidence under each, and how should I update? Exploration often finds scattered pieces of inconclusive evidence, and there’s a skill to integrating them well.
	- **It’s not too bad if you end up believing false things for a bit**, the key thing is to move fast and reflexively try to falsify any beliefs you form, so you don’t get stuck in a rabbit hole based on false premises. This means it’s totally fine to investigate case studies and qualitative data, e.g. a deep dive into a single prompt.
		- If you’re getting lots of (diverse) information per unit time you’ll notice any issues.
	- **It is also an issue if you are*****too*****skeptical** and don’t let yourself explore the implications of promising but unproven hypotheses, as this is crucial to designing good experiments
- For understanding, you want to be careful and precise about **what your experiments*****actually*****show you**, **alternative explanations** for your results, whether your **experiments make sense on a conceptual level**, etc.
	- Here the Bayesian frame is often helpful. It’s generally overkill to put explicit numbers on everything, but it reminds me to ask the question “**was this observation*****more*****likely under hypothesis A or B**”, not just whether it was predicted by my favourite hypothesis
	- In exploration it’s OK to be somewhat qualitative and case study focused, but here you want to be more quantitative. If you must do **qualitative case studies**, do them on **randomly sampled things**, (or at least several examples, if your sampling space is small) )since it’s *so* easy to implicitly cherry-pick
		- The one exception is if your hypothesis is “there exists at least one example of phenomenon X”, e.g. ‘[we found multidimensional SAE latents](https://arxiv.org/abs/2405.14860)’.
- For distillation, in addition to the above, it’s important to **avoid the temptations of choosing a narrative that looks good**, rather than the best way to communicate the truth.
	- E.g. [**publishing**](https://arxiv.org/abs/2502.16681)[**negative**](https://arxiv.org/abs/2410.19278)[**results**](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)
		- While it can be emotionally hard to acknowledge to *myself*that my results are negative, mechanistic interpretability has a healthy culture and **I’ve gotten nothing but positive feedback for publishing negative results**.
	- E.g. **exaggerating results** or stating an **overconfident narrative** to seem more publishable.
		- I find it pretty easy to tell when a paper is doing this - generally you should care more about impressing the more experienced researchers in a field, who are least likely to be fooled by this! So I don’t even think it’s a good selfish strategy.
	- E.g. **not acknowledging and discussing key limitations.**
		- If I notice a key limitation that a paper has not addressed or acknowledged, I think far less of the paper.
		- If a paper discusses limitations, and provides a nuanced partial rebuttal, I think well of it.

## Prioritisation

Ultimately, time is scarce. The space of possible actions you can take when doing research is wide and open ended, and some are far more valuable than others. **The difference between a failed and a great research project is often prioritisation skill.** Improved prioritisation is one of the key sources of value I add as a mentor

- Fundamentally, **good prioritisation is about having a clear goal (north star) in mind.**
	- You need **good judgement** about how well different actions achieve this goal
		- You need to **actually make the time** to think about how well actions achieve this goal!
	- You need to **be ruthless**about dropping less promising directions where necessary.
		- But **beware switching costs** - if you switch all the time without exploring anything properly you’ll learn nothing!
- The goals at each stage are:
	- *Ideation:***Choose a fruitful problem**
	- *Exploration*: **Gain information and surface area on the problem**
	- *Understanding*: **Find enough evidence to convince*****you*****of some key hypotheses**
	- *Distillation*: **Distill your research into concise, well-supported truth, and communicate this to the world.**
- Being great at prioritisation is pretty difficult, and requires good research taste, which will take a lot of time to develop. But **there’s often basic mistakes and low-hanging fruit to improve, if you just try.**
	- The first step is just making time to stop and ask yourself “**do I endorse what I’m doing, and could I be doing something better**?”
		- This advice may seem obvious, but is deceptively hard to put into practice! You need regular prompts  **Often it’s very easy to think of a better idea, but by default nothing prompts you to think.**
	- I like to **explicitly write goals down and regularly check in** that they’re being achieved - it sounds obvious, but you would be shocked at how effective it is to ask people if they’re doing the best thing for the project goals. I think in 3 tiers of goals:
		- Goal: What is the overall north star of the project? (generally measured in months)
		- Sub-goal: What is my current bit of the project working towards (measured in weeks)
		- Objective: What is the concrete short-term outcome I am aiming for right now (measured in days, e.g. 1 week)
	- I recommend **actually writing a plan**, and **estimate how long each step will take**, at least for the current research stage you’re in.
		- You don’t need to take it very seriously, and you’ll totally deviate a ton.
		- But **it forces you to think through the project**, notice uncertainties you could ask someone about, question if parts are really necessary to achieve your goals.
		- This is most important for understanding & distillation, though *can*be useful for exploration
		- **If you feel stuck,**[**set a 5 minute timer**](https://www.neelnanda.io/blog/post-28-on-creativity-the-joys-of-5-minute-timers) and brainstorm possible things you could do!
		- I typically wouldn’t spend more than a few hours on this
			- Unless you have a mentor giving high quality feedback - then it’s a great way to elicit their advice!
			- But even then, feel free to deviate - mentors typically have good research *priors*, but you know way more about your specific problem than them, which can be enough to make better decisions than even a very senior researcher
- **You need to prioritise at many different layers of abstraction**, from deciding when to move on from an experiment to deciding which hypothesis to test first to deciding when to give up on testing a hypothesis and pivot to something else (or just back to exploration)
- **Prioritising and executing are different mental modes and should not be done simultaneously**. Keep them separate, and make time to regularly reflect, and time to lock-in and execute on a plan without stressing about if it’s the best plan
	- Concrete advice: Work to a schedule where you regularly (ideally at least once a day, and with extended reflection at least once a week), zoom out and check that what you’re doing is your highest priority. E.g. work in pomodoros
	- **Having a**[**weekly review**](https://www.neelnanda.io/blog/39-reflection)**can be incredibly useful** -  where you zoom out and check in on what’s going on, any current issues, etc. Some useful prompts:
		- What is my goal right now?
		- What progress have I made towards that goal?
		- What’s consumed the most time recently?
		- What’s blocked me?
		- What mistakes have I made, and how could I systematically change my approach so it doesn’t happen again in future?
		- What am I currently confused about?
		- Am I missing something?
- See Jacob Steinhardt’s [excellent blog post on research prioritisation](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html).
- **Warning**: Different people need to hear different advice! (An eternal issue of writing public advice…). Some get stuck in rabbit holes and need to get better at moving on. Others get caught in analysis paralysis and never do *anything*, because they’re always waiting for the (non-existent) perfect opportunity.
	- **Real prioritisation is about a careful balance between exploration and exploitation**.
	- You probably know which failure mode you tend towards. **Please focus on the advice relevant to you, and ignore the rest**!

## Moving Fast

A core aspect of taking action in general is being able to move fast. Researchers vary a lot in their rate of productive output, and it gets very high in the best people - this is something I value a lot in potential hires.

This isn’t just about working long hours or cutting corners - there’s a lot of skill to **having fast feedback loops**, **noticing and fixing inefficiency** where appropriate, and **being able to take action or reflect where appropriate**. In some ways this is just another lens onto prioritisation.

- **Tight feedback loops are crucial**: A key thing to track when doing research is your feedback loops.
	- **Definition**: A **feedback loop** is the process from having an experiment idea and to results. Tight feedback loops are when the time taken is short.
	- It will make an enormous difference to your research velocity if you can get your feedback loops as tight as possible, and **this is a big priority**.
		- This is because you typically start a project confused, and **you need to repeatedly get feedback from reality to understand what’s going on**. This inherently requires a bunch of feedback loops that can’t be parallelised, so you want them to be as short as possible.
		- This is one of the big advantages of mech interp over other fields of ML - we can get much shorter feedback loops.
	- A mindset that I often find helpful is a deep-seated sense of impatience and **a feeling that something should be possible to do faster**. Sometimes I just need to accept that it will take a while, but often there is a better way, or at least a way that things can be reduced.
	- **Coding in a notebook is a lifesaver** (eg Jupyter, VS Code Interactive Mode or Colab)
	- Tips for tight feedback loops in mech interp:
		- Putting your data in a data frame rather than in a rigid plotting framework like Weights and Biases allows you to try arbitrary visualizations rapidly.
		- De-risking things on the smallest model you can, such as writing code and testing it on a small model before testing it on the model you're actually interested in.
		- Train things on fairly small amounts of data just to verify that you're seeing signs of life.
		- Sometimes there’s irreducible length, e.g. you need to train a model/SAE and this takes a while, but you can still often do something - train on less data, have evals that let you fail fast, etc.
- **Good tooling accelerates everything**. All stages benefit from **flexible exploration tools**(e.g., interactive notebooks, libraries like TransformerLens or nnsight), efficient infrastructure for running experiments, and helpful utilities (e.g., plotting functions, data loaders).
	- Flexible tooling tightens feedback loops by shortening the time between an arbitrary creative experiment idea and results, even if it’s less efficient for any given idea.
	- The balance shifts: more flexibility needed early, more optimization/robustness potentially useful later e.g. during the distillation stage it can make sense to write a library to really easily do a specific kind of fine-tuning run that happens a ton
- A corollary of this is that **you should (often) do fast experiments first**. It is far better to do a quick and dirty experiment to get some preliminary signs of life than an extremely long and expensive experiment that will produce conclusive data but only after weeks of work.
	- Realistically you should be prioritising by information gain per unit time.
	- This is especially important in exploration where it's hard to have a clear sense of which experiments are the most useful while estimating their tractability is pretty easy. When distilling you may know enough to be comfortable implementing a long running but conclusive experiment.
- **Audit your time**. It's all well and good to talk about the importance of speed and moving fast, but how do you actually do this in practice? One thing that might be helpful is to log how you spend your time and then reflect on it, and ways you might be able to go faster next time.
	- For example, you could use a tool like [Toggl](https://toggl.com/) to roughly track what you're doing each day and then look back on how long everything took you and ask, "**How could I have done this faster**? Was this a good use of my time?"
		- Often it’s easy to fix inefficiencies and the hard part is noticing them - e.g. making a util function for a common tedious task, or noticing things that an LLM could automate.
	- Note: It is *not*productive to look back and feel really guilty about wasting time. **Nobody is perfect and you will always waste time**. I am advocating for maintaining a mindset of **optimism that you will be able to do even better next time**.
- **Fail fast**. One of the largest time sinks possible is **investing weeks to months of effort into a failed research direction**. Thus, a key question to ask yourself is: if this direction is doomed, how could I discover this as fast as humanly possible?
	- I often try to think through what kind of confident predictions a hypothesis I care about makes in the understanding stage, or what fundamental assumptions make me think my domain is interesting at all in the exploration stage, and then think of the quickest and dirtiest experiments I can to test these.
		- It's often much better to have several quick and dirty experiments to attack different angles where you could fail fast than to put a lot of effort into one.
- **Are you moving*****too*****fast?**This is a natural pushback to the advice of ‘try hard to move fast’. It’s easy to e.g. be sloppy in the name of speed and introduce many bugs that cost you time in the long-run.
	- This is a hard balance, and I largely recommend just exploring and seeing how things go. But there *are*often things that can speed you up beyond ‘just push yourself to go harder in the moment’, which don’t have these trade-offs, like choosing the right experiments to run.
	- **Make sure you still regularly take time to think and reflect, rather than feeling pressure to constantly produce results**

### Taking action under uncertainty

A difficulty worth emphasising when trying to move fast is that there are a *lot*of possible next steps when doing research. And it’s pretty difficult to predict how they’ll go. Prioritisation remains crucial, but this means it’s also very hard, and **you will be highly uncertain about the best next step**. A crucial mindset is **being able to do something anyway, despite being so uncertain.**

- As a former pure mathematician, this is something I’ve struggled a fair bit with - I miss doing things grounded in pure, universal truth! But it’s learnable
- Ultimately, you just need to accept on an emotional level that you don’t get to know the “right” answer for what to do next - **in practice, there’s no such thing as the right answer**.
	- The ideal is to strive to carefully evaluate the extremely noisy evidence, make a best guess for what to do next, and act on it, while also being self-aware enough to notice if it no longer seems the best action. This is a hard balance to achieve, but super useful if you can do it.
- Especially when you’re starting out, this can be very low stakes: **the value of anything you do is dominated by the learning value**! If you make bad decisions you will learn and can do better next time, so it’s hard to really have a bad outcome.

# [My Research Process: Understanding and Cultivating Research Taste](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research)

by [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1?from=post_header)2nd May 2025*This is post 3 of a sequence on my framework for doing and thinking about research.*[*Start here*](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/hjMy4ZxS5ogA9cTYK)*. Thanks to my co-author Gemini 2.5 Pro*

# Introduction

Spend enough time around researchers, and you'll hear talk of "research taste." It's often presented as a somewhat mystical quality distinguishing the seasoned research from the novice – an almost innate sense for which research ideas will flourish and which will fail. While I believe research taste is very real, incredibly valuable, and a key differentiator I look for, I *don't* think it's mystical or innate. Talent plays an important role, but taste is largely learned, and with the right mindset you can learn faster.

**What is research taste?**As I define it, research taste is far broader than just picking the right problem at the outset. Research is full of key decisions that will affect the future of the project, without an obvious way to find the right answer: from choosing the research problem itself, to identifying which anomalies are and are not worth exploring, distinguishing an experiment that will be compelling from one that’ll have inconclusive results, etc. I think of taste as **the set of intuitions and good judgment that guide a researcher’s decisions*****throughout*****the research process**, any time an ambiguous or open-ended decision like this arises. This can just be gut feeling, but also having conceptual frameworks you reason through, having novel ideas spark in your mind, etc.

**Where does taste come from?**If you're new to research, feeling like you lack "taste" is completely normal and expected. You don't need perfect judgment to start. In fact, trying to force it early on can be counterproductive. Think of training your intuition like training a network. It starts poorly initialized and needs lots of diverse, high-quality training data (i.e., research experience). With time, people often develop fairly deep and sophisticated taste, as they see enough examples of research outcomes, but this generally isn’t something people start with.

**How to learn it?**In my opinion, research taste is one of the hardest skills to learn for being a good researcher. To see why, let's lean more into this analogy of training a neural network. The core problem is **you just don't get that much data**. Generally the shorter a feedback loop is the more data you will get. By definition research taste is about things that are not immediately obvious. For designing a good experiment, sometimes you can get results from hours to day, but feedback on whether a research idea was good can take months!

I think the main way to speed it up is by **getting more data**, and by **being more sample efficient** about the data that you have. To get more data the easiest way is to **lean on sources of supervised data:**ideally**a mentor**, or **seeing what worked in papers**. You can also get more from each data point - analyse it in detail before setting the feedback, **predict your mentor’s answers before they give them**, etc. When you have made a research decision and you eventually get feedback, do a post-mortem analyzing what did and did not work and why and what general themes you could look at in future.

But even with all that, **expect learning taste to take a while**, especially high level strategic things like choosing a project - learning speed depends on your feedback loops, and taste has very slow ones. Further, research taste often translates poorly from other fields, or comes with counter-productive habits

# What is Taste?

As discussed, I define **research taste** broadly: **it's the collection of intuitions and judgments that guide good decision-making throughout a research project,** especially where feedback loops are long, and the search space is large and open-ended.

I take such a broad definition, because I think that the ability to make good judgements is a fairly general skill, and improving at one facet often helps you improve at all of them, by e.g. getting better conceptual frameworks and domain knowledge.

While **Problem Selection** (strategic judgment about tractability and interest) is the most visible aspect, research taste also covers:

- **Exploration:** A tactical sense for which experiments yield the most insight, recognizing interesting anomalies versus noise, knowing when to dig deeper or move on from a thread. *Does this surprising result feel like a key insight or a distracting artifact?*
	- My internal experience here looks like a visceral science of excitement vs boredom or flinching away from messiness/ugliess. I tend to get excited about things that feel like unexpected structure, spark follow-up experiments, or relate to a deep curiosity I have.
- **Understanding:** Designing creative, elegant experiments that cleanly distinguish hypotheses, judging the plausibility and explanatory power of different theories, identifying crucial assumptions or potential confounds. *Is this experiment truly isolating the variable I care about? What's the simplest explanation for this data?*
	- My internal experience is that I may have a beautiful hypothesis I *want*to believe, but it feels uncertain, and this creates an uncomfortable sense of instability.
	- I try to probe at where the instability comes from, what predictions are made by that potential flaw, and design an experiment to target it.
	- A good experiment design feels very clean and reliable - I would trust the results - while for a bad one I still have this shifting sense of uncertainty and being able to generate many alternative explanations
- **Communication & Distillation:** Identifying the core, communicable claims within messy findings, structuring a compelling and *true* narrative, anticipating audience confusion, knowing what makes a result impactful *to others*. *What's the single most important takeaway here? How can I present this evidence most clearly and honestly?*
	- My internal experience of compression is about having a frustration and impatience with length and unnecessary conceptual detail - I want to distill the research down into what is truly important, and reach a point where I can cut no further without sacrificing something important.
	- If I’ve compressed too far, there’s a sense that there’s a missed opportunity - a really exciting thread that’s missed out.

## Decomposing Research Taste

Where does this "taste" come from? In my experience, it boils down to a few key ingredients:

1. **Intuition (System 1):** This is the fast, gut-level feeling - what people normally think of when they say research taste. A sense of curiosity, excitement, boredom, or skepticism about a direction, experiment, or result.
	1. "This feels promising," "This feels like a rabbit hole," "This anomaly seems *important*," "This explanation feels too simple/too complex."
	2. This is the part that feels most like "taste" and develops slowly through repeated exposure and feedback - when I refer to gathering data to train a network, I largely mean training your intuition.
	3. Empirically, my own recommendations based on this intuition have a decent hit rate, and experienced researchers are often fantastic (though not flawless!) at this, but this takes time.
2. **Conceptual Framework (System 2):** This is deep domain knowledge and understanding of underlying principles.
	1. This is crucial in mech interp, especially as it’s a pre-paradigmatic field, where you can’t just memorise and apply a standard method.
		1. I’d guess it’s still important in other domains, though I am less sure
	2. For mech interp, this includes:
		1. Understanding transformer mechanics and basic facts - they’re autoregressive, the residual stream is the central object, tokens are discrete while all activations are continuous vectors, etc
		2. Key results and heuristics: like superposition or the linear representation hypothesis, or the idea that features and circuits exist at all
		3. Common techniques and where to use them and what they can tell you: patching, SAEs, probing, prompting, etc.
			1. This can get pretty deep! See [my paper on how to think about activation patching](https://arxiv.org/abs/2404.15255).
		4. Foundational knowledge of relevant adjacent fields: linear algebra, ML theory, training ML models, basic software engineering, etc
	3. This conceptual framework allows you to generate hypotheses, evaluate plausibility *explicitly*, spot inconsistencies, design sensible experiments, and explain *why* your intuition feels a certain way. It provides the structured reasoning to back up or override gut feelings.
	4. Eventually, this conceptual framework should feel like [a gears-level model](https://www.alignmentforum.org/w/gears-level), where you can reason about the key moving parts, and what would make a project or experiment idea work vs fail vs be impractical.
3. **Strategic Big Picture:** Understanding the broader context of the field. What problems are important? What are the major open questions? What approaches have been tried? What constitutes a novel contribution?
	1. My motivations for doing mech interp partly stem from making AGI safe, so the main big picture is “what work translates into better outcomes for AGI”, and being able to break this down into near-term steps.
	2. But even for less goal directed fields, where the goal is just curiosity driven basic science, there’s often a useful big picture around what advances would unlock many future advances or be a dead end, what would people care about, etc.
	3. Ideally, you dwell on the big picture enough that your intuitive sense of curiosity and excitement starts to integrate it - it’s not about overriding your curiosity with strategic obligations, it’s about aligning them so you’re excited about what matters. I see this as one input to prioritisation, among many.
4. **Conviction & Confidence:** Research inevitably involves setbacks. A certain level of conviction – a belief in the direction, resilience to negative results – is often instrumentally useful for perseverance. It helps you push through the messy exploration phase or refine an idea that isn't working perfectly yet.
	1. Empirically, research taste also often leads to conviction - the intuitive feeling that an idea is exciting and important tends to also give motivation and focus.
	2. However, this is a **double-edged sword**. Your intuitions are not well calibrated. **Confidence doesn't mean correctness**. Generally people reach the level of having conviction far before they reach the level of having correct intuitions
	3. **The ideal is*****strategic*****conviction**: the ability to adopt a confident mindset to maintain momentum, while regularly zooming out to reflect and maintaining the capacity for zoomed-out skepticism and the willingness to update or abandon course based on evidence.
	4. **Track data**: Conviction is instrumentally useful, but so are correct beliefs. Generally, the best way to get calibrated is to pursue an exciting idea and see it fail in unexpected ways. Try to **write down prior predictions**, and *why*you think an idea is good, pursue it, and **reflect on what happened**.
		1. Corollary: **It’s fine to be uncalibrated at first**, this can help you get more research done and gather more data, if you’re paying attention you’ll often get over it.
			1. I often mentor people who start out by getting way too attached to flawed ideas, and don’t engage well with criticism. Seeing some of their exciting ideas fail tends to helps a lot.

**These components interact**. A strong conceptual framework sharpens intuition. Experience builds both intuition and framework knowledge. Strategic awareness helps channel conviction productively.

# Cultivating Research Taste

If taste is like an ML model, how can we speed up training? We want to improve the quantity (and quality) of data, and the sample efficiency of how much we learn from it.

- **Learning more from each data point**: You will learn something just from doing research. You'll get some feedback, some experience, and your intuitions and models will improve. But each data point is actually much richer than just a binary of success or failure!
	- My recommendation is to **make explicit predictions**, **review accuracy**, and make time to **reflect on what you missed** and how you could do better next time.
		- Keep a research log. Ask *why* things worked or failed. Was it luck, execution, or a fundamental judgment call (taste)?
	- **Reflect Deliberately:** After an experiment or project phase, ask: What worked? What didn't? What surprised me? What would I do differently next time? How does this update my model of this domain? ([Weekly reviews](https://www.neelnanda.io/blog/39-reflection) can be great for this).
- **Getting more data**: The obvious source of data is doing research. But there are other sources too!
	- **Leverage Mentors:** This is perhaps the biggest accelerator. A mentor provides high-quality, curated "labels", insights and feedback. You can think of this as supervised data, in contrast to the slow RL of doing research yourself.
		- **Predict their advice:** Before asking your mentor ("Should I run experiment A or B?", "Is this result interesting?"), predict their answer and reasoning.
		- **Analyze surprises:** When their answer differs from your prediction, *dig into why*. What perspective, heuristic, or piece of knowledge did they use that you lacked? This is incredibly valuable training data for your internal model.
			- **Strong recommendation**: Do this by **repeatedly paraphrasing their reasoning**. Try to repeat back their arguments in your own words, and ask what you’re missing. This is an excellent way to ensure you’ve processed correctly, and often highlights misunderstandings. This is one of my most effective tactics when learning from people.
		- **Absorb their frameworks:** Listen not just to *what* they advise, but *how* they reason. What questions do they ask? What principles do they seem to operate by?
	- **Learn Critically from Papers (Offline Data):** Papers are a biased dataset (publication bias!), but still useful.
		- Read actively: Predict methods, results, and limitations before revealing them.
		- Ask *why*: Why did the authors make these choices? What alternative approaches might they have considered? What makes this paper impactful (or not)?
		- Focus on *reasoning*: Try to reconstruct the authors' thought process, not just memorize the outcome.
		- Note: **Papers are*****very*****often flawed**! A common mistake in new researchers is assuming that everything in a paper was reasonable or done for principled reasons. Even in great papers, there’s a lot of janky crap or flaws in there. And many papers are just inherently flawed or outright false. Critically engaging with a paper’s flaws is also very educational
	- **Collaborate and Discuss:** Talk to peers. Explain your research plans and reasoning. Listen to theirs. Critique each other's logic. Explaining forces clarity and exposes flawed assumptions. Hearing others' perspectives provides diverse 'data points'.
	- **Prioritize Projects with Clearer Feedback:** Especially early on, projects where you can test intermediate hypotheses or get partial results relatively quickly can accelerate learning more than moonshots with year-long feedback loops.
- **Feedback loops**: The speed at which you complete each loop for each facet of taste determines how fast you learn that aspect.
	- **Short Loops/tactical taste:** Designing a specific experiment, debugging code, interpreting a single plot. Feedback is often quick (minutes to days). You'll likely improve *much*faster at skills with short feedback loops.
	- **Long Loops/strategic taste:** Choosing a research problem, deciding on a major strategic direction. Feedback might take months or even years. **Improvement here is inherently slower.**
	- **Implication:** Don't beat yourself up if your high-level strategic taste develops slower than your tactical experimental skills. This is expected.

I have less to say about other components of research taste like conceptual understanding or strategic picture - generally a similar mindset works there, though as it’s no longer really a black box I think it’s more straightforward, and is much easier to learn from reading papers and existing resources, and talking to mentors/experts. Conviction is more of a matter of personality and preference, in my experience.

# Conclusion: Patience and Process

Research taste isn't magic. It's a complex set of intuitions and frameworks built incrementally through experience, reflection, and learning from others. It governs the crucial, often implicit, decisions that shape a research project's success.

Because the feedback loops for high-level strategic taste are long and noisy, don't expect to master it quickly. It's perfectly normal, and indeed expected, to rely heavily on external guidance (like mentors or established research directions) early in your career. Focus first on mastering the skills with shorter feedback loops – coding, running experiments, analyzing data, clearly communicating simple results.

By actively engaging in research, deliberately reflecting on your decisions and their outcomes, and strategically leveraging the experiences of others, you can accelerate the development of your own research taste. Be patient with the process, especially the long-game aspects like problem selection. Trust that by doing the work and learning effectively from it, your intuition will improve over time.

*Post 4, on ideation/choosing a research problem, is coming out soon - if you’re impatient you can read a draft of the whole sequence*[*here*](https://docs.google.com/document/d/1YMkeMrhqsWxZcNDD9CIUWEK_DAOegeufnbc79U2hycg/edit?tab=t.0)*.*

# Research as a Stochastic Decision Process
*by Jacob Steinhardt*

In this post I will talk about an approach to research (and other projects that involve high uncertainty) that has substantially improved my productivity. Before implementing this approach, I made little research progress for over a year; afterwards, I completed one project every four months on average. Other changes also contributed, but I expect the ideas here to at least double your productivity if you aren't already employing a similar process.

Below I analyze how to approach a project that has many somewhat independent sources of uncertainty (we can often think of these as multiple "steps" or "parts" that each have some probability of success). Is it best to do these steps from easiest to hardest? From hardest to easiest? From quickest to slowest? We will eventually see that a good principle is to "reduce uncertainty at the fastest possible rate". After revealing issues with more simplistic approaches, I will articulate this principle in detail and show how to apply it. Throughout, I draw my examples primarily from problems in machine learning and mathematics, but I believe that the principles generalize to other situations as well.

### Warm-Up

Suppose you are embarking on a project with several parts, all of which must succeed for the project to succeed. For instance, a proof strategy might rely on proving several intermediate results, or an applied project might require achieving high enough speed and accuracy on several components. What is a good strategy for approaching such a project? For me, the most intuitively appealing strategy is something like the following:

**(Naive Strategy)**
Complete the components in increasing order of difficulty, from easiest to hardest.

This is psychologically tempting: you do what you know how to do first, which can provide a good warm-up to the harder parts of the project. This used to be my default strategy, but often the following happened: I would do all the easy parts, then get to the hard part and encounter a fundamental obstacle that required scrapping the entire plan and coming up with a new one. For instance, I might spend a while wrestling with a certain algorithm to make sure it had the statistical consistency properties I wanted, but then realize that the algorithm was not flexible enough to handle realistic use cases.

The work on the easy parts was mostly wasted--it wasn't that I could replace the hard part with a different hard part; rather, I needed to re-think the entire structure, which included throwing away the "progress" from solving the easy parts.

What might be a better strategy than the naive strategy above? Since the naive strategy has the problem that we waste effort on the easy components if the hard components are intractable, maybe it would be better to complete the components in *decreasing* order of difficulty, starting from the hardest and moving to the easiest.

This *might* be better, but our intuitive sense of hardness likely combines many factors--the likelihood that the task fails, the time it takes to complete, and perhaps others as well. Here is an example:

Task A is a detailed and tricky calculation, but you have done many similar calculations before and are confident that given a few days you will succeed. Task B will likely take much less time, but it is something you haven't done before (so it is more likely there will be an unforeseen difficulty or problem).

In this case, task B would be better to do first--if you do task A first and then B turns out doomed, you have wasted several days. Even if A also has some chance of failing (so that it is both more likely to fail and takes longer than B), we would still usually rather do B before A.

This reveals that harder tasks should not necessarily be prioritized. Rather, we should prioritize tasks that *are more likely to fail*(so that we remove the risk of them failing) but also tasks that *take less time* (so that we've wasted less time if one of the tasks does fail, and also so that we get information about tasks more quickly).

### A Better Strategy: Sorting by Information Rate

We can incorporate both of the above desiderata by sorting the tasks based on which are *most informative per unit time*.

**(Better Strategy)**
Do the components in order from most informative per unit time to least informative per unit time.

To implement this, we need a method for quantifying informativeness. I will present two methods below--one based on *expected time saved*, and one based on *failure rate*. Rather than define these rigorously upfront, I will work through several examples, which should make the general case evident.

**Method 1: Expected Time Saved**

If an earlier step fails, we save time by not having to attempt the later steps. We should therefore complete the steps in the order that maximizes the expected value of the time that we save. We assume for now that we can actually quantify the probability that each step succeeds, as well as the time it will take. Consider the following example:

Example 1: All of the steps of a project have roughly equal chance of success (80%, say) but take varying amounts of time to complete.

In this example we would want to do the quickest task first and slowest last, since the later a task occurs, the more likely we will get to skip doing it. Sorting "easiest to hardest" is therefore correct here, but it is rare that all steps have equal success probability.

Example 2: An easy task has a 90% success probability and takes 30 minutes, and a hard task has a 40% success probability and takes 4 hours.

Here we should do the easy task first: if it fails we save 240 minutes, so 0.1 \* 240 = 24 minutes in expectation; conversely if the hard task is done first and fails, we save 30 minutes, for 0.6 \* 30 = 18 minutes in expectation. But if the hard task takes 2 hours or the easy task has a 95% chance of success, we should do the hard task first.

Thus, in this method we formalized "most informative per unit time" by looking at how much time we save (in expectation) by not having to do the tasks that occur after the first failure. Our computations assumed that we only find out if a task succeeds or fails at the end, as opposed to in the middle; however, they can be modified to take such complications into account.

For more than two tasks, this calculation method quickly becomes intractable: for K tasks we have to consider all K! permutations to find the best one. The next method avoids this issue.

**Method 2: Failure Rate**

This next method models the occurrence of failures as a Poisson process: if a task takes 30 minutes and has a 15% chance of failure, then there is about a 0.5% chance that the failure will occur in each minute (actually, it is slightly more than that because of overlap among the failures; the actual value is the solution p to (1-p)^30 = 0.85). Note that this differs from our previous assumption that failures can only occur at the end. This alternate model will simplify our calculations.

Formally, assume that the probability that we realize the task fails in the next minute is independent of how long we have been doing the task. Then the occurrence of a failure is a [Poisson arrival process](https://en.wikipedia.org/wiki/Poisson_point_process#Interpreted_as_a_point_process_on_the_real_line) and the time at which a failure occurs [follows an exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution#Applications_of_exponential_distribution) with some rate parameter ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") , where ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") tells us how frequently failures occur per unit time. Using basic properties of Poisson processes (see Appendix A), we can compute ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") as

![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as7.png "image_tooltip") ,

where ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as9.png "image_tooltip") is the success probability of the task.

This rate exactly tells us how quickly we will encounter failures while doing a given task. Since we would like to front-load failures as much as possible, we would always like to sort the tasks in decreasing order of their rate ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") .

Returning to Example 2, we can compute the rate ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") for the two tasks:

Task 1: ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as1.png "image_tooltip")

Task 2: ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as14.png "image_tooltip")

This new computation reverses our previous conclusion: The hard task has a higher rate, so is actually (slightly) better to do first! The reason for this is that the Poisson assumption implies that the higher the failure probability of a task, the faster (in expectation) we will encounter the failure. This contrasts with the previous assumption that we only encounter failures at the end of a task. We should keep in mind that both of these assumptions are likely somewhat incorrect in practice.

The rate method extends easily to more than two tasks, since we can simply sort tasks in order of ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") .

**An Additional Example**

In the case of the time-consuming but certain task A and quicker but uncertain task B, task A might take 12 hours but have a 90% chance of success, while task B takes 2 hours but has a 65% chance of success.

First, let's see what we get using the time saved method:

- A first: 0.1 \* 2 = 0.2 hours
- B first: 0.35 \* 12 = 4.2 hours

Now suppose we use the rate method:

- A first: log(1/0.9)/12 = 0.009
- B first: log(1/0.65)/2 = 0.215

B dominates A on *both* failure prob and time, so doing B first looks substantially better under both methods.

**Caveats**

These numbers are all completely made up and in practice you won't be able to estimate things so well. I subjectively distinguish between different "buckets" of success probability, such as:

- "I am confident that this can be done and that there are no unforeseen difficulties" (~95%)
- "I am confident that this can be done modulo Murphy's law" (~90%)
- "I see the basic path to accomplishing this and all the steps seem like they should work" (~65%)
- "I have the intuition that this should be possible but only have a murky view of the path" (~30%)

On the other hand, I tend to have much better estimates of task completion times if I've been practicing (~30% average relative error, albeit with large tails). You can get better at this within a few weeks by estimating completion times for each task and then recording the actual completion times in a daily log. You should also practice decomposing tasks into small actionable chunks, each taking roughly 20 minutes to 2 hours.

### A further improvement: opening up the "task" black box

Sorting tasks in decreasing order of failure rate is a good start; it should improve efficiency by a factor of 2-3. However, we can do *much* better still by learning to front-load the information gained about each task. Front-loading information requires a mental finesse: rather than seeking to complete a task, we must seek information *about* a task.

Specifically, for each task we want a *cheap way to obtain high confidence about whether that task will be feasible*. This is called "de-risking". The following pattern is indispensable:

**(Basic Pattern)**
De-risk all components (to the extent feasible), then execute.

As an example, suppose we wish to set up a dataset and then train a suitable model on that dataset. However, setting up the dataset is arduous: we must download it to a place with enough disc space, parse it into a usable format, and incorporate auxiliary data sources (like noun/verb banks for natural language processing).

Setting up the dataset and training the model are both time-consuming and either one could fail. Even worse, it would seem that we are forced to set up the dataset first, even though it is probably the more time-consuming task.

To avoid this issue, we could first download a few thousand examples. We can then examine several examples by hand, as well as compute some aggregate statistics, to assess whether the dataset has the properties we want. Ideally, this will reduce a lot of uncertainty about whether the will dataset is suitable.[1](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html#fn1)

### General principle: stochastic decision process

We can unify and extend the above insights by modeling a research project as a *stochastic decision process*. Specifically, we think of research as a multi-round game, where in each round we take some action that gives us some information; the information we get is stochastic, and well as perhaps the time needed to complete the action. We have two competing goals:

- Maximize probability of eventual success (don't give up if it turns out we can eventually solve the problem).
- Minimize expected time spent (give up early if the problem is not feasible, and solve the problem quickly if it is feasible).

We are often either in "de-risking mode" (determining if the problem is infeasible as quickly as possible) or "execution mode" (assuming the problem is feasible and trying to solve it quickly).

**An aside: tooling.** This picture grows more complicated if we consider actions that could speed up a family of future actions (such as writing helpful scripts to automate tasks, or reducing the execution time of the system). Such "tooling" tasks are tricky to model, because it seems we should implement tooling as soon as we know we will eventually want it (since it speeds up things that come after it). However, this ignores that more experience often yields refined desiderata for the tools we implement. There is thus a trade-off between building tools earlier vs. building better-targeted tools. I won't say more about this here, but it is an important point to keep in mind.

Another complication is that our ultimate goal is often nebulous--we are not asking "is this problem possible" so much as "how interesting of a problem in this space is it feasible to solve"? But I don't think this substantially alters the above principles.

### Some further practical ideas

There are a number of useful patterns for putting the above principles into practice. I list several below.

**For empirical work, measuring "ceilings" (an upper bound of how high performance could possibly be) is often useful.**Example: suppose we wish to build a system with 3 components that interact in a complicated way. One of the components is difficult to implement, but we can easily substitute a "cheating" version of that component (e.g. by looking at the test set or by using information that won't be available at deployment time). We often benefit by building a prototype system that initially uses this cheating version:

- If the system works, we know that a sufficiently good implementation of the difficult component will yield a working system.
- If the system doesn't work, we've saved the time of implementing the difficult component.

We can choose which components to cheat on initially, and which to implement fully, using the "informativeness per unit time" heuristic from above. For instance, if the ability to do well on a specific component is the major source of uncertainty in the project, cheating on it might be counterproductive (we may instead want to cheat on *everything but that component*).

The counterpart to ceilings are *baselines*--simple or off-the-shelf methods that give a quick lower bound on achievable accuracy. Baselines provide an important sanity check, as complicated methods often underperform simple baselines. Together with ceilings, they delineate a range of possible performance, which helps us interpret our core results.

**Brute force.**If we know of an easy-to-implement brute force solution and a difficult-to-implement fast solution, starting with the brute force solution has many of the same advantages as using ceilings, as long as the slower running time doesn't bottleneck prototyping. A brute force implementation also facilitates debugging the fast solution, since we can compare the outputs of the two algorithms.

As with ceilings, brute force is most useful when implementing the fast solution is not a major source of uncertainty (e.g. it is routine but annoying, or is one of many sources of uncertainty).

**For theoretical work, looking for counterexamples is useful.**The simplest example of this: if we find a counterexample to the main result we want to prove, then we need to either give up or make stronger assumptions.

A more nuanced (and more common) example: if we are trying to prove that ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as6.png "image_tooltip") , and our current technique does this by proving ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as10.png "image_tooltip") and then ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as11.png "image_tooltip") , finding a counterexample to ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as10.png "image_tooltip") will rule out that technique.

Yet more nuanced/common: if we are trying to prove that ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as6.png "image_tooltip") , and our current technique applies equally well under assumptions ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as12.png "image_tooltip") and ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as0.png "image_tooltip") , then a counterexample to ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as13.png "image_tooltip") will rule out the technique.

**More generally, thinking about simplified instances of a problem is often useful.** This is because it provides intuition that often suggests/rules out approaches for the original problem. Similarly to de-risking, the ability to rule out entire approaches makes this tactic invaluable from the stochastic decision process perspective.

**Running simulations.**If we wish to prove X, first run simulations to check if X is actually true. This is easy when assessing the behavior of a specific algorithm, as we can simply run the algorithm. Simulations can also, for instance, help reveal the asymptotics of a random process, or be used to search for small counterexamples to a conjecture.

### Exponentially branching search trees

Another important mental framework focuses on the combinatorial aspect of a decision process:

**(Research as branching search)**
I often think about possible approaches to a problem as an exponentially branching search tree: we could try X, X', or X''. Then X could be combined with Y, Y', Y'', or X' could be combined with Z or Z', etc. This exponential blow-up poses barriers to projects with more than a small number of steps unless we have a way to systematically rule out entire branches of the tree.

Exponential branching often occurs because there are many ways to try a particular approach--perhaps we want to bound the moment generating function, and there are many ways to attempt this; or we think data augmentation will help our model generalize, but there are many ways to augment the data. With many possibilities for each step, even a two- or three-step approach creates a huge search space. For instance, if there are 10 ways to try bounding the moment generating function, and two other similar steps, then we have to try 1000 possibilities.[2](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html#fn2)

If the steps *factor*--meaning they can each be solved in isolation--this might be fine (we only have to try 3\*10 instead of 10^3 possibilities). However, I usually find that there is some interdependency between different steps. For a math problem, maybe how good of a bound I get from step 1 affects how hard I need to work for step 2. Or for an experiment, if any of 3 parts of the setup are wrong then the method just won't work, so I don't get signal until I've gotten a few things right simultaneously.

For this reason, I think it's *much* more useful to prune branches of the search tree at the level of conceptual approaches ("can the moment generating function give me sufficient control over the distribution I care about?") than at the level of a specific instantiation ("does this particular moment generating function bound work?"). This leads to adopting several principles:

**Whenever something doesn't work, I ask *why* it didn't work.** My goal is to avoid trying similar things that will fail for the same reason (or to notice that the reason why it didn't work is circumventable, and that a modified approach actually will work).

**Trying an experiment and seeing it fail gives little information by itself.** When an experiment fails, it is tempting to conclude "I tried X and it didn't work". However, if X is a high-level conceptual approach, then a more correct conclusion is "I tried an implementation comprising 0.1% of the possible implementations of X, and observed that that particular implementation did not work". For this reason, I am far less in favor than most people of publishing negative results, unless the negative result comes with insight into what caused the failure. In contrast to common concerns, negative results that come with such insights are [already publishable](https://acl2018.org/paper/1604/).

**Compared to other people I know, I try harder and earlier to show that my ideas can't work to solve a problem.** Importantly, it is often not obvious that multiple approaches to a problem all have the same issue. In the past, I have spent months trying different approaches to a problem before finally stepping back and realizing that they were all failing for the same reason. Moreover, I had all the data necessary to make this realization a couple weeks in but had failed to do so. I now save considerable time by ruling out ideas early on, and as a result I am usually bottlenecked on coming up with ideas rather than on implementing ideas.

**Additional Discussion**

In the previous section I talked about ruling out ideas. When ruling out ideas, it is important to hold oneself to a high standard. "This doesn't seem like it will work" or "I feel less motivated after trying a few things along this line that didn't work" are *not* ruling out an idea. We could perhaps think of them as updating the probabilities that a solution lies within a given subtree of the search tree. But these updates are rarely large updates, and I find them much less reliable than a solid argument for why an approach is doomed.

Note that I am *not* advocating that you should never trust your feelings. If you feel pessimistic about an approach, that is a great reason to try to show that the approach can't work! If I feel pessimistic about an approach but fail to rule out that it could work, I often then feel more optimistic.

I am also *not* advocating for the failure mode of only trying low-variance ideas, or of avoiding projects that lack an obviously promising approach. Part of the point of being able to systematically rule out ideas is to enable trying ideas that only have a low probability of working, or that do not immediately yield progress.

### Summary

Many of our default intuitions about how to pursue uncertain ideas are counterproductive:

- We often try easier tasks first, when instead we should try the most informative tasks first.
- We often conflate a high-level approach with a low-level instantiation of the approach.
- We are often too slow to try to disprove our own ideas.

Building frameworks that reify the research process as a concrete search problem can help unearth these incorrect intuitions and replace them with systematic reasoning.

### Appendix A: Poisson Process Calculation

In a Poisson process with rate ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") , the probability that a failure has already occurred by time t is ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as5.png "image_tooltip") , so in particular ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as8.png "image_tooltip") , where ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as4.png "image_tooltip") is the time to complete the task and ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as9.png "image_tooltip") is the success probability of the task. If we solve for this, we get that the rate ![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as3.png "image_tooltip") is equal to

![alt_text](https://cs.stanford.edu/~jsteinhardt/images/Research-as2.png "image_tooltip") ,

as claimed.

## Notes

---

1. This doesn't quite fit into the framework because if the dataset is unsuitable we can try again until we find a suitable dataset. But it could be that we try 4 datasets, they are all unsuitable, and we eventually conclude that there aren't any suitable datasets. The sort of de-risking above allows us to reach this conclusion much faster and avoid spending time trying to train a model on a broken dataset. [↩](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html#fnref1)
2. This is purely illustrative and in reality we can't necessarily decompose different attempts into a fixed number of discrete "ways" of attempting something. [↩](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html#fnref2)


# [Highly Opinionated Advice on How to Write ML Papers](https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers)

by [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1?from=post_header)12th May 2025## TL;DR

- **The essence of an ideal paper** is the **narrative**: a short, rigorous and evidence-based technical story you tell, with a takeaway the readers care about
	- **What?**A narrative is fundamentally about a contribution to our body of knowledge: **one to three specific novel claims** that fit within a cohesive theme
	- **Why?**You need **rigorous empirical evidence**that convincingly supports your claims
	- **So what?**Why should the reader care?
		- What is the **motivation**, the problem you’re trying to solve, the way it all fits in the bigger picture?
		- What is the **impact**? Why does your takeaway matter? The **north star** of a paper is ensuring the reader **understands**and **remembers**the narrative, and **believes** that the paper’s evidence supports it
- The first step is to **compress your research** into these claims.
- The paper must **clearly motivate these claims, explain them on an intuitive and technical level**, and **contextualise what’s novel**in terms of the prior literature
	- This is the role of the abstract & introduction
- **Experimental Evidence**: This is absolutely crucial to get right and aggressively red-team, it’s how you resist the temptation of elegant but false narratives.
	- **Quality > Quantity**: find compelling experiments, not a ton of vaguely relevant ones.
	- **The experiments and results must be explained in full technical detail** - start high-level in the intro/abstract, show results in figures, and get increasingly detailed in the main body and appendix.
		- **Ensure researchers can check your work**- provide sufficient detail to be replicated
		- **Define key terms and techniques** - readers have less context than you think.
- **Write iteratively**: Write abstract -> bullet point outline -> introduction -> first full draft -> repeat
	- Get feedback and reflect after each stage
	- Spend comparable amounts of time on each of: the abstract, the intro, the figures, and everything else - they have about the same number\_of\_readers \* time\_to\_read
- **Inform, not persuade**: Avoid the trap of overclaiming or ignoring limitations. Scientific integrity may get you less hype, but gains respect from the researchers who matter.
- **Precision, not obfuscation**: Use jargon where needed to precisely state your point, but not for the sake of sounding smart. Use simple language wherever possible.

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/081539a4d3b61418cfc6657e29bac2d234e1adf5331b4a7b3ea7974ad4d2bfc1/pact399wy3qd43yhrzx8)

***Case study**: The abstract of*[*refusal is mediated by a single direction*](https://arxiv.org/abs/2406.11717?)*, broken down into the purpose of each sentence*

## Introduction

**Your research only matters if people read, understand, and build upon it**. This means that **writing a good paper is a critical part of the research process**. Further, the process of writing forces you to clarify your own thinking in ways that often reveal gaps or new insights - I’ve often only properly understood an idea after writing it up. Yet, to many, writing feels less fun than research and is treated as an after thought - **a common but critical mistake**.

In my experience supervising 20+ papers and reading/appreciating/being annoyed by a bunch more, I've developed my own opinionated framework for what I think makes a paper good and how to approach the writing process. I try to lay this out in this post, along with a bunch of concrete advice.[[1]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnwwk8u16jdjf) This post assumes you’ve already done a bunch of technical research, and focuses on how to effectively share it with the world, [see my other posts](https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand) for advice on the research part.

**Caveat**: I mostly have experience with writing mechanistic interpretability papers and this advice is written with that flavour. I expect much of it to generalise to the rest of ML and some to generalise to other fields but it's hard for me to say. Further, this is very much my personal opinionated, and optimised more for truth-seeking than getting into conferences[[2]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fn5kbv6xekx6j). See other great advice [here](https://www.jakobfoerster.com/how-to-ml-paper) and [here](https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html).

**Caveat 2**: There are many reasonable objections to the academic paper as the format for communicating research. Alas, engaging with those is outside the scope of this post, it’s long enough as it is.

## The Essence of a Paper

At its core, a paper should **present a narrative** of **one to three specific concrete claims** that you believe to be true, that **build to some useful takeaway**(s). Everything else in the paper exists to support this narrative. The second pillar of the paper is **rigorous evidence for why they are true**- obviously there will always be some chance you’re wrong, but they should be compelling and believable, without obvious glaring flaws.

1. **Communicate the key idea**
	1. **Motivate** why someone should care about them
	2. **Contextualize** them in existing literature
2. **Communicate them precisely**, with all relevant technical detail, terminology and background context
3. **Provide sufficient evidence** to support them

### Crafting a Narrative

One of the critical steps that can make or break a paper is crafting a narrative. What does this actually mean? And how can you do it?

Research is about discovering new things and pushing forward our frontier of existing knowledge. I view a paper as something that finds insight and provides compelling evidence behind it. The way to tell when you could start writing a paper is when you have **learned something insightful**, in a way that could be **made legible to someone else**.

**This is far easier said than done**. A research project is often a mess of fun results, confusions, insights, and remaining mysteries. Even when you’ve made enough progress to write it up, you will likely have a great deal of tacit knowledge, interesting rabbit holes, dangling threads, etc - **projects rarely feel done**.

I find that converting a project into a great narrative is **a subtle and difficult skill**, one of the many facets of [**research taste**](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research). It’s something I’ve gotten much, much better at over time, and it’s hard to say what the best way to get better at it is, beyond experience. One exercise I’d recommend is taking papers you know, and trying to write down what their narrative is, and ask yourself what its strengths and weaknesses are. If at all possible, consult mentors/more experienced researchers for advice. But if you need to come up with one yourself, the right questions to ask look like:

- Which of these results would be most exciting to show someone?
- Actually show someone your findings and ask what they're most interested in
- What seems particularly important?
- Why should anyone care about this work?
- What was hard about what you did, that perhaps no one else has done?

A good, compelling narrative comes with motivation and impact. The key points to be sure to cover:

- The context of your insight
- The problem you're trying to solve
- Why this matters
- What you have shown
- Why the reader should believe it
- What the insight is

Why do you need this kind of compressed narrative? Often there’s far more insight in a research project than can be contained in this structure. But it is impossible to convey this level of nuance in a paper. Readers will rarely take away more than a few sentences of content. **Choose those sentences carefully**. These are the insights shared by your paper, **your contribution to the literature**. These are the specific, concrete claims that you want to communicate - you cannot reliably communicate much more. You will need to compress your research findings down into a handful of claims, prioritise those, and accept that you may need to drop a bunch of other detail, or move it to appendices. If you don’t deliberately de-prioritise some details, then something else will get dropped, which may have been far more important.

What do I mean by claims? For example:

- "Method X is the best approach on task Y (according to metric Z)"
- "A substantial part of the model's behavior in scenario A is explained by simple explanation B"
- "Technique C can fail in scenario D if conditions E and F hold"

One important claim, with sufficiently strong evidence, can be enough for a great paper! If you want multiple claims, I strongly recommend **choosing claims that fit together in a cohesive theme** - papers are far easier to understand, praise, share, etc if there is a **coherent narrative**, not just a grab-bag of unconnected ideas.

Depending on the strength of the evidence, you can adjust the confidence of a claim:

- **Existence-proof claims**: "We found at least one example where X happens" (like the indirect object identification paper providing an existence-proof for self-repair)
- **Systematic claims**: "X generally happens across a wide range of contexts" or "X is common"
- **Hedged claims**: “There is compelling/suggestive/tentative evidence that X is true”
- **Narrow claims**: “X is the best method for specific situations V & W, if your goal is objective Y”
- **Guarantees**: “X is always true”[[3]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnxjcznpnz657)

Generally, stronger statements make for more interesting papers, but require higher standards of evidence - resist the temptation to overclaim for clicks!

### When to Start?

Another thorny question is: When should you stop doing research and start writing up your research? This is a hard and subtle question that is, in many ways, a matter of [research taste](https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research), but here is my general guide:

1. Write down a list of things you've learned
2. Review that list carefully, and ideally show it to someone else
3. Ask yourself how comfortable you would be defending the claim that you have provided meaningful, positive evidence for these results
4. Think about reasons why others might care about this
5. Focus on things you've done that have been hard or non-trivial and look for exciting elements

But generally, this is unfortunately just a hard thing to tell when starting out, and gets far easier with time and experience. If you can consult a more experienced researcher, definitely do.

A more meta piece of advice when starting out is to try to choose projects where the narrative will be pretty obvious, e.g. method X beats SOTA method Y in domain Z on metric W

**Warning**: Before moving into paper-writing mode, **it's crucial to verify that your evidence is actually correct**. An unfortunate fact is that **many published papers are basically false or wildly misleading**. Don't let this happen to you! Carefully check your critical experiments and, if possible, re-implement them through alternate pathways. Ideally, verify all experiments worth mentioning in the paper, or at least 75% of them.

### Novelty

A common and confusing requirement for papers is that the results be novel, something that is not covered before. What exactly does this mean? Science is fundamentally about **building a large body of knowledge**. This means that your work exists in the context of what has come before. **Novelty means it expands our knowledge**.

The conventional definition of novelty can be annoying and, in my opinion, focuses too much on shininess and doesn't capture the more important aspect of whether our knowledge has expanded. Another way to put this is: Should I assign different probabilities to propositions I care about after observing the results of this paper?

Rigorous, at-scale replications of shaky results, negative results of seemingly promising hypotheses, and high-quality failed replications of popular papers are all very valuable contributions. I would personally consider these novel because they expand our knowledge. However, the revealed preferences of many reviewers and researchers suggest they do not feel the same way. Such is life.

I don’t want to go too far re criticising novelty: there are many cases where I am uninterested in a paper due to lack of novelty. This primarily occurs with methods that I expect to work when applied in standard settings, and I assign a high probability of success, so the project provides few bits of information. While *knowing*that such a method failed could be interesting, projects can also fail due to researcher incompetence or bad luck. Therefore, it is difficult to draw meaningful conclusions without evidence of researcher competence.

Leaving that aside, novelty can be hard to communicate. Given a paper on its own, it's difficult to tell what is and is not supposed to be novel:

- Are the techniques used innovative or just standard techniques?
- Does the claim represent a deep conceptual breakthrough?
- Is it a very simple extension of standard ideas?
- Is it a natural consequence of a more ambitious claim put forward in a different piece of work?

The main way to address this is to be extremely clear about what is and is not novel, especially in the introduction and related work, and to liberally cite the most relevant papers and explain why your work is and is not different.

How to find out what came before? **Use a large language model**. If you’re not already familiar with a relevant literature, LLMs are pretty great at doing quick literature reviews, e.g. [Gemini Deep Research](https://gemini.google/overview/deep-research/?hl=en-GB)[[4]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fn29hhedam5nx). Reading the literature yourself is much better of course, but takes way, way longer and should have been done at the start project.

One reason this is very important is that, depending on what’s claimed as novel, the same paper could be perceived as either inappropriately arrogant or making a modest incremental contribution, depending on how the claims are presented.

Contextualizing your work within existing literature is **particularly crucial for experienced researchers** who are familiar with the field. Clear explanation in the introduction helps them quickly engage with your work and see what’s interesting, else it blurs into all other superficially similar papers they’ve read and doesn’t seem worth the effort.

There are a few problems with novelty as it is traditionally thought of

- **Novelty is often overemphasized** - it incentivises going for ambitious but shaky claims over simple and rigorous insights.
	- This can mean that if there's an existing paper that provides a preliminary but shaky case for a claim, going and doing it properly can seem less exciting, even though this is in some ways a more useful scientific contribution, as it establishes a confident foundation for others to build upon.
- Another complex question arises when you have legitimate complaints about prior work, and your work superficially looks derivative, but this is because you identified a significant methodological flaw or bug.
	- I recommend being clear that you have criticism, but it's important to remain professional while explaining what was flawed and why this matters, and how your work resolves it, without critiquing the authors or their motivations.
- There are various social norms that are kind of annoying, such as citing being obliged to cite the first instance of a concept (even if later iterations are much clearer) and referencing a ton of vaguely relevant work even if it adds nothing to the paper - people can get offended if not cited. But this doesn’t detract from all the ways that citations genuinely strengthen a paper

I personally prefer to just do work that is optimised for scientific value, and shoe-horn it into a peer review friendly lens at the end, if applicable. But I’m in a fortunate position here, and there are real career incentives around getting published.

Two example papers of mine where being clear about novelty was tricky:

- In [my Othello work](https://arxiv.org/abs/2309.00941), I built directly on Kenneth Lee's paper that showed an Othello plane model had a world model found with non-linear probes. My contribution was demonstrating that it could be found with linear probes, which was interesting for a bunch of reasons to do with the linear representation hypothesis, but I needed to be careful to *not*claim credit for anything Kenneth did
- In [my refusal paper](https://arxiv.org/abs/2406.11717?), our key result was that refusal is mediated by a single direction. But it was *not*novel to find that *a*concept was linearly represented, the significant part was doing it for refusal: a particularly interesting concept.
	- Other work had loosely tried to do this for refusal, but had less compelling results, so we had to explain why our’s was better (much larger effect sizes, more models, downstream tasks, etc)
	- We also showed that we could now jailbreak the model by removing this direction from the weights - the novelty was less that we could jailbreak models (that's already known to be easy with finetuning), but that we could do it with interpretability tools, and so cheaply, one of the first practical applications of interpretability (even if, you know, not quite for safety...)

### Rigorous Supporting Evidence

**A paper is worth little unless it can*****convince***[[5]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnc844hgv7nzl)**the reader of its key claims**. To do this, you need evidence. In machine learning, this typically means experiments. Below, I discuss how I think about what good experimental evidence looks like - see [my research process sequence](https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand) for more advice.

With claims the priority is being able to communicate the intuitions to everyone, but with experiments **the priority is being able to justify it in full technical detail to an engaged, skeptical reader**. You also want to explain what’s going on intuitively, to support your claims, but this is less key than actually having good, legitimate evidence.

A particularly important thing to get right is **extensive red-teaming**: you should spend a good amount of your time, both during the original research and now, red teaming your narrative. One of the main traps introduced by the framing of “find a great narrative” is the temptation to ignore inconvenient contradictory results - don’t let this happen to you. Tips:

- Assume you've made a mistake - what is that mistake? Assume there's a hole in your case that your evidence supports your grand narrative - where is that hole? Try to break it apart.
- Try to get other researchers, especially more experienced ones, to weigh in.
- Make sure to extensively discuss limitations. If you notice issues, design and perform new experiments to test for them. This is all the more important the more ambitious or surprising your claims are.
	- When I read a paper with a bold claim, I have a strong prior that it is false, and I am constantly looking for holes. If I identify one, and the authors have not checked whether that is a real flaw, I will generally move on.
	- However, if they have preempted me and provide sufficient evidence that I can be confident it's not a flaw, then those papers can be incredibly exciting and insightful.

What does good evidence look like?

- **Good experiments distinguish between hypotheses**: Often, you will have several plausible hypotheses for some phenomena. The point of an experiment is to have results that vary significantly depending on which is true (i.e. that provide Bayesian evidence) - if the results vary enough, and the experiment is sufficiently reliable, then one good experiment can falsify many hypotheses
- **Can you trust your results?:**
	- **How reliable is my experiment?** Ask yourself: "How surprised would I be if it turned out to be complete bullshit due to a bug, error, noise, misunderstanding, etc.?" Investigate the most uncertain bits
	- **How noisy is my experiment?** If you ran similar experiments several times, how confident would you be that the results would be consistent? What is your sample size? What is your standard deviation? Are your results clearly distinguishable from noise? (There's a whole host of statistical theory here; your favourite LLM can probably effectively teach you about the basics.)
	- **Statistical rigour**: If you're doing some type of frequentist test[[6]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnn6o4myp4h8r), you probably shouldn't use p < .05 as a threshold. In stats heavy fields, like the social sciences, papers that report their central finding at .01 < p < .05, usually fail to replicate. If you're doing an exploratory approach, you should be skeptical of any result that isn't p < .001, as the number of possible hypotheses is vast.

		- Prior work discussing replicability is very strict on this point: "One prior study of 103 replication attempts [in psychology] indeed found a 74% replication rate for findings reported at p ≤ .005 and a 28% replication rate for findings at .005 < p < .05 (Gordon et al., 2021)". There are also various statistical reasons why true findings usually won't produce .01 < p < .05.[[7]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fni21bv7c6wsm)
	- This skepticism and sanity checking is especially key for particularly surprising or novel bits of evidence. Wherever possible, I will try to re-implement a key experiment from scratch or try to get at the same evidence via a somewhat different route, just to make sure that I'm not missing something crucial.
- **Ablation studies**: When a paper introduces a complex new method, there are often several moving parts. For example, they may make changes A, B, and C to standard practice. If they then only evaluate the standard method or the method with all three changes, it's impossible to tell which changes are actually effective and necessary. It's good practice to remove one change at a time, observe its effect, and then repeat this process for each change.
- **Unknown Unknowns:**How confident are you that there isn't some alternative explanation for your results that you're missing?
	- This is a gnarly one. You'll want to think hard about it, ideally ask other people for feedback and get their perspectives. However, ultimately, you may sometimes just need to move on after a reasonable effort and accept that you may have missed something.
- **Avoiding Misleading Evidence (Cherry-Picking and Post-Hoc Analysis):**
	- **Was this cherry-picked?** Researchers can, accidentally or purposefully, produce evidence that looks more compelling than it actually is. One classic way is cherry-picking: presenting only the examples that look most compelling. This is particularly dangerous with qualitative evidence, like case studies.
		- While qualitative evidence can be extremely valuable, it’s important to note *how* cherry-picked it was. Ideally, provide randomly selected examples for context to give a fairer picture.
		- The main exception is if your claim is an existence proof. In this case, one example suffices, if it’s a trustworthy result.
	- **Track pre/post-hoc analysis.** It's important to clearly track which experimental results were obtained *before* versus *after* you formulated your claim. Post-hoc analysis (interpreting results after they're seen) is inherently less impressive than predictions confirmed by pre-specified experiments.
		- Be aware that even complex predictions suggested by a hypothesis can turn out to be correct for the wrong reasons
		- For example, in [a toy model of universality](https://arxiv.org/abs/2302.03025), I came up with the key representation theory-based algorithm I thought the network would follow before we got our key pieces of empirical evidence. I felt very confident. However, follow-up work found that a different explanation, which also involved representation theory, was what was actually occurring.
- **Quality Over Quantity:**Try to prioritise having at least one really compelling and hard to deny experiment, over a bunch of mediocre ones.
	- If you do have many experiments, often some are more compelling than others. Highlight the ones that most strongly support your claims in the main text and consider moving others to an appendix or referencing them more briefly.
- **Diverse Lines of Evidence Are Robust:**On the flip side, it can be far better to have several *qualitatively different* lines of evidence all pointing to the same conclusion, rather than many very similar experiments that all use similar methodologies and standards of proof.
	- Qualitatively different basically means “given the result of experiment 1, how well can I predict the result of experiment 2?”
	- This can justify putting effort into weak lines of evidence; for example, qualitative analysis of some data points can be useful supporting evidence of a quantitative study, even if insufficient to carry a paper independently, as they make it less likely that the summary statistics hid a subtle flaw.
- **Distilling Experiments:**
	- Often, at the end of a project, you'll have run many experiments, some of which felt around the edges of your core claims. But by this stage, you likely have a much clearer idea of what the most promising kinds of evidence are. If practical (considering time and resources), consider going back to run a more conclusive, decisive experiment using what you now know.
	- This could also involve scaling up: using more models, larger sample sizes, sweeping hyperparameters more thoroughly, running on more diverse datasets, etc.
- **Baselines are Crucial:**
	- A common mistake is for people to try to show a technique works by demonstrating it gets "decent" results, rather than showing it achieves *better* results than plausible alternatives that people might have used or are standard in the field.
		- Implicitly you’re supporting the weak claim “method X works at all” not “method X is actually worth using in practice”
	- Sadly this is especially prevalent in fields like mechanistic interpretability, where the comparative need for qualitative evidence can lead to neglecting more rigorous and systematic quantitative comparisons against strong baselines - the best papers have both qualitative and quantitative evidence.
	- **The subtlety of baselines:** It's not enough to just *have* them; you must strive to have the *strongest* possible baselines. Put meaningful effort into making them good. Often, a "competitor" method can seem weak but can significantly improve with proper hyperparameter tuning, prompt engineering, or appropriate scaffolding.
		- There's a natural bias to invest more effort in making one's "cool, shiny" new technique look good than in optimizing "boring" baselines. Resist this. Rigorous comparison to strong baselines is critical for good science and for genuinely persuading informed readers.
- **The Guiding Question for Evidence:**
	- Ultimately, the question to ask about your evidence is: "Should this update a reader's beliefs about my claims?" not "Does this fit the stereotypical picture of a rigorous academic paper?" While the latter often correlates with the former, your primary goal is genuine persuasion through sound evidence.
	- Eg if reading several dataset examples by hand is genuinely strong evidence of your claim, just report that and justify why it’s great evidence!
- **Reproducibility & Publishing code**: Rigour can be in the eye of the beholder: if readers cannot understand or verify it for themselves, it’s far harder to consider it rigorous. So your paper will be made substantially more useful by providing more detail about your exact methods.
	- A particularly useful approach is sharing your code. This enables others to build on your work and clarifies any ambiguities left in the paper (there will always be some). More broadly, it provides transparency into your exact process.
	- If you have time, you should:
		- Ensure the codebase runs on a fresh machine
		- Write a helpful README that includes links to key resources like model weights or datasets (which can be easily hosted on Hugging Face)
		- Create a Python notebook demonstrating how to run the key components

Tragically, the world is complicated, and there is often no single clear recipe to deal with all edge cases in research. These considerations are guidelines to help navigate that complexity

### Paper Structure Summary

How are these claims and experiments translated into a paper?

- **Abstract**: Motivate the paper, present your narrative and the impact: explain your key claims and why you believe they are true - be as concise and high-level as possible, while still getting the point across. Give enough information for a reader to understand the key takeaways of your paper and whether to read further or not - they often won’t read further, so it’s key that they still get the gist!
	- The reader is coming in from a cold-start, and may have no idea what your paper is about - you need to help them orient *fast*, and indicate what “genre” your paper fits into
	- The rest of your paper exists to support the abstract
- **Introduction**: Basically an extended abstract that fleshes out your narrative - explain your key claims, motivate them, contextualise them in the *key*parts of the existing literature. Explain your key experiments and the results and why this supports your claims. Ensure the reader leaves understanding the narrative of the paper, and whether to read further or not - they often won’t. This is basically a self-contained paper summary, don’t worry about “spoiling” the paper or repetition - with a complex idea, you want to repeat it in varied ways so that it sticks.
	- The introduction sets the structure of the rest of the paper
- **Main content**: This is where the real technical detail lives. Clearly and precisely explain background concepts and results, your precise claims, what exactly you did for your experiments (in *full*detail, using appendices if need be, relevant baselines, etc), the results, what they mean and their implications, etc.
	- This should be tightly planned to support the key claims, not sprawling and comprehensive. **For section and subsection you should have a clear answer for how it contributes to the narrative**, and would be damaging to remove.
		- I recommend first planning out clear opening and closing sentences of each paragraph: what does the paragraph show and how does it fit into the paper?
- **Figures**: Figures and tables are a key medium for communicating experimental results. Diagrams are great for communicating key ideas and claims. Put a lot of effort into your figures.
	- Good captions are also crucial - you need to given context on what the figure shows, the nuance and intended interpretation, and key technical detail. Ideally the reader will understand everything from just the figure *and*just the caption, though this is ambitious
- **Related work**: This is a mini literature review - I generally put this after the main content and don’t think it’s super important. Giving context on a few key similar papers and how your work differs is crucial, but typically done in the intro.
- **Discussion** (/limitations/conclusion/etc): A place to put all the high-level reflections - limitations of your work, future work, key implications, etc. This is not essential, but is nice if you have something worthwhile to say. Acknowledging key limitations is very important, and papers that don’t do this are substantially weaker and less useful (in my opinion).
- **Appendices**: Everything else - in the main paper you need to care about being concise, but here you can do whatever you want. Often you want to briefly discuss something in the main paper and move all technical detail to the appendix. Appendices are pretty low stakes and rarely read except by superfans, so don’t stress them too much.

## Analysing My Grokking Work

This is all pretty abstract. To concretise this, let’s look at [my grokking paper](https://arxiv.org/abs/2301.05217) through this lens. I’ve broken it down into claims, evidence, context and motivation, with commentary thrown in. This is somewhat stylised for pedagogical reasons, but hopefully useful!

- **Meta**: This was a challenging paper to write!
	- There was significant technical detail to our claims and evidence, largely unfamiliar to readers - mech interp was very new, and we did something weird and novel. We needed to communicate our claims (the algorithm) and our experimental evidence, *and*justify why the evidence was believable, since there were no standard methods to follow
		- A good diagram was critical to explaining the algorithm:

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/65a5b7254debaeb354f0e6fd637b58c5abd07a533d472efb39d776799083c27f/tuw5q8dwrpyd3hegimjy)

*Figure 1 from the paper, and lead image in the tweet thread*

- This was more like two papers - the reverse engineering, and the study of circuit formation, and we needed to compress both into the same page limit. Fortunately, they did fit a cohesive theme

**Structure**:

- **Claim**: We fully reverse engineered a tiny transformer trained on modular addition
	- **Meta**: This is a general claim, but about a specific model
	- **Context**: We needed to explain the entire notion of reverse-engineering a model from its weights, as readers may not have been familiar. The motivation for why this is interesting is pretty obvious, but the goal is easy to misunderstand
	- **Evidence**: We show this with several lines of evidence: activation and weight analysis, and causal interventions
- **Claim**: This circuit forms gradually, well before the point of sudden grokking -> grokking is a gradual from memorisation followed by removing memorisation
	- **Context**: The entire notion of grokking as covered in prior work
		- **Meta**: This is critical context to understand my paper, but is also prior work - I need to explain enough detail for unfamiliar readers to follow, but without excessive repetition, and cite it for them to see further details.
	- **Motivation**: Grokking is a big deal and surprised many people. We show it’s fairly different from what people think.
	- **Evidence**: We show this by designing convincing progress measures to track the circuit, and show that they shift well before grokking

## The Writing Process: Compress then Iteratively Expand

***Note**: Check out*[*my paper writing checklist*](https://docs.google.com/document/d/1AoF6bPJp-muWnsZLMmfcxo1fmAu1izUzZXDFHar-35o/edit?tab=t.0)*for a concrete to-do list of what I recommend doing here*

So, you have a list of claims and key experiments. Now, all you need to do is write the paper! I recommend an iterative process - start with a bullet point narrative, then a full bullet point outline, then flesh it out into prose, taking time to reflect at each stage. See above or the next section for more details on the actual structure of a paper, here I just try to convey the high-level strategy

A key challenge in paper writing is the **illusion of transparency** - you have spent months steeped in the context of this research project. **You have tons of context, your reader does not**. You’ll need to help them understand exactly what you are doing, in the large space of all possible ML papers, and address all the misconceptions and possible misunderstandings, even though to you it all feels obvious. This is a difficult skill - wherever possible, get extensive feedback from others to address

**Spend far more time on early sections**: Realistically, tons of people read the title of your paper, many read the abstract, some read the introduction/skim the figures, and occasionally they read the whole thing. This means **you should spend about the same amount of time on each of: the abstract, the intro, the figures, and everything else**[[8]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fn8swe1i37vnm). (I’m only half joking)

### Compress

You should **start by compressing your work as much as possible**. Some tips:

- Verbally describe it to someone.
	- Bonus: Ask them what was most interesting, or to repeat it back to you
- Plan out a talk.
- Give your research notes to an LLM and ask it to summarize the key points.
- After each, think about what's missing, what's extraneous, what’s inaccurate or misleading, and iterate.

This compression step is crucial because it forces you to identify:

1. The 1-3 concrete claims you believe to be true
2. Why these claims matter (brief motivation)
3. The crucial experimental evidence for each claim (ideally 1-3 key experiments per claim)

Next, critically evaluate this compressed version:

- Do your experiments genuinely support your claims?
- Are there ways your experimental evidence could be flawed or misinterpreted?
- Could the evidence be true but the claim false? How?

As part of this process, write down common misconceptions, limitations, or ways someone might over-update on your work.

### Iteratively Expand

Once you have a compressed list of bullet points that you are satisfied with, you should start iteratively expanding and developing them. After each step, stop, reflect, read through, and edit - rushing a step can lead to a lot of wasted time at the next step.

If you have a research supervisor/mentor, it is very valuable to get feedback at each stage - I find it way faster *and*easier to give feedback on a narrative or bullet point outline than being sent 8 pages of dense prose! Even if you don’t have a mentor, try to get feedback from *someone*[[9]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnamgw22xkub).

1. Start with the **compressed bullet point narrative** - make sure you’re happy that this captures the narrative you want!
2. Write a **bullet point outline of the introduction** - **the north star here is to communicate what your claims are**, exactly, (including which parts are novel vs building on prior work), **why they matter**, and a high-level idea of **why they are true**
	1. This involves more detail, key citations to the literature, more detailed motivations, etc. Generally it won’t get too technical, but can involve explaining a few crucial concepts.
	2. Flow matters a lot here! Try to get feedback on how it feels to an unfamiliar reader, and how cohesive it feels
3. Write a **bullet point outline of the full paper** - covering the key experiments, results, methodology, background, limitations, etc
	1. **The north star is to convince a skeptical, engaged reader that your claims are true** - give them enough information to understand your experiments and the results
	2. **A good outline is tight and minimal** - every part of it should have a clear role in the overall narrative. If you don’t have a good answer to “what goes wrong if I cut this”, you should cut it.
	3. **Good figures are crucial**to communicate results - plan these out, but leave making them to step 4
	4. This can include writing the related work, or you can leave that to the end.
4. **Results**: Collect key experimental results and make first draft figures to show. Does this convincingly support your narrative? What’s missing? Which parts and complex and need more exposition, vs standard/unsurprising and can be sped through?
	1. You can’t always get this done in advance, but it’s *much*better if you do - more time to refine, iterate, etc.
5. **First draft**: Flesh this out into prose and full technical detail
	1. If you have writer’s block, try giving an LLM your outline, some relevant papers, and asking for a first draft. *Do not*just copy this into your paper, but I find that sometimes LLMs have good ideas, and that frustration with poor quality LLM write-ups can be a great way to break through writer’s block.
6. **Edit it**: Repeatedly pass over your first draft (and get feedback), clean it up, polish it, make the narrative as tight and clear as possible, cut out extraneous fluff, make the figures glorious, etc
	1. This is worth spending a *lot*of time on, it can make a big difference!

## The Anatomy of a Paper

OK, so what actually goes into a paper? What are the key components you’ll need to write, and what is the point of each?

### Abstract

*Check out the annotated abstract earlier for a concrete breakdown*

An abstract should **give a cold-start reader a sense of what the paper is about** - what sub-field, what type of paper, what key motivating questions, etc. This is a key manifestation of the illusion of transparency:  you know exactly what your project is about but to your reader there is a large space of possibilities, and without any context may have completely incorrect priors and wildly misinterpret.

People will often leave your abstract then move on, unless strongly compelled - it’s a big deal to get right, and deserves high polish

A common approach is:

- First sentence: Something uncontroversially true that clearly states which part of ML you're focused on (e.g., "Thinking models have recently become state-of-the-art across many reasoning tasks.")
- Second sentence: Something that makes clear there's a need, something unknown, or a problem for your paper to solve (e.g., "The transition to reasoning models raises novel challenges for interpretability.") - this should convey (some of) the motivation

Now the reader is situated, you need to *concisely* communicate your claims. Again, illusion of transparency - they often won’t know your techniques, the work you’re building on, key ideas, etc. **Abstracts should be as accessible as possible** - use simple language as much as you can

- Sentence 3: State the crucial contribution of this paper and why it is exciting - you’ll need to lose nuance, this is OK.
- Optional: Sentence 4 should provide clarifying details on that claim, such as its meaning and how evidence could be provided if not obvious.
	- Include key definitions for any necessary jargon, though jargon should be avoided if possible, unless it’s standard in the field and useful to contextualise the paper within the field.
- Each of the next few sentences should focus on either key experimental evidence or additional important claims. These can sometimes overlap, where a specific claim being true also supports the main claim.
	- Try to have 1 sentence per idea - this forces you to be concise, without getting overwhelming.
- If possible, include a concrete metric or result in any of the above that gives readers a sense that your results are real and substantial.
	- This can look like folding in key evidence of a claim into the sentence introducing the claim.

Finally, close with motivation:

- Final 1-2 sentences: Wrap up by reminding readers why the paper matters/is a big deal, its implications, and how it fits into the broader context.
	- This is also a good place to clearly state your standard of evidence, whether your work is:
		- A preliminary step towards…
		- Shows that method X should be used in practice
		- Shows that practitioners should take care when using method Y
		- Establishes best practices for Z
		- Provides compelling evidence that…

### Introduction

The introduction is broadly similar to the abstract but more extended and in-depth. I proceed in roughly this order:

- Paragraph 1: **Context** - What topic are we studying, what is the key motivating question, and why does it matter?
	- Optionally: 1 sentence on how our contribution answers it
	- It’s good to liberally cite papers here to establish things like ‘this is a real field’, ‘this problem matters’, ‘people are interested in it and have tried (and failed) to solve it/have solved variants’
- Paragraph 2: **Technical background** - what do we know about this problem? What are the established techniques our paper rests on? Etc
	- It’s good to cite liberally here to establish that what you’re using are standard methods and concepts, and to give the reader more context.
	- Here and in paragraph 1 you want to better situate your problem in the broader strategic picture of the field. Why does this matter? What other work has been done here, and why is it inadequate?
- Paragraph 3: **Key contribution** - What exactly is our main claim? Add key nuance, detail, context, etc.
- Paragraph 3.5[[10]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fn38hw5w3ydeg): **Our case** - summarise the most critical evidence we provide that our main claim is true
- [Optional]: More paragraphs for a second or third claim and the key case
- Paragraph 4: **Impact** - What should you take away from this paper? What are the implications, why is it a big deal, who should take different actions as a result of the results, etc. This may be emphasising practical utility, pushing forwards basic science, correcting common misconceptions, etc.
- **Contributions**: End with a bullet point list of concise descriptions of your key claims, ideally with concise descriptions of key evidence
	- You want something a reader can look at and decide if they’re impressed/interested

Note: Citing here isn't about performatively covering all the relevant papers[[11]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnqfem2sure9). It's about providing the context a reader needs to understand why your work is interesting and how it's limited. I try to have at least one citation for each step in an important argument, eg why

The introduction is where you have room to define key terms and concepts required to understand your claims, especially if they're somewhat technical.

It's often good to explicitly end with a bullet-point list of your contributions, which are basically just the concise claims you believe to be true, potentially with brief references to the supporting evidence.

### Figures

Figures are incredibly important. Having clear graphs can be the difference between a very clear and easy-to-read paper and an incomprehensible mess.

To create a good figure:

- Ask yourself, "What exactly is the information I would like someone to take away from this?" It's not just about finding the list of numbers output by your experiments and shoving them into some standard plotting software, you want to carefully choose a visualisation that emphasises the desired information and takeaway.
- Ask yourself, "Why does this experiment tie back to my core claims? How would I like the reader to interpret these results? Which parts do I want to draw their attention to?"
- Consider annotating a graph or, if there's one particularly important line, emphasising it
	- E.g. make it dark while the others are light and low opacity, or all the other ones of the same color.
- Include standard elements like axis titles, a clear caption that explains what the figure is, how to interpret it, or at least where in the text they should look to understand what's going on.
	- Make sure the axis title and ticks are large enough to read, and have a good clear legend.
- Often you can compress a fair amount of information into one graph - for example, using different sizes and shapes of markers on a scatterplot, different colors, etc.
- For heatmaps, if your data is positive and starts at zero, use a color scale that is white at zero and dark at the max (in plotly, "blues" is good). If your data is positive and negative with zero as a meaningful neutral point, use a color scale where zero is white (in plotly, "RdBu" is good).
- Avoid having reds and greens conveying key information, 4% of people are red-green colourblind

It can work well to combine several key graphs into one figure and make it your figure 1. E.g.:

[Language models represent space and time](https://arxiv.org/pdf/2310.02207):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/0c3d3f7b5a316bbc6ae17ed433c3de39c98ebbfc88b2b7ac629c8319c8c0954b/fqnwtulpjr6orkq1mp3j)

[Not all features are one-dimensionally linear](https://openreview.net/pdf?id=d63a4AM4hb):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8e16a323eb589967f6f16d49476037cdb1bbd722cc1ed62c1fa420f5d73b4cbc/dcacm5v0fr2gtsc9dqdg)

Another kind of figure is an explanatory diagram rather than a graph. This can be a high-effort but very effective figure one, that gives people a sense of roughly what is happening in the paper. This should be something that would catch people's eye if you put it as the first image in a tweet thread about your paper. Some diagrams I liked (intentionally at several different levels of effortful):

[Emergent Misalignment](https://www.emergent-misalignment.com/):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4d23b0480db579d24ff12c89031372b38f701222f393d197d6bd396a5bc03c87/hp4mfofcp4sdnn7loq4h)

[On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/c76ea31c53e60cd88e4b4559ecdf4ccb53410d0c75cfe58c5990bcb7600ccb4b/s2xy3kemvalmdsw2y6gs)

[CoT in the wild is not always faithful:](https://arxiv.org/pdf/2503.08679)

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6b9d49a36511ed41454f58686dad379917a4c473b5e7678bf66d98bd912d4e6e/xsqkni9iosbyt5mbdoet)

[Refusal is mediated by a single direction](https://arxiv.org/abs/2406.11717?):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/711cd46c3e32ab7ab39f28df69d0806d5451ce804556bba0db9a1870c690eedf/sjksooqcgwspk5cc43u8)

[My grokking modular addition work](https://arxiv.org/abs/2301.05217):

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cb657958d42a449c32dc6c7a4d4eec6e13407e8b585fd597799bf60dcbaaade5/edc4br2htkw6gxkgrsnz)

### **Main Body (Background, Methods and Results)**

Most of the actual paper, by word count, should be about **communicating your experiments and results in precise technical detail**. To do good science, it is important that researchers can understand exactly what you did and what you observed, so they can draw their own conclusions rather than needing to take things on faith. For example, in interpretability, there are ways that a method can give completely useless answers if misapplied, so it’s crucial that I know if a paper did that, even though the detail might seem totally unimportant to the authors!

Ideally, you want to communicate the information at several different layers of abstraction. It's your job to ensure that readers understand:

- The key background context required to disambiguate and understand your work - key terms, techniques, etc[[12]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fncamk72o22gu)[[13]](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fndj77nx985wb)
- What your results are and how to interpret those results and their significance
- What you actually did for your experiments
	- Why this was reasonable/well motivated/relevant to your claims
	- The specifics of various technical choices you made, and their implications for how to understand the results.

For structure, here’s a good default:

- **Background**: to explain the relevant context and terms - in particular, please define terminology and crucial techniques!
	- If pressed for space, you can put a glossary of key terms/definitions as an appendix, I always appreciate this
	- If you’re defining something new for this paper, put this in a section which is clearly *not*about reviewing known things (a new section or separate subsection)
- **Methods**: Explain the methods you used and why they are relevant to the problem
- **Results**: Specify exactly how the methods are applied as experiments, and what the results are
	- If you have a bunch of experiments using fairly similar methods, put each in a different subsection

If the experiments for each claim are more boutique, or if there are several claims with different styles of evidence, then I try to give each type of evidence its own section while explaining how it ties back to the overarching theme, rather than a methods -> results section. People will forget about the first method before they see its results.

### Discussion

Explaining the limitations of your work is a crucial part of scientific good practice. The goal of a paper is to contribute to our body of knowledge. Readers must understand the limitations of the evidence you provide to have a calibrated sense of what knowledge they have learned. And it's important that you put a good faith effort into documenting limitations because you know far more about your work than the readers, so they may miss things.

There is a common mistake of trying to make your work sound maximally exciting. Generally, the people whose opinions you most care about are competent researchers who can see through this kind of thing. And I generally have a much higher opinion of a piece of work if it clearly acknowledges its limitations up front. I’m not sure if this makes it easier or harder to get published.

This is also the place to discuss broader implications and general takeaways, future work you’d be excited about, reflections, etc.

Some people have conclusions too. Personally, I think conclusions are often kind of useless; the introduction should have explained this well. You can skip it

### Related Work

Generally, related work is often treated like a bit of an annoyance and afterthought. The feeling that you need to cite lots of things that aren't actually relevant can be annoying, but sometimes there is very important work that has done similar things to you, and a reader might have seen that and wonder why your paper is interesting.

It's very important to clearly explain why what you did is different or, if what you did is not very different, either acknowledge this ("that was parallel work") or explain why your work is still slightly interesting in this context, or how you fixed a mistake in prior work (stated politely).

But that said, there’s a lot of annoying norms here and related works often add little value IMO - needing to cite a lot so you look like you’ve put in enough effort, covering minor or obscure things that aren’t particularly relevant, citing low quality works to be polite, making sure to cite the first instance of each thing, etc. Contextualising in the literature is important, but ideally I’ve already covered it in the introduction.

Related work is often put as the second section of the paper. Personally, I generally prefer it to be the penultimate section. I think related work should only be upfront if it plays an important role in motivating the paper - if your paper is very heavily tied to the surrounding literature, plugging a gap, correcting a mistake, or unlocking a new capability that would enhance various bits of prior work.

### Appendices

Appendices are weird. They're basically the place you put everything that doesn't fit into the main paper. One way to think about it is that you're actually writing a much longer than nine-page paper - the main body *and* the appendices - but you've chosen a highlights reel for the first nine pages where you put all the absolutely key information. You place all the less crucial information in the appendices for readers to pick and choose from as they see fit.

In general, the crucial scarce resource you must manage is the reader's time and attention. The main body should be aggressively prioritized to make the most of this, be engaging, and communicate the most important pieces of information. But if you have a lot more to say than you can fit in there, then that's what appendices are for. A truly interested reader can go and take a look, though most won't.

Generally, appendices are held to a notably lower standard than the main body and will be read far less, so you should not feel obliged to put in meaningful effort polishing them. This is the standard solution to the dilemma when you want to include full technical detail but have done some fairly complex and convoluted work that just won't realistically fit.

## Common Pitfalls and How to Avoid Them

### Obsessing Over Publishability

Peer review is notoriously terrible for seeking truth. Reviewers often have biases, like favoring work that feels novel and shiny and exciting, or that doesn't feel weird or too new, or that doesn’t seriously challenge their existing beliefs. This has been shown in [rigorous RCTs](https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/), where NeurIPS 2021 gave some papers two sets of reviewers and compared their decisions. The results… aren’t great:

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eJGptPbbFPZGLpjsp/ovczzjp22rckjrhzecuo)

I personally think that, at least in safety, doing good work that people respect matters more than getting into conferences, though both are nice. I’ve generally had fairly good results with just trying to write high-integrity work that explains why I believe it is interesting, and just trying to do good science and the work that I think is highest impact, even if it doesn't fit the academic mold.

But it’s pretty plausible to me that many of the people reading this are not in such a fortunate position, and that getting first author papers into top conferences would be a meaningful career boost, especially your first 1-2 papers. The strategy I generally recommend for my mentees is to spend most of the project doing the best scientific work they can. Then, as we approach the end of the project, we figure out how to wrap it up in a maximally conference-friendly package while writing and submitting it. If we did anything that made the work noticeably worse, we can undo it before uploading to Arxiv.

### Unnecessary Complexity and Verbosity

Papers are seen as prestigious, formal, and highly intellectual artifacts. As a result, there's a tendency towards verbosity or trying to make things sound more complex and fancy than they actually are, so they *feel*impressive. I think this is a highly ineffective strategy. If I don’t understand a paper, I generally ignore it and move on, or assume it’s BS in the absence of strong evidence to the contrary. Often, the best papers just take some very simple techniques and apply them carefully and well. There’s a real elegance to being simple and effective.

People need to understand a paper in order to appreciate it and build on it and think it is interesting (except for superficial Twitter clickbait). Generally, you want to be precise, but within the constraint of being precise, be as simple and accessible as possible. Try to use plain language and minimize jargon except where the jargon is needed to precisely convey your meaning. You get points for quality technical insights, not for sounding fancy. Verbosity and overly complex language and jargon is actively detrimental to your paper’s prospects, IMO.

### Not Prioritizing the Writing Process

People often do not prioritize writing. They treat it like an annoying afterthought and do all the fun bits like running experiments, and leave it to the last minute. This is a mistake. Again, your work only matters if people read and understand it. Writing quality majorly affects clarity and engagement. Writing is absolutely crucial and is a major multiplier on the impact of your work.

I typically recommend that people switch from [understanding mode to distillation](https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand) and paper writing a month before a conference deadline, if at all possible. You should want to spend a lot of your time iterating on a write-up, getting feedback, trying to make it clearer, thinking about weaknesses, etc.

## Tacit Knowledge and Beyond

One irritation I have about the standard paper structure is that it heavily incentivizes being rigorous and maximally objective and defensible. Obviously, there are significant advantages to this, but I think that often a lot of the most valuable insights from a research project come in the form of tacit knowledge.

This might be:

- This was hard and here are the steps we had to follow to get it to work.
- Here are some ways we noticed our experiments catching fire and what we did to fix them.
- Here's my fuzzy intuition of what's going on in the big picture - I can't fully defend it, but I'm reasonably confident this is true after several months of screwing around in this domain.
- Here’s something I misunderstood for months before it suddenly clicked
- Here’s a common misconception in this domain, or way people often misunderstand or overreact to our results
- Here’s my advice to anyone replicating this work, especially how to find hyper-parameters and deal with the fiddly bits
- Fleshing out a plan for future work directions you find particularly exciting.

I think this is really important, and I find it a real shame that this is often just discarded. I am personally a big fan of putting this kind of stuff as appendix A or as an accompanying blog post, where you can take as many liberties as you like.

## Conclusion

Your research will only matter if people read it, understand it, engage with it, and ideally believe it. This means that good paper writing is a crucial skill, but often neglected.

The core process should be to find the concise claims you believe to be true, the strongest experimental evidence that you believe builds a robust case for these claims, and use this to craft a coherent narrative. Then flesh this out into a bullet point outline of the overall post, reflect on it, and ideally get feedback, and iteratively expand.

Again, this is a highly opinionated post about how I personally think about the process and philosophy of paper writing. I'm sure many researchers will strongly disagree with me on many important points, and the correct approach will vary significantly by field and norms.

1. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefwwk8u16jdjf)** Note: I am in no way claiming that *I*follow this advice, especially in blog posts - this is my attempt to paint a platonic ideal, and advise on how to be a better person than I. Personally, I find actually writing academic papers pretty frustrating and much prefer blog posts
2. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnref5kbv6xekx6j)** I think writing great papers certainly helps, and if you’re new I recommend just trying to write the best paper you can, but there’s still a lot of depressingly perverse incentives from ML peer review
3. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefxjcznpnz657)** I mention this purely for completeness. I have never seen a convincing guarantee in deep learning, neural networks are far too squishy
4. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnref29hhedam5nx)** OpenAI’s is also good, but Google’s is free!
5. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefc844hgv7nzl)** Well, at least “provide enough evidence to somewhat update their beliefs”, convince is a fairly high bar
6. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefn6o4myp4h8r)** Which, admittedly, is fairly rare in ML as far as I’m aware
7. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefi21bv7c6wsm)** Thanks to Paul Bogdan for these points on p-values
8. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnref8swe1i37vnm)** Also the title, though I haven’t figured out how to productively spend 20% of my time on that yet…
9. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefamgw22xkub)** Paper swaps are a great way to get feedback - find someone else also working on a paper and offer to give each other feedback. Even if you have less time before the paper deadline, this tends to be a mutually beneficial trade.
10. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnref38hw5w3ydeg)** This can be its own paragraph, or part of paragraph 3
11. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefqfem2sure9)** Save that for the related work section…
12. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefcamk72o22gu)** If something is *super*widespread knowledge, no need to cover it, but err towards defining things. E.g. I wouldn’t bother defining the transformer architecture or an LLM, but I would define a sparse autoencoder or steering vector
13. **[^](https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/eJGptPbbFPZGLpjsp#fnrefdj77nx985wb)** If this is too long, you can move most of it to an appendix

An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2
by Neel Nanda
7th Jul 2024
This post represents my personal hot takes, not the opinions of my team or employer. This is a massively updated version of a similar list I made two years ago

There’s a lot of mechanistic interpretability papers, and more come out all the time. This can be pretty intimidating if you’re new to the field! To try helping out, here's a reading list of my favourite mech interp papers: papers which I think are important to be aware of, often worth skimming, and something worth reading deeply (time permitting). I’ve annotated these with my key takeaways, what I like about each paper, which bits to deeply engage with vs skim, etc. I wrote a similar post 2 years ago, but a lot has changed since then, thus v2!

Note that this is not trying to be a comprehensive literature review - this is my answer to “if you have limited time and want to get up to speed on the field as fast as you can, what should you do”. I’m deliberately not following academic norms like necessarily citing the first paper introducing something, or all papers doing some work, and am massively biased towards recent work that is more relevant to the cutting edge. I also shamelessly recommend a bunch of my own work here, and probably haven't always clearly indicated which papers I was involved in, sorry!

How to read this post: I've bolded the most important papers to read, which I recommend prioritising. All of the papers are annotated with my interpretation and key takeaways, and tbh I think reading that may be comparable good to skimming the paper. And there's far too many papers to read all of them deeply unless you want to make that a significant priority. I recommend reading all my summaries, noting the papers and areas that excite you, and then trying to dive deeply into those.

Foundational Work
A Mathematical Framework for Transformer Circuits (Nelson Elhage et al, Anthropic) - absolute classic, foundational ideas for how to think about transformers. See my youtube tutorial (I hear this is best watched after reading the paper, and adds additional clarity). 



Deeply engage with:
All the ideas in the overview section, especially:
Understanding the residual stream and why it’s fundamental.
The notion of interpreting paths between interpretable bits (eg input tokens and output logits) where the path is a composition of matrices and how this is different from interpreting every intermediate activations
And understanding attention heads: what a QK and OV matrix is, how attention heads are independent and additive and how attention and OV are semi-independent.
Skip Trigrams & Skip Trigram bugs, esp understanding why these are a really easy thing to do with attention, and how the bugs are inherent to attention heads separating where to attend to (QK) and what to do once you attend somewhere (OV)
Induction heads, esp why this is K-Composition (and how that’s different from Q & V composition), how the circuit works mechanistically, and why this is too hard to do in a 1L model
Skim or skip:
Eigenvalues or tensor products. They have the worst effort per unit insight of the paper and aren’t very important.
Caveats (h/t Buck) - I think the paper somewhat overstates the degree to which we can fully and mathematically understand tiny attention-only transformers, and it may be worth checking out these critiques:
Its mathematical claim about one-layer transformers being equivalent to skip-trigrams is arguably wrong
Many people interpret the induction head hypothesis as being much stronger than evidence supports.
Understanding how a transformer works in detail is a pre-requisite for getting the most out of this paper, I recommend getting to the point where you can code a basic transformer (eg GPT-2) from scratch. I shamelessly recommend my Youtube tutorial on this (and accompanying tutorial notebook).
Superposition
Superposition is a core principle/problem in model internals. For any given activation (eg the output of MLP13), we believe that there’s a massive dictionary of concepts/features the model knows of. Each feature has a corresponding vector, and model activations are a sparse linear combination of these meaningful feature vectors. Further, there are more features in the dictionary than activation dimensions, and they are thus compressed in and interfere with each other, essentially causing cascading errors. This phenomena of compression is called superposition.

Toy models of superposition (Nelson Elhage et al, Anthropic) - absolutely foundational work on the idea of superposition, a ton of great ideas in there. I found reading it very useful for building my conceptual frameworks of what  My main critique is that they only study toy models (but, you know, it's in the name)

Deeply engage with:
The core intuitions: what is superposition, how does it respond to feature importance and sparsity, and how does it respond to correlated and uncorrelated features.
Read the strategic picture, and sections 1 and 2 closely.
Skim or skip:
No need to deeply understand the rest, it can mostly be skimmed. It’s very cool, especially the geometry and phase transition and learning dynamics part, but a bit of a nerd snipe and doesn’t obviously generalise to real models.
Finding Neurons In A Haystack (Wes Gurnee et al, during my MATS program). We try to find empirical evidence for and against superposition by identifying various attributes (eg “this text is in French” or “this is the second token in ‘social security’”) and then training sparse probes (ie at most k non-zero elements) on MLP neurons to find them. I recommend skimming over the sparse probing stuff and methodology, IMO the key stuff is the case studies, and building intuition for what's out there in models. Notably, we find monosemantic language neurons (eg is French), and evidence that compound word detection (eg “social security”) is done in superposition, distributed across many polysemantic neurons. Youtube walkthrough

I'm really proud of the exposition about superposition in appendix A, and think it’s highly worth reading, and clarifies some open thread left by Toy Models of Superposition
Fact Finding (Neel Nanda & Sen Rajamanoharan et al, Google DeepMind) - we tried to really understand how facts were stored in model MLP layers in superposition. Unfortunately, we failed - it seems cursed and complex. But I think we learned a fair bit in the process, provided further evidence that superposition is happening (it’s not just neuron aligned and there’s many more facts than neurons) and falsified some simpler hypotheses. The sequence is long, but I think it's worth reading post 1 carefully. Post 3 has the superposition relevant stuff

Note that this research was done before SAEs took off, and we do not use them.
Sparse Autoencoders
SAEs are a tool to interpret model activations in superposition - they’re a one hidden layer ReLU autoencoder (basically a transformer MLP layer), and are trained to reconstruct a model’s activations. L1 regularisation is applied to make the hidden layer activations sparse. Though not directly trained to be interpretable, the hope is that each unit (or feature) corresponds to an interpretable feature. The encoder + ReLU learns the sparse feature coefficients, and the decoder is a dictionary of feature vectors. Empirically, it seems to work, and I think they’re the one of the most promising tools in mech interp right now.

To understand the actual technique I recommend this ARENA tutorial, sections 1, 6 & 7 (exposition + code >> papers), but here are some related papers worth understanding. Note that all of these came out in the past year, this is very much where a lot of the mech interp frontier is at! However, our understanding of them is still highly limited, and there are many uncertainties and open problems remaining, and I expect our understanding and best practices to be substantially different in a year or two.


Towards Monosemanticity (Trenton Bricken et al, Anthropic): The foundational work in applying SAEs to models to find interpretable features, beautifully written, and with a lot of good exposition and tips for training SAEs well. I recommend reading in full. My main criticism is that they only looked at 1L models, so all the features lie in unembedding space (they get linearly mapped to the logits), so it’s not very surprising that they’re linear - but future work (below) shows their results hold up.

Sparse Feature Circuits (Sam Marks et al, David Bau’s group): A great initial attempt at circuit analysis with SAEs (ie using SAE features as circuit nodes). Uses attribution patching, introduces a variant with integrated gradients that empirically works better at early layers and seems fairly principled. I personally think the way they implement gradient computation is somewhat convoluted (lots of stop gradients) and it would be simpler to do it analytically (take the gradient with respect to the residual stream after the relevant layer, and then do linear algebra to find the gradient wrt an SAE feature)

Has some cool results applying their interpretability insights to reduce spurious correlations in a linear probe, reducing gender bias. I find this particularly exciting, as it feels like the start of a real-world application of SAEs, which I think is a big open problem for the field - if they are truly as useful as we think they are, they should be better than existing baselines in a fair fight on at least some real-world tasks. See this post from Sam on the long-term safety motivations here - we want to distinguish between behaviourally identical systems that tell us the truth vs telling us what we want to hear.
Transcoders Find Interpretable LLM Feature Circuits (Jacob Dunefsky, Philippe Chlenski et al, during my MATS program). Circuit analysis can either be causal intervention based (eg Interpretability in the Wild, or Sparse Feature Circuits), or weights based (eg Progress Measures for Grokking) - actually multiplying things out and seeing what happens. It’s very hard to do weights-based analysis on MLP layers in superposition though, as for any given feature, most neurons (and their non-linearities!) are implicated, and hard to decompose. Transcoders are an SAE variant that tries to solve this, by learning a sparse, interpretable replacement for an MLP layer, by training an “SAE” to map the MLP input to output. They seem to perform about as well as SAEs, and make circuit analysis much easier, though we still struggled to get true weights-based analysis working - many features have high alignment without co-occuring.

Interpreting Attention Layer Outputs with Sparse Autoencoders (Connor Kissane & Rob Krzyzanowski et al, during my MATS program): A nice post showing that SAEs can be applied to attention layer outputs, and that this just works (they do it pre W_O, which has nice benefits of being able to easily attribute things to heads). I particularly liked the focus on showing that these SAEs can be a useful tool for researchers, attacking problems previously out of reach. We found a deeper understanding of the semantics of the IOI circuit, got a sense of the kinds of features that form in attention layers, figured out the different roles of two induction heads in a layer, and roughly gauged the role(s) of every head in GPT-2 Small.

Towards principled evaluations of sparse autoencoders for interpretability and control (Alex Makelov & Georg Lange et al, during my MATS program). One of my favourite attempts to design better metrics for SAE quality, one of the major open problems in the field. We took the well-studied Indirect Object Identification circuit, trained SAEs on every head and layer, and evaluated two metrics: sparse control, whether the output could change from input A to input B by changing a sparse set of input A SAE features, and interpretability, whether we could use SAE features as probes to find the expected features. Because we roughly know what features to expect, we can also train dictionary vectors for them in a supervised way, measure the quality of these, and use this to get a baseline for our benchmark

One of the hard parts of this analysis is that, though we think we know what important features to expect in the IOI circuit (the value and position of the names), we can’t be sure we aren’t missing something (eg the gender of the names). We discuss this at length, and how we try to deal with it.
Gated SAEs (Sen Rajamanoharan et al, from my team at DeepMind): Introduced Gated SAEs, a new architecture for SAEs that gets similar reconstruction at half as many firing features while being either comparably or more interpretable. We also scaled SAEs to Gemma-7B. I (very biasedly) think this is worth reading as a good exemplar of how to rigorously evaluate whether an SAE change was an improvement, and because I recommend using Gated SAEs where possible.

Scaling and evaluating sparse autoencoders (Leo Gao et al, from the OpenAI superalignment team RIP 😢). Shows that top-k SAEs (ie replace the ReLU with “keep the top k pre-activations, set the rest to zero) are an effective technique, scale SAEs to GPT-4 (the largest model SAEs have been trained on!), do some rigorous exploration of SAE scaling laws, and propose several creative ideas for measuring SAE quality. I’m particularly excited about the ideas for how to measure SAE quality, since this is a big open problem in the field, and would be keen to see the metrics proposed fleshed out and applied in more detail. The work provided a feature viewer, but did little qualitative case studies itself - anecdotally the GPT-4 SAE features don’t seem that interpretable, but I haven’t explored this properly.

One big concern I had from this paper is how interpretable top-K SAEs are. Follow-up work from Anthropic showed that there’s no hit to interpretability, and confirmed that they are a significant performance improvement (comparable to gated SAEs)
Scaling monosemanticity (Adly Templeton et al, Anthropic) Similar to the OpenAI paper, the headline result is that SAEs scale to a (near) frontier model, Claude 3 Medium (Sonnet). But there’s a massive focus on case studies and qualitative analysis, which is very fun and worth reading, so it complements the OpenAI work nicely. They find a range of abstract and multimodal features eg an unsafe code feature that activates on pictures of “warning: this web page may be insecure”, and including some potential safety relevant ones. There’s a focus on showing that features are causally meaningfully variables and can be used to steer the model, similar to steering vectors.

The most famous part of this paper was Golden Gate Claude, the ultimate meme, an accompanying demo where they released a version of Sonnet that was steered with the Golden Gate Bridge feature to obsessively fixate on the bridge
A key nuance is that (in my opinion) the paper does not show that SAEs are the best way to steer (compared to lower tech methods like steering vectors, or even prompting). Rather, the goal was to show that SAEs do something real, by showing that they have an abstract, causal effect, and affect the model’s computation in sophisticated ways. I’ve seen many people see Golden Gate Claude and think they need an SAE to do steering, you probably don’t need to bother! 
There’s an accompanying blog post with a bunch of tweaks to significantly improve SAE training, which excitingly seem to stack with gated and top-K SAEs!
I really loved the feature completeness result - turns out that there’s a clean sigmoid relationship between the frequency with which a concept appears in the SAE training, the number of alive SAE features, and the probability that it’s learned by the SAE.
Activation Patching
Activation patching (aka causal mediation analysis aka interchange interventions aka causal tracing aka resample ablations - argh why can't we agree on names for things!) is a core mech interp technique, worth understanding in a lot of detail. The key idea is that, for a given model behaviour, only a sparse set of components (heads and neurons) are likely relevant. We want to localise these components with causal interventions. But, given any prompt, many model behaviours go into it. For example, if we want to know where the knowledge that Michael Jordan plays basketball lives, this is hard - we can do things like deleting components and seeing if the model still says basketball, but maybe we deleted the "this is about sports" part or the "I am speaking English part".

The key idea is to find contrast pairs - prompts which are as similar as possible, apart from the behaviour we care about, eg "Michael Jordan plays the sport of" and "Babe Ruth plays the sport of". If we patch activations from the Jordan token into the Ruth token (or vice versa), we control for things like "this is about sports" but change whether the sport is basketball or not, letting us surgically localise the right behaviour.

To understand the actual technique I recommend this ARENA tutorial (exposition + code >> papers), but here are some related papers worth understanding.


How to use and interpret activation patching (Stefan Heimersheim & me) An expository and highly opinionated note on how to think about activation patching. It’s a powerful but subtle and confusing technique, and we walk through some traps, how to use the technique well, and advice on interpreting results.
Note that this is not a paper, in the sense that it doesn’t produce original research, but rather an attempt to reduce Research Debt. I highlight it because I think it distils implicit ideas spread across many papers and the heads of many researchers in one place.
Causal scrubbing (Redwood) - This is my current favourite attempt to define a metric for how good your explanation of a model's behaviour is. The key idea is to identify all patches that are allowed by your explanation (eg resample ablating a head that isn't part of your hypothesis, or resample ablating the path between two heads that don't compose under your hypothesis) and to do all of them, and see how much damage it does. I found the sequence itself somewhat sprawling and hard to read, but found it valuable to get my head around the key ideas, and found it had multiple important methodological insights like the idea of resample ablation (rather than ablating a component by setting it to zero, replace its activation with the value taken from a random other input).
It's even stronger than just ablating all irrelevant nodes and edges - if some activation should have the same value on multiple prompts (eg "the previous token is ' cat'") then you are allowed to patch it between different prompts where the previous token is cat. And we can do this recursively. Unfortunately, this is a pain in the ass to implement, and I think you can basically get away with not doing it in practice?
Attribution Patching (me, work mostly done at Anthropic) - Activation patching at industrial scale, using a gradient based approximation. You can approximate the effect of patching a component from input A into input B with (act_A - act_B) . (grad_B). This works reasonably well in practice, and is blazingly fast - you go from doing a forward pass per component to doing two forward & one backward pass for every single component simultaneously! 
I think the key thing is just to know that the technique exists, but the post itself has a lot of good exposition on how to think about activation patching that I feel quite proud of. It doesn't do a good job of testing that attribution patching is actually a good approximation.
See the AtP* paper (Janos Kramar et al, Google DeepMind mech interp team) for a much more systematic set of experiments showing that attribution patching is a good approximation and the best way to use a limited compute budget (of several patching variants explored), and some improvements to the technique by handling saturated attention softmaxes better.

Marks et al (David Bau's group) introduces an integrated gradients based approach, which is slower, but seems to be superior.
Automated Circuit Discovery (Arthur Conmy et al) - The key idea is to take the kind of analysis done in IOI, finding the key sparse subgraph for a circuit, and automate it by doing it recursively and keeping the key nodes at each step. Importantly, it operates on edges, not nodes. Youtube walkthrough

I think the key insight is that this can be automated, and maybe to know that a specific method exists. I think the paper itself has lots of gory details that probably aren't worth engaging with in much detail?
Follow-up work (Aaquib Syed et al) showed that this can be done with attribution patching to be much faster
Distributed Alignment Search (Atticus Geiger et al). The key idea is that rather than activation patching a full component, which may not work if we aren't in a privileged basis, we can learn some subspace to patch instead! Honestly, I think this is the crucial idea: use gradient descent to find a subspace such that it has a causal effect when patched, and the rest of the paper isn't necessary. I also think that DAS can often be done with a 1D subspace, and the paper doesn't focus on getting the subspaces small. Atticus thinks about mech interp and causal graphs fairly differently from me, and engaging with how he frames things might be valuable for you! Youtube walkthrough (I've heard some people found the paper impenetrable, but found the walkthrough easy)
An Interpretability Illusion for Subspace Activation Patching (Aleksander Makelov & Georg Lange, during my MATS program) - A key accompaniment to thinking about distributed alignment search! It turns out that using gradient descent to find subspaces can be sketchy. Concretely, knowing that patching along a direction has a causal effect tells you that the direction both correlates with the feature in question and causally affects the output. if you have a direction that is dormant (matters causally but never changes) and a direction that is disconnected (causally irrelevant but correlates with the feature of interest), then their sum is both causal and correlated and so is valid to patch along. We show that you can find both of these directions everywhere in real models, and this illusion comes up in practice, especially when doing DAS on model layers rather than the residual stream. 

Narrow Circuits
A particularly important application of activation patching is finding narrow circuits - for some specific task, like answering multiple choice questions, which model components are crucial for performance on that task? Note that these components may do many other things too in other contexts, due to polysemanticity, but that is not relevant to this kind of analysis. At this point, there's a lot of narrow circuits work, but here's some of my favourites.

Note that this used to be a very popular area of mech interp work in 2023, but has fallen somewhat out of fashion. I am still excited to see work doing narrow circuits work in the SAE basis (eg Sparse Feature Circuits above), work using narrow circuits to understand or do something on real-world tasks, especially in larger models, and work automating circuit discovery, especially automating the process of finding the meaning of the circuit. But I think that there's not that much value in more manual Indirect Object Indentification style work in small models. 

Indirect Object Identification (Kevin Wang et al, Redwood) - The classic paper that used activation patching in detail to find a circuit used for a narrow task ("When John and Mary went to the store, John gave the bag to" -> " Mary"). I've gotten a ton of value out of thinking a lot about this circuit, replicating it, and thinking through the techniques. Just having a concrete example of a narrow circuit is high value. And I think there's several promising theories of change that factor through doing IOI style analysis on more important problems (eg deception, or 'why does Bing Chat gaslight users?')

I'm less confident it's worth a lot of effort close reading the actual paper lol, I think it was optimised for getting into ICLR, and there's a bunch of stuff on eg circuit completeness that isn't very interesting to me. 
Notable for being the first place I saw negative heads and backup heads studied, though IMO those are now best studied by reading copy suppression and the hydra effect respectively
The actual method used in the paper is mean ablation (mean taken over another, similar distribution) of circuit edges that they don't think are important. It's not clear to me that I'd use this myself (compared to eg patching between prompt pairs, possibly patching nodes rather than edges). I'd favour going through this tutorial to think about how to do IOI.
A Greater-Than Circuit (Michael Hanna et al, Redwood) - There's been a lot of IOI follow-up works, that look at circuits on other narrow tasks. This is one of my favourites, which includes some analysis of specific neurons. 
Task: "The war was fought from the year 1745 to 17" should be followed by 46 onwards not 44 or before
Does Circuit Analysis Interpretability Scale? (Tom Lieberum et al, DeepMind). Shows that IOI-style analysis scales by doing it on Chinchilla (70B) on the syntactic part of doing multiple choice questions - converting knowledge of the factual answer to the correct letter (not on finding the factual answer in the first place). Things basically scale! It's slow and painful and there's weird cursed shit (like heads that move the answer label to the end if and only if it is both correct and C), but nothing fundamentally breaks. I also think it's a fairly well presented and clean example of circuit analysis. 

I'm proud of the circuit analysis in my Fact Finding sequence (the focus of post 2), I think it's particularly clean and well-presented, and turned out fairly nicely.
Paper Back-and-Forths
One of my favourite phenomenons is when someone puts out an exciting paper, that gets a lot of attention yet has some subtle flaws, and follow-up work identifies and clarifies these. Interpretability is dark and full of terrors, and it is very easy to have a beautiful, elegant hypothesis that is completely/partially wrong, yet easily to believe by overinterpreting your evidence. Red-teaming your own work and being on guard for this is a crucial skill as a researcher, and reading examples of this in the literature is valuable training.

ROME + Follow-ups: A valuable thread in interpretability illusions. Rank-One Model Editing was a proposed factual editing technique that could insert facts into model MLP layers, by optimising for a rank one edit that would make the model say "The Eiffel Tower is in the city of" -> " Rome" (the optimisation target was that it would output Rome). The paper got a lot of attention, and the edit had fascinating effects, eg changing other answers about the Eiffel Tower to be consistent with the knowledge that it was in Rome. Can you see the flaw with this scheme?

My answer in rot13: Gur pber ceboyrz vf gung gurl qvq snpg vafregvba, abg snpg rqvgvat. Gb rqvg fbzrguvat lbh zhfg qryrgr gur byq guvat naq vafreg arj vasbezngvba, ohg EBZR unf yvggyr vapragvir gb rqvg jura vg pbhyq whfg vafreg n ybhq arj snpg gb qebja bhg gur byq bar. Guvf zrnaf gung gur ynlre vg jnf vafregrq va qvqa'g ernyyl znggre, orpnhfr vg whfg unq gb qrgrpg gur cerfrapr bs gur Rvssry Gbjre naq bhgchg Ebzr. Shegure gurer jrer yvxryl cngubybtvpny rssrpgf yvxr bhgchggvat 'ybbx ng zr' fb urnqf jbhyq nggraq zber fgebatyl gb gur Gbjre gbxra naq guhf bhgchg Ebzr zber, guvf yrq gb pregnva ohtf yvxr "V ybir gur Rvssry Gbjre! Onenpx Bonzn jnf obea va" -> " Ebzr". 
ROME (Kevin Meng & David Bau et al) - Given the above, I don't think it's worth engaging deeply with their method/tests, except as a pedagogical exercise. They also used causal tracing/activation patching to great effect, which legitimately was very cool, and significantly influenced the field.
Does Localization Inform Editing (Peter Hase et al) A follow-up showing that there was no correlation between the layers that were easiest to edit and the layers that mattered when patched. Unsurprising given the above! (I think the key info is given by the summary, and wouldn’t prioritise reading it deeply)
Detecting Edit Failures in Large Language Models (Jason Hoelscher-Obermaier & Julia Persson et al) A follow-up showing the "The Louvre is cool. Obama was born in" -> " Rome" example (unsurprising given the above!). (I think the key info is given by the summary, and wouldn’t prioritise reading it deeply)
The Interpretability Illusion for Subspace Patching paper discussed above shows (and almost proves) that there will exist such directions in any MLP layer before the fact retrieving heads
Othello + my follow-up: There was a very cool paper from Kenneth Li that trained a model to predict the next move in the board game Othello (like chess/Go) on synthetic games that randomly chose legal moves. They found that the model spontaneously learned a model of the board state internally, this could be probed for, and could be causally intervened on (with a very complex, galaxy brained method) and change model outputs in the desired way. Crucially, they found that non-linear probes (one hidden layer MLPs) could find the board state, but linear probes could not!

In my follow-up, I found that there was a linear world model hiding beneath! But that rather than saying whether a square was black or white, it said whether it had the current or opposing player's colour. Once you have this linear world model, you can causally intervene with simple vector arithmetic!
I think this is most exciting as real evidence for the linear representation hypothesis - the original paper seemed like a serious shot at falsifying it, and my follow-up showed that it survived falsification!
This paper brings me joy for (along with many other papers) conclusively falsifying the strong stochastic parrots narrative.
Emergent World Representations (Kenneth Li et al) Given the above, I don't actually think it's worth reading closely, except as a pedagogical exercise, but it's a cool paper!
Actually, Othello-GPT Has A Linear Emergent World Representation - the blog post form of my follow-up. Not obviously worth reading given the above, but I really like them, and think it has cool flow and research exposition. Post 3 has a blow by blow account of my research process and key decisions made at each point. 

Thanks to Andrew Lee, there's also a paper version - it's more rigorous, but less chatty and maybe less pegagogically good.
Bonus
I don't think the papers in here are essential reading, but are worth being aware of, and some are broadly worth reading if you have the time, especially if any specific ones catch your eye!

The induction heads paper (Catherine Olsson et al, Anthropic) - nothing important in mech interp has properly built on this IMO (the idea of induction heads is very important to know, but was introduced in A Mathematical Framework shortly beforehand), but there's a lot of cool stuff - a circuit was universal across scales, a circuit was crucial to in-context learning, phase transitions, bumps in the loss curve, etc.


Deeply engage with:
Key concepts + argument 1.
Argument 4: induction heads also do translation + few shot learning.
Getting a rough intuition for all the methods used in the Model Analysis Table, as a good overview of interesting interpretability techniques.
Skim or skip:
All the rigour - basically everything I didn’t mention. The paper goes way overboard on rigour and it’s not worth understanding every last detail
The main value to get when skimming is an overview of different techniques, esp general techniques for interpreting during training.
A particularly striking result is that induction heads form at ~the same time in all models - I think this is very cool, but somewhat overblown - from some preliminary experiments, I think it’s pretty sensitive to learning rate and positional encoding (though the fact that it doesn’t depend on scale is fascinating!)
Progress Measures for Grokking via Mechanistic Interpretability (Neel Nanda et al) - nothing important in mech interp has properly built on this IMO, but there's just a ton of gorgeous results in there. I think it's the one of the most truly rigorous reverse-engineering works out there, and the connections to phase transitions and explaining grokking were really fucking cool. Youtube walkthrough. See my blog post for more takes.

Also a good example of how actually understanding a model can be really useful, and push forwards science of deep learning by explaining confusing phenomena like grokking.
See this comment by Jason Gross suggesting some other highly rigorous works of mech interp on algorithmic models
Deeply engage with:
The key claims and takeaways sections
Overview of the modular addition algorithm
The key vibe here is “holy shit, that’s a weird/unexpected algorithm”, but also, on reflection, a pretty natural thing to learn if you’re built on linear algebra - this is a core mindset for interpreting networks!
Skim:
Reverse engineering modular addition - understanding the different types of evidence and how they fit together
Evolution of modular addition circuits during training - the flavour of what the circuits developing looks like during training, and the fact that once we understand things, we can just literally watch them develop!
The interactive graphics in the colab are way better than static images
The Phase Changes section - probably the most interesting bits are the explanation of grokking, and the two speculative hypotheses.
Logit Lens (nostalgebraist)

A solid early bit of work on LLM interpretability. The key insight is that we interpret the residual stream of the transformer by multiplying by the unembedding and mapping to logits, and that we can do this to the residual stream before the final layer and see the model converging on the right answer
Key takeaway: Model layers iteratively update the residual stream, and the residual stream is the central object of a transformer
Deeply Engage with:
The key insight of applying the unembedding early, and grokking why this is a reasonable thing to do.
Skim or skip:
Skim the figures about progress towards the answer through the model, focus on just getting a vibe for what this progress looks like.
Skip everything else.
The deeper insight of this technique (not really covered in the work) is that we can do this on any vector in the residual stream to interpret it in terms of the direct effect on the logits - including the output of an attn or MLP layer and even a head or neuron. And we can also do this on weights writing to the residual stream, eg the output weights of a neuron or SAE feature. This is called direct logit attribution, and is a really powerful technique.
Note that this tends only to work for things close to the final layer, and will totally miss any indirect effect on the outputs (eg via composing with future layers, or suppressing incorrect answers)
Steering vectors: A family of papers exploring the idea that language models can be controlled by adding vectors to their activations, and that these vectors can often be very cheaply found, eg taking the mean difference between activations with and without some property. I consider this more model internals work than mech interp, but I think it's worth being aware of, 
Activation Addition (Alex Turner et al, done with his MATS scholars before joining Google DeepMind) - The most conceptually simple version of this. You can eg take the difference in residual streams for "I love you" and "I hate you", add this vector in on a benign prompt like "I went up to my friend" and the model gets incredibly angry.
Inference-Time Interventions (Kenneth Li et al, Harvard) - Parallel work to Activation Addition, which found a truthful vector that improved TruthfulQA performance and reduced hallucinations

Representation Engineering (Andy Zou et al, a CAIS project) Following the above, this was a nice distillation of the ideas, that applied it to a range of more realistic settings

Refusal is Mediated by A Single Direction (Andy Arditi et al, during my MATS program). By applying existing techniques, we show in detail that chat-tuned models decide whether to refuse a harmful request (eg "How do I make a bomb?") via a single refusal vector - adding this to harmless prompts means they're refused, and ablating this on harmful prompts means they aren't refused. You can ablate this from the weights of the model to jailbreak it - it now rarely refuses things, has minimal damage done to its performance. This jailbreak is competitive with finetuning while being a fair bit easier, and I think is notably for being one of the more compelling real-world uses of model internals work so far.

The Hydra Effect (Tom McGrath et al, Google DeepMind) - makes the crucial point that self-repair aka backup in language models is a real thing (ie you delete a component and later components shift behaviour to compensate for its loss), but the paper is far longer than needed to get the key point and you can skim/skip most of it. Also observed in Interpretability in the Wild. Note that this happens even in models trained without dropout

Explorations of Self-Repair in Language Models (Cody Rushing et al, during my MATS program) A follow-up paper showing that self-repair happens across the full data distribution (not just narrow datasets), and exploring some of the mechanisms. Generally, self-repair is a mess, due to a range of mechanisms, only some of which we explore/catalogue. Notably, self-repair can be partially explained by the final LayerNorm's scale (if several components agree, and you delete one in a way that reduces the residual stream norm, the others get comparatively scaled up) - this explains a fraction of self-repair, up to 30% in extreme cases.
Copy Suppression (Callum McDougall, Arthur Conmy & Cody Rushing et al, during my MATS program). Really cool paper that IMO comes closest to really understanding a model component on the full pre-training distribution: we show that the main role of head L10H7 in GPT-2 Small is copy suppression: notice if earlier layers have decided to predict a token, check if that occurs earlier in the context, and if so attend to it and suppress it. Youtube walkthrough

The analysis is less complete than we hoped. We couldn't find any examples of it doing anything else, which I take as a big deal, but we only preserve 77% ish of the effect of the head when we ablate all other behaviours, and it's substantially worse if we restrict the query to just be the unembed of the relevant token. I don't know how to interpret this. 
Copy suppression heads have occured in the literature before as anti-induction heads (doing induction but suppressing the answer) and negative name movers in IOI - both were instances of this general algorithm!
Copy suppression explains part of self-repair - if you're suppressing an earlier prediction, and it goes away, then there's nothing to suppress.
Copy suppression is an important part of overall model calibration, and loss gets worse without it.
Linear Representations of Sentiment (Curt Tigges & Oskar Hollinsworth et al, during my MATS program) Really fun paper: we found that there's a "first principal component of sentiment" that seems to be shared across a wide range of sentiment-y tasks, matters across the pre-training distribution, matters causally on Stanford Sentiment Treebank, seems universal across models (we checked up to 7B), and can be found with a range of techniques which all broadly agree. To some degree, the paper is a model of doing a deep dive into interpreting a particular, interesting direction.

We also found the "summarization motif", where models store information about a sentence or phrase on punctuation like commas or full stops, and this matters causally for downstream effects.
Softmax Linear Units (Nelson Elhage et al, Anthropic) - We introduced a new activation function which we originally thought made MLP neurons monosemantic, but turned out to help a bit, but also to have a lot of superposition hidden under the hood. Not worth reading in detail, but worth it for grokking that illusions are everywhere in mech interp: we thought we'd solved superposition, but actually it was just smuggled in. Also for the section called qualitative results with a ton of cool examples of features

Language models can explain neurons in language models (Steven Bills et al, OpenAI) - Using GPT-4 to explain GPT-2 neurons. I wouldn’t prioritise reading in detail, but the core idea (and that GPT-4 is kinda good enough!) is very cool, and it's an important tool to be aware of. The work was a bit ahead of its time, as it came out a few months before SAEs became a big thing. Neurons are often not very interpretable, so the technique didn't work great, but SAE features are more interpretable so the technique is far more useful there.

An Interpretability Illusion for BERT (Tolga Bolukbasi et al, Google)
Good early paper on the limitations of max activating dataset examples - they took a seemingly interpretable residual stream channel (or neuron, note that it’s not an MLP hidden layer neuron) in BERT and took the max activating dataset examples on different (small) datasets, and observed consistent patterns within a dataset, but very different examples between datasets
Within the lens of the Toy Model paper, this makes sense! Features correspond to directions in the residual stream that probably aren’t neuron aligned. Max activating dataset examples will pick up on the features most aligned with that neuron. Different datasets have different feature distributions and will give different “most aligned feature”
Further, models want to minimise interference and thus will superpose anti-correlated features, so they should
Deeply engage with:
The concrete result that the same neuron can have very different max activating dataset examples
The meta-level result that a naively compelling interpretability technique can be super misleading on closer inspection
Skim or skip:
Everything else - I don’t think there’s too much value to the details beyond the headline result, which is presented well in the intro.
Multimodal Neurons in Artificial Neural Networks (Gabriel Goh et al, OpenAI)
An analysis of neurons in a text + image model (CLIP), finding a bunch of abstract + cool neurons. Not a high priority to deeply engage with, but very cool and worth skimming.

My key takeaways
There are so many fascinating neurons! Like, what?
There’s a teenage neuron, a Minecraft neuron, a Hitler neuron and an incarcerated neuron?!
The intuition that multi-modal models (or at least, models that use language) are incentivised to represent things in a conceptual way, rather than specifically tied to the input format
The detailed analysis of the Donald Trump neuron, esp that it is more than just a “activates on Donald Trump” neuron, and instead activates for many different clusters of things, roughly tracking their association with Donald Trump.
This seems like weak evidence that neuron activations may split more into interpretable segments, rather than an interpretable directions
The “adversarial attacks by writing Ipod on an apple” part isn’t very deep, but is hilarious

Thanks to Trenton Bricken and Michael Nielson for nudging me to write an updated version!

Open Problems in Mechanistic Interpretability

Lee Sharkey∗

Bilal Chughtai∗

Joshua Batson

Jack Lindsey

Jeff Wu

Lucius Bushnaq

Nicholas Goldowsky-Dill

Stefan Heimersheim

Alejandro Ortega

Joseph Bloom

Stella Biderman

Adria Garriga-Alonso

Arthur Conmy

Neel Nanda

Jessica Rumbelow

Martin Wattenberg

Nandi Schoots

Joseph Miller

Eric J. Michaud

Stephen Casper

Max Tegmark

William Saunders

David Bau

Eric Todd

Atticus Geiger

Mor Geva

Jesse Hoogland

Daniel Murfet

Tom McGrath

Apollo Research

Apollo Research

Anthropic

Anthropic

Anthropic†

Apollo Research

Apollo Research

Apollo Research

Apollo Research

Decode Research

Eleuther AI

FAR AI

Google DeepMind

Google DeepMind

Leap Laboratories

Harvard University

King’s College London and Imperial College London

MATS

MIT

MIT

MIT

METR

Northeastern University

Northeastern University

Pr(AI)2r group

Tel Aviv University

Timaeus

University of Melbourne

Goodfire

∗Correspondence to: lee@apolloresearch.ai and brchughtaii@gmail.com
†Work done prior to joining Anthropic.

1

Abstract

Mechanistic interpretability aims to understand the computational mechanisms underlying
neural networks’ capabilities in order to accomplish concrete scientific and engineering goals.
Progress in this field thus promises to provide greater assurance over AI system behavior
and shed light on exciting scientific questions about the nature of intelligence. Despite
recent progress toward these goals, there are many open problems in the field that require
solutions before many scientific and practical benefits can be realized: Our methods require
both conceptual and practical improvements to reveal deeper insights; we must figure out
how best to apply our methods in pursuit of specific goals; and the field must grapple with
socio-technical challenges that influence and are influenced by our work. This forward-facing
review discusses the current frontier of mechanistic interpretability and the open problems
that the field may benefit from prioritizing.

This review collects the perspectives of its various authors and represents a synthesis of their views by Apollo
Research on behalf of Schmidt Sciences. The perspectives presented here do not necessarily reflect the views
of any individual author or the institutions with which they are affiliated.

2

Contents

1 Introduction

1.1 The focus of this review: Open problems and the future of mechanistic interpretability . . . .
1.1.1 Why ‘mechanistic’ interpretability? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Types of open problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Open problems in mechanistic interpretability methods and foundations

and perform different tasks

Networks do not naturally decompose into architectural components.

2.1.3 Reverse engineering step 2: Describing the functional role of components

2.1 Reverse engineering: Identifying the roles of network components . . . . . . . . . . . . . . . .
2.1.1 Reverse engineering is necessary because AI and humans use different representations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Reverse engineering step 1: Neural network decomposition . . . . . . . . . . . . . . . .
. . . .
2.1.2a
. . . . . . . . . . . . .
2.1.2b Decomposition by dimensionality reduction methods.
Decomposition by sparse dictionary learning (SDL).
2.1.2c
. . . . . . . . . . . . . .
Current decomposition methods lack solid theoretical foundations. . . . . . .
2.1.2d
Intrinsic interpretability: Building more easily decomposable models . . . . .
2.1.2e
. . . . . . .
Explanations for what causes components to activate . . . . . . . . . . . . .
Explanations for the downstream effects of components . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
2.1.4a
2.1.4b Validating interpretability methods using benchmarks. . . . . . . . . . . . . .
2.2 Concept-based interpretability: Identifying components for given roles . . . . . . . . . . . . .
2.2.1 Concept-based probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Probes need carefully chosen data for well-defined concepts . . . . . . . . . . . . . . .
2.2.3 Probes detect correlations, rather than causal variables, in hidden activations . . . . .
2.2.4 Concept-based intrinsic interpretability . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Proceduralizing mechanistic interpretability into circuit discovery pipelines: A case study . .
2.4 Automating steps in mechanistic interpretability research . . . . . . . . . . . . . . . . . . . .

2.1.4 Reverse engineering step 3: Validation of descriptions

‘Model organisms’ facilitate hypothesis validation.

2.1.3a
2.1.3b

3 Open problems in applications of mechanistic interpretability

3.2.2 Using mechanistic interpretability for better control of AI system behavior

3.1 Axes of mechanistic interpretability progress.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Using mechanistic interpretability for better monitoring and auditing of AI systems for po-
tentially unsafe cognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Mechanistic interpretability-based evaluations could help us detect unsafe or unethical
AI cognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Enabling real time monitoring of AI systems for potentially unsafe cognition
3.2.1a
. . .
Improving our ability to red-team AI systems and elicit unsafe outputs
3.2.1b
. . . . . .
3.3 Using mechanistic interpretability for better predictions about AI systems . . . . . . . . . . .
3.3.1 Predicting behavior in novel situations . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.2 Predicting capabilities that arise during training or finetuning . . . . . . . . . . . . . .
3.4 Using mechanistic interpretability to improve our ability to perform inference, improve train-
ing, and make better use of learned mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 Using mechanistic interpretability for ‘microscope AI’
. . . . . . . . . . . . . . . . . . . . . .
3.6 Mechanistic interpretability on a broader range of models and model families . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
3.7 Human computer interaction with model internals

4 Open socio-technical problems in mechanistic interpretability

4.1 Translating technical progress in mechanistic interpretability into levers for AI policy and
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Social and philosophical problems in mechanistic interpretability . . . . . . . . . . . . . . . .

governance

5 Conclusion

3

4
4
4
5

7
7

7
8
8
9
9
13
13
14
14
16
17
18
19
19
19
20
20
21
21
22

24
24

24

24
26
26
27
28
28
29

30
30
31
31

33

33
34

36

1 Introduction

Recent progress in artificial intelligence (AI) has resulted in rapidly improved AI capabilities. These capa-
bilities are not designed by humans. Instead, they are learned by deep neural networks (Hinton et al., 2006;
LeCun et al., 2015). Developers only need to design the training process; they do not need to – and in almost
all cases, do not – understand the neural mechanisms underlying the capabilities learned by an AI system.

Although human understanding of these mechanisms is not necessary for AI capabilities, understanding them
would enhance several human abilities. For example, it would permit better human control over AI behavior
and better monitoring during deployment. It would also facilitate trust in AI systems, allowing us to fully
realize their potential benefits by enabling their deployment in safety-critical and ethically-sensitive settings.

Beyond the engineering benefits, understanding AI systems offers immense scientific opportunities. For the
first time in history, we can create and study artificial minds with a level of access and control that is simply
not possible in biological systems. What new laws of nature governing the mechanisms of minds might we
discover from studying the internal workings of AI systems?

The scientific opportunities are not limited to the field of AI. If an AI can outperform tools designed by
humans in a given scientific field, it suggests that the AI system is representing something about the world
currently unknown to us. We can develop a deeper comprehension of the world by understanding those
representations. What can we learn about protein folding from AIs that can successfully predict protein
structure? What insights can we glean about disease from a radiographer that performs beyond human
ability?

Mechanistic interpretability might unlock these benefits. This field of study aims to understand neural
networks’ decision-making processes. Here, we define “Understanding a neural network’s decision-making
process” as the ability to use knowledge about the mechanisms underlying a network’s decision-making
process in order to successfully predict its behavior (even on arbitrary inputs) or to accomplish other practical
goals with respect to the network. Such goals might include more precise control of the network’s behavior,
or improved network design. Interpretability promises greater assurances for AI systems through a better
understanding of what neural networks have learned, thus enabling us to realize their potential benefits.

1.1 The focus of this review: Open problems and the future of mechanistic interpretability

Several recent reviews of mechanistic interpretability research and related topics exist (Rauker et al., 2023;
Geiger et al., 2022; Bereska & Gavves, 2024; Ferrando et al., 2024; Rai et al., 2024; Anwar et al., 2024;
Davies & Khakzar, 2024; Mosbach et al., 2024; Mueller et al., 2024). Our review takes a more forward-
looking stance. We discuss not only where the frontier is today, but also which directions we might benefit
most from prioritizing in the future.

1.1.1 Why ‘mechanistic’ interpretability?

The distinction between interpretability and mechanistic interpretability is not always clear and is therefore
worth clarifying. The motivations and methods used in interpretability work are often diverse (Lipton,
2018; Doshi-Velez & Kim, 2017; Jacovi, 2023). As a result, there are many ways in which interpretability
research might be categorized. Prior categorizations of interpretability include causal vs.
correlational
methods, supervised vs. unsupervised methods, bottom-up vs. top-down methods, among others (Geiger
et al., 2024b; Mueller et al., 2024; Bereska & Gavves, 2024; Belinkov, 2022a; Zou et al., 2023a; Davies &
Khakzar, 2024). This review focuses specifically on mechanistic interpretability. But what distinguishes
‘mechanistic interpretability’ from interpretability in general? It has been noted that the term is used
In this review, we use the
in a number of (sometimes inconsistent) ways (Saphra & Wiegreffe, 2024).
term ‘mechanistic interpretability’ in a technical sense, referring specifically to work that investigates the
mechanisms underlying neural network generalization. Mechanistic interpretability represents one of three
threads of interpretability research, each with distinct but sometimes overlapping motivations, which roughly
reflects the changing aims of interpretability work over time.

4

The first thread aims to build AI systems that are interpretable by design. Much early interpretability
work focused on explaining the sensitivity of machine learning models to inputs and training data. This
work typically used small-to-medium sized models designed to be easily interpretable, such as decision trees
(Breiman, 1984; Hu et al., 2019), linear models (Roweis & Ghahramani, 1999), and generalized additive
models (Hastie & Tibshirani, 1986; Agarwal et al., 2021). These models could be used alongside attribution
methods such as influence functions (Hampel, 1974; Koh & Liang, 2017) and Shapley values (Shapley, 1997;
Lundberg & Lee, 2017), which were common techniques used to characterize model decision boundaries with
respect to inputs. “Interpretability by design” continues to be an active research area, including architectures
such as Concept-Bottleneck Models (Koh et al., 2020), Backpack Language Models (Hewitt et al., 2023),
Kolmogorov-Arnold Networks (Liu et al., 2024c), and sparse decision trees (Xin et al., 2022).

With the rise of larger-scale nonlinear neural networks (Krizhevsky et al., 2012; He et al., 2016), another
thread grew in importance, driven primarily by the question: Why did my model make this particular
decision? However, one challenge in interpreting larger networks was finding attribution methods that could
scale to large networks (Zeiler & Fergus, 2014). In response, a number of local attribution methods were
developed, including grad-CAM (Selvaraju et al., 2019), integrated gradients (Sundararajan et al., 2017), and
masking-based causal attribution (Fong & Vedaldi, 2017), SHAP (Lundberg & Lee, 2017), LIME (Ribeiro
et al., 2016), and many other methods, including backprop-based visualization methods (Simonyan et al.,
2014a; Nguyen et al., 2016b).

Inspired by, for example, Inception (Szegedy et al., 2015) and GPT-3 (Brown et al., 2020), another thread
emerged as models became capable of more profound generalization. Focused on the broader subject of
generalization, it was driven by the question: How did my model solve this general class of problems? Due
to its emphasis on the mechanisms underlying neural network generalization, the work in this category is
commonly referred to as ‘mechanistic interpretability’ (in addition to other, more cultural reasons, according
to (Saphra & Wiegreffe, 2024)). This kind of interpretability work is driven by a fundamental hypothesis
in deep learning that generalization arises from shared computation (LeCun et al., 2015). Early work in
this area, such as feature visualization (Olah et al., 2017b), or network dissection (Bau et al., 2020), sought
“global” explanations for model generalization by investigating the roles of model components across a class
of decisions. More recent work in this area looks at “circuits” of components (Wang et al., 2023), generalizable
patterns of information flow (Geva et al., 2023), representation subspaces (Geiger et al., 2024c; Zou et al.,
2023a) and probes (or self-supervised searches via sparse dictionary learning) for representation vectors that
carry information that generalizes across many instances for a particular task or set of tasks (Huben et al.,
2024; Bricken, 2023; Todd et al., 2024; Hollinsworth et al., 2024).

1.2 Types of open problems

The field of mechanistic interpretability ultimately aims to achieve concrete scientific and engineering goals.
For instance, we would like to be able to:

• Monitor AI systems for signs of cognition related to dangerous behavior (Section 3.2);
• Modify internal mechanisms and edit model parameters to adapt their behavior to better suit our

needs (Section 3.2.2);

• Predict how models will act in unseen situations or predict when a model might learn specific abilities

(Section 3.3);

• Improve model inference, training, and mechanisms to better suit our preferences (Section 3.4);
• Extract latent knowledge from AI systems so we can better model the world (Section 3.5).

Despite recent hopeful signs of progress, mechanistic interpretability still has considerable distance to cover
before achieving satisfactory progress toward most of its scientific and engineering goals.

To achieve these goals, the field not only needs greater application of current state-of-the-art mechanistic
interpretability methods, but also requires the development of improved techniques. The first major sec-
tion (Section 2) therefore discusses open problems related to the methods and foundations of mechanistic
interpretability.

5

We then explore key axes of research progress that will determine how far we can advance toward the goals
of mechanistic interpretability (Section 3.1). Section 3 outlines how applications of interpretability methods
have made progress toward the field’s goals, and, for each goal, discusses the specific axes along which
progress is likely needed to achieve specific objectives.

Finally, we note that the goals, applications, and methods of mechanistic interpretability do not exist in a
vacuum. Like any scientific field, they lie within a broader societal context. The final section of this review
examines open socio-technical problems in mechanistic interpretability (Section 4).
It discusses current
initiatives and possible pathways to translate technical progress into levers for AI governance, alongside
consequential social and philosophical challenges faced by the field.

6

2 Open problems in mechanistic interpretability methods and foundations

One way of thinking about what neural networks do internally is that they learn parameters that implement
neural algorithms. These neural algorithms take data as input and, through a series of steps, transform
their internal activations to produce an output. Different parts of the network learn different steps in the
algorithm; mechanistic interpretability aims to describe the function of different network components.

Broadly speaking, there are two methodological approaches to achieving this. The first approach, often
called ‘reverse engineering’, is to decompose the network into components and then attempt to identify the
function of those components (Section 2.1). This approach “identifies the roles of network components”.
Conversely, the second approach, sometimes referred to as ‘concept-based interpretability’, proposes a set of
concepts that might be used by the network and then looks for components that appear to correspond to
those concepts (Section 2.2). This approach thus “identifies network components for given roles”.

In this section, we will examine both approaches (‘Reverse engineering’ and ‘Concept-based interpretability’)
(Figure 1), discussing their methods and open problems. We will also touch on open problems that cut across
either approach, including proceduralizing the mechanistic interpretability pipeline (Section 2.3) and its uses
in automating interpretability research (Section 2.4).

2.1 Reverse engineering: Identifying the roles of network components

2.1.1 Reverse engineering is necessary because AI and humans use different representations and

perform different tasks

Large language models produce text that closely resembles human writing, and it is tempting to assume
that they generate it through cognitive processes similar to those of human writers1. However, humans and
AI models often solve problems in different ways. For example, a model that is only 1% the size of GPT-3
outperforms humans on next-token prediction tasks (Shlegeris et al., 2024).
Inversely, even state-of-the-
art multimodal LLMs struggle with tasks that a four-year-old could easily master, such as learning causal
properties of new objects involving simple lights and shapes (Kosoy et al., 2023). An even clearer sign that
humans and AI are using different representational processes are cases where humans cannot solve a problem
at all, as in the case of predicting a protein structure from sequence (Jumper et al., 2021; Lin et al., 2023).

Even when both humans and AI exhibit comparable levels of competence on a given task, they may use
different heuristics. For example, research shows that image models tend to rely more heavily on textural
features (such as recognizing elephants by their hide rather than their shape (Geirhos et al., 2019)) or rely on
dataset correlations (as when identifying fish by the fingers that proud fisherman use to hold them (Brendel
& Bethge, 2019)) to a greater degree than people do. Even simple algorithmic tasks, like modular addition,
which humans might solve with simple carries, were solved by a small transformer model by learning a
Fourier transform strategy that researchers only understood in retrospect (Nanda et al., 2023a).

To grasp the potentially alien cognition of these models, we must develop methods to uncover and understand
the previously unknown concepts and mechanisms implemented within them. In other words, we must be
able to reverse engineer these models (Olah, 2024).

Reverse engineering generally involves three steps, whether it is an engine, a piece of software, or a neural
network (Figure 2):

1. Decomposition: Breaking down the object of study into simpler parts (Section 2.1.2);

2. Description of components: Formulating hypotheses about the functional role of component

parts and how they interact (a process that can be called ‘interpretation’) (Section 2.1.3);

3. Validation of descriptions: Testing if our hypotheses are correct (Section 2.1.4).

1However, even if this assumption were true, understanding LLMs would remain challenging, as we also lack a mechanistic

understanding of the cognitive processes involved in human text creation!

7

Figure 1: Two approaches to neural network interpretability. (Left) Reverse Engineering is characterized by
decomposing networks into functional components and describing how those components interact to produce
the network’s behavior. It thus aims to ‘identify the roles of network components’ (Section 2.1). (Right)
Concept-based interpretability on the other hand attempts to discover human concepts within neural network
internals. It thus aims to ‘identify the network components for given roles’ (Section 2.2).

If our hypotheses are invalidated, we must either improve our decomposition or improve our hypotheses
regarding the functional roles of its components. We will systematically examine each step, analyzing current
methods and their respective shortcomings.

2.1.2 Reverse engineering step 1: Neural network decomposition

In mechanistic interpretability, our aim is to decompose a neural network and study its parts in isolation in
order to explain how neural networks generalize. This aim raises the question of how best to carve a neural
network “at its joints” for the purposes of interpretability.

2.1.2a Networks do not naturally decompose into architectural components.

The naive approach to decompose neural networks involves breaking them down into their architectural
components, such as individual neurons, attention heads, or layers.

Some early attempts to interpret deep artificial neural networks studied the responses or weight structure
of individual neurons or single convolutional filters (Erhan et al., 2009; Le et al., 2012; Krizhevsky et al.,
2012; Szegedy et al., 2014; Simonyan et al., 2014b; Zhou et al., 2015; Srivastava et al., 2014; Yosinski
et al., 2015; Mordvintsev, 2015; Olah et al., 2017a; Bau et al., 2020; Dalvi et al., 2019; Olah et al., 2020a;
Cammarata et al., 2020). These efforts paid homage to the ‘Neuron Doctrine’ in neuroscience, which posits
that individual neurons are the structural and functional unit of the nervous system (Cajal, 1924; Sherrington,
1906). However, researchers discovered that single neurons are ‘polysemantic’ – they seem to respond to
multiple kinds of features in both artificial (Wei et al., 2015; Nguyen et al., 2016c; Olah et al., 2017a) and
biological networks (Churchland & Shenoy, 2007; Rigotti et al., 2013; Mante et al., 2013; Raposo et al.,
2014). These observations support earlier theoretical work that suggested representations used by neural
networks do not necessarily align with the activation of individual neurons (Hinton, 1981).

Interpreting individual attention heads does not fare better than interpreting individual neurons, as attention
heads also exhibit polysemanticity (Janiak et al., 2023). More broadly, research suggests that studying the
attention patterns of models can often be misleading (Jain & Wallace, 2019; Pruthi et al., 2020).

Some work even suggests that representations in language models might span multiple layers (Yun et al.,
2021; Lindsey et al., 2024). This chimes with work that edits or intervenes on individual layers, which
indicates that this level is too coarse-grained to robustly carve the network at its joints (Meng et al., 2022b;
Wang et al., 2023).

If natural architectural components, such as individual neurons, attention heads, or layers, do not provide a
natural way to decompose neural network representations, then what does?

8

Concept Based InterpretabilityReverse Engineering
(1) Decomposing a network into simpler
Figure 2: The steps of reverse engineering neural networks.
components. This decomposition might not necessarily use architecturally-defined bases, such as individual
neurons or layers (Section 2.1.2). (2) Hypothesizing about the functional roles of some or all components
(Section 2.1.3). (3) Validating whether our hypotheses are correct, creating a cycle in which we iteratively
refine our decompositions and hypotheses to improve our understanding of the network (Section 2.1.4).

2.1.2b Decomposition by dimensionality reduction methods.

If individual neurons are not the right decomposition, perhaps groups or patterns of neurons are. Many
decomposition methods attempt to identify activation vectors that correspond to the basic unit of neural
network computation. One common approach is to provide models with a range of unlabeled inputs, col-
lect the resulting hidden activations, and then apply unsupervised dimensionality reduction techniques to
these hidden activations. The hope is that structure in the hidden activations corresponds to the structure
of neural computation. Commonly used dimensionality reduction methods include Principal Component
Analysis or Singular Value Decomposition (Hollinsworth et al., 2024; Marks & Tegmark, 2024; Huang et al.,
2024a; Bushnaq et al., 2024) and non-negative matrix factorization (Olah et al., 2018; Voss et al., 2021; Cam-
marata et al., 2020), though these techniques are no longer predominant methods used for mechanistically
decomposing language models (Friedman et al., 2024).

2.1.2c Decomposition by sparse dictionary learning (SDL).

According to the ‘superposition hypothesis’, neural networks are capable of representing more features than
they have dimensions, as long as each feature activates sparsely (Elhage et al., 2021) (Figure 3). This is a
key reason that dimensionality reduction methods are not considered state-of-the-art, because they cannot
identify more directions than there are activation dimensions. The superposition hypothesis, coupled with
the failure of dimensionality reduction methods to overcome it, motivated the search for methods that can
identify more directions than dimensions. Recent work has explored the use of sparse dictionary learning
(SDL) to this end (Elhage et al., 2021; Sharkey et al., 2022b; Huben et al., 2024; Bricken, 2023).

Currently, SDL is the most popular set of unsupervised decomposition methods in mechanistic interpretabil-
ity. SDL encapsulates a family of methods, including Sparse Autoencoders (SAEs) (Gao et al., 2024; Tem-
pleton et al., 2024; Rajamanoharan et al., 2024; Makelov et al., 2024; Kissane et al., 2024b; Braun et al.,
2024), Transcoders (Dunefsky et al., 2024), and Crosscoders (Lindsey et al., 2024).

In SDL, hidden activations are typically passed to a small neural network consisting of only two layers,
which correspond to an encoder and decoder respectively, with a wide hidden space. The encoder activations
represent how active each ‘latent’ 2 is, and the decoder matrix corresponds to a dictionary of latent directions.
We want to train the dictionary elements to align with ‘feature directions’ in the network’s hidden activations.
Since we assume that individual features are sparsely present in the activations, the encoder activations are
In the case of SAEs, the output is trained to either reconstruct the
trained to be sparsely activating.

2The term latent is often used instead of the word feature to refer to SDL dictionary elements, since the term ‘feature’ is

often used to refer to multiple different ideas (Smith, 2024b)

9

DecomposeDescribe
Validate
............def algorithm(input):
     blue = f(input)
     purple = g(blue)
     green = h(blue, purple)
     red = i(green)
     return red

NetworkDecompositionHypothesesValidationFigure 3: Three ideas underlying the sparse dictionary learning (SDL) paradigm in mechanistic inter-
pretability. (Left) The linear representation hypothesis states that the map from ‘concepts’ to neural acti-
vations is linear. (Middle) Superposition is the hypothesis that models represent many more concepts than
they have dimensions by representing them both sparsely and linearly in activation spaces. (Right) SDL
attempts to recover an overcomplete basis of concepts represented in superposition in activation space.

input or, in the case of transcoders (Dunefsky et al., 2024), to reconstruct the activations of the next layer.
Crosscoders (Lindsey et al., 2024) permit a wider class of inputs and outputs, potentially reconstructing the
activations of many layers simultaneously. Since the encoder is nonlinear, it is thought to be able to learn
to activate a latent only if a feature is ‘active’ in the hidden activations and remain ‘off’ if it is not.

Although SDL is considered a leading decomposition method for mechanistic interpretability, it has substan-
tial practical and conceptual limitations (Figure 4).

SDL reconstruction errors are too high: Large errors in SDL reconstruction raise the question whether
SDL methods can reconstruct the hidden representation sufficiently well such that the latents learned by SDL
are adequately faithful to the models being interpreted. To measure this, the model’s true hidden activations
can be replaced with sparse dictionary reconstructions, then subsequently evaluating the extent to which the
model’s performance decreases. In practice, the error results in significant performance reductions. When a
sparse dictionary with 16 million latents was inserted into GPT-4, the language modeling loss was equivalent
to a model with only 10% of GPT-4’s pretraining compute (Gao et al., 2024). Similarly, Makelov et al.
(2024) found that using reconstructions from sparse autoencoders decreased GPT-2 small performance by
10% when trained on task-specific data, and 40% when trained on the full distribution.

Making sparse dictionaries much larger and sparser to reduce errors is a feasible but computationally expen-
sive approach. Furthermore, in the limit, this results in merely assigning one dictionary latent per datapoint,
which is clearly less interpretable. One partial solution is using SDL methods with ‘error nodes’ (Marks et al.,
2024), to account for the discrepancies between the original and reconstructed activations. However, while
the sparse autoencoders identified interpretable latents, the error nodes contain ‘everything else’, making
them an inadequate solution to the problem. Engels et al. (2024b) found that these reconstruction errors
are not purely random, as much of the direction of the error and its norm can be linearly predicted from
the initial activation vector. This suggests that current SDL methods may systematically fail to capture
certain structured aspects of model representations, but also implies potential solutions. To overcome this
issue, it may be necessary to improve SDL training methods, or develop entirely new methods of network
decomposition.

SDL methods are expensive to apply to large models: SDL involves training a small neural network
for every layer of the AI model that we want to interpret. Typically, the sparse dictionaries have more
parameters at that layer than the original model does, and consequently will probably be relatively expensive
to train compared to the original model. 3 As AI models become larger, scaling costs of SDL also increase,
although it remains unclear whether relative scaling costs are sub- or supra-linear. For cost effectiveness,

3The actual relative cost is unclear since there are no public attempts to apply SDL to every vector space in a model,
although some work applies SDL to various layers (Marks et al., 2024; Gao et al., 2024; Braun et al., 2024; Lieberum et al.,
2024; Bloom & Lin, 2024a; Huben et al., 2024)

10

Superposition
Sparse Dictionary
Learning
Linear Representation
Hypothesis
Figure 4: Sparse dictionary learning has a number of practical and conceptual limitations that cause issues
when using it to reverse engineer neural networks (Paragraph 2.1.2c).

it may be important to develop intrinsically decomposable methods for training models, while remaining at
(or close to) state-of-the-art performance (Paragraph 2.1.2e). This approach will help avoid incurring the
expense of both training and decomposing AI models.

SDL assumes the linear representation hypothesis in nonlinear models: Other problems with SDL
arise from the assumptions on which SDL is based. One such assumption is the linear representation hypoth-
esis. The linear representation hypothesis observes that though neural networks are nonlinear functions and
could potentially use highly nonlinear representations (Black et al., 2022; Engels et al., 2024b; Kirch et al.,
2024), they tend to use representations that exhibit strikingly linear behavior (Smolensky, 1986; Mikolov
et al., 2013; Olah et al., 2020b; Elhage et al., 2021; Park et al., 2023a; Guerner et al., 2024; Gurnee &
Tegmark, 2024). The hypothesis formalizes this phenomenon by stating that high level concepts are lin-
early represented as directions (vectors) in neural network embeddings. Its two core claims are (i) that the
composition of multiple concepts can be represented as the addition of their corresponding feature vectors,
and (ii) that the intensity of a concept is represented by the scale of its corresponding feature vector (Olah
& Jermyn, 2024). Earlier work (Elhage et al., 2021) defined the linear representation hypothesis with the
additional assumption of one-dimensional features, but this definition has since been refuted (Yedidia, 2023;
Chughtai & Lau, 2024; Engels et al., 2024a) and clarified (Olah & Jermyn, 2024). Another recently proposed
criterion is that the intensity of concepts should be retrievable by a linear function of the embeddings, up
to a small error (Hänni et al., 2024; Olah & Jermyn, 2024).

11

SDL Reconstruction
errors are too high=+
??SDL describes
but not activationsmechanismsSDL cannot be straightforwardly
applied to all architectureshow do we apply SDL here?++??SDL leaves feature
geometry unexplainedJanFeb
Mar
...
activationinputoutputreconstructionerror
SDL assumes the Linear
Representation Hypothesis
in non linear modelsReLUSDL methods are expensive
to apply to large modelsSDL latents may not contain
the concepts we wantSparsity is not a good proxy
for interpretabilityFeature SplittingFeature Absorbtionmore featuresis sparser than++begins with “e”
except elephantbegins with “e”

“elephant”
“elephant”
catssiamese
catspersian catsmaine
coon
Practical and Conceptual
Limitations of
Sparse Dictionary Learning
(SDL)concept
we wantSAE∉A weak version of the linear representation hypothesis states that some concepts are linearly represented,
while a strong version may assert that all concepts are (Smith, 2024a). Some works have shown that the
strong version is false for some models (Black et al., 2022; Csordás et al., 2024). The weaker version of the
linear representation hypothesis is supported by the successes of linear probes (Section 2.2.1), activation
steering (Section 3.2.2), and the success of sparse autoencoders in finding seemingly interpretable latents
(Paragraph 2.1.2c).

Sparsity is not a good proxy for interpretability: Another of SDL’s key assumptions is that feature
activations are sparse. Therefore, SDL methods optimize their latents accordingly to be sparsely activating,
with the implicit assumption that sparser decompositions are more interpretable than denser ones. However,
this assumption may not necessarily hold. The (related) problems of feature splitting (Bricken, 2023), feature
absorption (Chanin et al., 2024) and composition (Till, 2024) suggest that with sufficient optimization
pressure, sparsity as a proxy for interpretability breaks down (though note that it is debated whether
feature splitting is actually a problem). Other proxies, such as minimum description length, might be better
optimization targets than sparsity per se (Ayonrinde et al., 2024). It also may be the case that no proxy
metric is sufficient.

SDL leaves feature geometry unexplained: SDL decomposes networks into single directions in the
activation space, which is a reasonable approach only if we consider feature activations akin to ‘a bag of
features’ (Harris, 1954) without any internal structure. However, the geometric arrangement of features in
relation to each other seems to reflect semantic and functional structure (Engels et al., 2024a; Gurnee &
Tegmark, 2024; Park et al., 2024b; Bussman et al., 2024). Understanding the geometric structure of a network
means understanding the position of one feature relative to (potentially) many others. Comprehending this
may be necessary if we want to know why and how a network treats certain features similarly or differently.
If understanding the global geometry (all-to-all relationships) of features is essential to understand neural
networks, this might pose a fundamental problem for current approaches to mechanistic interpretability
(Hänni et al., 2024; Mendel, 2024). However, if only local geometric relationships between features need to
be understood, understanding networks with a ‘bag of features’ approach may be more feasible.

SDL cannot straightforwardly be applied to all architectures: SDL was originally developed to iden-
tify features that may be represented in superposition across many neurons within a single layer. However,
representations may be spread over other network architecture components besides neurons. In transform-
ers, for example, representations may be spread across separate attention heads (Jermyn et al., 2023; Janiak
et al., 2023), and even across different layers (Yun et al., 2021; Lindsey et al., 2024). However, it is not
immediately obvious how to decompose representations distributed across attention heads with SDL (Math-
win et al., 2024; Wynroe & Sharkey, 2024). Nor is it straightforward to translate cross-layer distributed
representations into causal descriptions of neural network mechanisms (Lindsey et al., 2024).

SDL decomposes the input and output activations of network mechanisms, but not the mech-
anisms themselves: Ultimately, the aim of mechanistic interpretability is to understand the mechanisms
learned by neural networks. The parameters of the network, along with its nonlinearities and other architec-
tural components, implement these mechanisms, which are applied to the input’s hidden activations. SDL
identifies directions in activation space. Being activations, they only interact with the network’s mechanisms,
but are not the mechanisms themselves. Describing the mechanisms directly remains unresolved with SDL.
Gaining insights about the network’s mechanisms from SDL latents requires further post hoc analysis, which
can be labor intensive, computationally expensive, or data set dependent (Huben et al., 2024; Bricken, 2023;
Riggs et al., 2023; Marks et al., 2024). This is an instance of a more broad problem with current mechanistic
interpretability work; we primarily focus on understanding neural network activations, with little attention
paid to how this structure in activations is computed via weights (Chughtai & Bushnaq, 2025).

SDL latents may not contain the concepts needed for downstream use cases: When using SAEs
for practical tasks, there sometimes exists a single latent or small set of latents representing some concept of
interest for task performance (and not representing much else). For instance, Kantamneni et al. (2024) found
a single latent whose activation pattern was more accurate than official dataset labels on the NLP task GLUE
CoLA (Warstadt et al., 2019). More often than not though, a sparse set of latents that encode some useful
concept of interest do not exist. It is unclear what causes this problem. One hypothesis is that the concept

12

we want isn’t how the model ‘thinks’ about the concept, and the SAE is working as intended. Alternatively,
the SAE training distribution could be too narrow, resulting in the SAE not being incentivised to learn the
important latents. (Kissane et al., 2024c) found that SAEs trained on pretraining data generally do not have
good latents for the concept of ‘refusing’ harmful user requests, while SAEs trained on chat formatted data
do. Or, the SAE might not have a large enough dictionary size to learn all concepts of interest. Many more
hypotheses are plausible. A complicating factor in using SDL to identify the learned mechanisms of neural
networks is that the latents identified by depend on the data set used to train them. This is an undesirable
property for a decomposition method that was initially hoped to be capable of identifying the fundamental
units of computation in neural networks (Kissane et al., 2024c).

2.1.2d Current decomposition methods lack solid theoretical foundations.

Given the practical and conceptual issues with SDL, there is broad agreement that the question of how to
correctly decompose networks into atomic units remains a central problem, evidenced by the large amount
of effort focused on the direction in recent years. After investing considerable effort in SDL approaches, it is
apparent that improved conceptual clarity beyond the idea of superposition (Elhage et al., 2021) is needed
to advance neural network decomposition.

One of the most significant open questions is the absence of clarity around the nature of features, despite
being the central focus of SDL’s identification efforts. Satisfying formal definitions are elusive and con-
ceptual foundations are not yet established. However, even without foundations, progress in mechanistic
interpretability is possible — even confused concepts can be pragmatically useful (Henighan, 2024).

Without solid conceptual foundations, it remains unclear whether the superposition hypothesis, which under-
pins the SDL paradigm, is fundamentally valid or merely pragmatically useful (Henighan, 2024; Templeton
et al., 2024). If it is the latter, there may be better methods than SDL for carving neural networks at their
joints. Such methods may take into account feature geometry or take a more dynamic, developmental view
of how mechanistic structure emerges in the training process (Hoogland et al., 2024; Wang et al., 2024b).
Moreover, although there is much emphasis on how models represent features in superposition, there is
comparatively less work examining how they might perform computation on them natively in superposition.
Further conceptual work into this problem may suggest new methodologies for decomposing networks, or
provide bounds on the number of features we should expect models to be capable of learning (Hänni et al.,
2024; Adler & Shavit, 2024; Bushnaq & Mendel, 2024).

Mechanistic interpretability should ideally be built on more formal foundations. Some work attempts to
ground mechanistic interpretability in the formalisms of causality (Geiger et al., 2024a). However, this
approach has not yet yielded canonical causal mediators (Mueller et al., 2024) that may form a basis for
decomposition methods. How should we go about finding them? Given that our field’s objective is to
understand the learned structures that underlie networks’ generalization behaviors, exploring theories about
why neural networks generalize appears to be a promising avenue. But theories that attempt to characterize
why neural networks generalize, such as the spline theory of neural networks (Balestriero & richard baraniuk,
2018), theories of neural networks’ simplicity bias (Valle-Perez et al., 2019), deep learning theory involving the
neural tangent kernel (Jacot et al., 2018; Roberts et al., 2022), or singular learning theory (Watanabe, 2009;
Wei et al., 2023) have either not yet yielded mathematical objects that can be easily used for interpretability,
or simply have not been successfully linked to approaches for interpreting neural networks. Establishing
these connections would make significant progress toward carving neural networks at their joints to facilitate
mechanistic interpretability. And if we can carve trained networks at their joints, it may suggest ways to
train networks such that they come ‘pre-carved’. Thus, better theoretical foundations may also be important
for developing models that are intrinsically decomposable by design, which we discuss next.

2.1.2e

Intrinsic interpretability: Building more easily decomposable models

The current strategy of training a model solely for performance and then interpreting it post hoc may not
be optimal if our goal is a model that is both interpretable and performant. To this end, it may be beneficial
to prioritize interpretability during model training, for which there are several plausible approaches.

13

Instead of post-hoc decomposing trained network activations into discrete codes (Paragraph 2.1.2c), network
activations could be forced to use a discrete code from the outset, as in Tamkin et al. (2025). MLPs could
also be trained with sparser activation functions, such as TopK (Makhzani & Frey, 2013; Bills et al., 2023) or
SoLU (Elhage et al., 2022). Similar approaches could be potentially used to restrict attention superposition
(Jermyn et al., 2023) by limiting the number of heads attending to any query-key pair. Several approaches,
such as ‘mixture of experts’ (Shazeer et al., 2017; Fedus et al., 2022; He, 2024), use sparsely activating
components — with a large enough number of experts, and with sufficient activation sparsity, experts
may become individually interpretable (Warstadt et al., 2019). Many attempts to incentivize interpretable
activations directly so far have not been competitively performant, and have also allowed ‘superposition to
sneak through’, mitigating benefits.

We can also target weight sparsity directly during training, including approaches such as L0 regularization
(Louizos et al., 2018) or pruning (Mozer & Smolensky, 1988; Frankle & Carbin, 2019; Mocanu et al., 2018).
Han et al. (2015) achieve sparse weights by using magnitude pruning followed by finetuning. This highlights
a general strategy of finetuning with interpretability in mind (also used by Tamkin et al. (2025)), avoiding the
potentially excessive cost of training from scratch. Another related strategy is targeting modularity (Kirsch
et al., 2018; Andreas et al., 2016). For example, brain-inspired modular training (Liu et al., 2023) trains for
modularity by embedding neurons in a geometric space and encouraging geometrically local connections.

Existing research implements other approaches that can simplify linearity-reliant circuit analysis (Elhage
et al., 2021). For example, there is work removing the layer norm operations (Heimersheim, 2024), using
input-switched affine transformations for recurrence (Foerster et al., 2017). Other studies leverage archi-
tectures that are mathematically analyzable in other ways than linearity, such bilinear activations in MLPs
(Sharkey, 2023; Pearce et al., 2024).

After decomposing a network into components, whether through post hoc decomposition or by using intrinsi-
cally decomposable models, our task remains unfinished. We must provide an interpretation of the functional
role of each component (Step 2 – Section 2.1.3) and validate that interpretation (Step 3 – Section 2.1.4). In
the next section, we will discuss common methods of interpretation and their shortcomings.

2.1.3 Reverse engineering step 2: Describing the functional role of components

After decomposing networks into parts, the next step of reverse engineering is to “describe” the functional
role of these components. This step is best thought of as generating hypothesized ‘interpretations’ or ‘expla-
nations’. These explanations form candidate descriptions of the functional role of a given component, and
should not be taken to be definitive conclusions before they are thoroughly validated (see Section 2.1.4).

Descriptions of the functional role of neural network components can either indicate (1) the cause of a
component’s activation or (2) what occurs after that component has been activated, or – preferably – both
(Figure 5). In this section, we will discuss the existing set of tools available to mechanistic interpretability
researchers to describe the functional role of network components.

2.1.3a Explanations for what causes components to activate

Explanations of the causes of component activation can use three broad categories of methods, each with
several problems: (1) Highly activating data set examples; (2) Attribution methods; and (3) Feature syn-
thesis.

Highly activating data set examples. The simplest method is to use highly activating data set exam-
ples (sometimes called ‘exemplar representations’ (Hernandez et al., 2022)). These are inputs on which a
particular component is strongly activated. For a given component, analyzing apparent commonalities in
the inputs suggests hypotheses for what causes that component to activate. This step may be carried out
by humans or AI systems (Section 2.4).

Despite being widely used, this approach has several substantial issues. The first issue is that the method
relies on human prior beliefs, which may lead interpreters to project their human understanding onto models
that may, in fact, be using unfamiliar concepts. This bias could lead us to identify concepts in the model

14

Figure 5: To study the functional role of the blue component numerous approaches are possible. We
could study its causes: the purple input components or the red intermediate components via e.g.
feature
synthesis (Olah et al., 2017a; 2020b), maximum activating examples (Olah et al., 2017a; Bricken, 2023), or
attributions (Sundararajan et al., 2017). Or we could study its effects: the orange output components or
the green intermediate components via e.g. the logit lens (Nostalgebraist, 2020), activation steering (Turner
et al., 2024), or attributions (Marks et al., 2024).

that do not truly explain the mode’s functioning (Freiesleben & König, 2023; Donnelly & Roegiest, 2019;
Gale et al., 2020).

Another issue with this approach is the potential for ‘interpretability illusions’. Bolukbasi et al. (2021) show
that bias in data sets can create misleading explanations even when top activating examples are selected
from real data sets, as opposed to synthetically created data. Depending on the data set from which the
examples were drawn, human annotators identified dramatically different meanings for given directions in
the activation space of BERT (Devlin, 2018).

A third issue is that this approach often yields plausible explanations for arbitrarily chosen directions in the
activation space (Szegedy et al., 2014). This means that plausible explanations based on highly activating
data set examples cannot be solely relied upon to identify the basic units of computations in neural networks
— other methods are needed to accurately identify them. Furthermore, it is possible to develop adversarial
models that deliberately yield misleading feature visualizations (Geirhos et al., 2025).

Many of the issues with highly activating data set examples stem from the fact that they merely provide
correlational explanations for the activation of a network component, rather than causal explanations. To
identify causal explanations, other methods, such as attribution methods, are necessary.

Attribution methods are necessary for causal explanations but are often difficult to interpret.
Attribution methods (Simonyan et al., 2014a; Nguyen et al., 2016b; Selvaraju et al., 2019; Sundararajan
et al., 2017; Fong & Vedaldi, 2017; Lundberg & Lee, 2017; Ribeiro et al., 2016) are intended to measure
the causal importance of upstream variables (such as inputs) on downstream variables (such as a network
component). They are often gradient-based (Mozer & Smolensky, 1988; Simonyan et al., 2014a; Nguyen et al.,
2016b; Selvaraju et al., 2019; Sundararajan et al., 2017; Wang et al., 2024d) or sampling-, perturbation-,
or ablation-based (Fong & Vedaldi, 2017; Vig et al., 2020; Geiger et al., 2020; Ghorbani & Zou, 2020;
Meng et al., 2022b; Chan et al., 2022a; Nanda, 2023a). However, on a theoretical level, many gradient-
based methods identify only a first-order approximation of the ideal attribution, which is sometimes a poor
approximation (Watson, 2022). Adebayo et al. (2018) revealed more practical implications, demonstrating
that some gradient-based methods identify attributions that are independent both of the model and of the
data generating process. Furthermore, an adversary can train a model or perturb an input to reveal any

15

input causesintermediate causescomponent of interest
intermediate effects
output effects
irrelevant components

What is the functional
role of      ?attribution map (Dombrowski et al., 2019; Ghorbani et al., 2019; Heo et al., 2019; Kindermans et al., 2019;
Slack et al., 2020; Zhang et al., 2020)4. Perturbation methods present certain issues, such as taking models
off their training distribution and eliciting unusual behavior, among other theoretical complications (Feng
et al., 2018; Molnar et al., 2021; Hooker et al., 2021; Molnar et al., 2024; Freiesleben & König, 2023; Slack
et al., 2021). Developing efficient and accurate attribution methods thus remains an open problem.

Feature synthesis. Feature synthesis is a strategy that combines highly activating data set examples
and gradient-based attribution methods (Erhan et al., 2009; Szegedy et al., 2014; Olah et al., 2017a). This
approach attempts to synthesize inputs that maximize the activation of a component subject to some regular-
ization, such as consistency with a generative model (Nguyen et al., 2016a; 2017) or total variation distance
(Mahendran & Vedaldi, 2014). However, criticisms of feature synthesis methods suggest that natural dataset
examples may serve interpretation better (Zimmermann et al., 2021; Borowski et al., 2021) or show that
current methods struggle to identify trojans (Casper et al., 2023a).

2.1.3b Explanations for the downstream effects of components

Alternatively, the functional role of a component can be described through its downstream effects.

Studying the direct effect. The logit lens (Nostalgebraist, 2020) applies the models unembedding matrix
to an intermediate residual stream representations, converting it into a distribution over the model’s output
vocabulary. Direct logit attribution is a generalization of this technique, applying to any appropriately sized
vector space in the model, e.g. the output of MLP layers (Geva et al., 2021; 2022b;a; Dar et al., 2023),
attention blocks, gradients (Katz et al., 2024) of these, and SDL decoder weights (Bricken, 2023). Adding a
trainable affine or linear transformation before unembedding, as also referred to as the tuned lens, improves
decoding accuracy (Belrose et al., 2023a; Yom Din et al., 2024), at the cost of less faithfully representing
when the model has completed computation.

In the language of causality (Pearl, 2009), unembedding a residual stream vector measures the direct effect
of that vector on the output (McGrath et al., 2023). However, the logit lens cannot measure the indirect
effect, the effects resulting from the influence that the embedding has on the hidden activations of subsequent
layers. Other methods, such as causal interventions (see below), are necessary to measure the indirect effect.
In the future, it may be possible to extend logit-lens-like approaches to not only project the effects of network
components on directions in output vocabulary space, but also on intermediate downstream components.

Causal Interventions. Causal interventions typically substitute (“patch”) the value of some network
component, usually an activation vector, with a different value during a forward pass, observing the resulting
effect on the model. There exist a related set of techniques: ablation (for the special case of zero-ing or
otherwise attempting to delete activations entirely), activation patching, causal mediation analysis, causal
tracing, and interchange intervention (Vig et al., 2020; Geiger et al., 2020; Meng et al., 2022a; Chan et al.,
2022a; Nanda, 2023a). 5

More surgical patches are also sometimes also made on edges between network components. This approach,
known as “path” patching, allows us to isolate the effect of one particular component on only one other
component, instead of on the entire rest of the network (Goldowsky-Dill et al., 2023). Causal scrubbing
(Chan et al., 2022a) is a generalization of path patching that allows for testing hypotheses concerning any
given connection between a set of network components.

Causal intervention methods can also generate supervision signal to identify subspaces of interest. For
instance, distributed alignment search (Geiger et al., 2024c) learns a linear subspace that represents a
particular concept, using data with interventions on that concept as supervision (Geiger et al., 2024c; Guerner

4However, Freiesleben & König (2023) argue that adversarial examples do not truly undermine saliency maps as these are

highly dissimilar to real interpretability challenges.

5For instance, by patching activations from the corrupt prompt “the capital of Italy is” into the clean prompt “the capital of
France is”, we can observe the effect on the output (“Rome” vs.“Paris”). This tells us which component values are relevant for
the differing output between the two prompts, but not the information that remains consistent (e.g. the fact that the answer
is a city).

16

et al., 2024; Wu et al., 2023). Other work learns masks over network components to remove irrelevant
components (De Cao et al., 2020; Csordás et al., 2021; Davies et al., 2023).

A causal intervention typically requires a forward pass of the model. This may make performing one for
every network component in large models, long contexts, or when using finer-grained components such as
sparse autoencoder latents prohibitively expensive. Faster alternatives that work well in practice (Marks
et al., 2024; Templeton et al., 2024) include gradient-based approximations to activation patching, such as
attribution patching (Nanda, 2023a; Syed et al., 2024), AtP* (Kramár et al., 2024), and integrated gradients
(Sundararajan et al., 2017).

Observing the effects of components on sequential behavior. Another way to study the effects of
model components is to patch in activated components and observe their effect on model behavior. Steering
is one example of this (Rimsky et al., 2024; Turner et al., 2024), where components (activation vectors)
are activated in order to influence the network’s behavior, often in interpretable ways. To determine the
functional role of an activation vector, a related method is to have a language model itself decode the
activations (Chen et al., 2025; Watkins, 2023; Ghandeharioun et al., 2024; Huang et al., 2024b; Kharlapenko
et al., 2024). These approaches, known as “patchscopes”, patch activations from one forward pass of a model
into a different forward pass (perhaps in a different model). The context of the new forward pass is designed
to elicit relevant information from the activations of the original forward pass.

A related sequential behavior based technique is simply to read the chain-of-thought (Wei et al., 2024; Kojima
et al., 2024) produced by a language model’s output. While this could be considered an ‘interpretability
technique’ in the sense that it aims to explain model decisions, it does not use model internals in those
explanations, at least not directly. Recent research demonstrates that chains of thought are not entirely
faithful to the model’s underlying decision-making process (Agarwal et al., 2024; Atanasova et al., 2023;
Turpin et al., 2023; Lanham et al., 2023; Ye & Durrett, 2022). A promising future direction for interpretabil-
ity research may be to incorporate model internals into chain-of-thought training, which could incentivize
faithfulness. Another possibility is building monitors based on model internals to improve transparency in
chain-of-thought faithfulness.

2.1.4 Reverse engineering step 3: Validation of descriptions

Initial descriptions of network components’ functional roles should be treated as hypotheses that first require
validation to ensure that they are reasonable.

Conflating hypotheses with conclusions has regrettably been commonplace in mechanistic interpretability
research, making validation an important area for the field to improve (Madsen et al., 2024; Stander et al.,
2025). Unfortunately, it is often hard to distinguish faithful explanations of neural network components
from merely plausible ones. Numerous examples of model interpretations fail sanity checks (Adebayo et al.,
2018; Leavitt & Morcos, 2020; Miller et al., 2024); “interpretability illusions” in which seemingly convincing
interpretations of a model later turned out to be false (Bolukbasi et al., 2021; Makelov et al., 2023); or
instances where different approaches to explaining the same phenomenon yielded different interpretations
(e.g. Chan et al. (2022b) or Chughtai et al. (2023) vs. Stander et al. (2025) vs. Wu et al. (2024a)). Several
other instances were previously cited in this review (Section 2.1.3). Hypotheses in interpretability require
extensive validation beyond what appearances might imply.

Validating a hypothesis involves posing a simple question: Does the hypothesis make good predictions about
the neural network’s behavior? Testing the hypothesis often requires multiple approaches (Mueller et al.,
2024). To validate descriptions, many approaches simply apply a different description method than the
one used to generate the initial hypothesis (Section 2.1.3).
If one description method yields a different
description from another, it invalidates the hypothesis, which necessitates returning to an earlier step in the
reverse engineering cycle (Figure 2). However, methods for validating descriptions are not limited to other
component description methods. Hypothesis validation may take many forms, including the following:

Predicting activations and counterfactuals: By using natural language explanations of a given network
component’s function, it is possible to predict the component’s activation levels on different inputs. This
analysis can be carried out by humans or by AI systems (Section 2.4), as in (Hernandez et al., 2022; Bills et al.,

17

2023; Shaham et al., 2025; Juang et al., 2024). In essence, our interpretations should enable us to successfully
predict counterfactual scenarios in neural networks. For instance, if we ablate or activate particular network
components, we should be able to predict specific downstream effects on other components.

Predicting and explaining unusual failures or adversarial examples: Good explanations of neural
network behavior should help us identify and explain cases where that behavior fails to produce expected
outcomes. For instance, Hilton et al. (2020) validated their methods by explaining cases where a deep
reinforcement learning agent’s neural network failed to achieve maximum reward and also explained specific
hallucinations exhibited by the network. Another approach is to use use an interpretability approach to
handcraft an input for a network that functioned as an adversarial example (Carter et al., 2019; Casper
et al., 2023c; Mu & Andreas, 2020; Hernandez et al., 2022).

Handcrafting a network that reconstructs a network behavior: If our explanations for network
behavior are sufficient, we should be able to use them to build replacement parts for the original network.
Cammarata et al. (2020) validated their interpretation of a curve detector’s function in a convolutional neural
network by substituting its parts with simple handcrafted replacements.

Testing on ground truth: If the weights of a toy neural network were handcrafted by humans, it is possible
to obtain a ground truth explanation for how it works. This proves useful for testing explanations produced
by interpretability methods. For example, Conmy et al. (2023) validated a tool’s ability to attribute model
behaviors to internal components by running it on a simple model that implemented a known algorithm.
(See also Paragraph 2.1.4b).

Using the hypothesis to achieve particular engineering goals: Another way to test explanations
is to assess their utility in downstream applications (Doshi-Velez & Kim, 2017; Casper et al., 2023a). For
example, Templeton et al. (2024) discussed examples where manually editing a large language model based
on an interpretation led to predictable high-level changes in its behavior. Meanwhile, Marks et al. (2024)
showed how an interpretability tool could assist humans with debugging a classifier in a toy task. Farrell
et al. (2024) use unlearning (Section 3.2.2) to demonstrate that learned SDL latents don’t quite match human
concepts, and might not be optimal for particular downstream use cases, highlighting potential issues with
SDL.

Using the hypothesis to achieve specific engineering goals competitively: Achieving not only useful,
but competitive methods sets an even higher standard. For interpretability tools, the highest evaluation
criteria require fair comparisons against relevant baselines on real-world tasks instead of cherry-picking
them. However, the practice of conducting evaluations using non-cherry-picked tasks remains relatively
uncommon. Although attempts have been made to use techniques in the current interpretability toolkit in
such evaluations, they have not proven to be consistently useful (Adebayo et al., 2020; Denain & Steinhardt,
2023; Casper et al., 2022b; Hase et al., 2023; Durmus et al., 2024). Some of the most promising research
directions are to use interpretability methods to achieve things that would be hard or impossible to achieve
without them (Schut et al., 2023). Unless interpretability methods demonstrate that they are competitive
with alternative approaches to achieve engineering goals, then the act of demonstrating their usefulness may
lead to a bias toward developing methods that only perform well in best-case scenarios and on simple tasks,
rather than those that can handle worst-case scenarios and practical challenges.

Interpretability researchers have historically faced challenges in adequately validating their hypotheses due
to the high costs in terms of time and cognitive labor. In the following section, we explore two potential
solutions that could simplify the validation process: model organisms and interpretability benchmarks.

2.1.4a

‘Model organisms’ facilitate hypothesis validation.

Although interpretability is sometimes motivated by achieving engineering goals, it is often also approached
through the perspective of the natural sciences (Olah et al., 2020b). In certain natural sciences, such as
genomics and neuroscience, it is common for researchers to investigate a few extensively studied species known
as ‘model organisms’ or ’model systems’. By conducting in-depth studies on a select group of organisms, like
E. coli, fruit flies, mice, and macaque monkeys, researchers can leverage the insights and tools gained from
those organisms and apply them to other species. For example, imaging specific types of neural activity in

18

mice is more tractable due to existing hypotheses about which proteins should be fluorescently labeled in
order to identify specific types of neurons. The use of model organisms allows for cross-checking results with
previous work, enabling stronger validation of hypotheses.

Currently, interpretability researchers lack consensus on which networks should serve as model organisms.
Essentially, what should the Drosophila melanogaster of mechanistic interpretability be? In mechanistic
interpretability, an ideal model organism should be open source, easy and cheap to use, representative of
a broad range of systems and phenomena, have a replicable training process with open source training
data, and have multiple instances with different random seeds, among other criteria (Sharkey et al., 2022a).
Thus far, researchers have mostly used model organisms that possess only some of these criteria, such as a
transformer than can perform modular addition (Nanda et al., 2023a) or GPT-2 (Radford et al., 2018).

Model organisms not only support cross-validation of hypotheses, but also facilitate the progressive construc-
tion of experimental infrastructure by providing a reliable foundation for experiment design. This simplifies
the process of rigorous hypothesis testing, thus helping prevent oversimplification and ‘interpretability illu-
sions’.

Studying solely model organisms, instead of more directly pursuing engineering goals, risks merely making
true statements about neural network structure, rather generating insights that are of immediate practical
benefit. For mechanistic interpretability to make the fastest and most substantial progress toward engineering
goals, both scientific and engineering wins should be pursued in parallel.

Furthermore, certain choices made while studying model organisms risk steering the field in suboptimal
directions. For instance, interpretability research is often motivated by the engineering goal of understanding
state-of-the-art models thoroughly enough to make assurances of their safety (Bereska & Gavves, 2024;
Tegmark & Omohundro, 2023; Dalrymple et al., 2024). However, limiting its focus by studying small toy
models (e.g. Nanda et al. (2023a)) or how larger models accomplish select subtasks (Arditi et al., 2024),
risks incentivizing research and methods that fail to generalize to more safety-relevant real-world settings.

2.1.4b Validating interpretability methods using benchmarks.

Beyond validating individual hypotheses, we may wish to validate entire interpretability methods. Bench-
marking is a proven approach to making incremental improvements in other areas of machine learning, with
several approaches to benchmarking interpretability methods being developed in recent years.

One desideratum for interpretability benchmarks is to evaluate interpretations against ground truth expla-
nations (Freiesleben & König, 2023; Zhou et al., 2022). Benchmarks can be established using models with
known ground truth explanations. Such models can be created by compiling simple programs into weights
of models that exactly implement the known program (Lindner et al., 2023; Weiss et al., 2021; Thurnherr
& Scheurer, 2024; Gupta et al., 2024). Alternatively, predetermined explanations can be enforced at train-
ing time in conventional models using Interchange Intervention Training (Gupta et al., 2024; Geiger et al.,
2022). Other interpretability benchmarks that evaluate specific steps in the interpretability pipelines also
exist, such as model decomposition (Huang et al., 2024a; Makelov et al., 2024), generating descriptions of
network component functions (Schwettmann et al., 2023) , or testing natural language explanations (Huang
et al., 2023).

2.2 Concept-based interpretability: Identifying components for given roles

2.2.1 Concept-based probes

When attempting to localize a human-interpretable concept within the network, an intuitive approach is to
‘probe’ for it (Köhn, 2015; Gupta et al., 2015; Alain & Bengio, 2017; Ettinger et al., 2016). A concept-based
probe is a classifier trained to predict a concept from the hidden representation of another model (Hupkes
& Zuidema, 2018). Probing requires a labeling function that assigns classification labels to input data,
indicating the ‘value’ of the concept on that data (Note: A binary value indicating the presence or absence
of a concept is a special case of this approach). Once the labels are assigned, a probe, which is a simple

19

parameterized model, is trained to predict concept labels based on hidden activations. If the probe is a linear
model, then we have localized the concept as a vector in latent space.

Probes were first introduced in NLP (Köhn, 2015; Gupta et al., 2015) and have since been extensively
explored in the field (Conneau et al., 2018; Tenney et al., 2019; Rogers et al., 2020; Gurnee et al., 2023;
Peters et al., 2018; Burns et al., 2023; Marks & Tegmark, 2024). They have also been applied in vision
(Alain & Bengio, 2017; Kim et al., 2018b) and deep reinforcement learning (McGrath et al., 2022; Forde
et al., 2023). Probing also includes concept activation vectors (Kim et al., 2018a), information-theoretic
probing (Voita & Titov, 2020), and structural probing (Hewitt & Manning, 2019).

Although relatively simple to implement, probing has two main challenges (Ravichander et al., 2021; Belinkov,
2022b): (1) The need for carefully chosen data for well-defined concepts, and (2) probes detect correlations
instead of causal variables in hidden activations.

2.2.2 Probes need carefully chosen data for well-defined concepts

Concept-based probing requires a labeling function that assigns labels to input data. Obtaining a labeling
function is not always trivial and may require substantial human effort to define a data set for a single
concept. Moreover, it is only possible to identify concepts that we have defined precisely enough to create
high-quality data. This limitation implies that concept-based probing can only identify concepts that we
were already looking for, rather than reveal unexpected features in the network. Another approach, known
as Contrast-Consistent Search (CCS), probes not for single concepts but an axis in activation space that
corresponds to positive or negative propositions by enforcing probabilistic consistency conditions (Burns
et al., 2023). Despite being more unsupervised than standard concept-based probing, even CCS requires
the construction of data sets with clear positive and negative cases. A potential path forward for concept-
based decomposition is to develop methods that automatically develop data sets for probing and concept
localization (Shaham et al., 2025) (Section 2.4).

2.2.3 Probes detect correlations, rather than causal variables, in hidden activations

Probes are tools for a correlational analysis, measuring if hidden activations serve as signals for a given
concept. Indeed, from an information-theoretic point of view, an arbitrarily powerful probe measures the
mutual information between a hidden representation and a concept (Hewitt & Liang, 2019; Pimentel et al.,
2020).

However, training a probe to associate a concept with specific hidden activations does not necessarily imply
that those activations causally mediate how that concept is used by the network, or even if the network uses
the concept at all (Ravichander et al., 2021; Geiger et al., 2024b; Elazar et al., 2021; Belinkov, 2022b). Probes
can be successfully trained on hidden activations that lack any causal connection to the output, only localizing
correlated hidden activation vectors. For this reason, probing should be used only to generate hypotheses
about which network components might be causally linked to a concept. Confirming such hypotheses requires
further investigation using causal interventions or other probing methods.

To improve the causal relevance of probe vectors, one approach is to use counterfactual data, which involves
intervening on the concept of interest (for instance, observing the resulting output if the dog in an image
was changed to a cat) (Elazar et al., 2021; Mueller, 2024; Geiger et al., 2024b). Methods include distributed
alignment search (Geiger et al., 2024c; Wu et al., 2023; Huang et al., 2024a), causal probing (Guerner et al.,
2024), using attribution methods to measure the effect of concept vectors on network predictions (Kim et al.,
2018b), and various concept erasure methods (Ravfogel et al., 2020; 2022; Elazar et al., 2021; Belrose et al.,
2023b;b). While these methods can identify causal mediators of concepts in hidden representations, they
require more specialized data than probes do.

At times, it might be acceptable for probes to identify merely correlated hidden activations if the correla-
tions generalize to the test distribution. However, probing approaches face a considerable risk of not only
discovering correlated features, but also spurious correlations due to the high dimensionality of hidden ac-
tivations. Validating probes is therefore essential to avoid overfitting. This includes evaluating them on
out-of-distribution test data that varies along task-specific dimensions to ensure that general purpose fea-

20

tures have been found. One question that remains unanswered is how to use regularization to achieve good
probe generalization.

2.2.4 Concept-based intrinsic interpretability

Although probes begin with a trained network and search for specific concepts within it, it is also feasible
to leverage the concepts in the network training process itself, such as in the case of concept bottleneck
models (Koh et al., 2020). This is beneficial as it is more likely that the components to which concepts are
assigned are causally relevant. Instead of specific concepts, networks can also be trained to use particular
causal structures (Geiger et al., 2022). While it may not be possible to prespecify all relevant concepts
or structures, integrating this approach with methods for disentangling concepts could prove useful (Chen
et al., 2018). Cloud et al. (2024) incentivize modularity via applying data-dependent, weighted masks to
gradients during backpropagation.

2.3 Proceduralizing mechanistic interpretability into circuit discovery pipelines: A case study

How should we codify the process of mechanistic interpretability to yield the deepest possible insights? To
form a complete pipeline by combining various methods, several methodological choices regarding decompo-
sition, description, and validation, must be made. Circuit discovery has emerged as a prominent pipeline in
recent mechanistic interpretability research (Wang et al., 2023; Hanna et al., 2023; Heimersheim & Janiak,
2023). Its objective is to describe how a neural network performs a task of interest while making specific
choices for network decomposition, component description, and hypothesis validation. In this section, we
look at the typical choices in each step in greater depth and discuss how this popular pipeline could be
improved.

The ‘circuit discovery’ pipeline takes the following steps:

1. Task Definition. For a given model we want to study, we select a task that the model can perform,
and a dataset on which the network performs that task. This is a concept-based step, since the
definition of the task was based on how human researchers define a task distribution.

2. Decomposition. During the decomposition step, it is common to think of the neural network as a
directed acyclic graph (DAG), where activations are represented by nodes and the “abstract weights”
between them represented by edges. Most work thus far has selected architectural components
(Section 2.1.2), such as attention heads and MLP layers, to be the nodes. However, more recent
work has also used SDL latents for nodes (Marks et al., 2024)

3. An initial description step: Identify task-relevant vs. -irrelevant subgraphs. The circuit
discovery procedure then identifies task-relevant nodes and edges. Typically, causal interventions are
used, drawing samples from some “clean” and “counterfactual” data sets. Circuit discovery methods
are generally based on iterative activation patching (Wang et al., 2023; Chan et al., 2022a; Lieberum
et al., 2023) or integrated gradients (Marks et al., 2024).

4. An iterative description-validation loop. After obtaining a task-relevant subgraph, the next
step involves describing the function of each node or edge individually. This step is less formulaic
than previous steps. Researchers rely on their intuition, attempting to create testable hypotheses
for the function of a component or edge of the circuit, and then design custom experiments to
validate or invalidate their hypothesis. Only after several iterations of hypothesis testing through
experimentation are researchers finally satisfied with their explanation. In research papers, this loop
is rarely made explicit, as only the final description is presented. However, Chan et al. (2022b) detail
this process for understanding the induction task, and Nanda (2023b)provides another description
of such a loop (building on work by Li et al. (2023)).

5. Final Validation. Circuits are commonly evaluated based on three attributes (Wang et al., 2023):
faithfulness, which refers to how closely the circuit approximates the entire network’s behavior, min-
imality, which assesses if nodes in the subgraph are unnecessary, and completeness, which determines

21

whether any nodes not included in the subgraph are important for task behavior. Additional ad hoc
validation methodologies also exist. For example, Wang et al. (2023) generate adversarial examples
for their task based on their mechanistic understanding, while Shi et al. (2024) devise a suite of
formal statistical hypothesis tests for circuit efficacy.

This circuit discovery procedure has yielded valuable insight, but falls short using current methods. The
pipeline has several issues:

Task definition is concept-based. Defining circuits has thus far been with respect to tasks defined by
humans. Miller et al. (2024) demonstrate that the within-task variance of model performance across the
distribution of data points in a task is large, implying that the circuit provides a good approximation of the
average case performance on the dataset, but a poor one for any individual data point. This suggests that
the process of first selecting a task and then studying how the model performs it may not be an effective
approach to achieve “reverse engineering” -style interpretability. Thus, it might be worth learning the task
decomposition instead (Haani et al., 2024).

Network decomposition methods are flawed. Perhaps most importantly, prior circuit discovery work
has attempted to decompose models in either architectural bases (Wang et al., 2023; Conmy et al., 2023) or
sparse autoencoder latents (Marks et al., 2024; Huben et al., 2024), which are imperfect ways to decompose
neural networks for mechanistic interpretability (Section 2.1.2). Future work could locate circuits in improved
decompositions or simultaneously learn both network decompositions and circuits.

Circuit faithfulness is low. Simple early circuits were found to be unfaithful (Chan et al., 2022b; 2023).
Miller et al. (2024) show that existing measures of faithfulness depend on the causal intervention imple-
mentation used, and further demonstrate that such metrics are misleading when applied to several complex
end-to-end circuits. Makelov et al. (2023) argue that subspace activation patching via distributed alignment
search may lead to interpretability illusion mechanisms, although these findings are contested by Wu et al.
(2024c).

Scalable methods are only approximate. Identifying relevant components through individual interven-
tions is costly when there are many components. Attribution patching Syed et al. (2024) was designed to
identify potential relevant candidates for further testing through intervention, which becomes more impor-
tant as the number of components expands significantly through sparse dictionary learning (Marks et al.,
2024). However, attribution patching uses gradients, which only yield a first-order approximation of the
effect of ablating components (Wu et al., 2024c; Molchanov et al., 2017), leaving it unclear whether this
method and any improvements on it (Kramár et al., 2024) produce adequate approximations.

Circuit discovery algorithms struggle with backup and negative behavior. Additional challenges
for circuit analysis arise from the effects of “backup” and “negative” behavior (Wang et al., 2023; McGrath
et al., 2023; McDougall et al., 2024), which actively suppress task performance and are thus not captured
by maximizing task performance metrics. Despite this, they remain important factors to consider; Mueller
(2024) provides further discussion of these issues.

Streetlight Interpretability: The tasks studied so far have been deliberately selected to be simple to
define and study mechanistically (Wang et al., 2023). This gives a misleading impression of the level of
difficulty involved in implementing circuit discovery for any arbitrary task that a network implements.
Indeed, attempts to study arbitrary circuits have proceeded less successfully (Nanda et al., 2023b).

Solving issues with current mechanistic interpretability pipelines remains an open challenge that promises sig-
nificant benefit. Upon establishing reasonable procedures, automating the overall pipeline will become more
feasible. However, some individual steps in mechanistic interpretability can already be fruitfully automated,
as discussed in the next section (Section 2.4).

2.4 Automating steps in mechanistic interpretability research

Historically, mechanistic interpretability research has required considerable manual researcher effort, though
it typically studies models that are smaller than those at the frontier. To make interpretability useful for
downstream use cases, scalable approaches are crucial. In this section, we discuss automated interpretability

22

methods. We will explore previous cases where manual tasks in mechanistic interpretability have been
successfully automated and address open problems in further automation.

Automating feature description and validation. A task that is amenable to automation is ‘describing
the functional role of model components’ (Section 2.1.3). With the increasing sophistication of language
models, researchers have generated descriptions of the functional role of neurons in image models (Hernandez
et al., 2022), neurons in language models (Bills et al., 2023), and sparse autoencoder latents in language
models (Huben et al., 2024; Bricken, 2023; Juang et al., 2024) using highly activating data set examples.
These interpretations are validated by assessing how effectively a human or model can use them to predict the
activation of a feature in a given data set example, or predict where a feature is active within a single image or
text excerpt. The success of these predictions can be used as a quantitative measurement of ‘interpretability’.
This was previously used to measure progress toward a decomposition method that carves networks at the
joints of their generalization structure, assuming that such a decomposition would be maximally interpretable
(Section 2.1.2). While imperfect, these methods for interpretation hypothesis generation and validation might
be improved by automating the generation of inputs to test the interpretation hypotheses by ensuring that
generations activate the interpreted feature (Huang et al., 2023), or defining more rigorous statistical tests
(Bloom & Lin, 2024b). Automatic component labeling could expand in the future to include descriptions
of feature effects, relationships between features (Bussman et al., 2024), or how components interact during
runtime produce behavior.

Automating circuit discovery pipelines. An approach called Automated Circuit DisCovery’ (ACDC)
automates part of the pipeline discussed in Section 2.3 to identify computational subgraphs involved in
particular tasks (Conmy et al., 2023). Several works have since improved upon and accelerated this process
(Syed et al., 2024; Kramár et al., 2024; Marks et al., 2024). Note that ACDC-like approaches in general
assist in identifying relevant subgraphs for a pre-defined task, but do not automate important subsequent
steps, such as describing the functional role of subgraph components.

While significant progress has been made toward automating steps of mechanistic interpretability pipelines,
fully automating current pipelines would not yield satisfactory explanations of model behavior6. Further
methodological progress is required for fully automated neural network interpretability to be capable of
generating the quality of interpretations necessary to achieve our goals.

6For one attempt at this using leading decomposition and description methods, see Marks et al. (2024)

23

3 Open problems in applications of mechanistic interpretability

Ultimately, we need mechanistic interpretability methods that enable us to solve concrete scientific and
engineering problems (Figure 6). While predicting the impact of fundamental science in advance is difficult,
having concrete goals in mind during research is usually beneficial. We want mechanistic interpretability
methods to help us achieve various outcomes, such as monitoring and auditing AI systems more effectively
(Section 3.2), controlling of AI system behavior more precisely (Section 3.2.2), predicting AI system outcomes
more accurately (Section 3.3), enhancing AI system capabilities (Section 3.4), and extracting knowledge from
AI systems (Section 3.5). We should also anticipate that mechanistic interpretability will uncover “unknown
problems” present in systems, revealing that the true realm of challenges and possibilities is greater than
what we currently perceive it to be.

As highlighted in the previous section, progress in mechanistic interpretability methods is multifaceted. Each
axis of methodological advancement leads to varying degrees of progress toward different goals. Before we
discuss open questions in its applications, we identify distinct axes of methodological progress that lead to
different amounts of progress toward different goals.

3.1 Axes of mechanistic interpretability progress.

Decomposition vs. description of network components: Improvements in network decomposition
versus component description methods offer varying benefits for different goals. Decomposition methods
vary in their efficacy at carving networks at the joints of their generalization structure, while description
methods can yield descriptions that vary in depth. Deeper descriptions of a component are typically more
causal or mechanistic, whereas shallower descriptions may rely more on correlations and only connect to
inputs or outputs without referencing intermediate causes or effects (Figure 5. Deeper descriptions thus
attempt to explain more about how the component interacts with other components within the network’s
algorithm. Certain goals can be achieved with minimal or no progress in decomposition or description, while
others may demand substantial progress.

Extent of network decomposition or description: The extent of network decomposition or description
needed may vary depending on the goal. Certain goals only require an understanding of specific network
components (such as an individual features or a circuit), while others might require enumerating or under-
standing of larger circuits or the entire model (as in ‘enumerative safety’).

Extent of task distribution analyzed: The scope of task distribution analysis also depends on the
intended goal. For instance, monitoring a model for a single kind of behavior might only require decomposing
or understanding the model only over a narrow task distribution, while others, such as formal verification,
might demand understanding over the entire distribution of tasks.

Mechanistic understanding post vs. during training: Understanding the mechanisms of a fixed model
might suffice for some goals, but more ambitious goals might require an understanding of not only the models’
mechanisms, but also how they change during the learning process.

In this section, we’ll discuss how mechanistic interpretability has been used or could be leveraged to further
the field’s various goals. We’ll assess the progress made thus far, and identify the advancements along
different axes of methodological progress that will be most crucial to success.

3.2 Using mechanistic interpretability for better monitoring and auditing of AI systems for potentially

unsafe cognition

3.2.1 Mechanistic interpretability-based evaluations could help us detect unsafe or unethical AI

cognition

Currently, we rely on “black box” evaluations to understand a model’s capabilities, but studying input-
output behavior alone may not reveal all dangerous behaviors. Such behaviors include deceiving users (Park
et al., 2023b; Ward et al., 2023; Scheurer et al., 2024; Meinke et al., 2024); for instance, by intentionally
underperforming on evaluations (“sandbagging”, van der Weij et al. (2024)), leveraging situational awareness

24

Figure 6: A summary of problem areas for applications of mechanistic interpretability.

(Laine et al., 2024); or giving dishonest responses tailored to match the user’s beliefs (“sycophancy”; Sharma
et al. (2024b)).
Interpretability techniques could be used to uncover the mechanisms underlying these
potentially harmful behaviors and thus help to detect and characterize them. This becomes increasingly
important as the capabilities of models increases, especially when using training methods that incentivize
models to feign particular properties for the purpose of passing evaluation.

Using interpretability methods to identify internal signs of concern (also known as “white-box” evaluations
(Casper et al., 2024) or “understanding-based” evaluations (Hubinger, 2023)) is therefore an important
problem. White-box evaluation methods could serve as tools to detect potential biases that arise when
models learn to use spurious correlations (Gandelsman et al., 2024a; Casper et al., 2022a; Abid et al., 2022).
However, human judgment might be required to determine which features are ‘supposed’ to be relevant to
the task (Marks et al., 2024; Kim et al., 2018a; Goyal et al., 2022).

25

Monitoring and Auditing
ControlPredictionsMicroscope AI
More Models
and ModalitiesHuman-Computer
Interaction
Policy and GovernanceImproved Inference, Training,
and Mechanisms
Despite current shortcomings in decomposition and description, white-box evaluations are likely feasible
today. Even shallow, correlation-based descriptions could signal potentially concerning cognition. For ex-
ample, developing new methods that reliably distinguish between features that merely recognize deceptive
behavior vs. mechanisms that cause deceptive behavior may be challenging. However, a correlation-based
method that flags both can facilitate catching the latter. To be useful, it may not even be necessary to
decompose or describe the entire network; having descriptions for components that are used on concerning
subdistributions of model behavior might suffice. For instance, imperfect interpretability methods may help
evaluators develop hypotheses about how models will behave, thus guiding further inquiry. Meanwhile, re-
cent work proposes incorporating SDL (Paragraph 2.1.2c) into safety cases for advanced AI systems. By
monitoring internal representations, it could aid in detecting potential sabotage or deceptive behavior before
deployment (Grosse, 2024). While such approaches show promise, they have difficulty in validating whether
learned features capture all concerning patterns of reasoning reliably.

Evaluations for unsafe cognition may be a particularly important use case as it plays well to the comparative
advantages of mechanistic interpretability relative to the other areas of machine learning. The majority
of other areas of machine learning already focus on controlling or steering the behavior of AI systems to
alter input-output behavior. It is therefore unclear that this is to mechanistic interpretability’s comparative
advantage. On the other hand, interpretability is perhaps the only research area that attempts to understand
the mechanisms of model cognition. This implies that it might be particularly fruitful for interpretability
researchers to tackle problems that become easier to solve through improving such understanding: auditing
for unsafe cognition, debugging unexpected behavior, and monitoring systems in deployment.

3.2.1a Enabling real time monitoring of AI systems for potentially unsafe cognition

Beyond white-box evaluations, interpretability has further applications in monitoring. For instance, internals
could be used to passively monitor the system during deployment, much like content moderation systems
currently in use today. Alternatively, internals could be used to flag when a model takes an action for
abnormal reasons even in absence of satisfactory descriptions (Section 2.1.3), known as “mechanistic anomaly
detection” (Christiano, 2022; Johnston et al., 2024), which may be a sign of suspicious behavior. Mechanistic
anomaly detection primarily requires progress in decomposition methods (Section 2.1.2) as it is necessary to
be confident about what constitutes an individual mechanism within the network. Current SDL methods
identify active latents, but not active mechanisms, which are implemented by network parameters. It may
not be necessary to have deep descriptions of the function of individual mechanisms as long as anomalies
can be detected.

3.2.1b Improving our ability to red-team AI systems and elicit unsafe outputs

Beyond white-box evaluations, leveraging interpretability could improve our ability to conduct adversarial
attacks, red-team, or jailbreak AI systems (Casper et al., 2024). This process is beneficial as it exhibits
failure modes models may display in the wild, when facing adversarial pressure from wide deployment or
malicious actors, thereby enabling developers to effectively preempt and address them. Furthermore, it may
form a significant element of safety cases (Clymer et al., 2024; Balesni et al., 2024a; Grosse, 2024; Goemans
et al., 2024) for AI systems, by providing assurances of form: We tried hard to red-team the system, yet
failed to exhibit concerning behavior despite having more affordances than users may have. One reasonable
assumption is that developers may have white-box access to models, while users may not. Although many
existing red-teaming methods (Perez et al., 2022; Zou et al., 2023b) already require gradient access, we could
additionally leverage interpretability insights to accelerate red-teaming. For instance, Arditi et al. (2024)
discovered a universal “refusal direction” in chat-finetuned language models, which is causally important
for models engaging in the behavior of refusing harmful requests. Lin et al. (2024) used this to red-team
models by optimizing for inputs that minimize the projection of the residual stream onto this direction
during the forward pass. This approach may be more efficient than optimizing over the whole model, as
previous methods did (Zou et al., 2023b). Mechanistic interpretability techniques also promise to improve
our ability to attribute model outputs to their corresponding inputs paragraph 2.1.3a. This could enhance
human red-teamers’ ability to find key input features responsible for bad behavior in models, thus speeding

26

up iteration cycles. Though such approaches are possible today with only feature-based understanding, they
might improve with more crisp mechanism-based understanding.

3.2.2 Using mechanistic interpretability for better control of AI system behavior

Ensuring the safe deployment of AI first requires effective control over their behavior. Currently, the tech-
niques used for this purpose are mostly unrelated to interpretability (e.g. Christiano et al. (2017); Rafailov
inter alia), but sometimes inspired by it (Rimsky et al. (2024), Zou et al. (2024), Kirch
et al. (2023);
et al. (2024); inter alia). Mechanistic interpretability could assist in interpreting (Lee et al., 2025) and
improving (Conmy & Nanda, 2024) these control methods, or in developing new ones. In this section, we
outline interpretability-inspired control methods, and envision future possibilities with further progress in
interpretability methods.

One new control method derived from mechanistic interpretability insights is activation steering (a.k.a.
activation addition). A fixed activation vector, hypothesized to linearly represent a model concept, is added to
an intermediate activation of a model at inference time (Li et al., 2024a; Turner et al., 2024; Zou et al., 2023a;
Rimsky et al., 2024). Turner et al. (2024) introduced activation steering, directly inspired by the Linear
Representation Hypothesis (discussed in Paragraph 2.1.2c). This technique results from the hypothesis,
and its success can be thought of as evidence for the hypothesis. Moderate success can be achieved in
steering using basic decomposition and description methods. Mechanistic interpretability decomposition
methods enable the steering of models toward a narrower range of behaviors with fewer side effects (Chalnev
et al., 2024). Advancements in mechanistic interpretability methods are likely to result in improved steering
capabilities, such as activating entire mechanisms instead of individual features.

Machine unlearning was originally defined as the problem of scrubbing the influence of particular data
points on a trained machine learning model (Cao & Yang, 2015). In the context of modern generative models,
machine unlearning is more broadly defined as removing particular undesirable knowledge or capabilities
(’unlearning targets’) from models, while preserving model performance on tasks involving non-targets (Liu
et al., 2024a). Targets for unlearning that are of particular interest include sensitive private or copyrighted
data, model biases (Liu et al., 2024b), and hazardous knowledge that could be misused by malicious actors
(Li et al., 2024b); for instance, information regarding the creation of bioweapons. A better understanding
of how knowledge or capabilities are implemented within model internals can help in the development of
new techniques for machine unlearning (Belrose et al., 2023b; Zou et al., 2024; Guo et al., 2024; Pochinkov
& Schoots, 2024; Ashuach et al., 2024), as well as to better evaluate unlearning efficacy through white-box,
non-behavioral, techniques (Lynch et al., 2024; Deeb & Roger, 2024; Hong et al., 2024). Thus far, mechanistic
interpretability methods that modify intermediate activations (but not weights) for unlearning have yet to
yield competitive results (Farrell et al., 2024).

Unlearning falls under the broader aim of model (knowledge) editing, which seeks to make precise
modifications to a machine learning model that incorporates specific knowledge with desirable generalization
properties, while minimizing the impact on other knowledge (Wang et al., 2024c). By attempting to carve
neural networks at their joints, mechanistic interpretability could improve our ability to make interventions
on knowledge with few side effects. Meng et al. (2022a) make initial progress toward interpretability-based
model editing with their ROME technique. However, Thibodeau (2022) and Hase et al. (2023) highlight
flaws in the technique, indicating that mechanistic interpretability has not yet found appropriate model
components to intervene on (Section 2.1.2). With better comprehension of neural networks, we should
anticipate more surgical model editing techniques in the future.

Editing any given capability or piece of knowledge presents a greater challenge than deleting them. Meaning-
ful progress in unlearning and editing methods may depend on improved network decomposition methods,
as it would require isolating the individual mechanisms that correspond to specific knowledge or capabili-
ties. Progress in mechanistic interpretability may elucidate the structure of knowledge and capabilities in AI
models, leading to a better understanding of what kinds of model edits possibilities are realistic in future.
Knowledge and capabilities could, in fact, be part of large mechanisms that overlap with each other, making
it challenging to isolate them into discrete components. For mechanistic interpretability to effectively guide

27

editing, strong description methods will be necessary to understand how to modify specific targets without
affecting others.

Finally, mechanistic interpretability may provide tools to rigorously understand how finetuning alters
models. This may assist in debugging instances in which finetuning leads to undesired and spurious effects
(Casper et al., 2023b). Recent work (Jain et al., 2024; Prakash et al., 2024; Lee et al., 2025) suggests that ex-
isting finetuning methodologies primarily make shallow edits to existing model representations and circuitry.
Importantly, this suggests that harmlessness training (which trains models to refuse to answer harmful re-
quests) may be cheaply undone. Empirical evidence supports this claim, both with further finetuning (Gade
et al., 2024; Lermen et al., 2024), as well as with causal interventions on the forward pass (Arditi et al.,
2024). Separately, localizing knowledge and capabilities within models may improve the sample efficiency
of finetuning, by selectively modifying only relevant parameters (as in, e.g. Wu et al. (2024b)). Further
advancement in tools for comparing feature-level differences between models (such as Lindsey et al. (2024))
may accelerate our ability to debug finetuning or other control methods (Bricken et al., 2024). Mechanis-
tic interpretability work has thus yielded several insights into how finetuning changes models and how to
improve it, and may provide further insights and improvements in the future.

3.3 Using mechanistic interpretability for better predictions about AI systems

Accurately predicting model behavior in new scenarios or regimes is difficult (arguably impossible) without
understanding model internals. Interpretability could hopefully facilitate two kinds of predictions:

• Predicting model behavior in novel situations
• Predicting capabilities that arise during training or finetuning

3.3.1 Predicting behavior in novel situations

In order to determine whether an AI system may potentially underperform poorly or pose a safety risk in
new situations, the ability to predict its behavior in untested settings is imperative. A model’s behavior,
which may only become apparent in unforeseen circumstances, cannot be fully captured by its performance
on a finite set of behavioral evaluations.

By understanding the mechanisms of jailbreaking, we can anticipate the means through which a user might
bypass existing safeguards (Lee et al., 2025; Arditi et al., 2024). Similarly, if models have “trojans”, backdoors
(Hubinger et al., 2024), adversarial examples, or biases, comprehending a model’s internal mechanisms could
improve our ability to predict when models will display undesirable behavior, even if these scenarios were not
encountered during standard training or behavioral evaluations. Casper et al. (2023a) benchmark feature
synthesis tools through their ability to aid developers in identifying trojans, while interpretability assisted
in identifying cases of adversarial examples (Gandelsman et al., 2024b; Mu & Andreas, 2020; Wang et al.,
2023; Kissane et al., 2024a), and SDL was used to uncover biases based on spurious correlations in an LLM-
based classifier (Marks et al., 2024). Beyond specific failures, interpretability methods can also be used to
gain a broader understanding of network behavior. For example, prior work identified signatures in model
internals that predict a model’s likelihood of hallucinating (Yu et al., 2024) or its knowledge of particular
facts (Gottesman & Geva, 2024).

Generally, being able to predict an AI system’s behavior in advance is more challenging – but also more
desirable – than merely being able to monitor its behavior and cognition. Mechanistic interpretability could
allow us to make a certain type of claim, namely, “there exists no mechanisms that would cause the model
to deliberately behave undesirably” (Olah, 2023). For a strong version of this claim, substantial progress in
both decomposition and description methods is necessary. However, weaker versions of the claim, addressing
specific undesirable behaviors, might be more feasible with near-term methods. For instance, if it is possible
to decompose networks and identify all components, even basic description methods might let us recognize
that there are no mechanisms relating to bioweapons or illicit substances within the network, thus letting
us predict that models are probably not capable of instructing users how to fabricate bioweapons or illicit
substances (a possibility sometimes referred to as “enumerative safety” (Elhage et al., 2021; Olah, 2023)).
As AI systems become increasingly agentic, claims about even more general behavior may be possible.

28

Understanding their values or goals (or, less anthropomorphically, ‘the internal mechanisms that determine
their action plans and actions’) should enable us to better predict their behavior across a broad range of
contexts (Colognese & Jose, 2023).

When deploying AI in high-stakes scenarios, rigorous and reliable predictions are necessary, much like those
demanded of safety-critical software applications. Sometimes, such software is formally verified, thereby
ensuring certain safety-critical aspects of its behavior are guaranteed, since its compliance with specific
properties is mathematically proven. In the context of mechanistic interpretability, the equivalent would be
formal verification of AI systems (Dalrymple et al., 2024; Tegmark & Omohundro, 2023; Critch & Krueger,
2020): mathematically proving that an AI system’s behavior will satisfy a desired property on any input in
a given distribution. Formal verification of AI systems remains an unresolved issue at present. The level
of understanding of AI necessary to enable formal verification of large, general AI systems for nontrivial
properties is well beyond the current capabilities of mechanistic interpretability. However, some recent
studies using toy models provide a glimpse into what solving formal verification of AI might look like.
Approaches inspired by mechanistic interpretability have been used to prove accuracy bounds on a single-
layer transformer trained on a synthetic task, albeit with great difficulty (Gross et al., 2024). Program
synthesis through mechanistic analysis offers an alternative approach by converting simple trained neural
networks into more interpretable, controllable, and verifiable programs (Michaud et al., 2024).

Several open questions remain about the tractability of scaling these approaches from toy models to frontier
systems. For instance, for program synthesis, it is uncertain to what extent computations within real-world
neural networks can be reduced to operations that can be cleanly represented in symbolic code, or what the
total length of such code would be. Ensuring the safety of agentic systems with formal guarantees is further
complicated by the need to model a system’s interactions in an arbitrarily complex environment that might
not be formalizable (Seshia et al., 2022; Wongpiromsarn et al., 2023; Dalrymple et al., 2024).

3.3.2 Predicting capabilities that arise during training or finetuning

The most competitive methods of AI development systems result in uninterpretable systems that often fail
in ways that surprise their developers (OpenAI et al., 2024; Team et al., 2024; Anthropic, 2024). Applying
mechanistic interpretability to alleviate this issue is a key area for future research.

Improved mechanistic understanding of model training could enhance the ability to predict when certain
capabilities will appear. For instance, it has been observed that new model capabilities can emerge as a
function of scale (Wei et al. (2022), though also see Schaeffer et al. (2023)). Evidence suggests that new
capabilities may be learned in a somewhat discrete Michaud et al. (2023) or stagewise (Wang et al., 2024b)
fashion, and that in synthetic data settings, the emergence of new capabilities coincides with abrupt changes
in the trajectory of model parameters (Park et al., 2024a).

Other work shows a correlation between in-context learning capabilities and the emergence of induction heads,
an attention-based circuit mechanism (Olsson et al., 2022). By connecting these threads of research, the
long-term hope for mechanistic interpretability research is to link small-scale mechanistic structure to larger-
scale structure, such as the evolving shape of the loss landscape during model scaling (Olah, 2023). To make
progress toward this goal, research needs to move beyond simply improving ‘decomposition’ (Section 2.1.2)
or ‘description’ (Section 2.1.3) quality and instead be capable of describing the dynamic changes in the
mechanistic structure of networks throughout the learning process.

We may also want to link the emergence of capabilities to specific properties of the training data set. Through
mechanistic interpretability, we can create data sets that facilitate training models to demonstrate desirable
attributes and predict their behavior. By attributing model outputs to specific training examples, influence
functions have been applied to LLMs (Koh & Liang, 2017; Grosse et al., 2023) to predict limitations in
their generalization abilities, such as a lack of robustness when the order of certain phrases was flipped
(Berglund et al., 2024). Other work examines how data set composition shapes the emergence of in-context
and weights-based learning (Reddy, 2024).

A related problem of interest involves predicting which model capabilities, that may not be present in
a given model, can be “elicited” with sufficiently advanced prompting or finetuning strategies (Greenblatt

29

et al., 2024). Prakash et al. (2024) find evidence that finetuning improves capabilities primarily by enhancing
existing circuits, rather than developing fundamentally new mechanisms. Relatedly, (Jain et al., 2024) and
Lee et al. (2025) show that finetuning can mask capabilities present in a base model in a way that can
easily be reversed via simple changes to the model. Improved mechanistic understanding of finetuning could
help reveal capabilities obscured in this fashion. Since capabilities are behaviors that often span multiple
sequential steps, it may be necessary to have mechanistic interpretability methods that examine mechanisms
spanning multiple time steps. However, current mechanistic interpretability research is primarily focused on
understanding mechanisms involved in predictions at a single time step.

3.4 Using mechanistic interpretability to improve our ability to perform inference, improve training,

and make better use of learned mechanisms

A mechanistic understanding of AI models could be leveraged to improve their utility, from faster inference
and better training, to enhancing and manipulating representations.

By understanding the internal generation process of AI models, we could accelerate their inference. For
example, it could help identify which parts of the computation could be skipped without changing the
model’s final output (Voita et al., 2019; Din et al., 2024; Voita et al., 2024; Gromov et al., 2024). The ability
to inspect the information or functions implemented in a model could facilitate the development of more
effective distillation methods by recognizing gaps that should be distilled (Gottesman & Geva, 2024) and
discovering novel ways to distill them (Zhang et al., 2024).

Another aspect that mechanistic interpretability could enhance is model training. Interpreting how the model
processes specific examples and using this information to influence its predictions (Koh & Liang, 2017; Grosse
et al., 2023) may inform the selection of better training data to improve the model’s capabilities in desired
ways. Moreover, better monitoring of the training process can be achieved by correlating certain drops in
training loss with capability gains (Olsson et al., 2022; Wang et al., 2024a) or identifying a general order
in which specific capabilities emerge during training (Michaud et al., 2023).
In addition, identifying the
contributing components of a given task could help to devise novel, parameter-efficient training methods.
Finally, being able to decompose networks into their functional components presents possibilities to build
components that lend themselves to learning computational structures that we better understand (Crowson
et al., 2022; Fu et al., 2023).

Mechanistic interpretability has the potential to not only accelerate AI inference and training, but also
enhance its utility. Intervening in the model’s computation has the potential to remove unwanted bugs in its
reasoning abilities, and achieve better balance between its knowledge recall process and latent reasoning (Yu
et al., 2023; Jin et al., 2024; Biran et al., 2024; Balesni et al., 2024b). More broadly, understanding the inner
workings of different models could lead to better recombination of what they have learned, such as combining
model parameters (Wortsman et al., 2022) and transferring representations across models (Ghandeharioun
et al., 2025; Csiszárik et al., 2021).

3.5 Using mechanistic interpretability for ‘microscope AI’

Current approaches for knowledge discovery from data involve statistical or causal analysis, dimensionality
reduction, or using machine learning models that are inherently interpretable. These techniques can be
valuable, but are influenced by human priors, typically assume linear relationships between variables, and
cannot handle massive multimodal data. On the other hand, neural networks can do these things. Deep
learning models are capable of encoding complex, non-linear relationships and extracting meaningful features
from massive data sets without human intervention. Historically, these abilities had limited scientific value,
as without methods to interpret these models, we could not understand the patterns they found. However,
with ongoing advancements in interpretability research, this is beginning to change.

Applying interpretability for knowledge discovery is sometimes called microscope AI. This approach in-
volves training a neural network to model a data set, then applying interpretability techniques to the model
to gain insight into any (potentially novel) predictors it discovers.
In this way, the superhuman pattern
matching skills of deep neural networks can serve as a tool to parse complex data sets.

30

Current methods allow for versions of microscope AI, depending on the kind of insights that we want to
learn. Some examples of these applications include extracting novel chess concepts from AlphaZero and
teaching them to top grandmasters (Schut et al., 2023) using a CNN trained on defendant mugshots and
judge decisions to reveal how facial features affect judgments (Ludwig & Mullainathan, 2023), transforming
psychology articles into a causal graph with an LLM to enable link prediction and produce expert-level
hypotheses (Tong et al., 2024b), and analyzing a CNN to learn previously unknown morphological features
for predicting immune cell protein expression (Cooper et al., 2022), among several other studies (O’Brien
et al., 2023b; Hicks et al., 2021; Narayanaswamy et al., 2020; Korot et al., 2021). As interpretability methods
improve along various axes, deeper insights in and across more domains will become possible.

Currently, the majority of scientists are unable to access microscope AI due to the need for specialized
expertise in machine learning, interpretability, and domain knowledge to recognize significant new patterns,
a combination of skills that is rare in many fields. This may change as interpretability research becomes
more widely adopted in the sciences and as interpretability becomes increasingly automated and accessible.

3.6 Mechanistic interpretability on a broader range of models and model families

The vast majority of mechanistic interpretability research to date has focused on just three model families:
CNN-based image models (e.g. Erhan et al. (2009); Nguyen et al. (2016c); Olah et al. (2020b)), BERT-based
text models (e.g. Devlin (2018); Rogers et al. (2020)) and GPT-based text models (e.g. Elhage et al. (2021);
Wang et al. (2023); Nanda et al. (2023a)). The degree of generalizability of these findings to other models
and contexts is currently a somewhat open question. Given that future frontier models may use architectures
that differ from the current state-of-the-art, and are expected to be multimodal by default, interpretability
researchers may need to expand the range of models and modalities that they study and try to identify
universal approaches that can be applied to all of them.

Assessing how well interpretability methods apply to architectures beyond those for which they were devel-
oped, and whether we can develop techniques that generalize effectively across architectures remain open
questions. This is especially important due to the recent success of other competitive architectures as alter-
natives to CNNs and transformers. Notable alternatives include diffusion models (Sohl-Dickstein et al., 2015;
Rombach et al., 2022) and Vision Transformers (Dosovitskiy et al., 2021) for image generation/classification
and RWKV (Peng et al., 2023) and later state space models (SSMs) (Gu & Dao, 2024) for language model-
ing. Recent studies show that certain methods transfer from CNNs to SSMs (Paulo et al., 2024), and from
transformers to some SSMs e.g. (Meng et al. (2022b) vs. Sharma et al. (2024a)) and (Wang et al. (2023) vs.
Ensign & Garriga-Alonso (2024)).

Beyond the transferability of interpretability methods, a related open question concerns the transferability
of conclusions across model families. The overwhelming majority of mechanistic interpretability research
focuses on the transformer model family and therefore does not distinguish between observations that are
model-specific and those that are not. Consequently, we may be overlooking valuable insights that could be
obtained by comparing the results across multiple model families. These insights may, for example, evince
or refute the ‘universality hypothesis’, which states that (Li et al., 2015; Olah et al., 2020b) different neural
networks learn similar features and circuits to one another.

3.7 Human computer interaction with model internals

As we saw in Section 3.3, the ability to control and understand neural networks is tightly linked. Thus,
mechanistic interpretability has great potential to facilitate new types of human-AI interaction. Systems
amenable to human comprehension and control would allow diverse users to intuitively manipulate and
interact with them based on their preferences, greatly broadening their utility. As a starting point, AI
engineers who build and test AI systems have an obvious interest in working with the internal workings of
neural nets. If experts could visualize and interact with internal representations, it would unlock obvious
benefits for scientific research.

However, interactive tooling has a much broader constituency, including policy-focused AI auditors, as well
as end users. Consider an auditor looking for bias or safety issues in a neural net. With a way to probe

31

the network directly instead of relying on testing behavior, the auditor can be much more successful in
discovering potential low-probability but high-stakes errors. For end users, transparency into a network
facilitates appropriate calibration of trust. One recent idea proposes a dashboard that can show, in real
time, the internal features that influence a chatbot’s answers during a text chat (Chen et al., 2024; Zou
et al., 2023a; Viégas & Wattenberg, 2023). Such a dashboard might help users spot AI errors, or display
warnings if safety-relevant features activate. More generally, results from mechanistic interpretability could
be used to blend direct-manipulation interfaces with text interfaces, providing users with a richer palette of
controls (Carter & Nielsen, 2017).

32

4 Open socio-technical problems in mechanistic interpretability

Effective practical application of mechanistic interpretability brings both technical challenges and complex
social ramifications. It could enable us to act on AI policy and governance, presenting a valuable opportunity
to implement regulatory standards and social ideals through technical means (Section 4.1). Such consequen-
tial impacts inevitably come with important social and philosophical considerations, which likewise require
rigorous inquiry if we are to fully realize the potential benefits of AI (Section 4.2).

4.1 Translating technical progress in mechanistic interpretability into levers for AI policy and

governance

Current frontier AI governance efforts rarely specify concrete ways in which a mechanistic understanding of
AI models might be used to help implementation. For example, OpenAI’s Preparedness Framework commits
to mitigating biological risks posed by its AI systems (OpenAI, 2023), but the framework lacks details on
specific measures that might be taken. Progress in interpretability could potentially enable the removal
of any knowledge from the model which could aid users in creating biological weapons (Li et al., 2024b).
However, it remains uncertain how technical progress will translate into better AI governance, largely due to
the numerous open technical problems in mechanistic interpretability. However, there are several promising
routes toward better levers for AI policy and governance.

These avenues include assisting companies and governments to identify risks through evaluations and enhanc-
ing forecasts about new AI developments; more thorough oversight of AI systems in deployment; simplifying
how AI systems operate within existing liability law through clearer explanations of AI decisions; enabling
governments to establish risk mitigation regulations and companies to commit to concrete mitigation com-
mitments; and protecting copyright law.

An understanding of model internals can help AI labs assess the risks from frontier models (Chang et al.,
2024; Shevlane et al., 2023; Casper et al., 2024) and thus better fulfill their obligations under the EU AI
Act to “perform model evaluation ... with a view to identifying and mitigating systemic risk” (Council of
the European Union, 2024). More specifically, a mechanistic understanding of models could help evalua-
tors elicit dangerous capabilities via improved finetuning (United Kingdom AI Safety Institute, 2024), guide
their adversarial red-team attempts (Tong et al., 2024a), and ensure that AI systems are not intentionally
underperforming evaluations (van der Weij et al., 2024). Advancements in mechanistic interpretability could
also assist companies and governments in anticipating when or if AI models will obtain specific dangerous
capabilities. Improved forecasting capabilities could enhance threat modeling by reducing the “reasonable
disagreement amongst experts over which risks to prioritize” (Anthropic, 2024). For instance, it may help
build consensus on whether language models are just stochastic parrots (Bender et al., 2021) or if they
have coherent world models (Li et al., 2023). We also might be able to use interpretability to build ev-
idence supporting or refuting different models of catastrophic threat, such as determining the validity of
mesa-optimization (Hubinger et al., 2021) or inner alignment concerns (Carlsmith, 2023).
It would also
give additional time for companies to prepare adequate risk mitigation measures, (OpenAI, 2023), and for
governments to establish appropriate guidance or regulation (UK Government, 2022).

The EU AI Act mandates that developers of General Purpose AI models with systemic risk have obligations
to report incidents involving their system to the AI office.
Interpretability tools have the potential to
continuously monitor AI inference and detect incidents that require reporting. Compared to incidents in
other domains (for instance, nuclear security), AI systems allow us to log all inputs and system states, even
those that may lead to catastrophic harm. Access to a small number of such data points may greatly improve
our ability to mitigate similar future failures (Greenblatt & Shlegeris, 2024). For example, interpretability
could be used to investigate the critical “features” of the input that led to the AI incident. This could
improve our ability to further red-team the system (Section 3.3) and generate more similar data points
that could result in similar incidents. This, in turn, may help us reduce the likelihood of future incidents
(Chan et al., 2024). The incident could be utilized more directly in the construction of test-time monitors
to detect similar future incidents (see Roger (2023)). More speculatively, interpretability could be used to
verify companies’ compliance with domestic regulation (O’Brien et al., 2023a), or to authenticate states’
compliance with future international treaties regarding the use of AI (Aarne et al., 2024).

33

Leveraging a mechanistic understanding of model internals could also make decision rationales for AI model
outputs more easily obtainable. This could aid in enforcing citizens’ rights under the EU General Data
Protection Regulation “to obtain an explanation of the decision reached” by a system “based solely on
automated processing” (GDP, 2016; Gilpin et al., 2019). Model editing tools could also resolve problems
regarding the copyright status of existing generative models (Grynbaum & Mac, 2023). According to the US
Copyright Act (United States Congress), for any copyrighted work, an artifact “from which the work can
be perceived, reproduced, or otherwise communicated. . . with the aid of a machine or device” is considered
a copy of the work (Lee et al., 2024). Interpretability tools could help detect and remove memorized works
that can be reproduced verbatim by generative models.

4.2 Social and philosophical problems in mechanistic interpretability

The ability to interpret advanced AI systems holds immense potential to advance the science of AI and
increase our ability to control it (Critch & Krueger, 2020; Tegmark & Omohundro, 2023; Dalrymple et al.,
2024).
In this paper, we provide an overview of various interpretability tools that offer novel insights.
Nonetheless, interpretability research has thus far produced few tools that are used to make state-of-the-art
systems safer in the real world (Rauker et al., 2023). Modern AI systems are still generally trained, evaluated,
monitored, and debugged using techniques that do not rely on understanding their internal workings.

The absence of paradigmatic clarity is a major socio-technical factor for this. Questions such as which
goals the field of interpretability should pursue, how success should be graded, and how we should define
interpretability warrant more thoughtful answers than exist at present. In interpretable AI research, the
motivations and methods employed are often described as “diverse and occasionally discordant” (Lipton,
2018). At times, the objective of AI interpretability research is articulated as advancing a fundamental
“understanding” or “uncovering the true essence” (Christensen & Cheney, 2015) of what is happening inside
black-box models. The intrinsic validity of this paradigm deserves philosophical inquiry.

However, from an engineer’s perspective, pursuing “understanding” without a practical downstream appli-
cation misses an engineer’s objective. Proponents of this view may contend that quantifiable benchmarks
linked to concrete practical goals are accurate measures of the success of interpretability. This motivation
is concrete and useful, but some have criticized interpretability research as artificially limiting the solution
space to engineering problems. When interpretability tools are studied with motivations such as fairness or
safety, Krishnan (2020) argues that, “Since addressing these problems need not involve something that looks
like an ‘interpretation’ (etc.) of an algorithm, the focus on interpretability artificially constrains the solution
space by characterizing one possible solution as the problem itself.” Thus, some argue that interpretability
research has failed to produce competitive techniques (Rauker et al., 2023; Casper et al., 2022b), omitted
non-interpretability baselines (Rudin, 2019; Krishnan, 2020), and graded interpretability tools on their own
curve (Doshi-Velez & Kim, 2017; Miller, 2019; Rauker et al., 2023). In all safety-relevant applications of
mechanistic interpretability, it is important to assess the usefulness of interpretability against alternative
methodologies. Failing to do so or misrepresenting these comparisons can lead to follow-up work that rests
on false assumptions (Lipton & Steinhardt, 2019; Leech et al., 2024).This is particularly problematic when
it impacts the efforts of critical safety work.

Despite these concerns, it is not always imperative for mechanistic interpretability to strictly outperform
uninterpretable baselines: It may still be helpful to develop methods that offer interpretability-based advan-
tages, along with the benefits of more competitive uninterpretable methods (e.g. Rimsky et al. (2024)). While
interpretability-based methods do not currently outperform black-box baselines, if they perform in a simi-
lar ballpark, further progress in mechanistic interpretability could soon lead to better methods, especially
in problems which we believe might disproportionately benefit from improved mechanistic understanding
(Section 3).

Another potential reason for lack of clarity is that models are often studied in a vacuum: Smart & Kasirzadeh
(2024) emphasize that the usefulness or correctness of model interpretations can depend on the broader
context of the model’s development or deployment. For example, it may be hard to identify representations
of fairness within models, since understanding this requires an understanding of broader contexts. The same
data may lead to different conclusions under different definitions of fairness.

34

Finally, the interpretability community must exercise caution in their communication to minimize potential
abuses of the results of its work. Unfortunately, selective transparency can be used to actively mislead
(Ananny & Crawford, 2018). Furthermore, interpretability is at risk of being used for purposes that might
serve corporate interests at the potential expense of safety. The field of AI interpretability is highly influ-
enced by research teams in the private sector. On one hand, industry resources and research contributions
have significantly advanced interpretability research. On the other hand, compared to academia, corpora-
tions publish selectively, often have financial conflicts of interest, and may provide limited transparency.
Meanwhile, the recent definite — but ultimately modest — progress in mechanistic interpretability has been
used to lobby against specific AI regulation by falsely claiming that, “Although advocates for AI safety
guidelines often allude to the ‘black box’ nature of AI models, where the logic behind their conclusions is not
transparent, recent advancements in the AI sector have resolved this issue, thereby ensuring the integrity of
open-source code models.” (Andreessen Horowitz, 2023).

35

5 Conclusion

While mechanistic interpretability has made meaningful progress in both methods and applications, signifi-
cant challenges remain before we can achieve many of the field’s ambitious goals.

The path forward requires progress along multiple axes. We would benefit from stronger theoretical founda-
tions for decomposing neural networks at the joints of their generalization structure. Current methods like
sparse dictionary learning, while promising, face both practical limitations in scaling to larger models and
deeper conceptual challenges regarding their underlying assumptions. We must also develop more robust
methods for validating our interpretations of model behavior, moving beyond correlation-based descriptions
to capture true causal mechanisms. Additionally, we need better techniques to understand how mechanisms
evolve during training and how they interact to produce complex behaviors.

These methodological advances could unlock several promising applications. Improved interpretability meth-
ods could enable more effective monitoring of potential risks, better control over model behavior, and more
accurate predictions of capabilities. For AI capabilities, mechanistic understanding could lead to more effi-
cient architectures, better training procedures, and more targeted ways to enhance model performance. In
various scientific domains, microscope AI approaches could help extract valuable insights from model inter-
nals. To achieve these diverse goals, the field must ensure a focus on generating insights that have real-world
utility. This will involve establishing better benchmarks and comparing interpretability-based approaches
to non-interpretability baselines. However, fastest progress will likely come from the field pursuing both
scientific and engineering goals simultaneously, rather than one at the expense of the other.

The practical impact of progress in mechanistic interpretability extends beyond technical achievements.
Interpretability tools could provide crucial mechanisms for governance and oversight. They could help verify
compliance with safety standards, detect potential risks before deployment, and provide clearer attribution
of model decisions. However, realizing these benefits will require careful attention to the risks of potential
misuse and of giving false assurance about AI safety.

Looking toward the future, many expect current AI capabilities to be only a foretaste of what is to come.
As AI capabilities advance, the need for a mechanistic understanding of their decision-making processes
becomes increasingly urgent. While the black-box nature of AI models remains unresolved, the untapped
potential of mechanistic interpretability is what makes it such an exciting research area, and highlights the
importance of solving its many open research problems.

36

Author Contributions

LS managed the project and made major contributions to planning, writing, and editing content, as well
as coordinating other contributors and synthesizing their perspectives where necessary. BC substantially
contributed to the framing, writing and editing of the final manuscript, and made all of the figures. All
authors (LS, BC, JBn, JL, JW, LB, NGD, SH, AO, JBm, SB, AGA, AC, NN, MW, NS, JM,
EJM, SC, MT, WS, DB, ET, AG, MG, JH, DM, TM) contributed to the initial writing and editing
of various sections and giving feedback on versions of the manuscript.

We also greatly thank Fayth Tan for assistance with editing and Schmidt Sciences for their feedback at
various stages of the project and for funding the work.

74

A Summary of open questions

A.1 Open problems in mechanistic interpretability methods and foundations

A.1.1 Reverse engineering: Identifying the roles of network components

A.1.1a Reverse engineering step 1: Neural network decomposition

1. How should we decompose networks into more interpretable constituent parts?

a. What isomorphism or what approximation of a neural network (or parts of it) is the best way

to express it for the purposes of interpreting it?

b. How should we coarse grain neural networks?
c. How should we build higher level abstractions on top of low-level network components?

2. How true is the linear representation hypothesis?

a. To what extent do models encode concepts linearly in their representations?
b. How should we characterize representations that are not linearly represented in neural networks?
c. What properties of a concept, or of the training distribution, result in a particular concept

becoming encoded linearly (or not)?

3. Is the combination of the linear representation hypothesis and superposition the right frame for

thinking about computation in neural networks?

a. Can we fully determine the causes of feature superposition and polysemanticity within neural

networks?

b. How should we understand superposition in attention blocks?
c. How should we understand cross-layer superposition?
d. What new theoretical insights can be gleaned from considering how networks perform com-
putation natively in superposition, rather than treating superposition purely as a compression
strategy?

4. Can the problems with SDL be overcome?

a. What lies in SDL reconstruction errors? Will the errors converge to zero with methodological

progress?

b. Is sparsity the correct proxy for interpretability?
c. Can the approach be scaled to the largest models?
d. Does SDL make sense if we don’t believe in the linear representation hypothesis?
e. Is sparsity the best possible proxy for interpretability?
f. Is it correct to think of SDL features as ‘bags of features’, or is there important information

contained within the geometry of representation space?

g. If SDL finds compositions of the “true” features, is this a problem?
h. Can SDL be applied to all architectural components to fully decompose networks?
i. How can we better measure the success of SDL techniques?
j. How should we connect sparsely activating features into circuits? Will this be practically pos-

sible, and the best possible description of network mechanisms?

k. Can we develop new methods that address conceptual and practical issues with SDL?

5. How important is the geometry of activation space for explaining neural network behavior?

a. How can we identify the underlying functional structure of networks (which defines why acti-

vations are located in particular geometric arrangements in activation space)?

75

b. Must we understand global feature geometry or only local feature geometry in order to under-

stand computation in neural networks?

6. Can we connect theories for how neural networks generalize to interpretability?

a. Can we distinguish parts of networks that underlie generalization from parts that underlie

memorization?

b. What mechanisms underlie the relationship between interpretability and generalization?
c. Are there connections between adversarial robustness and superposition?
d. Can we connect interpretability to theories of deep learning like SLT?

7. Can we build intrinsically more interpretable models at low performance cost? How helpful is this?

a. Interpretability training: Can we train networks that are interpretable by default at low per-

formance cost?

b. Interpretable inference: Can we convert already-trained models into forms that are much easier

to completely interpret at little performance cost?

c. How can we train large-scale models such that the concepts they use are naturally understand-

able to humans?

d. How can we design training objectives so that the model is incentivized to use known specific

abstractions?

e. How can we localize concepts we want to control (e.g., long-term plans) during training?

A.1.1b Reverse engineering step 2: Identifying the functional role of components

1. Can we improve on max-activating input data set examples for understanding the causes of network

component activations?

a. How can we avoid imposing human bias to explanations?
b. Can we progress toward deeper descriptions based on internal mechanisms?
c. How might we develop interpretation methods that can recognize and work with unfamiliar

concepts - computational patterns that don’t map cleanly to human intuitions?

2. How can we develop attribution methods that faithfully and efficiently compute which network

components are important for some downstream metric?

a. How can we develop attribution methods that capture higher-order effects beyond first-order

approximations of model behavior?

b. Is it possible to create perturbation-based methods that don’t force models to operate outside

their training distribution?

c. Can we develop hybrid approaches that combine the strengths of different attribution methods

while mitigating their individual weaknesses?

3. How can we better measure the downstream effects of model components?

a. How can we reliably distinguish between true causal pathways and compensatory effects like

the “Hydra effect” when performing interventions?

A.1.1c Reverse engineering step 3: Validation of descriptions

1. Can we improve our ability to validate mechanistic explanations for model behavior in ways that do

not depend on researcher intuition and are computationally tractable to use?

a. Can we improve on methodologies for evaluating hypotheses through their predictive power on

activations of network components?

76

b. Can we develop methodologies for evaluating hypotheses through their predictive power for

model behavior (e.g. unusual failures or adv examples)?

c. Can we develop methodologies for handcoding weights that faithfully represent some hypotheses,

as a drop in replacement for subnetworks we claim to understand?

d. Can we develop a wider suite of networks with known ground truth explanations to validate

techniques against?

e. Can we use mechanistic explanations to achieve engineering goals?

f. Can we use mechanistic explanations to achieve engineering goals in a way that improves upon

black box baselines?

2. Can we develop “model organisms” as a community, which are understood deeply, and seen as a

test-bed for new unproven interpretability methodologies to be tested?

3. Can we establish standardized baselines and benchmarks for comparing different interpretability

approaches on real-world, non-cherry-picked tasks, where the ground truth is known?

4. What would constitute a comprehensive set of “stress tests” for interpretability hypotheses that

could reliably detect interpretability illusions?

5. How might we design evaluation frameworks that assess interpretability methods on their average

case and worst-case performance rather than just best-case scenarios?

6. How can we ensure that our understanding of internals generalizes to out-of-distribution inputs?

A.1.2 Concept-based interpretability: Identifying network components for given roles

1. How can we reliably distinguish causal from merely correlated features when probing neural net-

works?

2. Can we develop automated systems to generate high-quality probing data sets, reducing the current

heavy reliance on human effort?

3. What regularization and validation techniques can be used to prevent spurious correlations while

ensuring probes find generalizable features?

4. How can we improve probing for concepts that may not have clear positive/negative examples?

A.1.3 Proceduralizing mechanistic interpretability into circuit discovery pipelines

1. Can we develop techniques that build on lower level methods that provide deeper or more complete

insights about neural networks?

2. How much can we learn from further work in the existing circuit discovery paradigm?

a. Should we expect circuit discovery to benefit from further methodological progress in decom-
posing neural networks? Will faithfulness go up and explanation description length go down?

b. Can we remove the constraint that task definition for circuit discovery is inherently concept-

based, which may be complicating mechanistic analysis?

c. Can we get around the practical issues of negative and backup behavior?

d. Will circuit discovery provide insights into arbitrary tasks, or will it only be helpful in cases

where we are able to crisply define tasks?

77

A.1.4 Automating steps in mechanistic interpretability research

1. Can we improve on AI automated feature description and validation methods?

a. Through automating the generation and testing of arbitrary hypotheses?
b. Through describing differences between features?
c. Through descriptions of how components interact?

2. Can we improve on ACDC-like circuit discovery methods?

3. Can we automate other parts of the mechanistic interpretability pipeline?

a. Conceptual interpretability research?
b. Decomposition method discovery?
c. More ad hoc validation of hypotheses?

4. Should we take steps to mitigate potentially misaligned AI systems sabotaging AI automated inter-

pretability?

A.2 Open problems in applications of mechanistic interpretability

A.2.1 Using mechanistic interpretability for better monitoring and auditing of AI systems for

potentially unsafe cognition

1. Can we effectively use interpretability for safety evaluations?

a. Can we develop robust “white box” evaluations that detect concerning internal patterns without

needing to understand the entire network?

b. Can we reliably distinguish between features that merely recognize deceptive behavior versus

mechanisms that generate deceptive behavior?

c. How can we validate that learned features capture all concerning patterns of reasoning?
d. Can we reliably identify which features are appropriately versus spuriously relevant to a given

task?

2. Can we leverage interpretability to enhance red-teaming and system testing?

a. Can we use interpretability insights to make red-teaming more efficient than current methods?
b. How can we best use feature attribution to help human red-teamers identify problematic input

patterns?

3. Can we develop effective test-time monitoring systems based on interpretability?

a. Can we get mechanistic anomaly detection to work?
b. Can we create passive monitoring systems based on model internals that effectively flag con-

cerning internal patterns during deployment?

c. Can we develop monitoring systems that work with only feature-level understanding rather than

requiring deep mechanical insights?

A.2.2 Using mechanistic interpretability for better control of AI system behavior

1. Can we improve steering methods through interpretability?

a. How can we make activation steering more precise and reduce its side effects?
b. Can we develop methods to steer entire mechanisms rather than just single features?

2. Can we achieve reliable model unlearning and editing?

a. Will carving the network at its true joints help us improve on model unlearning and editing?
b. Can mechanistic interpretability help us develop better methods for evaluating unlearning effi-

cacy?

78

c. Can mechanistic interpretability help us determine which classes of model edit are even possible,

without damaging generalization in undesirable ways?

3. Can we better understand and improve finetuning through interpretability?

a. Can we make finetuning more sample-efficient by targeting specific parameters?
b. Can we develop better tools for analyzing feature-level or mechanism-level differences between

model versions?

A.2.3 Using mechanistic interpretability for better predictions about AI systems

1. Can we predict model behavior in novel situations outside of the distribution of inputs we have

access to with mechanistic understanding?

a. Can we reliably predict when and how jailbreaking or safety bypasses might occur?
b. How can we identify internal signatures that predict specific failure modes like hallucination?
c. Can we develop methods to predict model behavior without requiring behavioral evaluations?
d. Can we find “values” or “goals” in systems that might be indicative of behavior in more gener-

ality?

e. Is it possible to prove the absence of specific dangerous capabilities through mechanistic anal-

ysis?

2. Can we develop formal verification methods for AI systems?

a. Can current toy model verification approaches scale to frontier systems?
b. How much of neural computation can be reduced to verifiable symbolic operations?
c. Can we create formal guarantees about system behavior in complex, non-formalizable environ-

ments?

d. What level of mechanistic understanding is necessary for meaningful formal verification?

3. Can we make rigorous claims about model safety?

a. Can we definitively prove the absence of specific dangerous mechanisms?
b. How can we verify claims about model values and goals in a rigorous way?
c. What types of safety claims are possible with current interpretability methods?
d. Can we develop “enumerative safety” approaches that reliably identify all relevant mechanisms?

4. Can we better predict AI capability development through interpretability?

a. Can we identify early signatures that predict emergent capabilities?
b. How do model mechanisms evolve dynamically during training?
c. Can we map the connection between small-scale circuits and large-scale capabilities?
d. How does the loss landscape’s structure relate to capability emergence?

5. Can we understand the relationship between training data and capabilities?

a. How do specific training examples influence the development of model mechanisms?
b. Can we predict model limitations based on training data composition?
c. Can we design training data sets to reliably produce specific desired capabilities?
d. How does data set structure affect the balance between in-context and weights-based learning?

6. Can we predict latent or maskable capabilities?

a. How can we identify capabilities that could be ‘unlocked’ through prompting or finetuning?
b. Can we detect when finetuning has masked rather than removed capabilities?
c. How do we analyze mechanisms that span multiple timesteps or sequential behaviors?
d. Can we predict which model capabilities are fundamental versus superficially trained?

79

A.2.4 Using mechanistic interpretability to improve our ability to perform inference, improve training

and make use of learned representations

1. Can we use interpretability to make inference more efficient?

a. How can we identify skippable computations without affecting outputs?
b. Can we create more effective distillation methods through mechanistic understanding?
c. How can we optimize model architecture based on component function analysis?
d. Can we identify and optimize critical computational pathways?

2. Can we improve training through mechanistic insights?

a. Can we better select training data by understanding example influence?
b. How can we monitor and optimize capability emergence during training?
c. Can we develop more parameter-efficient training methods through component analysis?
d. Can we create better architectures through component understanding?
e. Can we identify and enhance components with specific functionalities?

3. Can we instill capabilities directly into networks?

a. Can we design better inductive biases based on mechanistic insights?
b. Is it possible to create modular architectures with swappable components?
c. Can we develop reliable methods for combining model parameters?
d. Is it possible to transfer specific capabilities between models?

A.2.5 Using mechanistic interpretability for ’microscope AI’

1. Can we leverage AI models for scientific discovery?

a. How can we extract novel patterns and predictors that models have found?
b. Can we make microscope AI techniques accessible to domain experts?
c. How do we validate scientific insights derived from model interpretability?
d. Can we extend microscope AI beyond current simple correlational discoveries?

2. Can we develop better knowledge extraction methods?

a. How can we detect when models have found genuinely novel patterns?
b. Can we automate the process of finding scientific insights in model weights?
c. How do we bridge the gap between model features and scientific concepts?
d. Can we make these techniques usable without deep machine learning expertise?

A.2.6 Mechanistic interpretability on a broader range of models and model families

1. Can interpretability methods generalize across architectures?

a. Do current interpretability methods (SDL, circuit analysis) transfer to SSMs? Or, like the

transition from CNNs to transformers, are new approaches necessary?

b. Which insights are model-specific versus universal?
c. How can we adapt methods for multimodal models?

2. How do different models trained on similar data compare mechanistically?

a. Is the “universality hypothesis” true across models? To what extent do neural networks learn

similar features and circuits to each other (and to humans?)
b. Do different architectures learn fundamentally different features?
c. How do mechanisms of particular tasks differ between transformers, CNNs, and SSMs?
d. Are there insights we can gain from comparing architectures?

80

3. Can we future-proof interpretability research?

a. How can we prepare for interpreting novel architectures?
b. Should we focus on architecture-specific or general methods?
c. Can we identify truly fundamental interpretability principles?
d. Will current methods work on future frontier models?

A.2.7 Human computer interaction with model internals

1. Can we create interfaces that use mechanistic understanding to enhance human-neural network

interaction?

a. How can we visualize model internals in an intuitive way?
b. Can we develop real-time interpretability dashboards?
c. What’s the right balance between simplicity and depth in these interfaces?
d. How do we make complex model mechanisms understandable to non-experts?

2. Can we develop interpretability tools to help auditors?

a. How can we help auditors find potential failure modes directly?
b. Can we develop tools to detect bias at the mechanism level?
c. What interfaces would make auditing more efficient and thorough?
d. How can we present technical findings to policy makers?

3. Can we improve end-user interaction with AI?

a. How can transparency features help users calibrate trust?
b. Can we create intuitive controls based on model mechanisms?
c. Can we create intuitive ways to steer model behavior?

A.2.8 Governance

1. Can mechanistic analysis help identify and prevent failures?

a. Can we identify specific mechanisms that caused AI failures?
b. How do we map the causal chain of mechanisms leading to incidents?
c. Can we detect when similar mechanisms are about to activate?
d. Is it possible to isolate and modify failure-causing mechanisms?

2. Can we study mechanism patterns related to governance?

a. Can we identify mechanisms responsible for specific dangerous capabilities?
b. How do we detect deceptive or evasive mechanisms?
c. Can we map the mechanisms involved in model decision-making?
d. Is it possible to verify the absence of specific harmful mechanisms?

3. Can mechanistic insights verify compliance?

a. How can we trace decision mechanisms to explain model outputs?
b. Can we identify mechanisms that process copyrighted content?
c. Is it possible to detect mechanisms that encode specific knowledge?
d. How do we verify modifications to problematic mechanisms?

81

A.2.9 Open socio-technical problems in mechanistic interpretability

A.2.9a Translating technical progress in mechanistic interpretability into levers for AI policy
and governance

1. Can we use a mechanistic understanding to better evaluate AI capabilities?

a. How can we use interpretability to improve capability elicitation?
b. Can we use interpretability to reliably detect when models are strategically underperforming

capabilities evaluations?

2. Can we use a mechanistic understanding to improve our ability to forecast when or whether new

capabilities will arise ahead of time?

3. How can we use interpretability to better estimate the likelihood of different threat models?

4. Can we use interpretability to prevent AI incidents?

a. Can we use interpretability to construct reliable test-time monitors to detect AI incidents?
b. Can we use reliably prevent similar incidents in the future, by using interpretability to design

new evaluation tasks on incident scenarios?

5. Can interpretability help verify which workloads GPUs are being used for?

6. How should interpretability inform copyright law?

7. How can mechanistic understanding help resolve copyright challenges in generative AI, particularly

regarding the detection and removal of memorized copyrighted works?

A.2.9b Social and philosophical challenges in mechanistic interpretability

1. What is interpretability?

a. What are the goals of the field?
b. How should success be graded?
c. Should we treat interpretability as a science or an engineering discipline? What implications

does this have on what research should be done?

2. How can we mitigate downside risks of interpretability research?

a. How can we communicate the results of our research such that the risk of their misuse is

minimized?

82

A PRIMER ON THE INNER WORKINGS OF
TRANSFORMER-BASED LANGUAGE MODELS

Javier Ferrando1∗, Gabriele Sarti2, Arianna Bisazza2, Marta R. Costa-jussà3
1Universitat Politècnica de Catalunya, 2CLCG, University of Groningen, 3FAIR, Meta

ABSTRACT
The rapid progress of research aimed at interpreting the inner workings of ad-
vanced language models has highlighted a need for contextualizing the insights
gained from years of work in this area. This primer provides a concise techni-
cal introduction to the current techniques used to interpret the inner workings of
Transformer-based language models, focusing on the generative decoder-only ar-
chitecture. We conclude by presenting a comprehensive overview of the known in-
ternal mechanisms implemented by these models, uncovering connections across
popular approaches and active research directions in this area.

4
2
0
2

t
c
O
3
1

]
L
C
.
s
c
[

3
v
8
0
2
0
0
.
5
0
4
2
:
v
i
X
r
a

Figure 1: Survey overview. Section 2 introduces the Transformer language model and its compo-
nents. Section 3 and Section 4 present interpretability techniques used to analyze models’ inner
workings. Finally, Section 5 presents known inner workings of Transformer language models.

∗Correspondence to: jferrandomonsonis@gmail.com.

1

Section 2 Components of a Transformer Language ModelSection 2.1 The Transformer LayerSection 2.2 Prediction Head and                    Transformer DecompositionsSection 3 Behavior LocalizationSection 4 Information DecodingSection 5 Discovered Inner BehaviorsSection 3.1 Input AttributionSection 3.2 Model Component ImportanceSection 4.1 ProbingSection 4.2 Linear Representation Hypothesis                         and Sparse AutoencodersSection 4.3 Decoding in Vocabulary SpaceSection 5.1 Attention BlockSection 5.2 Feedforward BlockSection 5.3 Residual StreamSection 5.4 Emergent Multi-component BehaviorA report about the Impressionists hasWhy has instead of have?

1

INTRODUCTION

The development of powerful Transformer-based language models (LMs; Radford et al., 2019;
Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2023) and their widespread utilization
underscores the significance of research devoted to understanding their inner mechanisms. Gaining
a deeper understanding of these mechanisms in highly capable AI systems holds important implica-
tions in ensuring the safety and fairness of such systems, mitigating their biases and errors in critical
settings, and ultimately driving model improvements (Wei et al., 2022; Costa-jussà et al., 2023).
As a result, the natural language processing (NLP) community has witnessed a notable increase in
research focused on interpretability in language models, leading to new insights into their internal
functioning.

Existing surveys present a wide variety of techniques adopted by Explainable AI analyses (Räuker
et al., 2023) and their applications in NLP (Madsen et al., 2022; Lyu et al., 2024). While previ-
ous NLP interpretability surveys primarily focused on encoder-based models like BERT (Devlin
et al., 2019; Rogers et al., 2021), the success of decoder-only Transformers (Radford et al., 2018)
prompted further developments in the analysis of these powerful generative models, with concur-
rent work surveying trends in interpretability research and their relation to AI safety (Bereska &
Gavves, 2024). By contrast, this work provides a concise, in-depth technical introduction to relevant
techniques used in LM interpretability research, focusing on insights derived from models’ inner
workings and drawing connections between different areas of interpretability research. Moreover,
throughout this work, we employ a unified notation to introduce model components, interpretability
methods, and insights from surveyed works, shedding light on the assumptions and motivations be-
hind specific method designs. We categorize LM interpretability approaches surveyed in this work
along two dimensions: i) localizing the inputs or model components responsible for a particular
prediction (Section 3); and ii) decoding information stored in learned representations1 to understand
its usage across network components (Section 4). Finally, Section 5 provides an exhaustive list of
insights into the inner workings of Transformer-based LMs, and Section 6 provides an overview of
useful tools to conduct interpretability analyses on these models.

2 THE COMPONENTS OF A TRANSFORMER LANGUAGE MODEL

Auto-regressive language models assign probabilities to sequences of tokens. Using the probability
chain rule, we can decompose the probability distribution over a sequence t = ⟨t1, t2 . . . , tn⟩ into a
product of conditional distributions:

P (t1, . . . , tn) = P (t1)

n−1
(cid:89)

i=1

P (ti+1|t1, . . . , ti).

(1)

Such distributions can be parametrized using a neural network optimized to maximize the likelihood
of a corpus used for training (Bengio et al., 2003). In recent years, the Transformer architecture
by Vaswani et al. (2017) was widely adopted for this purpose thanks to its expressivity and its
scalability (Kaplan et al., 2020). While several variants of the original Transformers were proposed,
we focus here on the decoder-only architecture (also known as GPT-like) due to its success and
popularity.2 A decoder-only model f has L layers, and operates on a sequence of embeddings
x = ⟨x1, x2 . . . , xn⟩ representing the tokens t = ⟨t1, t2 . . . , tn⟩. Each embedding x ∈ Rd is a row
vector corresponding to a row of the embedding matrix WE ∈ R|V|×d, where V is the model
vocabulary. Intermediate layer representations, for instance, at position i and layer l, are referred to
i.3 By X ∈ Rn×d we represent the sequence x as a matrix with embeddings stacked as rows.
as xl
Likewise, for intermediate representations, X l
≤i is the layer l representation matrix up to position i.
Appendix A provides a summary of the notation used in this work.

Following recent literature regarding interpretability in Transformers, we present the architecture
adopting the residual stream perspective (Elhage et al., 2021a).
In this view, each input em-
bedding gets updated via vector additions from the attention (Section 2.1.2) and feed-forward

1In this work we use representations and activations interchangeably, and we refer to the fundamental unit

of information encoded in model activations as features, representing human-interpretable input properties.

2Most of the insights presented in this work remain relevant for encoder-only and encoder-decoder models.
3Note that x0

i = xi.

2

blocks (Section 2.1.3), producing residual stream states (or intermediate representations). The final
layer residual stream state is then projected into the vocabulary space via the unembedding ma-
trix WU ∈ Rd×|V|(Section 2.2), and normalized via the softmax function to obtain the probability
distribution over the vocabulary from which a new token is sampled.

2.1 THE TRANSFORMER LAYER

In this section, we present the Transformer layer components following their computations’ flow.

2.1.1 LAYER NORMALIZATION

Layer normalization (LayerNorm) is a common operation used to stabilize the training process
of deep neural networks (Ba et al., 2016). Although early Transformer models implemented
LayerNorm at the output of each block, modern models consistently normalize preceding each
block (Xiong et al., 2020; Takase et al., 2023). Given a representation z, the LayerNorm com-
putes ((z−µ(z))/σ(z)) ⊙ γ + β, where µ and σ calculate the mean and standard deviation, and γ ∈ Rd
and β ∈ Rd refer to learned element-wise transformation and bias respectively. Layer normaliza-
tion can be interpreted geometrically by visualizing the mean subtraction operation as a projection
of input representations onto a hyperplane defined by the normal vector [1, 1, . . . , 1] ∈ Rd, and the
d norm as a mapping of the resulting representations to a hypersphere (Brody
following scaling to
et al., 2023; Riechers, 2024). Kobayashi et al. (2021) notes that LayerNorm can be treated as an
affine transformation zL + β, as long as σ(z) is considered as a constant (Appendix B). In this
view, the matrix L computes the centering and scaling operations. Furthermore, the weights of the
affine transformation can be folded into the following linear layer (Appendix C), simplifying the
analysis.

√

We note that current LMs such as Llama 2 (Touvron et al., 2023) adopt an alternative layer normal-
ization procedure, RMSNorm (Zhang & Sennrich, 2019), where the centering operation is removed,
and scaling is performed using the root mean square (RMS) statistic.

2.1.2 ATTENTION BLOCK

Attention is a key mechanism that allows Transformers to contextualize token representations at
each layer. The attention block is composed of multiple attention heads. At a decoding step i, each
attention head reads from residual streams across previous positions (≤ i), decides which positions
to attend to, gathers information from those, and finally writes it to the current residual stream. We
adopt the rearrangement proposed by Kobayashi et al. (2021) and Elhage et al. (2021a) to simplify
the analysis of residual stream contributions.4 In particular, every attention head computes

Attnl,h(X l−1

≤i ) =

=

(cid:88)

j≤i
(cid:88)

j≤i

al,h
i,j xl−1

j W l,h

Value vector
V W l,h

O

al,h
i,j xl−1

j W l,h
OV .

(2)

The learnable weight matrices W l,h
O ∈ Rdh×d, where dh represents the dimen-
sion of each head, are combined into the OV matrix W l,h
OV ∈ Rd×d, also referred to
as OV (output-value) circuit. The attention weights for every key (≤ i) given the current query (i)
are obtained as:

V ∈ Rd×dh and W l,h

O = W l,h

V W l,h

4The original implementation considers a concatenation of each attention head output before projecting into
O ∈ Rdh×d, matrices

O into per-head weight matrices W l,h
O can be joined in a single matrix W l,h
OV .

the weight matrix W l
and W l,h
W l,h
V

O ∈ RH·dh×d. By splitting W l

3

Figure 2: Unrolled Transformer LM with expanded views of the Attention and Feedforward net-
work blocks, including model weights (gray) and residual stream states (green). Based on figures
from (Ferrando & Voita, 2024; Voita et al., 2023).

al,h

i = softmax






i W l,h
xl−1

≤i W l,h

K )⊺

Query vector
Q (X l−1
√
dh






Key vectors’ matrix

= softmax

(cid:32) xl−1

QKX l−1
i W h
√
dh

≤i

⊺

(cid:33)

,

(3)

= W h

∈ Rd×dh and W l,h
K

with W l,h
∈ Rd×dh combining as the QK (query-key) circuit
Q
⊺
QK ∈ Rd×d. The decomposition introduced in Equations (2) and (3) enables a
W h
QW h
K
view of QK and OV circuits as units responsible for reading from and writing (in the case of the
OV circuit) to the residual stream. The attention block output is the sum of individual attention
heads, which is subsequently added back into the residual stream:

Attnl(X l−1

≤i ) =

H
(cid:88)

h=1

Attnl,h(X l−1

≤i ),

xmid,l
i

= xl−1

i + Attnl(X l−1

≤i ).

(4)

(5)

2.1.3 FEEDFORWARD NETWORK BLOCK

in ∈ Rd×dffn and W l

The feedforward network (FFN) in the Transformer block is composed of two learnable weight
matrices5: W l
, and
its result is passed through an element-wise non-linear activation function g, producing the neuron
activations. These get transformed by W l
), which is then added
back to the residual stream:

in reads from the residual stream state xmid,l

out to produce the output FFN(xmid

out ∈ Rdffn×d. W l

i

i

FFNl(xmid,l

i

) = g(xmid,l

i W l

in)W l

out.

(6)

5We omit bias terms, following the practice of recent models such as Llama (Touvron et al., 2023),
PaLM (Chowdhery et al., 2023) and OLMo (Groeneveld et al., 2024), which also exclude biases from attention
matrices.

4

KeysValuesi = xmid,l
xl

i

+ FFNl(xmid,l

i

).

(7)

in) stored in columns of W l

The computation described in Equation (6) was equated to key-value memory retrieval (Geva et al.,
2021), with keys (wl
in acting as pattern detectors over the input sequence
(Figure 2 right) and values wl
out, being upweighted by each neuron activation. We
use the term “neuron” to refer to each value after an element-wise non-linearity, and use “unit” or
“dimension” for other individual values in any other representation. Provided that the output of the
FFN is a linear combination of wl
out values, Equation (6) can be rewritten following the key-value
perspective:

out, rows of W l

FFNl(xmid,l

i

) =

=

dffn(cid:88)

u=1

dffn(cid:88)

u=1

gu(xmid,l

i wl
inu

)wl

outu

uwl
nl

outu

,

(8)

(9)

with nl ∈ Rdffn being the vector of neuron activations, and nl

u the u-th neuron activation value.

The elementwise nonlinearity inside FFNs creates a privileged basis (Elhage et al., 2022b), which
encourages features to align with basis directions. For instance, given a linear network f (x) =
xW1W2, the representations extracted from its first layer, xW1, are rotationally invariant, since
we can rotate them by an orthogonal matrix O, giving xW1O, and invert the rotation having the
output of the network untouched, f (x) = xW1OO−1W2 (Brown et al., 2023). However, having
an elementwise nonlinear function on the output of the first layer breaks the rotational invariance of
the representations, making the standard basis dimensions (neurons) more likely to be independently
meaningful, and therefore better suitable for interpretability analysis.

2.2 PREDICTION HEAD AND TRANSFORMER DECOMPOSITIONS

The prediction head of a Transformer consists of an unembedding matrix WU ∈ Rd×|V|, sometimes
accompanied by a bias. The last residual stream state gets transformed by this linear map con-
verting the representation into a next-token distribution of logits, which is turned into a probability
distribution via the softmax function.

Prediction as a sum of component outputs. The residual stream view shows that every model
component interacts with it through addition (Mickus et al., 2022). Thus, the unnormalized scores
(logits) are obtained via a linear projection of the summed component outputs. Due to the properties
of linear transformations, we can rearrange the traditional forward pass formulation so that each
model component contributes directly to the output logits:

f (x) = xL
(cid:16) L
(cid:88)

nWU
H
(cid:88)

=

l=1

h=1

Attnl,h(X l−1

≤n ) +

L
(cid:88)

l=1

FFNl(xmid,l

n

) + xn

(cid:17)

WU

L
(cid:88)

H
(cid:88)

=

Attnl,h(X l−1

≤n )WU +

l=1

h=1
Attention head logits update

L
(cid:88)

l=1

FFNl(xmid,l

n

)WU + xnWU .

(10)

FFN logits update

This decomposition plays an important role when localizing components responsible for a prediction
(Section 3) since it allows us to measure the direct contribution of every component to the logits of
the predicted token (Section 3.2.1).

Prediction as an ensemble of shallow networks forward passes. Residual networks work as
ensembles of shallow networks (Veit et al., 2016), where each subnetwork defines a path in the
computational graph. Let us consider a two-layer attention-only Transformer, where each attention
head is composed just by an OV matrix: f (x) = x1 + W 2
OV (x). We
can decompose the forward pass (Figure 3) as

OV (x1), with x1 = x + W 1

5

Figure 3: Forward pass decomposition in a simplified Transformer LM. The direct path (red), full
OV circuits (yellow) and virtual attention heads (grey) expressed in Equation (11) are highlighted.

Direct path
f (x) = xWU + xW 1

Full OV circuits
OV W 2

OV WU + xW 1

OV WU + xW 2

OV WU .

(11)

Virtual attention heads (V-composition)

The first term in Equation (11), linking the input embedding to the unembedding matrix, is referred
to as the direct path (first path in Figure 3). The paths traversing a single OV matrix are instead
named full OV circuits (second and fourth path in Figure 3). Often, full OV circuits are written
as WEWOV WU ∈ R|V|×|V|, stacking as rows the logits effect of each input embedding through
the circuit. Lastly, the path involving both attention heads is referred to as virtual attention heads
doing V-composition, since the sequential writing and reading of the two heads is seen as OV ma-
trices composing together. Elhage et al. (2021a) propose measuring the amount of composition as:
∥W 1
. Q-composition and K-composition, i.e. compositions of WQ and
WK with the WOV output of previous layers, can also be found in full Transformer models.

OV ∥F/∥W 1

OV ∥F ∥W 2

OV ∥F

OV W 2

3 BEHAVIOR LOCALIZATION

Understanding the inner workings of language models implies localizing which elements in the
forward pass (input elements, representations, and model components) are responsible for a specific
prediction.6In this section, we present two different types of methods that allow localizing model
behavior: input attribution (Section 3.1) and model component attribution (Section 3.2).

3.1

INPUT ATTRIBUTION

Input attribution methods are commonly used to localize model behavior by estimating the contribu-
tion of input elements (in the case of LMs, tokens) in defining model predictions. We refer readers
to Madsen et al. (2022) for a broader overview of post-hoc input attribution methods with a focus
on classification tasks in NLP. Additionally, for a more focused examination of these techniques as
applied to Transformer architectures, we recommend the work of Fantozzi & Naldi (2024).

Gradient-based input attribution. For neural network models like LMs, gradient information
is frequently used as a natural metric for attribution purposes (Simonyan et al., 2014; Li et al.,
2016; Ding & Koehn, 2021). Gradient-based attribution in this context involves a first-order Tay-
lor expansion of a Transformer at a point x, expressed as ∇f (x) · x + b. The resulting gradient
∇fw(x) ∈ Rn×d = (grad fw)(x) captures intuitively the sensitivity of the model to each element
in the input when predicting token w.7 While attribution scores are computed for every dimension
of input token embeddings, they are generally aggregated at a token level to obtain a more intuitive
overview of the influence of individual tokens. This is commonly done by taking the Lp norm of the
gradient vector w.r.t the i-th input embedding:

AGrad

fw(x)←ti

= ∥∇xifw(x)∥p .

(12)

6Commonly referred to as local explanation in the interpretability literature (Lipton, 2018)
7Vocabulary logits or probability scores are commonly used as differentiation targets (Bastings et al., 2022).

6

Figure 4: Three approaches to compute inter-token contributions (ci,j) towards context mixing in
attention heads. Relying only on attention weights overlooks the magnitude of the vectors they
operate on. This limitation can be addressed by accounting for the norm of the value-weighted or
output-value-weighted vectors (x′
j). Finally, distance-based analysis estimates the contribution of
weighted vectors from their proximity to the attention output.

By taking the dot product between the gradient vector and the input embedding ∇xifw(x) · xi,
known as gradient × input method (Denil et al., 2015), this sensitivity can be converted to an
importance estimate. However, these approaches are known to exhibit gradient saturation and shat-
tering issues (Shrikumar et al., 2017; Balduzzi et al., 2017). This fact prompted the introduction
of methods such as integrated gradients (Sundararajan et al., 2017) and SmoothGrad (Smilkov
et al., 2017) to filter noisy gradient information. For example, integrated gradients approximate
the integral of gradients along the straight-line path between a baseline input ˜x and the input x:
(xi − ˜xi) (cid:82) 1
0 ∇xifw(˜x + α(x − ˜x))dα, and subsequent adaptations were proposed to accommo-
date the discreteness of textual inputs (Sanyal & Ren, 2021; Enguehard, 2023). Finally, approaches
based on Layer-wise Relevance Propagation (LRP) (Bach et al., 2015) have been widely applied
to study Transformer-based LMs (Voita et al., 2021; Chefer et al., 2021; Ali et al., 2022; Achtibat
et al., 2024). These methods use custom rules for gradient propagation to decompose component
contributions at every layer, ensuring their sum remains constant throughout the network.

Perturbation-based input attribution. Another popular family of approaches estimates input im-
portance by adding noise or ablating input elements and measuring the resulting impact on model
predictions (Li et al., 2017). For instance, the input token at position i can be removed, and the
resulting probability difference fw(x) − fw(x−xi) can be used as an estimate for its importance. If
the logit or probability given to w does not change, we conclude that the i-th token has no influence.
A multitude of perturbation-based attribution methods exist in the literature, such as those based
on interpretable local surrogate models such as LIME (Ribeiro et al., 2016), or those derived from
game theory like SHAP (Shapley, 1953; Lundberg & Lee, 2017). Notably, new perturbation-based
approaches were proposed to leverage linguistic structures (Amara et al., 2024; Zhao & Shan, 2024)
and Transformer components (Deiseroth et al., 2023; Mohebbi et al., 2023) for attribution purposes.
These methods relate directly to causal interventions discussed in Section 3.2.2. We refer readers
to Covert et al. (2021) for a unified perspective on perturbation-based input attribution.

Context mixing for input attribution. While raw model internals such as attention weights were
generally considered to provide unfaithful explanations of model behavior (Jain & Wallace, 2019;
Bastings & Filippova, 2020; Lopardo et al., 2024), recent methods have proposed alternatives to
attention weights for measuring intermediate token-wise attributions. Some of these alternatives
include the use of the norm of value-weighted vectors (Kobayashi et al., 2020) and output-value-
weighted vectors (Kobayashi et al., 2021), or the use of vectors’ distances to estimate contribu-
tions (Ferrando et al., 2022b) (Figure 4 provides a visual description). A common strategy among
such approaches involves aggregating intermediate per-layer attributions reflecting context mixing
patterns (Brunner et al., 2020) using techniques such as attention rollout (Abnar & Zuidema, 2020),
resulting in input attribution scores (Ferrando et al., 2022b; Modarressi et al., 2022; Mohebbi et al.,

7

Attention weightsOutput-value-weightedvectorsDistance-based analysis2023).8 Such context mixing approaches have shown strong faithfulness compared to gradient and
perturbation-based methods on classification benchmarks such as ERASER (DeYoung et al., 2020).
However, rollout aggregation has recently been criticized due to its simplistic assumptions, and re-
cent research has attempted to fully expand the linear decomposition of the model output presented
in Equation (10) (Modarressi et al., 2023; Yang et al., 2023; Oh & Schuler, 2023) as a sum of linear
transformations of the input tokens, linearizing the FFN block (Kobayashi et al., 2024).

Contrastive input attribution. An important limitation of input attribution methods for interpret-
ing language models is that attributed output tokens belong to a large vocabulary space, often having
semantically equivalent tokens competing for probability mass in next-word prediction (Holtzman
et al., 2021). In this context, attribution scores are likely to misrepresent several overlapping fac-
tors such as grammatical correctness and semantic appropriateness driving the model prediction.
Recent work addresses this issue by proposing a contrastive formulation of such methods, produc-
ing counterfactual explanations for why the model predicts token w instead of an alternative token
o (Yin & Neubig, 2022). As an example, Yin & Neubig (2022) extend the vanilla gradient method
of Equation (12) to provide contrastive explanations (ContGrad):
AContGrad

= ∥∇xi (fw(x) − fo(x))∥p .

(13)

fw¬o(x)←ti

Limitations of input attribution methods. While input attribution methods are commonly used
to debug failure cases and identify biases in models’ predictions (McCoy et al., 2019), popular ap-
proaches were shown to be insensitive to variations in the model and data generating process (Ade-
bayo et al., 2018; Sixt et al., 2020), to disagree with each others’ predictions (Atanasova et al., 2020;
Crabbé & van der Schaar, 2023; Krishna et al., 2024) and to show limited capacity in detecting
unseen spurious correlations (Adebayo et al., 2020; 2022). Importantly, popular methods such as
SHAP and Integrated Gradients were found provably unreliable at predicting counterfactual model
behavior in realistic settings Bilodeau et al. (2024). Apart from theoretical limitations, perturbation-
based approaches also suffer from out-of-distribution predictions induced by unrealistic noised or
ablated inputs, and from high computational cost of targeted ablations for granular input elements.

Training data attribution. Another dimension of input attribution involves the identification of
influential training examples driving specific model predictions at inference time (Koh & Liang,
2017). These approaches are commonly referred to as training data attribution (TDA) or instance
attribution methods and were applied to identify data artifacts (Han et al., 2020; Pezeshkpour et al.,
2022) and sources of biases in language models’ predictions (Brunet et al., 2019), with recent ap-
proaches proposing to perform TDA via training run simulations (Guu et al., 2023; Liu et al., 2024).
While the applicability of established TDA methods was put in question (Akyurek et al., 2022), es-
pecially due to their inefficiency, recent work in this area has produced more efficient methods that
can be applied to large generative models at scale (Park et al., 2023b; Grosse et al., 2023; Kwon
et al., 2024). We refer readers to (Hammoudeh & Lowd, 2022) for further details on TDA methods.

3.2 MODEL COMPONENT IMPORTANCE

Early studies on the importance of Transformers LMs components highlighted a high degree of
sparsity in model capabilities. This means, for example, that removing even a significant fraction
of the attention heads in a model may not deteriorate its downstream performances (Michel et al.,
2019; Voita et al., 2019b). These results motivated a new line of research studying how various
components in an LM contribute to its wide array of capabilities.

3.2.1 LOGIT ATTRIBUTION

Let us call f c(x) the output representation of a model component c (attention head or FFN) at a
particular layer for the last token position n. The decomposition presented in Equation (10) allows
us to measure the direct logit attribution9 (DLA, Figure 5) of each model component for the output
token w ∈ V:

fw(x)←c = f c(x)WU [:,w],
8The attention flow method is seldom used due to its computational inefficiency, despite its theoretical

ADLA

(14)

guarantees (Ethayarajh & Jurafsky, 2021).

9Note that the softmax function is shift-invariant, and therefore the logit scores have no absolute scale.

8

Figure 5: Direct Logit Attributions (DLA) on output token w. (a) DLA of an attention head Attnl,h,
(b) DLA of an intermediate representation xl−1
via an attention head, (c) DLA of an FFN block,
and (d) DLA of a single neuron.

1

where WU [:,w] is the w-th column of WU , i.e. the unembedding vector of token w. In practical
terms, the DLA for a component c expresses the contribution of c to the logit of the predicted token,
using the linearity of the model’s components described in Section 2.2.

Geva et al. (2022b) exploit the fact that the FFN block update is a linear combination of the rows of
Wout weighted by the neuron activation values (Equation (8)). Thus, it is possible to measure the
DLA of each neuron as:

= nl
Similarly, Ferrando et al. (2023) makes use of the decomposition of an attention head as a weighted
sum of residual stream transformations (Equation (2)) and proposes assessing the DLA of each path
involving the attention head:

WU [:,w],

fw(x)←nl
u

ADLA

uwl

(15)

outu

ADLA

fw(x)

h←−xl−1

j

= al,h

n,jxl−1

j W l,h

OV WU [:,w].

(16)

The Logit difference (LD) (Wang et al., 2023a) is the difference in logits between two tokens,
fw(x) − fo(x). DLA can be extended to measure direct logit difference attribution (DLDA):

ADLDA

fw¬o(x)←c = f c(x)WU [:,w] − f c(x)WU [:,o].
including its neuron and head-specific variants of Equation (15) and Equation (16). Similarly to the
contrastive attribution framework described in Section 3.1, a positive DLDA value suggests that c
promotes token w more than token o.

(17)

3.2.2 CAUSAL INTERVENTIONS

We can view the computations of a Transformer-based LM as a causal model (Geiger et al., 2021;
McGrath et al., 2023), and use causality tools (Pearl, 2009; Vig et al., 2020) to shed light on the
contribution to the prediction of each model component c ∈ C across different positions. The
causal model can be seen as a directed acyclic graph (DAG), where nodes are model computations
and edges are activations. We can intervene in the model by changing some node’s value f c(x)
computed by a model component10 in the forward pass on target input x, to those from another value
˜h, which is referred to as activation patching (Figure 6).11 We can express this intervention using
the do-operator (Pearl, 2009) as f (x|do(f c(x) = ˜h)). We then measure how much the prediction
changes after patching:

f (x)←c = diff(f (x), f (x|do(f c(x) = ˜h))).
Popular choices for the diff(·, ·) function include KL divergence and logit/probability differ-
ence (Zhang & Nanda, 2024). The patched activation (˜h) can be originated from various sources.

APatch

(18)

10Alternatively, we can patch residual stream states f l(x).
11Also referred to in the literature as Causal Mediation Analysis (Vig et al., 2020), Causal Tracing (Meng

et al., 2022), and Interchange Interventions (Geiger et al., 2020; 2021).

9

KeysValuesKeysValues(a)(b)(c)(d)Figure 6: Activation (resample) patching. The FFN output activation from the forward pass with a
source input ˜x (left) is placed in the forward pass with target input x (right), making the prediction
flip from “Italy” to “France”.

A common approach is to create a counterfactual dataset with distribution Ppatch, where some input
signals regarding the task are inverted. This approach leads to two distinct types of ablation:

• Resample intervention12, where the patched activation is obtained from a single example
of Ppatch, i.e. ˜h = f c(˜x), ˜x ∼ Ppatch (Heimersheim & Janiak, 2023; Hanna et al., 2023;
Conmy et al., 2023).

• Mean intervention, where the average of activations of multiple Ppatch examples is used for

patching, i.e. ˜h = E˜x∼Ppatch[f c(˜x)] (Wang et al., 2023a).

Alternatively, other sources of patching activations include:

• Zero intervention, where the activation is substituted by a null vector, i.e. ˜h = 0 (Olsson

et al., 2022; Mohebbi et al., 2023).

• Noise intervention, where the new activation is obtained by running the model on a per-

turbed input, e.g. ˜h = f c(x + ϵ), ϵ ∼ N (0, σ2) (Meng et al., 2022).

An important factor to consider when designing causal interventions experiments is the ecological
validity of the setup, since zero and noise ablation could lead the model away from the natural
activations distribution and ultimately undermine the validity of components’ analysis (Chan et al.,
2022; Zhang & Nanda, 2024).

Following the distinction of Kramár et al. (2024), we note that the activation patching methods
presented above adopt a noising setup, since the patching is performed during the forward pass
with the clean/target input, i.e. f (x|do(f c(x) = ˜h)) (Wang et al., 2023a; Hanna et al., 2023).
Alternatively, the same interventions can be performed in a denoising setup, where the patch ˜h
is taken from the clean/target run and applied over the patched run on source/corrupted input, i.e.
f (˜x|do(f c(˜x) = ˜h)) (Meng et al., 2022; Lieberum et al., 2023). We refer readers to Heimersheim
& Nanda (2024) for a comprehensive overview of metrics, good practices and pitfalls of activation
patching.

Component modeling (Shah et al., 2024) uses an estimator to measure the effect of interventions
on subsets of model components. Specifically, Shah et al. (2024) demonstrate that a linear func-
tion (component attribution) can effectively predict the effects of these interventions, providing a

12Commonly named ablation in the literature. We use the more neutral intervention here since activations

are not actually ablated, but rather replaced.

10

ParisisinRomeisinFranceFranceItalyPatchFigure 7: Left: Distributed Interchange Interventions (subspace activation patching) on a 1-
dimensional subspace (direction) u. Right: Single-layer Transformer. Path patching replaces the
edges of different paths connecting two nodes (sender and receiver) representing model components.
For instance, we can measure the direct effect of the attention head on the output f (x) or the indirect
effect of the attention head on the output f (x) via the FFN.

clear explanation of how the individual components interact to generate predictions. Other forms of
causal interventions use differentiable binary masking on subsets of units or neurons of intermediate
representations (De Cao et al., 2020; Csordás et al., 2021; De Cao et al., 2022), or entire attention
heads outputs (Voita et al., 2019b; Michel et al., 2019) which can be cast as a form of zero ablation.

Subspace Activation Patching.
It is hypothesized that models encode features as linear subspaces
of the representation space (Section 4.2). Geiger et al. (2023b) proposed distributed interchange in-
terventions (DII), which aim to intervene only on these subspaces.13 It provides a tool that allows
for a fine-grained intervention, rather than relying on patching full representations. Formally, as-
suming a model component c takes values in Rd, we seek to find a linear subspace U ⊂ Rd, where
by replacing the orthogonal projection of f c(x) on U with that of f c((cid:101)x) we substitute the feature of
interest present in f c(x) by that in f c((cid:101)x). Following the do-operation notation for the intervention
process, f (x|do(f c(x) = ˜h)), the patched activation is computed as follows:

˜h = f c(x) − f c(x)U ⊺U
(cid:125)

(cid:124)

(cid:123)(cid:122)
projU ⊥ f c(x)

+f c(˜x)U ⊺U

(19)

where U ∈ Rn×d is an orthonormal matrix whose rows form a basis for U . If the feature is encoded
as a direction (Figure 7 left), i.e. in a 1-dimensional subspace, then the patched activation becomes

Patched activation

Target projection
˜h = f c(x) − f c(x)u⊺u
(cid:125)

(cid:124)

(cid:123)(cid:122)
proju⊥ f c(x)

Source projection

+ f c(˜x)u⊺u .

(20)

Target projection subtraction

3.2.3 CIRCUITS ANALYSIS

The Mechanistic Interpretability (MI) subfield focuses on reverse-engineering neural networks into
human-understandable algorithms (Olah, 2022). Recent studies in MI aim to uncover the existence
of circuits, which are a subset of model components (subgraphs) interacting together to solve a
task (Cammarata et al., 2020). Activation patching, logit attribution, and attention pattern analysis
are common techniques for circuit discovery (Wang et al., 2023a; Stolfo et al., 2023a;b; Heimer-
sheim & Janiak, 2023; Geva et al., 2023; Hanna et al., 2023).

Edge and path patching. Activation patching propagates the effect of the intervention throughout
the network by recomputing the activations of components after the patched location (Figure 6). The
changes in the model output (Equation (18)) allow estimating the total effect of the model compo-
nent on the prediction. However, circuit discovery also requires identifying important interactions

13Subspace causal interventions were also used as causal probes by Guerner et al. (2023).

11

between components. For this purpose, edge patching exploits the fact that every model compo-
nent input is the sum of the output of previous components in its residual stream (Section 2.2), and
considers edges directly connecting pairs of model components’ nodes (Figure 7 right). Path patch-
ing generalizes the edge patching approach to multiple edges (Wang et al., 2023a; Goldowsky-Dill
et al., 2023), allowing for a more fine-grained analysis. For example, using the forward pass de-
composition into shallow networks described in Equation (11), we could visualize the single-layer
Transformer of Figure 7 (right) as being composed as

Attn direct path to logits

Attn indirect path to logits via FFN

f (x) = Attn(X≤n) Wu + FFN( Attn(X≤n) + xn)Wu + xnWu,

(21)

where each copy of the sender node AttnL(X L−1
≤n ) is relative to a single path. In this example,
patching separately each of the sender node copies (Goldowsky-Dill et al., 2023) allows us to esti-
mate direct and indirect effects (Pearl, 2001; Vig et al., 2020) of AttnL(X L−1
≤n ) to the output logits
f (x). In general, we can apply path patching to any path in the network and measure composition
between heads, FFNs, or the effects of these components on the logits.

Limitations of circuit analysis with causal interventions. Circuit analysis based on causal inter-
vention methods presents several shortcomings:

1. it demands significant efforts for designing the input templates for the task to evaluate,

along with the counterfactual dataset, i.e. defining Ppatch.

2. isolating important subgraphs after obtaining component importance estimates requires hu-

man inspection and domain knowledge.

3. it has been shown that interventions can produce second-order effects in the behavior of
downstream components (Makelov et al., 2024a, see Wu et al., 2024d for discussion), in
some settings even eliciting compensatory behavior akin to self-repair (McGrath et al.,
2023; Rushing & Nanda, 2024). This phenomenon can make it difficult to draw conclusions
about the role of each component.

Overcoming the limitations.
Conmy et al. (2023) propose an Automatic Circuit Discovery
(ACDC) algorithm to automate the process of circuit identification (Limitation 2) by iteratively
removing edges from the computational graph. However, this process requires a large amount of
forward passes (one per patched element), which becomes impractical when studying large mod-
els (Lieberum et al., 2023). A valid alternative to patching involves gradient-based methods, which
have been extended beyond input attribution to compute the importance of intermediate model com-
ponents (Leino et al., 2018; Shrikumar et al., 2018; Dhamdhere et al., 2019). For instance, given
the token prediction w, to calculate the attribution of an intermediate layer l, denoted as f l(x), the
gradient ∇fw(f l(x)) is computed. Sarti et al. (2023) extend the contrastive gradient attribution
formulation of Equation (13) to locate components contributing to the prediction of the correct con-
tinuation over the wrong one using a single forward and backward pass. Nanda (2023); Syed et al.
(2023) propose Edge Attribution Patching (EAP), consisting of a linear approximation of the pre-
and post-patching prediction difference (Equation (18)) to estimate the importance of each edge in
the computational graph. The key advantage of this method is that it requires two forward passes
and one backward pass to obtain attribution scores of every edge in the graph. Hanna et al. (2024)
propose combining EAP with Integrated Gradients (EAP-IG) and show improved faithfulness of
the extracted circuits, a method also used by Marks et al. (2024) to identify sparse feature circuits.
Further work on Attribution Patching by Kramár et al. (2024) finds two settings leading to false neg-
atives in the linear approximation of activation patching, and proposes AtP∗, a more robust method
preserving a good computational efficiency. Recently, Ferrando & Voita (2024) propose finding
relevant subnetworks, which they name information flow routes, using a patch-free context mixing
approach, requiring only a single forward pass, avoiding the dependence on counterfactual examples
and the risk of self-repair interferences during the analysis.

Causal Abstraction. Another line of research deals with finding interpretable high-level causal
abstractions in lower-level neural networks (Geiger et al., 2021; 2022; 2023a). These methods in-
volve a computationally expensive search and assume high-level variables align with groups of

12

Figure 8: A binary probe trained to predict the input sentiment with positive P and negative N
sentences. Binary linear classifier probes work within a 1-dimensional subspace (direction u) in the
representation space.

units or neurons. To overcome the limitations, Geiger et al. (2023b) propose distributed alignment
search (DAS), which performs distributed interchange interventions (DII, Section 3.2.2) on non-
basis-aligned subspaces of the low-level representation space found via gradient descent.14 DAS
interventions have been shown to be effective in finding features with causal influence in targeted
syntactic evaluation (Arora et al., 2024), and in isolating the causal effect of individual attributes of
entities (Huang et al., 2024a). Recently, learned edits on subspaces of intermediate representations
during the forward pass have been proposed as an efficient and effective alternative to weight-based
Parameter-efficient fine-tuning (PEFT) approaches (Wu et al., 2024b). A DAS variant named Bound-
less DAS has been used to search for interpretable causal structure in large language models (Wu
et al., 2023b). In this context, Causal Proxy Models (CPMs) were proposed as interpretable proxies
trained to mimic the predictions of lower-level models and simulate their counterfactual behavior
after targeted interventions (Wu et al., 2023a).

4

INFORMATION DECODING

Fully understanding a model prediction entails localizing the relevant parts of the model, but also
comprehending what information is being extracted and processed by each of these components.
For example, if the grammatical gender of nouns is assumed to be relevant for the task of corefer-
ence resolution in a given language, information decoding methods could look at whether and how a
model performing this task encodes noun gender. A natural way to approach decoding the informa-
tion in the network is in terms of the features that are represented in it. While there is no universally
agreed-upon definition of a feature, it is typically described as a human-interpretable property of the
input15, which can be also referred to as a concept (Kim et al., 2018).

4.1 PROBING

Probes, introduced concurrently in NLP by Köhn (2015); Gupta et al. (2015) and in computer vision
by Alain & Bengio (2016) serve as tools to analyze the internal representations of neural networks.
Generally, they take the form of supervised models trained to predict input properties from the repre-
sentations, aiming to asses how much information about the property is encoded in them. Formally,
the probing classifier p : f l(x) (cid:55)→ z maps intermediate representations to some input features (la-
bels) z, which can be, for instance, a part-of-speech tag (Belinkov et al., 2017), or semantic and
syntactic information (Peters et al., 2018). For example, for a binary probe seeking to decode the

14Alternatively, Lepori et al. (2023) proposes employing circuit discovery approaches for this purpose.
15Although we have evidence that models learns human-interpretable features even in instances that exceed
human performance (McGrath et al., 2022), Olah (2022) argues that the definition of feature should include
properties that are not human-interpretable.

13

Whatagreatamount of input sentiment information within an intermediate representation (Figure 8) we build
two sets: {f l(x) : x ∈ P} and {f l(x) : x ∈ N }, with the representations obtained when providing
positive and negative sentiment sentences respectively. After training the classifier we evaluate the
accuracy results on a held-out set.

Although performance on the probing task is interpreted as evidence for the amount of information
encoded in the representations, there exists a tension between the ability of the probe to evaluate
the information encoded and the probe learning the task itself (Belinkov, 2022). Several works
propose using baselines to contextualize the performance of a probe. Hewitt & Liang (2019) use
control tasks by randomizing the probing dataset, while Pimentel et al. (2020) propose measuring
the information gain after applying control functions on the internal representations. Voita & Titov
(2020) suggest evaluating the quality of the probe together with the “amount of effort” required to
achieve the quality. This is done by measuring the minimum description length of the code required
to transmit labels z given representations f l(x). We refer the reader to Belinkov & Glass (2019);
Belinkov (2022) for a larger coverage of probing methods.

Probing techniques have been largely applied to analyze Transformers in NLP. Although probes are
still being used to study decoder-only models (CH-Wang et al., 2023; Zou et al., 2023; Burns et al.,
2023; MacDiarmid et al., 2024), a significant portion of the research in this area has focused on
BERT (Devlin et al., 2019) and its variants, leading to several BERTology analyses (Rogers et al.,
2021). Probing has provided evidence of the existence of syntactic information within BERT repre-
sentations (Tenney et al., 2019b; Lin et al., 2019; Liu et al., 2019), from which even full parse trees
can be recovered with good precision (Hewitt & Manning, 2019). Additionally, some studies have
analyzed where syntactic information is stored across the residual stream suggesting a hierarchical
encoding of language information, with part-of-speech, constituents, and dependencies being repre-
sented earlier in the network than semantic roles and coreferents, matching traditional handcrafted
NLP pipelines (Tenney et al., 2019a). Rogers et al. (2021) summarizes results on BERT in detail. Im-
portantly, highly accurate probes indicate a correlation between input representations and labels, but
do not provide evidence that the model is using the encoded information for its predictions (Hupkes
et al., 2018; Belinkov & Glass, 2019; Elazar et al., 2021).

4.2 LINEAR REPRESENTATION HYPOTHESIS AND SPARSE AUTOENCODERS

Linear Representation Hypothesis. The linear representation hypothesis states that features are
encoded as linear subspaces of the representation space (see Park et al. (2023a) for a formal dis-
cussion). Mikolov et al. (2013) were the first to show that Word2Vec word embeddings capture
linear syntactic/semantic word relationships. For example, adding the difference between word
representations of “Madrid” and “Spain”, f (“Madrid”) − f (“Spain”), to the “France” represen-
tation, f (“France”), would result in a vector close to f (“Paris”). This presumes that the vector
f (“Madrid”) − f (“Spain”) can be considered as the direction of the abstract capital_of feature.
Instances of interpretable neurons (Radford et al., 2017; Voita et al., 2023; Bau et al., 2020), i.e.
neurons that fire consistently for specific input features (either monosemantic or polysemantic), also
exemplify features represented as directions in the neuron space. Recent work suggests the linearity
of concepts in representation space is largely driven by the next-word-prediction training objective
and inductive biases in gradient descent optimization (Jiang et al., 2024).

Erasing Features with Linear Interventions Feature directions can be found in LMs using linear
classifiers (linear probes, Section 4.1). These models learn a hyperplane that separates representa-
tions associated with a particular feature from the rest. The normal vector to that hyperplane, the
probe direction u ∈ Rd, can be considered the direction representing the underlying feature (Fig-
ure 8). For instance, the sensitivity of model predictions to a feature can be computed as the direc-
tional derivative of the model in the direction u, ∇f (f l(x)) · u, treating the model as a function
of the intermediate activation (Kim et al., 2018). This linear feature representation was exploited
by Ravfogel et al. (2020; 2022); Belrose et al. (2023b) to erase concepts, preventing linear classi-
fiers from detecting them in the representation space. Linear concept erasure was shown to mitigate
bias (Ravfogel et al., 2020) or induce a large increase in perplexity after removing part-of-speech
information (Belrose et al., 2023b). In presence of class labels, linear erasure models can be adapted
to ensure the removal of all linear information regarding class identity (Singh et al., 2024c; Bel-

14

Figure 9: Left: Feature directions in a 2-dimensional space. (a) features as directions not aligned
with the standard basis, we observe polysemanticity. (b) features aligned with the standard basis,
monosemanticity. (c) more features than dimensions (superposition), hence features can’t align with
the standard basis and polysemanticity is inevitable. Right: Sparse autoencoder (SAE) trained to
reconstruct a model’s internal representations z. Interpretable SAE features are found in rows of
Wdec. Biases are omitted for the sake of clarity.

rose, 2023). Finally, Elazar et al. (2021) exploits linear erasure to address the correlational nature of
probing classifier, validating the influence of probed properties on model predictions.

Steering Generation with Linear Interventions As mentioned in Section 4.1, a fundamental
problem of probing lies in its correlational, rather than causal, nature. Recent work (Nanda et al.,
2023b; Zou et al., 2023) shows the effectiveness of linear interventions on language models using
directions identified by a probe. For instance, adding negative multiples of the sentiment direction
(u) to the residual stream, i.e. xl′
← xl − αu, is sufficient to generate a text matching the opposite
sentiment label (Tigges et al., 2023). This simple procedure is named activation addition (Turner
et al., 2023). Other unsupervised methods for computing feature directions include Principal Com-
ponent Analysis (Tigges et al., 2023), K-Means (Zou et al., 2023), or difference-in-means (Marks &
Tegmark, 2023). For instance, Arditi et al. (2024) use the difference-in-means vector between resid-
ual streams on harmful and harmless instructions to find a “refusal direction” (Zheng et al., 2024)
in LMs with safety fine-tuning (Bai et al., 2022). Projecting out this direction from every model
component output, i.e. f c′
← f c − f cu⊺u, leads to bypass refusal. Recent studies set distributed
alignment search (Section 3.2.3) as the best performing method for causal intervention across math-
ematical reasoning and linguistic plausibility benchmarks (Tigges et al., 2023; Arora et al., 2024;
Huang et al., 2024a), and leveraged it for efficient inference-time interventions aimed at improv-
ing task-specific model performance (Wu et al., 2024b). Finally, the MiMic framework (Singh
et al., 2024c) was recently proposed to craft optimal steering vectors, exploiting insights from linear
erasure methods and class labels from the data distribution. We note that the effectiveness of steer-
ing approaches involving linear interventions was recently observed to extend to non-Transformer
LMs (Paulo et al., 2024).

Polysemanticity and Superposition. A representation produced by a model layer is a vector that
lies in a d-dimensional space. Neurons are the special subset of representation units right after
an element-wise non-linearity (Section 2.1.3). Although previous work has identified neurons in
models corresponding to interpretable features, in most cases they respond to apparently unrelated
inputs, i.e. they are polysemantic. Two main reasons can explain polysemanticity. Firstly, features
can be represented as linear combinations of the standard basis vectors of the neuron space (Figure 9
left (a)), not corresponding to the basis elements themselves. Therefore, each feature is represented
across many individual neurons, which is known as distributed representations (Smolensky, 1986;
Olah, 2023). Secondly, given the extensive capabilities and long-tail knowledge demonstrated by
large language models, it has been hypothesized that models could encode more features than they
have dimensions, a phenomenon called superposition (Figure 9 left (c)) (Arora et al., 2018; Olah
et al., 2020b). Elhage et al. (2022b) showed on toy models trained on synthetic datasets that super-
position happens when forcing sparsity on features, i.e. making them less frequent on the training
data. Recently, Gurnee et al. (2023) have provided evidence of superposition in the early layers of
a Transformer language model, using sparse linear probes.

Sparse Autoencoders (SAEs). A possible strategy to disentangle features in superposition in-
volves finding an overcomplete feature basis via dictionary learning (Olshausen & Field, 1997).
Autoencoders with sparsity regularization, also known as sparse autoencoders (SAEs), can be used

15

Dictionary(a)(b)(c)SAEfor dictionary learning by optimizing them to reconstruct internal representations z ∈ Rd of a neural
network exhibiting superposition while simultaneously promoting feature sparsity. That is, we ob-
tain a reconstruction z = SAE(z) + ϵ, where ϵ is the SAE error term. Sharkey et al. (2022); Bricken
et al. (2023); Cunningham et al. (2023) propose training SAEs (see Figure 9 right) of the form

SAE feature activations h(z)
SAE(z) = ReLU(cid:0)(z − bdec)Wenc + benc)(cid:1) Wdec + bdec

Dictionary SAE features

(22)

on language models’ representations with a loss defined as

Reconstruction loss term

Sparsity loss term

L(z) = ∥z − SAE(z)∥2

2 + α ∥h(z)∥1 .

(23)

of

on

the

latent

sparsity

inducing

representation

SAE feature

activations
By
h(z) = ReLU(zWenc + b) ∈ Rm and setting m > d, we can approximate z as a sparse lin-
ear combination of the rows of the learned Wdec ∈ Rm×d dictionary, from which we can extract
interpretable and monosemantic SAE features.16 Since the output weights of each SAE feature
interact linearly with the residual stream, we can measure their direct effect on the logits (Sec-
tion 3.2.1) and their composition with later layers’ components (Section 2.2) (He et al., 2024).
An initial assessment of reconstruction errors (ϵ) in SAEs trained on LM activations highlighted
their systematic nature, driving a shift in next token prediction probabilities much higher than
random noise (Gurnee, 2024). Marks et al. (2024) also found these errors account for 1–15% of z
variance. While this finding might undermine the faithfulness of component analyses relying on
SAE features, Marks et al. (2024) proposes an adaptation of the causal model framework outlined
in Section 3.2.2 aiming to incorporate SAE features and errors as nodes of the computational graph.
Using edge attribution patching (Section 3.2.3), they recover sparse feature circuits providing more
intuitive overviews of features driving model predictions.

SAEs Evaluation. The goal of SAEs is to learn sparse reconstructions of representations. To
assess the quality of a trained SAE in achieving this it is common to compute the Pareto frontier of
two metrics on an evaluation set (Bricken et al., 2023). These metrics are:

• The L0 norm of the feature activations vector h(z), which measures how many features are
“alive” given an input. This metric is averaged across the evaluation set, Ez∼D ∥h(z)∥0.
• The loss recovered, which reflects the percentage of the original cross-entropy loss of the
LM across a dataset when substituting the original representations with the SAE recon-
structions.

A summary statistic proposed by Bricken et al. (2023) is the feature density histogram. Feature
density is the proportion of tokens in a dataset where a SAE feature has a non-zero value. By
looking at the distribution of feature densities we can distinguish if the SAE learnt features that are
too dense (activate too often) or too sparse (activate too rarely). Finally, the degree of interpretability
of sparse features can be estimated based on their direct logit attribution and maximally activating
examples (see Figure 10 left, we introduce these concepts in Section 4.3). This process can be done
manually or automated, using a LLM to produce natural language explanations of SAE features.
Although recent studies have compared various SAE architectures (Makelov et al., 2024b; Karvonen
et al., 2024), developing robust evaluation frameworks to compare between architectures remains a
critical area for future research.

Gated SAEs (GSAEs). The sparsity penalty used in SAE training promotes smaller feature acti-
vations, biasing the reconstruction process towards smaller norms. This phenomenon is known as
shrinkage (Tibshirani, 1996; Wright & Sharkey, 2024). Rajamanoharan et al. (2024a) address this
issue by proposing Gated Sparse Autoencoders (GSAEs) and a complementary loss function. GSAE
is inspired by Gated Linear Units (Dauphin et al., 2017; Shazeer, 2020), which employ a gated ReLU
encoder to decouple feature magnitude estimation from feature detection (Figure 10 right):

16We present in Appendix D some SAEs’ implementation details currently debated.

16

Figure 10: Left: SAE feature visualization on Neuronpedia (Lin & Bloom, 2024). It shows the
promoted/suppressed tokens, feature density, logits distribution, and maximally activating examples
of a name mover feature found in GPT-2 Small (Kissane et al., 2024b). Right: Gated Sparse Au-
toencoder with encoder weight sharing. Biases are omitted for the sake of clarity.

GSAE features’ gate

GSAE feature activations’ magnitude

GSAE(z) = 1[((z − bdec)Wgate + bgate) > 0] ⊙ ReLU(cid:0)(z − bdec)Wmag + bmag

(cid:1)

Wdec + bdec,

(cid:124)

(cid:123)(cid:122)
h(z)

(cid:125)

(24)
where 1 is the step function. The features’ gate and activation magnitudes are computed by sharing
weight matrices, Wmag[i,j] = Wgate[i,j]er[j] , being r ∈ Rm a learned rescaling vector, thus Wgate
can be considered the encoder matrix Wenc (Figure 10 right). Rajamanoharan et al. (2024a) show
GSAE is a Pareto improvement over the standard SAE architecture on a range of models, scaling
GSAEs up to Gemma 7B (Gemma Team et al., 2024).

TopK SAEs. An alternative approach designed to avoid the L1 penalty is proposed by Gao et al.
(2024), who use k-sparse autoencoders (Makhzani & Frey, 2014), a variant that only keeps the k
largest features

TopK-SAE(z) = TopK(cid:0)(z − bdec)Wenc

(cid:1)Wdec + bdec,

(25)

and simplifies the loss to consider only the reconstruction error term (see Equation (23)). TopK SAEs
empirically outperform standard ReLU autoencoders (Equation (22)) on the sparsity-reconstruction
frontier while increasing monosemanticity. TopK-SAEs constrain each token to use exactly k fea-
tures. To introduce flexibility in per-sample feature activation, (Bussmann et al., 2024) propose
BatchTopK-SAEs, which apply the TopK operation across the flattened batch instead.

JumpReLU SAEs.
Rajamanoharan et al. (2024b) propose a modification to the standard SAE
architecture (Equation (22)). Specifically, they substitute the ReLU activation function by the
JumpReLU (Erichson et al., 2019), JumpReLUθ(z) = z ⊙ H(z − θ), where H is the step function,
and θ, a learnable vector acting as a threshold. Intuitively, the output value is zero below the thresh-
old, and the identity afterwards, with a discontinuous jump at the threshold. The threshold θ enables
JumpReLU SAEs to decouple feature activation decisions from the estimation of active feature
magnitudes. Additionally, JumpReLUs are trained using the L0 sparsity penalty. Notably, Lieberum
et al. (2024) recently released a set of pre-trained autoencoders on different sites of Gemma 2 mod-
els (Team et al., 2024).

4.3 DECODING IN VOCABULARY SPACE

The model engages with the vocabulary in two primary ways: firstly, through a set of input tokens
facilitated by the embedding matrix WE, and secondly, by interacting with the output space via the
unembedding matrix WU . Hence, and due to its interpretable nature, a sensible way to approach
decoding the information within models’ representations is via vocabulary tokens.

Decoding intermediate representations. The logit lens (nostalgebraist, 2020) proposes project-
ing intermediate residual stream states xl by WU . The logit lens can also be interpreted as the
prediction the model would do if skipping all later layers, and can be used to analyze how the model
refines the prediction throughout the forward pass (Jastrz˛ebski et al., 2018). This technique has
also proven effective in analyzing encoder representations in encoder-decoder models Langedijk

17

DictionaryGSAE𝟙et al. (2023). However, the logit lens can fail to elicit plausible predictions in some particular mod-
els (Belrose et al., 2023a). This phenomenon have inspired researchers to train translators, which
are functions applied to the intermediate representations prior to the unembedding projection. Din
et al. (2023) suggest using linear mappings, while Belrose et al. (2023a) propose affine transforma-
tions (tuned lens). Translators have also been trained on the outputs of attention heads, resulting in
the attention lens (Sakarvadia et al., 2023). More generally, we can also think of WU as the weights
learned by a probe whose classes are the subwords in the vocabulary (Section 4.1), and inspect at
any point in the network the amount of information encoded about any subword.

Patchscopes. Patchscopes (Ghandeharioun et al., 2024) is a framework that generalizes patching
to decode information from intermediate representations.17 Recall from Section 3.2.2 that patching
an activation into a forward pass f (x|do(f c(x) = ˜h)) serves to evaluate the output change with
respect to the original clean run f (x). Patchscope defines a function acting on the patched repre-
sentation m(˜h), a target model f ∗ for the patched run, which can differ from the original f , a target
prompt x∗, and a target model component c∗ that can be at a different position and layer. It then
evaluates f ∗(x∗|do(f c(x)∗ = m(˜h))), either by inspecting the output logits, probabilities, or gen-
erating from it a natural language explanation. The election of f ∗, m, x∗, and c∗ defines the type
of information to extract from ˜h independently of the original context, allowing a higher expressiv-
ity. For instance, the future lens (Pal et al., 2023), used to decode future tokens from intermediate
representations can be considered as a patchscope where f ∗ = f , c∗ = c and x∗ is a learned prompt.

Decoding model weights. As seen in previous sections, W h
OV , Wout, Win and WQK interact
linearly with the residual streams. Dar et al. (2023) suggest analyzing matrix weights in vocabulary
space by projecting them by WU , and find that some weight matrices interact with tokens with
related semantic meanings. Millidge & Black (2022) propose to factorize these matrices via the
singular value decomposition (SVD). In the “thin” SVD, a matrix is factorized as W = U ΣV ⊺,
with U ∈ Rd×r, Σ ∈ Rr×r, V ⊺ ∈ Rr×d, and r = rk(W ), the rank of W . The largest right singular
vectors (rows of V ⊺)18 represent the directions along which a linear transformation stretches the
most. Then, multiplying z by W (Figure 11 left) can be expressed as

zW = (zU Σ)V ⊺ =

r
(cid:88)

(zuiσi)v

⊺
i .

i=1

(26)

where ui ∈ Rd×1 can be seen as a key that is compared to the query (z) via dot product, weighting
⊺
the right singular vector v
i (McDougall, 2023; Molina, 2023), similar to Equation (8). By projecting
⊺
the top right singular vectors onto the vocabulary space via the unembedding matrix (v
i WU ) we
reveal the tokens the matrix primarily interacts with (Figure 11 right). We can instead use the SVD
⊺
to find a low-rank approximation (cid:99)W (k) = (cid:80)k
i , where rk( (cid:99)W (k)) = k < r, and study
the model predictions by substituting the original matrix by (cid:99)W (k) (Sharma et al., 2024b). Katz
et al. (2024) propose extending the projection of weight matrices (Dar et al., 2023) to the backward
pass. Specifically, the backward lens projects the gradient matrices of the FFNs to study how new
information is stored in their weights.

i=1(uiσi)v

⊺
u,1, . . . , V

⊺
u,N }, where V

Logit spectroscopy. Cancedda (2024) proposes an extension of the logit lens, the logit spec-
troscopy, which allows a fine-grained decoding of the information of internal representations via
the unembedding matrix (Wu). Logit spectroscopy considers splitting the right singular matrix of
⊺
⊺
Wu into N bands: {V
u,1 and V
u,N each contain a set of singular vectors,
the former associated with the largest singular values and the latter with the lowest. If we consider
the concatenation of matrices associated with different bands, e.g. from the j-th to the k-th band,
⊺
we form a matrix V
u,j:k whose rows span a linear subspace of the vocabulary space. We can use the
⊺
operator Φu,j:k = Vu,j:kV
u,j:k to evaluate the orthogonal projection zΦu,j:k of representations z
onto different subspaces. Alternatively, we can suppress the projection from the representation, i.e.
z′ ← z − zΦu,j:k, leaving its orthogonal component with respect to the subspace. Similarly, bands
of singular vectors of the embedding matrix can be considered in the analysis.

17A concurrent similar approach is presented by Chen et al. (2024c).
18Note that we left multiply by W .

18

Figure 11: Left: Multiplication of an internal representation z by the SVD decomposition of a
⊺
matrix W . Right: The top right singular vector v
1 represents the direction along which the trans-
formation stretches the most, revealing the tokens the matrix primary interacts with when projecting
onto the vocabulary space. The input representation z and the associated left singular vector u1 act
as a query and a key respectively, being v

⊺
1 the value associated with the key.

Maximally-activating inputs. The features encoded in model neurons or representation units
have been largely studied by considering the inputs that maximally activate them (Zhou et al., 2015;
Zeiler & Fergus, 2014).
In image models this can be done either by generating synthesized in-
puts (Nguyen et al., 2016), e.g. via gradient descent (Simonyan et al., 2014), or by selecting exam-
ples from an existing dataset. The latter approach has been used in language models to explain the
features that units (Dalvi et al., 2019) and neurons (Nanda, 2022b) respond to. However, Bolukbasi
et al. (2021) warn that just relying on maximum activating dataset examples can result in “inter-
pretability illusions”, as different activation ranges may lead to varying interpretations. Maximally-
activating inputs can produce out-of-distribution behaviors, and were recently employed to craft
jailbreak attacks aimed at eliciting unacceptable model predictions (Chowdhury et al., 2024), for
example by crafting maximally-inappropriate inputs for red-teaming purposes (Wichers et al., 2024).

Natural language explanations from LMs. Modern LMs can be prompted to provide plausible-
sounding justifications for their own or other LMs’ predictions. This can be seen as an edge case
of information decoding in which the predictor itself is used as a zero-shot explainer. A notable
example is the work by Bills et al. (2023) where GPT-4 is prompted to describe shared features
in sets of examples producing high activations for specific neurons across GPT-2 XL. Subsequent
work by Huang et al. (2023) shows that neurons identified by Bills et al. (2023) do not have a causal
influence over the concepts highlighted in the generated explanation, underscoring a lack of faith-
fulness in such approach. Additional investigations in the consistency between input attribution and
self-explanations in language models highlighted the tendency of LMs to produce explanations that
are very plausible according to human intuition, but unfaithful to model inner workings (Atanasova
et al., 2023; Parcalabescu & Frank, 2023; Turpin et al., 2023; Lanham et al., 2023; Madsen et al.,
2024; Agarwal et al., 2024).

5 DISCOVERED INNER BEHAVIORS

The techniques presented in Sections 3 and 4 have equipped us with essential tools to understand
the behavior of language models. In the following sections, we provide an overview of the internal
mechanisms that have been discovered within Transformer LMs.

5.1 ATTENTION BLOCK

As seen in Section 2.1.2, each attention head consists of a QK (query-key) circuit and an OV (output-
value) circuit. The QK circuit computes the attention weights, determining the positions that need to
be attended, while the OV circuit moves (and transforms) the information from the attended position
into the current residual stream. A substantial body of research has been dedicated to analyzing
attention weights patterns formed by QK circuits (Clark et al., 2019; Kovaleva et al., 2019; Voita
et al., 2019b), fueling a debate on whether these weights serve as explanations (Bibal et al., 2022).
However, our understanding of the specific features encoded in the subspaces employed by circuit
operations is still limited. Here, we categorize known behavior of attention heads in two groups:
those having intelligible attention patterns, and those with meaningful QK and OV circuits.

19

Promoted tokens=GermanySpainAustraliaFigure 12: Left: Induction mechanism. An early previous token head writes information of A into
B’s residual stream via WOV . This information (shown in red) gets read by the WQK matrix of a
downstream induction head (K-composition), which serves it to attend to B and copy its information,
increasing the likelihood of B for the next token prediction. Right: Copy suppression mechanism.
The copy suppression head detects that a token in the context (A) is being confidently predicted at the
current residual stream, for instance, thanks to a previous copying head (shown in red). The copy
suppression head attends to it and suppresses its prediction, improving model calibration. Other
components are hidden for the sake of clarity.

5.1.1 ATTENTION HEADS WITH INTERPRETABLE ATTENTION WEIGHTS PATTERNS

Positional heads. Clark et al. (2019) showed some BERT heads attend mostly to specific positions
relative to the token processed. Specifically, attention heads that attend to the token itself, to the pre-
vious token, or to the next position. A similar pattern is also observed in encoders of neural machine
translation models (Voita et al., 2019b; Raganato & Tiedemann, 2018). Previous token heads are an
essential part of induction heads, and have been shown necessary for circuits in GPT2-Small (Wang
et al., 2023a). Their main role has been associated with copying previous token information to the
following residual stream, such as concatenating two-tokens names (Nanda et al., 2023c). Ferrando
& Voita (2024) show previous token heads are important across several textual domains.

Subword joiner heads. First discovered in machine translation encoders Correia et al. (2019),
subword joiner heads have been observed as well in large language models (Ferrando & Voita,
2024). These heads attend exclusively to previous tokens that are subwords belonging to the same
word as the currently processed token.

Syntactic heads. Some attention heads attend to tokens having syntactic roles with respect to
the processed token significantly more than a random baseline (Clark et al., 2019; Htut et al., 2019).
Particularly, certain heads specialize in given dependency relation types such as obj, nsubj, advmod,
and amod. Chen et al. (2024a) show these heads appear suddenly during the training process of
masked language models playing a crucial role in the subsequent development of linguistic abilities.

Duplicate token heads. Duplicate token heads attend to previous occurrences of the same token in
the context of the current token. Wang et al. (2023a) hypothesize that, in the IOI task (Section 5.4),
these heads copy the position of the previous occurrence to the current position.

5.1.2 ATTENTION HEADS WITH INTERPRETABLE QK AND OV CIRCUITS

Copying heads. Several attention heads in Transformer LMs have OV matrices that exhibit copy-
ing behavior. Elhage et al. (2021a) propose using the number of positive real eigenvalues of the
full OV circuit matrix WEWOV WU as a summary statistic for detecting copying heads. Positive
eigenvalues mean that there exists a linear combination of tokens contributing to an increase in the
linear combination of logits of the same tokens.

Induction heads. An induction mechanism (Figure 12 left) that allows language models to com-
plete patterns was discovered first by Elhage et al. (2021a) and further studied by Olsson et al.

20

+CopyingheadInductionheadCopy suppressionheadPrevious tokenhead-+(2022).19 This mechanism involves two heads in different layers composing together. Specifically, a
previous token head (PTH) and an induction head. The induction mechanism learns to increase the
likelihood of token B given the sequence A B ... A, irrespective of what A and B are. To do so, a PTH in
an early layer copies information from the first instance of token A to the residual stream of B, specif-
ically by writing in the subspace the QK circuit of the induction head reads from (K-composition).
This makes the induction head at the last position to attend to token B, and subsequently, its copying
OV circuit increases the logit score of B. Olsson et al. (2022) demonstrate that the OV and QK cir-
cuits of the induction head can perform fuzzy versions of copying and prefix matching, giving rise
to generating patterns of the kind A∗ B∗ ... A → B, where A and A∗, and B and B∗ are semantically
related (e.g. the same words in different languages). Overall, induction heads have been shown to
appear broadly in Transformer LMs (Nanda, 2022a), with those operating at an n-gram level being
identified as important drivers of in-context learning (Akyürek et al., 2024). Recent work showed
that these heads display both complementary and redundant behaviors, likely shaped by competi-
tive dynamics during optimization (see Section 5.4) (Singh et al., 2024a). Relatedly, redundancy
was also observed in the connections between early-layer PTHs and subsequent induction heads.
Finally, the emergence rate of induction heads is impacted by the diversity of in-context tokens,
with higher diversity in attended and copied tokens delaying the formation of the two respective
sub-mechanisms (Singh et al., 2024a).

Copy suppression heads. Copy suppression heads, discovered in GPT2-Small (McDougall et al.,
2023) reduce the logit score of the token they attend to, only if it appears in the context and the
current residual stream is confidently predicting it (Figure 12 right). This mechanism was shown to
improve overall model calibration by avoiding naive copying in many contexts (e.g. copying “love”
in “All’s fair in love and ___”). The OV circuit of a copy suppression head can copy-suppress
almost all of the tokens in the model’s vocabulary when attended to. This behavior is confirmed
by analyzing the “effective QK circuit” of GPT2-Small. The key input is the FFN1 output of every
token, and the query input the unembedding of any token, WU WQKFFN1(WE), and shows the
diagonal elements rank higher. Copy suppression is also linked to the self-repair mechanism since
ablating an essential component deactivates the suppression behavior, compensating for the ablation.

Successor heads. Given an input token belonging to an element in an ordinal sequence (e.g. “one”,
“Monday”, or “January”), the ‘effective OV circuit’: FFN1(WE)WOV WU of the successor heads
increases the logits of tokens corresponding to the next elements in the sequence (e.g. “two”, “Tues-
day”, “February”). Specifically, Gould et al. (2024) show the output of the first FFN block represents
a common ‘numerical structure’ on which the successor head acts. Gould et al. (2024) find these
heads in Pythia (Biderman et al., 2023), GPT2 (Radford et al., 2019) and Llama 2 (Touvron et al.,
2023) models.

5.1.3 OTHER NOTEWORTHY ATTENTION PROPERTIES

Domain specialization. The attention heads previously described serve specific functions in aid-
ing the model to predict the next token. However, the degree of specialization of components across
different domains and tasks remains unclear. Ferrando & Voita (2024); Chughtai et al. (2024); Lv
et al. (2024) identify some specialized heads that contribute only within specific input domains,
such as non-English contexts, coding sequences, or specific topics. An analysis of the top singular
vectors of their OV matrices (Section 4.3) reveal these heads mainly promote tokens related to the
semantics of the input they participate in.

Attention sinks. Early investigations into BERT (Kovaleva et al., 2019) revealed most attention
heads exhibit “vertical” attention patterns, mainly focusing on special (CLS, SEP) and punctuation
tokens. Clark et al. (2019); Vig & Belinkov (2019) hypothesized a head may attend to special tokens
when its specialized function is not applicable (no-op hypothesis). Kobayashi et al. (2020) showed
the norm of the value vectors (Section 3.1) associated with special tokens, periods, and commas tend
to be small, canceling out the effect of large attention weights, thereby supporting the no-op hypoth-
esis. Furthermore, it was shown that attention to the end-of-sequence token in MT models is used to
ignore the contribution of the source sentence (Ferrando & Costa-jussà, 2021), useful when predict-
ing some function words such as the particle “off” in “She turned off the lights.”. In auto-regressive

19We follow the mechanistic formulation by Elhage et al. (2021a). See (Variengien, 2023) for a discussion.

21

Figure 13: Attention sink mechanism. An attention head at a token C attends to BOS token. Its OV
circuit squeezes the BOS residual stream representation resulting in a negligible update, leaving the
residual stream of C unchanged. Cancedda (2024) suggests that early FFNs in Llama 2 write into a
“dark subspace” (in red) in the BOS residual stream that allows later heads to exploit this behavior.
Gurnee et al. (2024) find specific neurons in previous layer FFNs of GPT-2 that control the extent to
which the query attends to BOS (in orange).

LMs, these patterns are observed mainly in the beginning of sentence (BOS) token (Figure 13), al-
though other tokens play the same role (Ferrando & Voita, 2024). According to Xiao et al. (2023),
allowing attention mass on the BOS token is necessary for streaming generation, and performance
degrades when the BOS is omitted. Using the logit spectroscopy (Section 4.3), Cancedda (2024)
finds that early FFNs in Llama 2 write relevant information (for the attention sink mechanism to
occur in later layers) into the residual stream of BOS. These FFNs write into the linear subspace
spanned by the right singular vectors with the lowest associated singular values of the unembedding
matrix. Cancedda (2024) refers to this as a dark subspace due to its low interference with next to-
ken prediction, and finds a significant correlation between the average attention received by a token
and the existence of these dark signals in its residual stream. These dark signals reveal as massive
activation values acting as fixed biases (Sun et al., 2024), a crucial prerequisite for the attention sink
mechanism to take place (Puccetti et al., 2022; Bondarenko et al., 2023). On the other hand, specific
neurons in the FFN of the layer before the attention head have been found to control the amount to
which the tokens attend to BOS (Gurnee et al., 2024) (Figure 13).

Features in attention heads. Sparse autoencoders have been trained on the outputs of the atten-
tion layers to better understand the features computed by each head. Results presented in Kissane
et al. (2024a) show that, on a two-layer Transformer, a large number of features (76%) are non-
dead, with the majority of them being interpretable (82%). Three specific features are studied in
detail. The “board by induction” feature promotes the token board, and is present on the output of
an induction head (Section 5.1.2), being part of the induction features family. The “in questions
starting with Which” feature is instead part of the local context features, promoting the prediction
of ? when Which appears in the context. Lastly, “in texts related to pets” is an example of an high-
level context feature that activates for almost the entire context, with its related head attending to
pet-related context tokens. Notably, Kissane et al. (2024a) detect the presence of non-induction
features in the output of induction-heads, providing evidence of attention head polysemanticity,
initially observed by Heimersheim & Janiak (2023). Further investigations (Kissane et al., 2024b)
reveal the same three feature families appear on GPT2-Small, as well as successor features, name
mover features, suppression features and duplicate token features associated with heads matching
their respective behaviors. Krzyzanowski et al. (2024) conduct a finer-grained analysis of features
in GPT-2 Small attention heads, focusing on the top 10 features in each head and concluding that
most heads do multiple tasks, with only around 10% of those being monosemantic. Their findings
point out that early layers (0-3) mainly focus on shallow syntactic features, with the following layers
encoding increasingly more complex syntactic features. Middle layers (5-6) contain the least inter-
pretable features, while later layers (7-10) encode complex abstract features like time and distance
relationships and high-level context concepts. The heads in the last attention block show mostly
grammatical adjustments and bigram completions.

22

''No-op'' updateAttentiondeactivation neurons5.2 FEEDFORWARD NETWORK BLOCK

The dimensions in the FFN activation space (neurons), following the non-linearity, are more likely
to be independently meaningful (Section 2.1.3), and have therefore been the object of study of recent
interpretability works.

Neuron’s input behavior. The behavior of neurons in language models has been extensively stud-
ied, with examinations focusing on either their input or output behavior. In the context of input
behavior analysis, Voita et al. (2023) show neurons firing exclusively on specific position ranges.
Other discoveries include skills neurons, whose activations are correlated with the task of the input
prompt (Wang et al., 2022), concept-specific neurons (Suau et al., 2020; 2022; Gurnee et al., 2023)
whose response can be used to predict the presence of a concept in the provided context, such as
whether it is Python code, French (Gurnee et al., 2023), or German (Quirke et al., 2023) language.
Neurons responding to other linguistic and grammatical features have also been found (Bau et al.,
2019; Durrani et al., 2023).

Neuron’s output behavior. Regarding the output behavior of neurons, Dai et al. (2022) use the
Integrated Gradients method (Section 3.1) to attribute next-word facts predictions to FFNs neurons,
finding knowledge neurons. The key-value memory perspective of FFNs (Section 2.1.3) offers a
way to understand neuron’s weights. Specifically, using the direct logit attribution method (Sec-
tion 3.2.1) we can measure the neuron’s effect on the logits. Geva et al. (2022b) show that some
neurons promote the prediction of tokens associated with particular semantic and syntactic con-
cepts. Ferrando et al. (2023) illustrate that a small set of neurons in later layers is responsible for
making linguistically acceptable predictions, such as predicting the correct number of the verb, in
agreement with the subject. Gurnee & Tegmark (2024) find neurons that interact with directions in
the residual stream that are similar to the space and time feature directions extracted from probes.
Tang et al. (2024) show language-specific neurons are key for multilingual generation, demonstrat-
ing one can steer the model output’s language by causally intervening on them. Stolfo et al. (2024)
discover token frequency neurons, which increase or suppress the token’s logit proportionally to
its log frequency, shifting the output distribution towards or away from the unigram distribution.
Finally, neurons suppressing improbable continuations, e.g. the repetition of the last token in the
sequence, have recently been identified (Voita et al., 2023; Gurnee et al., 2024).

Polysemantic neurons. Recent work highlighted the presence of polysemantic neurons within
language models. Notably, most early layer neurons specialize in sets of n-grams, functioning as
n-gram detectors (Voita et al., 2023), with the majority of neurons firing on a large number of
n-grams. Gurnee et al. (2023) suggest superposition appears in these early layers, and via sparse
probing they find sparse combinations of neurons whose added activation values disentangle the
detection of specific n-grams, such as the compound word “social security” from other bigrams
containing only one of the two terms. Even though polysemanticity and superposition arise in early
layers, several dead neurons were observed in OPT models20 (Voita et al., 2023). Furthermore,
Elhage et al. (2022a) hypothesize models internally perform “de-/re-tokenization”, where neurons
in early layers respond to multi-token words or compound words (Elhage et al., 2022a), mapping
tokens to a more semantically meaningful representation (detokenization). In contrast, in the latest
layers, neurons aggregate contextual representations back into single tokens (re-tokenization) to
produce the next-token prediction.

Universality of neurons. Whether different models learn similar features remains an open ques-
tion (Olah et al., 2020b). For instance, various computer vision models were found to learn Gabor
filters in early layers (Olah et al., 2020a). In a recent study, Gurnee et al. (2024) investigated whether
neurons respond to features similarly across different models (Bau et al., 2019). Their analysis used
the pairwise correlation of neuron activations across GPT2 models trained from different random
initializations as a proxy measure, revealing a subset of 1-5% of neurons activating on the same
inputs. As expected, within the cluster of universal neurons there is a higher degree of monose-
manticity. This group includes alphabet neurons, which activate in response to tokens representing
individual letters and on tokens that start with the letter, supporting the re-tokenization hypothe-
sis. Additionally, there are previous token neurons that fire based on the preceding token, as well

20OPT models use ReLU activation functions, allowing for zero activation values (Zhang et al., 2022).

23

Figure 14: Example of the local context “President” feature in a Sparse Autoencoder (SAE) trained
to reconstruct the second layer residual stream of GPT2-Small (Bloom, 2024).

as unigram, position, semantic, and syntax neurons. In terms of output behavior, universal neu-
rons include attention (de-)activation neurons, responsible for controlling the amount of attention
given to the BOS token by a subsequent attention head, and thus setting it as a no-op (Section 5.1.3).
Lastly, Gurnee et al. (2024) hypothesize that some neurons act as entropy neurons, modulating the
model’s uncertainty over the next token prediction. Stolfo et al. (2024) later explained how entropy
neurons regulate their confidence by writing to an effective null space of the unembedding matrix
and leveraging the layer normalization.

High-level structure of the role of neurons.
It has been suggested that the overall arrangement
of neurons in language models mirrors that of neuroscience (Elhage et al., 2022a). Early layer
neurons exhibit similarities to sensory neurons, responding to shallow patterns of the input, mostly
focusing on n-grams. Moving into the middle layers, activation tends to occur around more high-
level concepts (Bricken et al., 2023; Gurnee et al., 2023). An example of this is the neuron identified
in Elhage et al. (2022a), which represents numbers only when they refer to the amount of people.
Finally, later layers’ neurons bear a resemblance to motor neurons in the sense that they produce
changes in the distribution of the next-token prediction, either by promoting or suppressing sets of
tokens.

Features in Feedforward Networks. SAEs are able to identify significantly more interpretable
features than the model’s neurons themselves (Bricken et al., 2023), as noted both by human and
automated analyses in one-layer transformers. The features detected by SAEs trained to reconstruct
FFN activations (Bricken et al., 2023) appear to split into increasingly more fine-grained distinctions
of the feature as more dimensions (dictionary entries) are added, demonstrating that 512 neurons can
encode tens of thousands of features. Examples of features found by Bricken et al. (2023) include
those firing in the presence of Arabic or Hebrew scripts and promoting tokens in those scripts, and
features responding to DNA sequences or base64 strings.

5.3 RESIDUAL STREAM

We can think of the residual stream as the main communication channel in a Transformer. The “di-
rect path” (Section 2.2) connecting the input embedding with the unembedding matrix, xWU does
not move information between positions, and mainly models bigram statistics (Elhage et al., 2021a),
while the latest biases in the network, localized in the prediction head, are shown to shift predictions
according to word frequency, promoting high-frequency tokens (Kobayashi et al., 2023). However,
alternative paths involve the interaction between components, which write into linear subspaces (El-
hage et al., 2021a) that can be read by downstream components, or directly by the prediction head,
potentially doing more complex computations. Heimersheim & Turner (2023) observed that the
norm of the residual stream grows exponentially along the layers over the forward pass of multi-
ple Transformer LMs (Millidge & Winsor, 2023; Merrill et al., 2021). A similar growth rate appears
in the norm of the output matrices writing into the residual stream, WO and Wout, unlike input

24

leadershipofPresidentDictionarySAEPromoted tokensObamaBushDonaldmatrices (WQ, WK, WV and Win), which maintain constant norms along the layers. It is hypothe-
sized that some components perform memory management to remove information stored in the
residual stream. For instance, there are attention heads with OV matrices with negative eigenval-
ues attending to the current position, and FFN neurons whose input and output weights have large
negative cosine similarity (Elhage et al., 2021a), meaning that they write a vector (FFN value) on
the opposite direction to the direction they read from (FFN key). Notably, Gurnee et al. (2024) find
that these neurons activate very frequently. Dao et al. (2023) evaluate a small Transformer LM and
provide convincing evidence of multiple attention heads removing the information written by a first
layer head.

Outlier dimensions (Kovaleva et al., 2021; Luo et al., 2021) have been identified within the residual
stream. These rogue dimensions exhibit large magnitudes relative to others and are associated
with the generation of anisotropic representations (Ethayarajh, 2019; Timkey & van Schijndel,
2021). Anisotropy means that the residual stream states of random pairs of tokens tend to point
towards the same direction, i.e. the expected cosine similarity is close to one. Furthermore, ablating
outlier dimensions has been shown to significantly decrease downstream performance (Kovaleva
et al., 2021), suggesting they encode task-specific knowledge (Rudman et al., 2023). The magni-
tudes of these outliers have been shown to increase with model size (Dettmers et al., 2022), posing
challenges for the quantization of large language models. The presence of rogue dimensions has
been hypothesized to stem from optimizer choices (Elhage et al., 2023), with higher levels of reg-
ularization reducing their magnitudes (Ahmadian et al., 2023). Puccetti et al. (2022) identified a
high correlation between the magnitude of the outlier dimensions found in token representations
and their training frequency. They concluded that these dimensions contribute to enabling the
model to focus on special tokens, which is known to be associated with “no-op” attention up-
dates (Bondarenko et al., 2023) (see attention sinks in Section 5.1.3). Recent works have proposed
architectural modifications to reduce the presence of these outliers (Bondarenko et al., 2023; Hu
et al., 2024). In Vision Transformers, high-norm residual stream states have been identified as ag-
gregators of global image information, appearing in patches with highly redundant information, such
as those composing the image background (Darcet et al., 2024).

The specific features encoded within the residual stream at various layers remain uncertain, yet
sparse autoencoders offer a promising avenue for improving our understanding. Inital SAEs were
trained to reconstruct residual stream states of small language models such as GPT2-Small (Cun-
ningham et al., 2023; Bloom & Lin, 2024; Bloom, 2024) showing highly interpretable features (Fig-
ure 14). Since residual stream states gather information about the sum of previous components’
outputs, inspecting SAE’s features can illuminate the process by which they are added or trans-
formed during the forward pass. Given the type of features intermediate FFNs and attention heads
interact with, we also expect the residual stream at middle layers to encode highly abstract features.
Tigges et al. (2023) provide some preliminary evidence by showing that causally intervening on the
residual stream in middle layers is more effective in flipping the sentiment of the output token, sug-
gesting that the latent representation of sentiment is most prominent in the middle layers. Bloom &
Lin (2024) study the features learned by a SAE in layer 8 of the 12-layer model GPT2-Small. Based
on their output behavior via the logit lens (Section 4.3) the authors first find local context features
promoting small sets of tokens. Secondly, they highlight the presence of partition features, which
promote and suppress two distinct sets of tokens. For instance, a partition feature might promote
tokens starting with capital letters and suppress those starting with lowercase letters. Finally, akin
to suppression neurons (Voita et al., 2023; Gurnee et al., 2024), they note the presence of suppres-
sion features aimed at reducing the likelihood of specific sets of tokens. Templeton et al. (2024b)
scale residual stream SAEs to LLMs, finding millions of features in Claude 3 Sonnet (Anthropic,
2024). They identified abstract features, such as a “code error feature” firing on variables incor-
rectly named and invalid inputs in function calls. Additionally, they found safety-related features
on bias or racism, sycophancy features that activate on empathetic text, and others related to decep-
tion and power-seeking. Templeton et al. (2024b) also showed the model behavior can be steered by
clamping the activation values of these features. Other notable features in Claude 3 Sonnet include
an addition function feature representing the addition operation, similar to function vectors dis-
covered by Hendel et al. (2023); Todd et al. (2024), and the presence of features corresponding to
intermediate computations in prompts requiring multi-step inference.

25

Figure 15: Simplified version of the IOI circuit in GPT2 Small discovered by Wang et al. (2023a).

5.4 EMERGENT MULTI-COMPONENT BEHAVIORS

In previous sections we presented some of the different mechanisms that attention heads and FFNs
implement, as well as an overview of the properties of the residual stream. However, in order to
explain the remarkable performance of Transformers, we also need to account for the interactions
between the different components (Wen et al., 2023; Cammarata et al., 2020).

Evidence of multi-component behavior. The induction mechanism presented in Section 5.1.2
is a clear example of two components (attention heads) composing together to complete a pattern.
Recent evidence suggests that multiple attention heads work together to create “function” or
“task” vectors describing the task when given in-context examples (Hendel et al., 2023; Todd et al.,
2024).
Intervening in the residual stream with those vectors can produce outputs in accordance
with the encoded task on novel zero-shot prompts. Variengien & Winsor (2023) study in-context
retrieval tasks involving answering a request where the answer can be found in the context. The
authors identify a high-level mechanism that is universal across subtasks and models. Specifically,
middle layers process the request, followed by a retrieval step of the entity from the context done by
attention heads at later layers.

Additionally, Neo et al. (2024); Yu & Ananiadou (2024) reveal that individual neurons within
downstream FFNs activate according to the output of previous attention heads, interacting in
specific contexts. However, the most compelling evidence of particular behaviors emerging from
the interaction between multiple components is found in the circuit analysis literature (Wang et al.
(2023a); Stolfo et al. (2023b); Heimersheim & Janiak (2023); Geva et al. (2023); Hanna et al. (2023),
among others). As an illustration, we present the circuit found in GPT2 Small for the Indirect Object
Identification (IOI) task (Wang et al., 2023a), depicted in Figure 15. In the IOI task the model is
given inputs of the type “When Mary and John went to the store, John gave a drink to ___”. The
initial clause introduces two names (Mary and John), followed by a secondary clause where the two
people exchange an item. The correct prediction is the name not appearing in the second clause,
referred to as the Indirect Object (Mary). The circuit found in GPT2 Small mainly includes:

• Duplicity signaling: duplicate token heads at position S2, and an induction mechanism
involving previous token heads at S1+1 signal the duplicity of S (John). This information
is read by S-Inhibition heads at the last position, which write in the residual stream a token
signal, indicating that S is repeated, and a position signal of the S1 token.

• Name copying: name mover heads in later layers copy information from names they attend
to in the context to the last residual stream. However, the signals of the previous layers
S-Inhibition heads modify the query of name mover heads so that the duplicated name (in
S1 and S2) is less attended, favouring the copying of the Indirect Object (IO) and therefore,
pushing its prediction.

Besides, Wang et al. (2023a) discovered Negative mover heads, which are instances of copy sup-
pression heads (Section 5.1.2) downweighing the probability of the IO. While the IOI is an attention-

26

WhenMaryandJohnwenttothestore,JohngaveadrinktoS-InhibitionHeadsMaryMary+Name MoverHeadsInduction HeadDuplicate TokenHeadPrevious tokenheadIOS1S1+1S2centric circuit, examples of circuits involving both FFNs and attention heads are also present. For
instance, Hanna et al. (2023) reverse-engineered the GPT2-Small circuit for the greater-than task,
which involves sentences like The war lasted from the year 1814 to the year 18__, where the model
must predict a year greater than 1814. The authors demonstrate that downstream FFNs compute a
valid year by reading from previous attention heads, which attend to the event’s initial date.

Generality of circuits.
Prakash et al. (2024) show that the functionality of the circuit compo-
nents remains consistent after fine-tuning and benefits of fine-tuning are largely derived from an
improved ability of circuit components to encode important task-relevant information rather than an
overall functional rearrangement. Fine-tuned activations are also found to be compatible with the
base model despite no explicit tuning constraints, suggesting the process produces minimal changes
in the overall representation space. The findings of Prakash et al. (2024) are additionally supported
by Jain et al. (2024) in controlled settings. While a common critique of mechanistic interpretability
work is the limited scope of identified circuits, Merullo et al. (2024) show that low-level findings
about specific heads and higher-level findings about general algorithms implemented by Transformer
models can generalize across tasks, suggesting that large language models could be explained as
functions of few task-general sparse components. The results of Merullo et al. (2024) also suggest
that circuits are not exclusive, i.e.
the same model components might be part of several circuits.
Other studied dimensions of discovered circuits include their faithfulness (Hanna et al., 2024) and
their completeness (Wang et al., 2023a). Ferrando & Costa-jussà (2024) provide evidence of iden-
tical circuits solving subject-verb agreement in both English and Spanish, supporting the generality
of discovered circuits across languages.

Grokking as Emergence of Task-specific Circuits. Transformer models were observed to con-
verge to different algorithmic solutions for tasks at hand (Zhong et al., 2023). Nanda et al. (2023a)
provide convincing evidence on the relation between circuit emergence and grokking, i.e. the sud-
den emergence of near-perfect generalization capabilities for simple symbol manipulation tasks at
late stages of model training (Power et al., 2022). Merrill et al. (2023) suggest the grokking phase
transition can be seen as the emergence of a sparse circuit with generalization capabilities, replacing
a dense subnetwork with low generalization capacity. According to Varma et al. (2023), this hap-
pens because dense memorizing circuits are inefficient for compressing large datasets. In contrast,
generalizing circuits have a larger fixed cost but better per-example efficiency, hence being preferred
in large-scale training. Huang et al. (2024b) connect the learning dynamic converging to grokking
to the double descent phenomenon (Loog et al., 2020). According to this view, the emergence of
specialized attention heads might be seen as a mild grokking-related phenomenon (Olsson et al.,
2022; Bietti et al., 2023).

5.4.1 FACTUALITY AND HALLUCINATIONS IN MODEL PREDICTIONS

Intrinsic views on hallucinatory behavior. The generation of factually incorrect or nonsensical
outputs is considered a significant limitation in the practical usage of language models (Ji et al.,
2023; Minaee et al., 2024). While some techniques for detecting hallucinated content rely on quan-
tifying the uncertainty of model predictions (Varshney et al., 2023), most alternative approaches
engage with model internal representations. Approaches for detecting hallucinations directly from
the representations include training probes and analyzing the properties of the representations lead-
ing to hallucinations. CH-Wang et al. (2023) and Azaria & Mitchell (2023) find probing classifiers
predictive of the model’s output truthfulness, achieving the highest accuracy using middle and last
layers representations. Zou et al. (2023) and Li et al. (2023a) find “truthfulness” directions with
causal influence on the model outputs, i.e.
intervening in the internal representations with the
found directions enhance the output truthfulness. Li et al. (2023a) locate these causal directions in
the specific attention head activation. Chen et al. (2024b) use the eigenvalues of responses’ repre-
sentations covariance matrix to measure the semantic consistency in embedding space across layers,
while Chen et al. (2024d) observe that logit lens (Section 4.3) scores of the predicted attribute (an-
swer) in higher-layers representations of context tokens are informative of the answer correctness.

A related area of research with overlapping goals is that of hallucination detection in machine trans-
lation (MT). An MT model is considered to hallucinate if its output contains partially or fully de-
tached content from the source sentence (Guerreiro et al., 2023b). Prediction probabilities of the
generated sequence and attention distributions have been used to detect potential errors (Fomicheva

27

Figure 16: Simplified version of the factual recall circuit.

et al., 2020) and model hallucinations (Guerreiro et al., 2023a;b). Recently, methods measuring the
amount of contribution from the source sentence tokens (Ferrando et al., 2022a) were found to per-
form on par with external methods based on semantic similarity across several categories of model
hallucinations (Dale et al., 2023a;b). Detection methods show complementary performance across
hallucination categories, and simple aggregation strategies for internals-based detectors outperform
methods relying on external semantic similarity or quality estimation modules (Himmi et al., 2024).

The underlying mechanisms involved in the prediction of hallucinated content for LLMs remain
largely unexplained. Most of the research in this area focuses on studying the ability of language
models to recall facts, which we discuss in the next section.

Recall of factual associations. Recent research has delved into the internal mechanisms through
which language models recall factual information, which is directly related to the hallucination
problem in LLMs. A common methodology involves studying tuples (s, r, a), where s is a subject,
r a relation, and a an attribute. The model is prompted to predict the attribute given the subject
and relation. For instance, given the prompt: “LeBron James plays the sport of”, the model is
expected to predict basketball. Meng et al. (2022) and Geva et al. (2023) make use of causal inter-
ventions (Section 3.2.2) to localize a mechanism responsible for recalling factual knowledge within
the language model. Early-middle FFNs located in the last subject token add information about
the subject into its residual stream. On the other hand, information from the relation passes into
the last token residual stream via early attention heads. Finally, later layers attention heads ex-
tract the right attribute from the last subject residual stream. Yuksekgonul et al. (2024) find
that, in similar settings, attention to relevant tokens in the prompt correlates with LLM’s factual
correctness. Importantly, the division of responsibilities between lower and upper layers was also
observed in attention-less models based on the Mamba architecture (Gu & Dao, 2023; Sharma et al.,
2024a). While this might be motivated by implicit context-mixing akin to Transformers’ causal self-

28

LeBronJamesplaysthesportofRelationHeadsMixedHeadsSubjectHeads+games+basketball+TokenConcatenationSubjectEnrichmentAttributeExtractionRelationPropagationSubjectRelationbasketballsurﬁng+basketballattention (Ali et al., 2024), it suggests the organization of these mechanisms might be driven by the
language modeling optimization process rather than architectural constraints.

Subsequent research has moved from localizing model behavior to studying the computations per-
formed to solve this task. Hernandez et al. (2024) show that attributes of entities can be linearly
decoded from the enriched subject residual stream, while Chughtai et al. (2024) investigate how
attention heads’ OV circuits effectively decode the attributes, proposing an additive mechanism.
More precisely, using the direct logit attribution by each token via the attention head (Equation (16))
they identify subject heads responsible for extracting attributes from the subject independently from
the relation (not attending to it), as well as relation heads that promote attributes without being
causally dependent on the subject. Additionally, a group of mixed heads generally favor the correct
attribute and depend on both the subject and relation. The combination of the different heads’ out-
puts, each proposing different sets of attributes, together with the action of some downstream FFNs
resolve the correct prediction (Figure 16). Nanda et al. (2023c) provide a detailed explanation of
the subject enrichment phase by studying names of athletes as subjects. They suggest that the first
layers’ attention heads concatenate the athlete’s name on the final name token residual stream
through addition, and subsequent FFNs map the obtained athlete’s name representation into a linear
representation of the athlete’s sport that can be easily linearly extracted by the downstream attribute
extraction heads.

Merullo et al. (2023) report that for solving relational tasks, such as predicting a country’s
capital given in-context examples, middle layers prepare the argument, e.g. Poland, of a
get_capital() function that is applied downstream via an FFN update, giving place to
get_capital(Poland) = Warsaw. Further research replicates Merullo et al. (2023)’s analysis on
zero-shot settings (Lv et al., 2024) and finds specific attention heads “passing” the argument from
the context (Poland), but also promoting the capital cities (Warsaw). Downstream FFNs “activate”
relevant attention heads in the previous layer and add a vector guiding the residual stream toward
the correct capital direction.

Recent works aim to shed light on how the model engages in factual recall vs. grounding. Follow-
ing the aforementioned (subject, relation, attribute) structure of facts, an answer is considered to
be grounded if the attribute is consistent with the information in the context of the prompt. Given
prompts of the type “The capital of Poland is London. Q: What is the capital of Poland? A:___”,
Yu et al. (2023a) find in-context heads and memory heads by using the difference logit attribution
(Section 3.2.1, Equation (17)) of attention heads. These heads favor, respectively, the in-context
answer London and the memorized answer Warsaw, showing a “competition” between mecha-
nisms (Ortu et al., 2024). Furthermore, upweighting the output of each head type reveals a bias
towards one of the two answers. Similar to the in-context heads, Variengien & Winsor (2023) show
that a set of downstream attention heads retrieve the correct answer (an attribute) from the context
via copying, preceded by a processing of the request (a question) in middle layers. Wu et al. (2024a)
study these type of heads, which they coin retrieval heads in arbitrarily long-contexts, and show
they are crucial for solving the Needle-in-a-Haystack tests (Kamradt, 2023). Monea et al. (2024)
complement the findings of Yu et al. (2023a) and Meng et al. (2022) and show that FFNs in the
last token of the subject have higher contributions on ungrounded (memorized) answers as
opposed to grounded answers, while suggesting that grounding could be a more distributed pro-
cess lacking a specific localization. Haviv et al. (2023) show that the recall of “memorized” idioms
largely depends on the updates of the FFNs in early layers, providing further evidence of their role as
a storage of memorized information. This is further observed in the study of memorized paragraphs,
with lower layers exhibiting larger gradient flow (Stoehr et al., 2024). On the other hand, Sharma
et al. (2024b) show that substituting the original FFN matrices by lower-rank approximations (Sec-
tion 4.3) leads to improvements in model performance, especially in later layers of the model. They
show that, in the factual recall task, the components with smaller singular values encode the correct
semantic type of the answer but the wrong answer, thus their removal benefits the accuracy. To con-
clude, we draw a connection with a decoding strategy (DoLa) proposed to improve the factuality of
language models (Chuang et al., 2024). DoLa contrastively compares the logit-lens next-token dis-
tributions between an early layer and a later layer (Li et al., 2023b), promoting tokens that undergo
a larger probability change, suggesting that the factual knowledge injection is done in a distributed
manner across the network.

29

Factuality issues and model editing. Factual information encoded in LMs might be incorrect
from the start, or become obsolete over time. Moreover, inconsistencies have been observed when
recalling factual knowledge in multilingual and cross-lingual settings (Fierro & Søgaard, 2022; Qi
et al., 2023), or when factual associations are elicited using less common formulations (Berglund
et al., 2023). This sparked the interest in developing model editing approaches able to perform
targeted updates on model factual associations with minimal impact on other capabilities. While
early approaches proposed edits based on external modules trained for knowledge editing (De Cao
et al., 2021; Mitchell et al., 2022a;b), recent methods employ causal interventions (Section 3.2.2) to
localize knowledge neurons (Dai et al., 2022) and FFNs in one or more layers (Meng et al., 2022;
2023), informed by factual recall mechanisms described in the previous paragraph. However, model
editing approaches still present several challenges, summarized in (Yao et al., 2023; Li et al., 2024),
including the risks of catastrophic forgetting (Gupta et al., 2024a;b) and downstream performance
loss (Gu et al., 2024). Importantly, Hase et al. (2023) show that effective localization does not al-
ways result in improved editing results, and that distributed edits across different model sections can
result in similar editing accuracy. Steerable-by-design architectures such as the Backpack Trans-
former (Hewitt et al., 2023) were recently proposed as possible alternatives to localization-driven
methods, exploiting the linearity of component contributions (Section 4.2) as an inductive bias to
enhance controllability. We refer readers to Wang et al. (2023b) for further insights on LM editing.

6 LM INTERPRETABILITY TOOLS

Several open-source software libraries were introduced to facilitate interpretability studies on
Transformer-based LMs. In this section, we briefly summarize the most notable ones and highlight
their main points of strength.

Captum (Kokhlikyan et al., 2020) is a library in the Pytorch ecosys-
Input attribution tools.
tem providing access to several gradient and perturbation-based input attribution methods for any
Pytorch-based model. It notably supports training data attribution methods (Section 3.1), and re-
cently added several utilities for simplifying attribution analyses of generative LMs (Miglani et al.,
2023). Several Captum-based tools provide convenient APIs for input attribution of Transformer-
based models: Transformers Interpret (Pierse, 2021), ferret (Attanasio et al., 2023) and
Ecco (Alammar, 2021) are mainly centered around language classification tasks, while Inseq (Sarti
et al., 2023) is focused specifically on generative LMs and supports advanced approaches for
contrastive context attribution (Sarti et al., 2024) as well as context mixing evaluation (Sec-
tion 3.1). SHAP (Lundberg & Lee, 2017) is a popular toolkit mainly centered on perturbation-
based input attribution methods and model-agnostic explanations for various data modalities. The
Saliency (PAIR Team, 2023) library provides framework-agnostic implementations for mainly
gradient-based input attribution methods. LIT (Tenney et al., 2020) is a framework-agnostic tool
providing a convenient set of utilities and an intuitive interface for interpretability studies spanning
input attribution, concept-based explanations and counterfactual behavior evaluation. It notably in-
cludes a visual tool for debugging complex LLM prompts (Tenney et al., 2024).

Component importance analysis tools. Tools supporting work on circuit discovery and causal
interventions play a fundamental role in mechanistic studies, balancing the complexity and model-
specific nature of intervention-based methods with a broad support for various pre-trained LM ar-
chitectures. TransformerLens (Nanda & Bloom, 2022) is a Pytorch-based toolkit to conduct mech-
anistic interpretability analyses of generative language models inspired by the closed-source Garçon
library (Elhage et al., 2021b). The library reimplements popular Transformer LM architectures, pre-
serving compatibility with the popular transformers library (Wolf et al., 2020) while also provid-
ing utilities such as hook points around model activations and attention head decomposition to facili-
tate custom interventions. NNsight (Fiotto-Kaufman, 2024) provides a Pytorch-compatible interface
for interpretability analyses. Its usage is not restricted to Transformer models, but it provides utilities
to streamline the usage of transformers checkpoints. Its main peculiarity is the ability to compile
an intervention graph that can be processed through delayed execution, enabling the extraction of
arbitrary internal information from large LMs hosted on remote servers. Pyvene (Wu et al., 2024c)
is a Pytorch-based library supporting complex intervention schemes, such as trainable (Geiger et al.,
2023a) and mid-training interventions (Geiger et al., 2022), alongside various model categories be-
yond Transformers. Notably, it supports the serialization of intervention schemes to simplify anal-

30

yses and promotes reusability. Several tools are currently used for the development of SAEs (Sec-
tion 4.2), providing overlapping sets of features. For example, SAELens (Bloom & Channin, 2024)
supports advanced visualization of SAE features, while dictionary-learning (Marks & Mueller,
2023) is an actively developed tool built on top of NNsight, supporting various experimental features
to address SAEs’ weaknesses. Other examples include Cooney (2023); Jeffrey Wu (2024), provid-
ing standard TransformerLens-compatible SAE implementations, and Belrose (2024) for Top-K
SAE training.

tools

internals. Several

for visualizing model

such as BERTViz (Vig, 2019),
Tools
exBERT (Hoover et al., 2020) and InterpreT (Lal et al., 2021) were developed to visualize attention
weights and activations in Transformer-based LMs. LM-Debugger (Geva et al., 2022a) is a toolkit to
inspect intermediate representation updates through the lens of logit attribution (Section 3.2.1), while
VISIT (Katz & Belinkov, 2023), Ecco (Alammar, 2021) and Tuned Lens21 (Belrose et al., 2023a)
simplify the application of naive and learned vocabulary projections to inspect the evolution of pre-
dictions across model layers. CircuitsVis (Cooney, 2022) provides reusable Python bindings for
front-end components that can be used to visualize Transformers internals and predictions, and was
adopted by various interpretability tools. Penzai (Daniel Johnson, 2024) is a JAX library supporting
rich visualizations of pytree data structures, including LM weights and activations. LM-TT (Tufanov
et al., 2024) allows inspecting the information flow in a forward pass, faciliting the examination of
the contributions of individual attention heads and feed-forward neurons. TDB (Mossing et al., 2024)
is a visual interface to interpret neuron activations in LMs supporting automated interpretability
techniques and SAEs. Neuronpedia22 (Lin & Bloom, 2024) provides an open repository for visu-
alizing activation of SAE features trained on LM residual stream states (Section 4.2). Notably, it
includes a gamified experience to facilitate the annotation of human-interpretable concepts in SAE
feature space. Lastly, sae-vis (McDougall & Bloom, 2024) is a SAELens-compatible library to
produce feature-centric and prompt-centric interactive visualizations of SAE features.

Other notable interpretability-related tools. The “Restricted Access Sequence Processing Lan-
guage” (RASP, Weiss et al., 2021) is a sequence processing language providing a human-readable
model for transformer computations. Tracr (Lindner et al., 2023) is a compiler converting RASP
programs into decoder-only Transformer weights, automating the creation of small Transformer
models implementing specific desired behaviors. RASP and Tracr were adopted for promoting in-
terpretable behaviors via constrained optimization (Friedman et al., 2023) and validating the effec-
tiveness of circuit discovery techniques (Conmy et al., 2023). Pyreft23 (Wu et al., 2024b) is a toolkit
based on Pyvene for fine-tuning and sharing trainable interventions (Section 3.2.3, Causal abstrac-
tion) aimed at optimizing LM performance on selected tasks, in a similar but more targeted and
efficient way than parameter-efficient fine-tuning methods (PEFT, Han et al., 2024). Going beyond
the textual modality, ViT Prisma (Joseph, 2023) is a toolkit to conduct mechanistic interpretability
analyses on vision and multimodal models. Finally, MAIA (Shaham et al., 2024) is a multimodal
language model augmented with tool use to automate common interpretability workflows such as
neuron explanations, example synthesis and counterfactual editing.

7 CONCLUSION AND FUTURE DIRECTIONS

In this paper, we have offered an overview of the existing interpretability methods useful for un-
derstanding Transformer-based language models, and have presented the insights they have led to.
Although the focus of this work is on practical methods and findings, we acknowledge theoretical
studies related to the interpretability of Transformers, such as investigations explaining in-context
learning (Akyürek et al., 2023; Von Oswald et al., 2023; Xie et al., 2022), explorations of Trans-
formers through the lens of data compression and representation learning (Yu et al., 2023b; Voita
et al., 2019a), the study of Transformers’ learning dynamics (Tian et al., 2024; 2023; Tarzanagh
et al., 2024), or the analyses on their generalization properties on algorithmic tasks (Nogueira et al.,
2021; Anil et al., 2022; Zhou et al., 2024).

21https://github.com/AlignmentResearch/tuned-lens
22https://neuronpedia.org
23https://github.com/stanfordnlp/pyreft

31

Looking forward, we believe that the ultimate test for insights collected in years of interpretabil-
ity work remains their applicability in debugging and improving the safety and reliability of future
models, providing developers and users with better tools to interact with them and understand the
factors influencing their predictions (Longo et al., 2024). To ensure such requirements are met,
future developments in interpretability research will be faced with the challenging task of moving
from functionally-grounded evaluations (i.e. no human evaluation, only toy settings) to actionable
insights and benefits for real-world tasks (Doshi-Velez & Kim, 2017). From an analytical stand-
point, this involves moving from methods and analyses operating in model component space to
human-interpretable space, i.e from model components to features and natural language explana-
tions, as suggested by Singh et al. (2024b), while still faithfully reflecting model behaviors (Siegel
et al., 2024). Directions we deem promising in this area involve the usage of LMs as verbal-
izers (Feldhus et al., 2023; Bills et al., 2023; Wang et al., 2024; Chen et al., 2024c) for scaling
input and component attribution analyses, especially when paired with verification mechanisms to
ensure counterfactual consistency (Avitan et al., 2024), and circuit discovery methods leveraging
interpretable features to enable interventions motivated by human-understandable concepts (Marks
et al., 2024). More accessible insights might also unlock gains in model performance and efficiency,
translating interpretability-driven insights into downstream task improvements (Wu et al., 2024b).
Importantly, interdisciplinary research grounded in the technical developments we summarize in this
survey will play a key role in broadening the scope of interpretability analyses to account for the
perceptual and interactive dimensions of model explanations from a human perspective (Liao et al.,
2020; Dhanorkar et al., 2021; Vasconcelos et al., 2023). Ultimately, we believe that ensuring open
and convenient access to the internals of advanced LMs will remain a fundamental prerequisite for
future progress in this area (Bau et al., 2023; Casper et al., 2024; Hudson et al., 2024).

ACKNOWLEDGEMENTS

Javier Ferrando is supported by the Spanish Ministerio de Ciencia e Innovación through the project
PID2019-107579RB-I00 / AEI / 10.13039/501100011033. Gabriele Sarti and Arianna Bisazza
acknowledge the support of the Dutch Research Council (NWO) as part of the project InDeep
(NWA.1292.19.399). The authors express their gratitude to Paul Riechers, Dmitrii Troitskii and
Alex Loftus for their useful comments.

A MATHEMATICAL NOTATION

Notation

Definition

n
V
t = ⟨t1, t2 . . . , tn⟩
x = ⟨x1, x2 . . . , xn⟩
d
dh
dFFN
H
L
i ∈ Rd
xl
xmid,l
∈ Rd
i
f c(x) ∈ Rd

f l(x) = xl

n ∈ Rd

W l,h

Al,h ∈ Rn×n
K , W l,h
O ∈ Rdh×d

Q , W l,h
W l,h
in ∈ Rd×dFFN , W l

out ∈ RdFFN×d
W l
WE ∈ Rd×|V| and WU ∈ R|V|×d

V ∈ Rd×dh

Sequence length
Vocabulary
Input sequence of tokens
Input sequence of token embeddings
Model dimension
Attention head dimension
FFN dimension
Number of heads
Number of layers
Residual stream state at position i, layer l
Residual stream state at position i, layer l, after the attention block
Component c output representation at the last position
Residual stream state at the last position, layer l
Attention matrix at layer l, head h
Queries, keys and values weight matrices at layer l, head h
Output weight matrix at layer l, head h
FFN input and output weight matrices at layer l
Embedding and unembedding matrices

Table 1: Notation and definitions of the main variables used in this work.

B LINEARIZATION OF THE LAYERNORM

The LayerNorm operates over an input z as: LN(z) = z−µ(z)
σ(z) ⊙ γ + β, where µ and σ compute
the mean and standard deviation of z, and γ and β refer to the element-wise transformation and bias
respectively. Holding σ(z) as a constant, the LayerNorm can be decomposed into zL + β, where L
is a linear transformation:

L :=

1
σ(z)






γ1
0
· · ·
0

0
γ2
· · ·
0

· · ·
· · ·
· · ·
· · ·












0
0
· · ·
γd

C FOLDING THE LAYERNORM

d−1
d
− 1
d
· · ·
− 1

− 1
d
d−1
d
· · ·
d − 1

d




.



· · · − 1
d
· · · − 1
d
· · ·
· · ·
d−1
· · ·
d

(27)

Any Transformer block reads from the residual stream by normalizing before applying a linear layer
(with weights W and bias b) to the resulting vector:

LN(z)W + b

(28)

Following the decomposition in Equation (27) we can fold the weights of the LayerNorm into those
of the subsequent linear layer as follows:

LN(z)W + b = (zL + β)W + b
= zLW + βW + b
= zW ∗ + b∗,

(29)

where the new weights and bias are W ∗ = LW and b∗ = βW + b respectively.

D IMPLEMENTATION DETAILS OF SAES

• During training, a feature receives a zero gradient signal if it does not activate. When
this occurs frequently, it can lead to a dead feature. Bricken et al. (2023) propose resam-
pling these features by reinitializing their encoder and decoder weights periodically during

56

training. An alternative approach to resampling is ghost gradients (Jermyn & Templeton,
2024), which adds an auxiliary loss term that supplies a gradient signal to promote the
reactivation of dead features. However, recent results have found this approach subopti-
mal (Rajamanoharan, 2024; Conerly et al., 2024).

• Setting the β1 parameter of Adam to 0 has been found to reduce the number of “dead”
features in larger autoencoders (Templeton et al., 2024a; Rajamanoharan et al., 2024a).
Yet, Conerly et al. (2024) rely on β1 = 0.9.

• Although intially the norm of the decoder’s rows24 was recommended to be equal to
one (Bricken et al., 2023), recent released SAEs also consider an unconstrained norm set-
ting (Conerly et al., 2024).

24Note that we consider the decoder weight matrix Wdec ∈ Rm×d.

57

* A Comprehensive Mechanistic Interpretability Explainer & Glossary
  * **Warning**: This was written in Dec 2022 and has not been kept up to date. The contents should still be accurate, but are missing a lot!
  * __Written by [Neel Nanda](https://www.neelnanda.io/) to accompany the [TransformerLens library](https://github.com/neelnanda-io/TransformerLens) for mechanistic interpretability of language models. I love hearing if people find my work useful, please [reach out](mailto:neelnanda27@gmail.com)! If you want to get involved in mechanistic interpretability research, check out [my guide to getting started](https://neelnanda.io/getting-started)__
  * Navigating This Doc
    This is an extremely long doc! To make it more managable, I've hosted it on Dynalist, which I think has a much nicer UI, but takes some getting used to. Some tips on how to use it. If this is too annoying, there's an [HTML version here](https://neelnanda.io/mechanistic-interpretability/glossary)
    * **The recommended way to view this doc:**
      * Hover over the main title of the doc
      * Click the three lines to the left
      * Click expand to level -> Level 2 to see just the section headers
      * Click expand to level -> Level 3 to see the high-level definitions of everything
    * Other UI tips:
      * **Click on the bullet point to expand** a closed point/section you want to read
      * **Click on the bullet point to collapse** an open point/section you don't want to read. Try it on this bullet!
        * You can also do collapse all siblings to collapse every point in that section, for readability
      * If you just want to look up a specific term, **search in the top right to filter for it**.
      * Hover over a point/section and **click the magnifying glass to focus on it**. Try it on this section!
      * Points I think are particularly worth reading are highlighted in green
      * [Discuss and comment here](https://www.lesswrong.com/posts/vnocLyeWXcAxtdDnP/a-comprehensive-mechanistic-interpretability-explainer-and)
    * Table of Contents
      * [Introduction](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iJffSbaz_Vj8ujdvRb9IlQ_8)
        * [Why does this doc exist?](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=EEGvuK3NricH4Z8cDqt4vHPi)
        * [How to read this doc](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=6xKhX3sR-7dyjzhRggHOJzMB)
      * [Mechanistic Interpretability](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=eL6tFQqNwd4LbYlO1DVIen8K)
        * [General](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=BQds7CQ8ytq2rolt7p0XQPbt)
        * [Representations of Features](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KiDMIteKCEXt_EkR2sZCOfbG)
        * [Superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2)
        * [Toy Models of Superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=EuO4CLwSIzX7AEZA1ZOsnwwF)
        * [The Broader Interpretability Field](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bsKRuga24AHSMztSeter7oGX)
        * [Linear Algebra](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=HM3KfFp0X4P2HPSE6Luq80Pi)
        * [Circuits As Computational Subgraphs](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=Pky4iQrwVO1hN3HUO5aeJC22)
      * [Machine Learning](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=FU_mI4vJX1gBv3X70upDyjnn)
        * [Basic Concepts](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=LgWRcThnOwW8Uc2qeK_6E5mm)
        * [Training Concepts](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=gw6rZcDY2uyqgmTcWv7nHVSC)
        * [Training Dynamics](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=gw6rZcDY2uyqgmTcWv7nHVSC)
        * [Misc](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=gJWBGbXh3bXBH_hMFYmoc2o2)
      * [Transformers](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=pndoEIqJ6GPvC1yENQkEfZYR)
        * [Transformer Basics](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=6SOSlUPQukfjSYIjo-rXNaLe)
        * [Transformer Components](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=83xppOkmpR8Enfc5O8m1uw7b)
        * [Attention Heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=94hCeljxx7mX_WoIBNjpszE1)
        * [Misc Transformer Words](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=PS0Czs5DoAG74iySI9p-gqeW)
        * [Training](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=ROPHo0DxpSgg5Cub-gjxztPR)
      * [Transformer Circuits](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=L1NKUgo0EBsRc-fBZTtjaHtx)
        * [Language Modelling](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=mHPjBbUJWc0UiIzCxjLibN82)
        * [A Mathematical Framework for Transformer Circuits](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=aGu9fP1EG3hiVdq169cMOJId)
        * [Induction Circuits](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ)
        * [SoLU](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NnlcCwpsX2yJpu4js1EnluiF)
        * [The Indirect Object Identification Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa)
      * [Techniques](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=8m6oU03bXO60iaRnGzFHBHN9)
        * [Mechanistic Interpretability Techniques](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=M3vTmlndMJTgnL1-t1IAsTaR)
        * [Non-MI Techniques](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=mwSnqyC-_MzCZGcGGPSUxs7s)
        * [Tooling](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=tKdEkEU5X0U-1zVKv0PO0L3q)
      * [Notable Models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=1Z1d-y3LE8Y6TVqglA0UrGZS)
        * [Open Source GPT-Style Models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h)
        * [My Interpretability-Friendly Models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m)
        * [Other Open Source Models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=urEWXO4Oj-V36CCCARRRZwkT)
  * Introduction
    * Why does this doc exist?
      * The goal of this doc is to be a comprehensive glossary and explainer for [Mechanistic Interpretability](https://distill.pub/2020/circuits/zoom-in/) (focusing on transformer language models), the field of studying how to reverse engineer neural networks. 
      * There's a lot of complex terms and jargon in the field! And these are often scattered across various papers, which tend to be pretty well-written but not designed to be an introduction to the field as a whole. The goal of this doc is to resolve some [research debt](https://distill.pub/2017/research-debt/) and strives to be a canonical source for explaining concepts in the field
      * I try to go beyond just being a reference that gives definitions, and to actually dig into how to __think__ about a concept. Why does it matter? Why should you care about it? What are the subtle implications and traps to bear in mind? What is the underlying intuition, and how it fits into the rest of the field?
      * I also go outside pure mechanistic interpretability, and try to define what I see as the key terms in deep learning and in transformers, and how I think about them. If you want to reverse engineer a system, it's __extremely__ useful to have a deep model of what's going on inside of it. What are the key components and moving parts, how do they fit together, and how could the model use them to express different algorithms?
    * How to read this doc
      * The first intended way is to use this a **reference**. When reading papers, or otherwise exploring and learning about the field, coming here and looking up any terms and trying to understand them.
      * The second intended way is to treat this as a **map to the field**. My hope is that if you're new to the field, you can just read through this doc from the top, get introduced to the key ideas, and be able to dig into further sources when confused. And by the end of this, have a pretty good understanding of the key ideas, concepts and results! 
      * I frequently go on long tangents giving my favourite intuitions and context behind a concept. The intent is that people who find them useful can read and engage, and people who find them confusing or irrelevant can skim or just move on - they aren't necessary to follow what's going on.
      * It's obviously not practical to fully explain __all__ concepts from scratch! Where possible, I link to sources that give a deeper explanation of an idea, or to learn more. 
        * More generally, if something’s not in this glossary, you can often find something good by googling it or searching on [alignmentforum.org](http://alignmentforum.org). If you can’t, [let me know](mailto:neelnanda27@gmail.com)!
  * Mechanistic Interpretability
    Key concepts and terms in mechanistic interpretability
    * General
      Basic ideas, like features and circuits, and the surrounding intuitions and how best to think about them
      * **MI/mech int/mech interp/mechanistic interpretability**: The field of study of reverse engineering neural networks from the learned weights down to human-interpretable algorithms. Analogous to reverse engineering a compiled program binary back to source code
        * **Mechanistic** refers to the emphasis on trying to understand the actual mecahnisms and algorithms that compose the network
          * In contrast, many forms of interpretability seek to explain how the network's outputs relate high level concepts without referencing the actual functioning of the network. [Saliency maps](https://christophm.github.io/interpretable-ml-book/pixel-attribution.html) are a classic example, as are "build an interpretable model" techniques such as [LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/). 
          * See further discussion of the relationship to the rest of the field [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bsKRuga24AHSMztSeter7oGX)
        * Further reading:
          * [Circuits: Zoom In](https://distill.pub/2020/circuits/zoom-in/)
          * [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)
          * [Interpretability vs Neuroscience](http://colah.github.io/notes/interp-v-neuro/)
          * [Analogies between biology and deep learning](http://colah.github.io/notes/bio-analogies/)
      * **Interpretability**: The broader subfield of AI studying why AI systems do what they do, and trying to put this into human-understandable terms.
        * There isn’t a clear consensus on exactly what **interpretability **is about, what the goals of the field are, the right standards of and types of evidence, etc.
          * [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf) and [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/pdf/1702.08608.pdf) are good attempts to disentangle this.
        * See **The Broader Interpretability Field** for more discussion of this, and how MI differs from/overlaps with the broader field.
      * A **feature** is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image)
        * This is a fuzzy and non-rigorous idea, best illustrated by examples:
          * This part of the image contains a curve
            * This is a feature in a convnet, where there’s a neuron activation per image patch - thus “part of image”
          * This part of the image contains a dog fur-like texture
          * This token is the final token in the phrase “Eiffel Tower”
            * In a factual recall circuit, this can get looked up to produce the feature “is in Paris”
            * This is a feature in a transformer, where there are separate activations for each token in a sequence, thus “this token”
          * This text is Python code
            * This token is the name of a variable corresponding to a list in Python code
          * This token is in a news headline in a Reuters article
          * This token corresponds to [a number that is being used to describe a group of people](https://transformer-circuits.pub/2022/solu/index.html#section-6-3-4)
          * This text is a Bible verse
          * This text is an excerpt from Harry Potter
        * [Chris Olah's answer to "What is a feature":](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)
          * Defining a "feature" in a satisfying way is surprisingly hard. This isn't necessarily a bad thing. Sometimes when you're trying to understand something, you need to struggle with lots of definitions. (Lakatos' famous Proofs and Refutations is a beautiful articulation of this.)
          * The easy answer would be to say a feature is something like curve detector or car detector neurons found in InceptionV1 – some meaningful, articulable property of the input which the network encodes as a direction in activation space. The unsatisfactory thing about this definition is that it's human centric: it excludes the possibility of features humans don't understand.
          * An alternative definition, which I find more satisfactory, is to say that features are whatever the "independent units" a neural network representation can be decomposed into are. That definition is a bit fuzzier, but I suspect it might be getting at something more fundamental. Another idea for a potential human-independent definition is that features are the things a network would ideally dedicate a neuron to if you gave it enough neurons.
          * Some researchers define features simply as any function of the input. The thing I find unsatisfactory about this is that under this definition, any mixture of features is also a feature. It seems to me that there's something importantly different about a "car detector" (which seems to correspond to an important latent variable) from "car and cat detector" (which seems to be an arbitrary mixture of things). I'd like for a definition of a feature to make this distinction.
        * Implicit fuzzy things about how the term is used in practice:
          * This is normally used to describe a property of an example input that is actually internally represented in the model, rather than a feature that could be in theory.
          * It’s used to refer to properties of inputs that generalise across inputs, eg “is this a dog” rather than “is this the specific picture that appeared in the training data”
        * Often used interchangeably with the more specific term **interpretable features**
          * It’s somewhat fuzzy whether you __can __have a feature that is not interpretable. An edge case is described in [Adversarial Examples are Features Not Bugs](https://arxiv.org/abs/1905.02175), where they find that adversarial examples seem to pick up on subtle features in images that humans can’t detect, but which have predictive power and transfer to images that were not trained on - it’s not obvious to me whether these count as interpretable or not.
        * Features are the fundamental building block of models - the model’s internal activations represent features, and the model’s weights and non-linearities are used to apply computations to produce later features from earlier features. 
      * **Decomposability**: When a network representation can be described in terms of independently understandable features. 
        * This is key because it allows us to attack the curse of dimensionality. 
        * ([fleshed out in Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#motivation))
      * **Circuit**: The subset of a model’s weights and non-linearities used to map a set of earlier features to a set of later features is called a [circuit](https://distill.pub/2020/circuits/zoom-in/)
        * **Circuit** is also a fairly fuzzy and poorly defined term. Intuitively, a circuit means “the sub part of a model that does some understandable computation to produce some interpretable features from prior interpretable features”. A special case is when some of the features are the inputs or the outputs, which are inherently interpretable (hopefully!). Ideally, the intermediate steps of computation also represent interpretable features, but this is not essential.
          * Examples of well understood circuits:
            * [Curve circuits](https://distill.pub/2020/circuits/curve-circuits/)**: **A circuit in inception that identifies which parts of an image contain curves (with a separate neuron for each curve orientation)
            * [Induction circuits](https://transformer-circuits.pub/2021/framework/index.html#induction-heads): A circuit in generative language models that involves two attention heads (a **previous token head **and an **[induction head](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)**) composing to detect and continue repeated subsequences.
            * The [Indirect Object Identification Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa)  (**IOI**)**: **A circuit used to complete the sentence “John and Mary went to the shops, then John gave a bottle of milk to” with “ Mary” not “ John”
      * A special case is an **end-to-end circuit**, where the circuit describes how the input to the model is converted to the output (ideally with several interpretable intermediate computations).
        * [Induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY) and Indirect Object Identification are end-to-end circuits, curve circuits are not (because the output of a circuit curve is a neuron that represents that curve orientation)
      * **Intervening on **or **editing **an activation means to run the network, then to stop it once it’s computed an activation, edit or replace that activation, and then resume running the model with the edited activation replacing the old one.
        * An example intervention is **pruning**. Pruning a neuron means to intervene on the neuron’s activation and set it to zero, so later layers in the model cannot use that neuron’s output.
      * **Equivariance / Neuron families: **When there is a family of neurons or features that are distinct but analogous, and where we expect understanding of one to translate to understanding of the others.
        * Eg neurons that detect lines or curves of different orientations, or [neurons that detect whether the token “ die” is in English, German or Dutch text](https://transformer-circuits.pub/2022/solu/index.html#section-6-3-2)
      * **Neuron splitting: **Where a feature in one model gets decomposed into several features in a larger model.
        * Eg “a character in hexadecimal” -> “the character 3 in hexadecimal” (and for the other 15 characters!)
      * **Universality**: The hypothesis that the same circuits will show up in different models.
        * This is a somewhat fuzzy concept - my interpretation is the idea that there is some “best” or “correct” way to complete some task on some data distribution in some model architecture, and that different models trained to do similar tasks on similar data with similar architectures are likely to converge on it.
        * The bolder (and IMO probably false) hypothesis is that circuits represent some deep principles of how neural networks learn, that there is some finite (and hopefully not too large!) family of important circuits to understand, and that we can characterise eg language model training by seeing which circuits develop at which points in training.
      * **[Motif](https://distill.pub/2020/circuits/zoom-in/#claim-2-motifs): **A fuzzy notion of some abstract notion that recurs between circuits or features in different models/contexts
        * Neuron splitting and equivariance are one example
        * Attention heads composing to form [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY) are another example
        * [Induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY) themselves are an example, as they seem to underlie more complex behaviour like [translation](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#performing-translation) and [few-shot learning](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#pattern-matching)
      * A model behaviour is **localised **or **sparse **when it is determined by only a few components in the model.
        * Eg, the behaviour of predicting the next token in a repeated subsequence can be localised to the previous token head and induction head.
        * This is again a fuzzy notion:
          * Model behaviour can mean many things (why does a head look here? Why does this neuron activate? Why does the model get high loss on this task?).
            * It can also be __relative __to some default behaviour. Eg, in the IOI task ”why does it have a higher indirect object logit then the subject logit?” tracks the fact that the model predicts the indirect object more than the subject, __given __that it predicts a name that occurred earlier in the sentence at all, but doesn’t eg check whether it predicts a word like “ the” higher than either name.
          * “Determined by” is also fuzzy - normally almost all parts of the model contribute a bit, but some contribute far more than others. See **[causal scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc) **for an attempt to operationalise this.
        * Localisation matters because it makes model behaviour much easier to reverse engineer, and suggests that there is some legible circuit to be understood.
        * In practice, a surprising amount of behaviours are localised, but many are not! Often careful setups of the task and behaviour in question and what is being controlled for can help.
      * **[Microscope AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#5__Microscope_AI) **is the idea that, if we train a superhuman model, rather than needing to __use __it (with all of the associated dangers), we could instead reverse engineer it, learn what it has learned about the world, and use this knowledge ourselves.
        * I don’t think it’s particularly practical, but it’s a nice aspiration for what a world with amazing mechanistic interpretability __could __look like!
    * Representations of Features
      How should we think about the way features are represented in the model
      * __See [the introduction of Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#motivation) for an excellent distillation of these ideas__
      * The **curse of dimensionality **is the idea that things can get weird and confusing when examining high-dimensional systems (relative to low-dimensional systems)
        * Specifically in mechanistic interpretability, the problem is that neural network activations live in a very high dimensional space, and the weights lie in an even higher dimensional space. This is basically impossible to understand intrinsically, so we need a way to break the high-dimensional object down into lower dimensional pieces that can be understood independently-ish.
        * The main way of doing this is to find a way of decomposing the model’s internal activations into **features**, and using this to decompose the weights into **circuits **connecting up features. Obviously, we need to be careful to do this in a way that is principled and actually true to the ground truth of the model, and not just projecting what we want to be true!
      * Key properties of features and their representation inside the model. Note that this is both "things that we really __want__ to be true", and "things that we think models __actually__ do"
        * **Decomposability** aka **decomposable**: We can describe the models internal activations in terms of independently understandable features.
          * This is a crucial claim. If we can't reason about the features (semi-independently, then we don't have any way to resolve the curse of dimensionality!
          * This is also a subtle claim, because features are often highly correlated. It is not at all obvious that we can decompose highly correlated features!
            * For example, let's say that the model has "this is a dog" and "this is a pet" features. A dog is normally a pet, but not always (eg a stray dog). If the model's representation is decomposable, this means that it can __separately__ check whether something is a dog or a pet, and choose which one (or both!) to use in further computation. But we could also imagine a model which does not bother disentangling these, and which is unable to think about the dog feature in the absence of the pet feature.
        * The features are **useful** for computing the model’s output (ie, there is mutual information between that feature and the correct output).
        * The features can be **recovered** from the model’s activations
      * **Linear representations** aka **linearity** aka **features as directions** aka **the linear representation hypothesis** is the hypothesis that features are represented in the model as directions in activation space. Notably, this means that they can be **recovered** by projecting onto the relevant direction.
        * The intuition behind this is that the main thing a model is capable of doing is linear algebra - addition and matrix multiplication, which further breaks down into addition, scalar multiplication, and projecting onto specific directions. Given these capabilities, features as directions is an __extremely __natural way to represent things - if a later layer wants to access a feature it can project onto that feature’s direction, a neuron can easily access and combine multiple features, features can vary independently, and the component in that feature direction represents the strength of that feature.
        * Note that this is a much more specific claim than the fuzzy notion that “model’s internal activations represent features”. I would say that the broader claim means that there is __some __function that can recover the features from these activations, even if we don’t know what that function is, and even if there’s no way to do this without a bunch of noise. 
          * The broad category of "other ways to represent features" is called **non-linear representations**
          * If, specifically, features are directions in activation space, then the function to recover a feature from activations would be projecting onto that feature’s direction. Ideally the feature directions would be orthogonal, so that they can be perfectly recovered with a projection.
        * Note that in this section I am trying to emphasise what representations the __model __is likely to find useful, rather than what I __want __it to do. See **[superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2)** for discussions of limitations of this framework.
        * Implicit in the description of features as directions is that the feature can be represented as a scalar, and that the model cares about the __range __of this number. That is, it matters whether the feature
          * This is an important and confusing point, and it’s worth digging into.
        * Overly technical aside: Implicit in this is the idea that a model cares about the __range __of values that its representation of a feature takes on. It wants to represent the feature as a direction, and to apply linear maps to that, and so it wants to preserve the __magnitude __of that feature.
          * This is in contrast to a purely **binary feature**, which is either true or false about an input.
            * This __may__ still be represented as a direction, but in practice the model just cares about whether that point in space is there, rather than the size of a component in a direction. Given that a model’s core operation is matrix multiplication, it’s not clear to me how much it’s worth representing a binary feature as a direction, vs compressing things together much more.
            * Eg, “if the component in this direction is 0.4 it’s feature A, 0.6 it’s Feature B, and 1.0 it’s feature C” is a much cheaper way to represent binary features.
          * In practice, this is a pretty imprecise description. Most features will be binary(-ish), but the model’s ability to __tell __will often be much more continuous, and involve weighing up evidence.
          * A better phrasing might be whether the feature is **bimodal **(ie, it’s pretty obvious whether it’s true or false, and very few inputs are ambiguous), vs something that’s more uncertain with many ambiguous examples.
            * Eg A feature like “is the current token the” or “is this computer code vs a romance novel” will be pretty obvious and binary, and the model likely won’t care about the magnitude.
            * But for more complex questions (eg, the next word is a female vs male pronoun), the model will likely do a significant amount of weighing up and evaluating evidence, and will care about tracking exactly how confident it is.
          * **Continuous features**: Features that lie on a spectrum, where they can be more or less present for an input.
            * Eg, this is an image of water
            * And, implicitly, where it __matters __how present they are, rather than being boring.
      * An **interpretable basis **is a set of directions in activation space where each direction corresponds to some interpretable feature.
        * In the weak sense, this means a set of directions where we expect each to be an interpretable feature, but we don’t necessarily know what it is. In the strong sense, we have that __and __we know what all of the features are!
      * For any activation in the model, the **standard **or **canonical basis** is the basis by which that activation is represented internally in the computer (and in the code) as a tensor of floats.
        * A key example is neuron activations after a non-linearity - if there are n neurons, their activations is a direction in R^n. R^n is the **activation space**, and each neuron is a **direction**. We call the basis given by neuron directions the **standard basis.**
      * A **privileged basis** is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.
        * Note that a space can have an **interpretable basis **__without __having a **privileged basis**. In order to be privileged, a basis needs to be interpretable __a priori__ - ie we can predict it solely from the structure of the network architecture.
          * Eg It is possible that we could infer an interpretable basis from the weights of the model with a dimensionality reduction technique (like **[SVD](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=4X51Ok83UzyX8JNNhOOMs-Ma)**), but that if we retrained the model this technique would give a totally different interpretable basis.
          * This is a useful distinction, because if we can identify a privileged basis __and __have reason to think that it’s (weakly) interpretable, then we can directly decompose the model’s activations into interpretable features.
        * The main important example of a privileged basis is the basis of neuron directions immediately after an elementwise non-linearity, like ReLU or GELU. (ie the standard basis of neuron activation space) - in the standard basis a ReLU acts on each coordinate independently, but if we change basis then ReLU now affects ranges of coordinates in a weird and confusing way.
        * This is a confusing concept, because we tend to focus on privileged bases, but actually all bases are non-privileged by default - vector spaces are a geometric object, and there’s no intrinsic meaning to any particular basis. We need a __reason __to think that a basis is privileged (like a ReLU).
          * Another framing - if we’re only doing linear algebra, then there’s no such thing as a privileged basis. Operations like addition, matrix multiplication, dot products, etc are unchanged under any change of basis. So we need to look for a special non-linear operation affecting that vector space that might give it a privileged basis.
        * Caveat: Technically, privileged/non-privileged bases is a somewhat leaky abstraction, and the standard basis is always __slightly __privileged. Floating point representations and Adam inherently privilege the standard basis, and are not the same under rotation. I mostly think of it as a spectrum from privileged to not privileged than a binary.
      * A **bottleneck activation/dimension **is an intermediate activation in a low dimensional space between a map from a larger space and a map to a larger space.
        * Most activations in a transformer are bottleneck dimensions: the residual stream, keys, queries and values. (I don’t believe that any activations in Inceptionv1 are bottlenecks?)
          * The residual stream is subtle - it’s not the intermediate activation between a single map in and a single map out, but instead many layers read and write from it, but each operation is purely linear. Not all of these spaces have a higher dimension, but in __aggregate__ many more dimensions go in than come out.
        * Importantly, there are __no __non-linearities involved, so the bottleneck activation has no privileged basis (ish) - all spaces have no privileged basis by default!
          * Intuition: It’s often useful to think about it as an intermediate step when multiplying by a low-rank factorization of a bigger matrix.
      * **Features as neurons **is the more specific hypothesis that, not only do features correspond to directions, but that each **feature **corresponds to a specific **neuron**, and that the neuron’s activation is the strength of that feature on that input. (and that the direction of that feature is the basis element corresponding to the neuron)
        * Aka, the standard basis of neuron activations is interpretable.
        * Importantly, this is not obvious, and probably not entirely true! The only reason this is even somewhat plausible is that neuron activations.
        * There is a decent amount of empirical evidence that this is mostly true of Inceptionv1, but this seems at best only slightly true for transformers.
          * Check out [a bunch of interpretable neurons in CLIP](https://distill.pub/2021/multimodal-neurons)!
        * Note: In a convnet, every activation space is immediately after a ReLU, so this describes every activation. In a transformer it’s somewhat more complicated - this only describes the internal activation space of an MLP layer, and not the residual stream.
      * **Enumerative safety **is the (ambitious!) idea that we could reach a point where we understand __every __feature in the model, and could check through all of these to look for undesirable behaviour, eg deception
    * Superposition
      How to think about superposition, and concepts from A Toy Model of Superposition
      * **[Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)** is when a model represents __more __than n features in an n dimensional activation space. That is, features still correspond to directions, but the set of interpretable directions is larger than the number of dimensions. Intuitively, this is the model __simulating a larger model__
        * This set of >n directions is sometimes called an **overcomplete basis**. (Notably, this is __not __a basis, because it is not linearly independent)
          * A key consequence is that if superposition is being used, there __cannot __be an interpretable basis. In particular, **features as neurons** cannot perfectly hold.
          * [Sparse coding](http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/#:~:text=Sparse%20coding%20is%20a%20class,1ai%CF%95i)** **is a field of maths that finds techniques to find an overcomplete basis for a set of vectors such that each vector is a __sparse __linear combination of these basis vectors.
        * Importantly, if we try to read each feature vs projecting onto some direction, these __cannot __all be orthogonal, so we cannot perfectly recover each feature.
          * Equivalently, because any set of >n directions is not linearly independent, any activation can be written as infinitely many different linear combination of those directions, and so can’t be uniquely interpreted as a set of features.
      * There are two kinds of superposition worth caring about in a transformer: (these terms are how I think about it, but are not standard notation)
        * **Bottleneck superposition** - this is when a bottleneck dimension experiences superposition. (Eg keys, queries, the residual stream, etc)
          * This is not very surprising! If there’s 50,000 tokens in the vocabulary and 768 dimensions in the residual stream, there almost __has __to be more features than dimensions, and thus superposition.
          * Intuitively, bottleneck superposition is just used for **“storage”**, bottleneck dimensions are intermediate states of linear maps and we do not expect them to be doing significant computation.
        * **Neuron superposition** - this is when neuron activations experience superposition. Ie, there are more features represented in neuron activation space than there are neurons.
          * Intuitively, neuron superposition represents doing **computation in superposition** - using n non-linearities to do some processing that outputs more than n features.
        * Intuitively, bottleneck superposition is easier than neuron superposition - the only interference to care about is when projecting onto a feature direction is other features with non-zero dot product with that direction. While in neuron superposition, if one neuron has significant contribution from multiple features, then if one of those feature changes then that will affect all the other features in a weird and messy way.
          * Is this actually harder to deal with in practice for a model? I have no idea! This is a pretty open question.
      * **Neuron polysemanticity **is the idea that a single neuron activation corresponds to multiple features. Empirically we might observe that, a neuron activates on multiple clusters of seemingly unrelated things like [pictures of dice and pictures of poets](https://microscope.openai.com/models/contrastive_v2/image_block_4_2_Add_6_0/878).
        * Subtlety: Neuron superposition implies polysemanticity (since there are more features than neurons), but __not __the other way round. There could be an interpretable basis of features, just not the standard basis - this creates polysemanticity but not superposition.
          * Alternately, neuron polysemanticity is equivalent to saying that the standard basis is not interpretable.
        * Conversely, a neuron is **monosemantic **if it corresponds to a single feature.
          * In practice, the standards for calling a neuron monosemantic are somewhat fuzzy and it’s not a binary - if a neuron activates really strongly for a single feature, but activates a bit on a bunch of of other features, I’d probably call it monosemantic.
        * We can both use polysemanticity to refer to the neuron layer as a whole, or to refer to a specific neuron as being polysemantic. A layer of neurons could contain both polysemantic and monosemantic neurons.
        * Note: Polysemanticity isn’t used to refer to bottleneck dimensions, because there’s no privileged basis to be polysemantic in.
      * **High-Level Concepts:**
      * Intuitively, superposition is a form of **lossy compression**. The model is able to represent more features, but at the cost of adding noise and interference between features. Models need to find an optimal point balancing between the two, and it’s plausible that the optimal point will not be zero superposition.
      * There are two key aspects of a feature:
        * Its **importance** - how useful is it for achieving lower loss? Important features are more useful to represent, and interference with them is more expensive
        * Its **sparsity** - how __frequently __is it in the input? Controlling for importance, if a feature is sparse it will interfere with other features less.
        * In general, problems with many sparse, unimportant features will show significant superposition.
      * An underlying concept is that of the **feature importance curve**. There is, in theory, an arbitrary amount of features that matter, with a long tail of increasingly niche and unimportant features (like whether text occurs in a glossary about mechanistic interpretability!) which are still better than nothing. We can imagine enumerating all of these features, and then ordering them in decreasing order of importance. We’ll begin with incredibly important and frequent features (eg, “this is a new article” or “this is Python code”), and steadily drop off. Under this framing, we should expect models to __always __want to do non-zero superposition, as there will always be some incredibly sparse but useful feature it will want to learn (which may be extremely hard to detect!)
    * Toy Models of Superposition
      A follow-on to the [Superposition section](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2), describing the insights about superposition from Anthropic’s [Toy Model of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) paper. Note that these descriptions are heavily informed by my takes and interpretations, and I'm not sure the authors would agree!
      * **Toy Model:** The model studied in the aforementioned paper. The model is an **autoencoder** - it's trained to map its input to its output. The model has two layers, and has a large input dimension, a small hidden dimension, and a large output dimension, which means that we can study how it compresses its input features into that hidden dimension, and use it to study superposition! 
        * It's worth getting the technical details straight, [check out the paper for this](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup)
        * Roughly:
          * We generate a vector of $$d_{in}$$ iid random uniform numbers, which we make **sparse** by having each feature be zero with probability $$p$$. 
            * This simulates the fact that any given feature is not present in most inputs
          * We learn a linear map $$W$$ that's $$d_{in}, d_{hid}$$ (where $$d_{hid} < d_{in}$$) and map this down to a hidden space
          * We apply $$W^T$$ to map things back up, with __no__ non-linearity in the middle
          * We add a bias and then apply a ReLU
          * We take the mean squared error of this output with respect to the original input $$x$$. 
            * We weight each feature by a constant, the **importance** of that feature. This simulates how important that feature is for reducing the loss.
        * The overall function is thus $$x \to \textrm{ReLU}(W^T W x + b)$$
        * This is a concrete model for **[bottleneck superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=MjDx_NdXa8sSxeq-tiLALtl2)**. The model has more features that it wants to represent than dimensions, and so needs to compress them into some linear dimension. 
          * By training an autoencoder, we measure "how well can the model compress features into a lower dimensional space, such that they can be later recovered"
          * This is a reasonably good model for bottlenecks in the network, like the [queries, keys and values](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=94hCeljxx7mX_WoIBNjpszE1) in a transformer or the **[residual stream](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=DHp9vZ0h9lA9OCrzG2Y3rrzH)**, less so for neuron polysemanticity
          * Though the paper also explores some **neuron superposition** with [an absolute value model](https://transformer-circuits.pub/2022/toy_model/index.html#computation) ([discussed later](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=Gf_Atpykc-Hv8S6gE6XjyFSv))
      * The first big takeaway is that the model uses superposition at all! For sufficiently sparse data, it compresses features into fewer dimensions. It uses the final ReLU to clean up some of the interference.
      * As we increase feature **sparsity**, the model is happy to tolerate more superposition and compresses things more.
        * To understand this, it's worth disentangling interference into two ways that feature 1 and feature 2 can interference:
          * **Simultaneous interference**: Interference where feature 1 __and__ feature 2 are present. 
          * **Alternating interference**: Interference when exactly one of feature 1 and feature 2 are present
        * If feature 1 and feature 2 are independent and occur with probability $$p$$, alternating interference happens with probability around $$2p$$, simultaneous with probability around $$p^2$$. So with high sparsity the model __only__ needs to care about alternating sparsity.
        * Concrete example: Let's say that the model wants to fit two features $$x_1,x_2$$ into one dimension $$y$$ - we can set $$y=x_1 - x_2$$. This is called an **antipodal pair**. 
          * Alternating interference is easy. Because $$x_1,x_2 \in [0, 1]$$, $$\textrm{ReLU}(y)=x_1$$ __if__ $$x_2=0$$ and $$\textrm{ReLU}(-y)=x_2$$ __if__ $$x_1=0$$
          * Simultaneous interference is hard, if both are present then the answer is all over the place!
          * Thus the higher the sparsity is, the better the antipodal pair structure becomes.
        * ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2FIrrWzTqqj8.png?alt=media&token=f6ffe7e8-0a37-462c-be65-a3c42fc83827)
          * ([Diagram in context](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results))
      * As a feature becomes more **important**, interference becomes more costly, so it's more likely to have a dedicated dimension
        * Note that this only describes __relative__ importance - when the feature is more important than the other features that can be learned. The model is trading off which features to learn against each other, so there's not a meaningful sense of __absolute__ importance
      * Superposed features of uniform importance tend to cluster in [geometric configurations](https://transformer-circuits.pub/2022/toy_model/index.html#geometry), where features cluster into subspaces, where each cluster lies in orthogonal subspaces but are in superposition within that cluster. 
        * If clusters form multiple orthogonal subspaces, these subspaces are said to be in a **tegum product**. (See [Adam Jermyn’s post](https://www.alignmentforum.org/posts/3cR2YH9dpr7SmKvCb/toy-models-and-tegum-products) for some intuition on why this happens).
          * Eg if four features are in a cluster in a 3D subspace, it can form a tetrahedron - ie where each feature points to evenly spaced points in 3D space.
        * Does this generalise to real models? Extremely unclear!
        * A key geometric configuration is that of **antipodal pairs**. That is, a single direction, which represents feature 1 in the positive direction and feature 2 in the negative direction. I would guess this is the most likely configuration to generalise.
          * A key observation is that (with ReLUs) antipodal pairs work totally fine if at most one of feature 1 and feature 2 is present, but breaks if both are. If both are sparse, this is totally fine - if they’re there 1% of the time, then this is useful 2% of the time and terrible 0.01% of the time.
        * ![](https://lh4.googleusercontent.com/CRyKoRvP_DP08hoFKDa6qQNuMAxjtcWCRhaH6DhMN5EHt8SDDDfriU7PbH5rhEOZoW7Oy6VX8vpSuIbmlOsR__U7SD-hClkKNQ8gwBvGEUOfcbzGIElsZiCfj032PtrRKlYYq7fBp1UgC01jJTq9BE8gmX-URJwlu9r1GqhRs5ZpZHRXkMml1KUW-8I_)
      * **Correlated **features will interfere more and tend to be more orthogonal, while **anti-correlated **features will interfere less and tend to be within the same tegum product.
        * There’s some evidence that correlated features may form **locally almost-orthogonal bases **- that is, if we take the subset of features that are pairwise correlated, because the model wants low interference, these correspond to orthogonal-ish directions that can be interpreted, even if features __outside __that set will interfere significantly.
        * **![](https://lh3.googleusercontent.com/9-Lv6tlq9Zfw-u1JDctGyHbs-mUKV8jUNBGVUXqf_4jh7xFi_ZPnw2jq9RisZ-o6ed1JEVU285W0DRwo2LUkjWrS9keRb4BbXwhqSB_Fxz_eFV3Q3eB8nFiqaaS03ErP6V8eBJVwRDYd4qGwjousZO6guHmRgNHEn5dZoY3iJ3dwv-5bGsMh16yUXc9V)**
      * The paper mostly focuses on bottleneck superposition, but has some fascinating results on [neuron superposition](https://transformer-circuits.pub/2022/toy_model/index.html#computation). Concretely, they adapt the toy model above to have inputs that are uniform in $$[-1, 1]$$, and train it to output the **absolute values**
        * They find that this still exhibits superposition! Further, it tends to keep the most important neurons as monosemantic, and less important as polysemantic. It tolerates more and more superposition as sparsity increases until all neurons are polysemantic
          * ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2FpVYMrYLIBZ.png?alt=media&token=89506ee5-89db-4057-b73d-722359f362d8)
        * A particularly exciting result is finding the **[asymmetric superposition motif](https://transformer-circuits.pub/2022/toy_model/index.html#computation-asymmetric-motif)**. (I’m not happy with this explanation - go check out the paper!)
          * Rather than having “symmetric” superposition, where the two features interfere equally, the model has **asymmetric superposition**, where feature 1 interferes much more with feature 2 than vice versa.
          * It then uses a separate neuron to clean up the interference, where the presence of feature 1 suppresses any impact of that neuron on the output for feature 2.
          * Ie, if we have both feature 1 and feature 2, the neuron just thinks it has feature 1 - feature 1 has a much larger coefficient than feature 2.
        * ![](https://lh3.googleusercontent.com/RiQU5_gxiDH7JhHAOfNC34MfAmVRtf8ezXeplyxbqts8E_4Q9xedRQNX4nMeJ7Gc4QzCwMUQPCUWdYqDpU1Xjb-QdruCroR5WpGT435Q-xpR13H-TAz4Z2D1wx0D8X93FHF9cDBAwqDnkUzrVjML3YhJVol4tt_JjNPz0g8COfBPPWNBjrdeN9iCOhpG)
      * Obviously, these results are in a toy model, and we ultimately care about real transformers! And it's not obvious how much this generalises. The key question to ask yourself with any toy model paper is how faithfully it captures the real problem. My personal takes:
        * I'd guess that the insights around how sparsity, importance and correlation affect superposition are legitimate and do generalise. And the idea that superposition happens at all! 
          * I expect that correlation and anti-correlation are a very big deal in how superposition manifests in real networks - most features are either correlated or anti-correlated, and this seems like it should significantly affect how things shake out!
        * Some of the motifs __might__ generalise, but I'm more sceptical, especially of the fancy geometry. Features self-organising into orthogonal subspaces seems really useful if true, though I would weakly guess that it doesn't happen much, if at all. 
          * I'd guess that antipodal pairs, important features as monosemantic neurons, and asymmetric motifs are the most likely to generalise, as they feel most generic. 
    * The Broader Interpretability Field
      A brief attempt to position Mechanistic Interpretability in the context of the field of AI interpretability
      * __An attempt to outline the field of interpretability to put MI in context. Note that I am both biased and much better placed to speak about MI than interpretability as a whole! Take the below with a pinch of salt__
      * **Black-box interpretability: **Studies the model as a black box mapping inputs to outputs, and tries to interpret it.
        * Sometimes uses the fact that the model is differentiable to produce interpretations of the input (eg saliency maps), but is more focused on explaining the model’s behaviour than exploring the internals and what they’re doing.
        * This is very different from MI’s strong focus on model internals and understanding the underlying computations
      * **White-box interpretability **aka **inner interpretability: **Techniques that involve looking at the internal activations and weights of the model and trying to understand what they mean. [Towards Transparent AI](https://arxiv.org/abs/2207.13243) is a good survey of the field.
        * This has much more overlap with MI, and arguably MI is a sub-field of this.
        * Often these techniques focus on understanding, eg, what the model’s activations represent, tries to further understand __how __these features are computed from earlier features by reverse engineering the weights, or using causal interventions.
        * MI investigations are more often about studying concrete models on concrete tasks in a lot of detail and fully understanding them, which is a less common emphasis here. But this is a fuzzy description and far from universal, both as a description of MI and non-MI work.
      * **Explainability **aka **XAI **aka **Explainable AI**: The field of trying to __explain __model behaviour, and why it outputs what it does (and why it doesn’t output other things!).
        * Often used interchangeably with interpretability, and the distinctions are fuzzy. The standards of evidence can skew more towards “is this explanation useful to a user” than “is this explanation true to the model’s internal computation”, though obviously the two correlate!
        * A focus is often **Human-Computer Interaction**, making systems more usable and easier for people to interact with (both being safe and doing what users want)
      * **BERTology: **The subfield of interpretability specifically studying and trying to understand BERT. Covers a range of techniques and questions, see [A Primer in BERTology](https://arxiv.org/abs/2002.12327) for an overview
      * Meta: MI is a pretty young field, still trying to figure out its exact definitions and boundaries and goals, so distinguishing it from standard interpretability is somewhat fuzzy. It mostly __feels __distinct to me, in terms of culture, the general research taste and what results and directions people find most exciting and interesting, and the kinds of evidence people most care about. The above is my attempt to articulate these vibes.
    * Linear Algebra
      A very brief + incomplete overview of linear algebra terms
      * __If you want to learn linear algebra, check out __[__3Blue1Brown__](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)__ or __[__Linear Algebra Done Right__](https://linear.axler.net/)__ - this is just a refresher of key concepts that are relevant to mechanistic interpretability.__
      * A **basis** is a set of n vectors corresponding to coordinate axes for an n-dimensional vector space R^n. Any vector can be uniquely expressed as a weighted sum of these vectors, the coefficients are the coordinates of the vector
        * A key mental move in mechanistic interpretability is thinking about the internal activations of the model as living in some vector space, and switching between thinking about the vector as a geometric object in R^n, vs as a tuple of n coordinates in some specific basis, vs as a different tuple of n coordinates in some other basis.
          * We often refer to the vector space of the activations as **activation-space**
          * The main important activation space in a transformer is **residual stream space **- the d_model dimensional vector space that the residual stream lives in. Each layer’s input and output lives in residual stream space
      * If the n basis vectors are all orthogonal and unit length then this is an **orthonormal basis**
      * A key intuition about neural networks is that their internal state consists of activations (tensors) and their main operation is multiplying these activation vectors by matrices. So having good linear algebra intuitions is extremely important! (I recommend building this by doing exercises, and exploring the above resources)
    * Circuits As Computational Subgraphs
      Discussion of how to think about circuits and alternate framings
      * __This section is pretty in the weeds, feel free to skip__
      * [Redwood Research](https://redwoodresearch.org/) have suggested that the right way to think about circuits is actually to think of the model as a **computational graph**. In a transformer, nodes are components of the model, ie **attention heads** and **neurons** (in MLP layers), and edges between nodes are the part of input to the later node that comes from the output of the previous node. Within this framework, **a circuit is a computational subgraph** - a subset of nodes and a subset of the edges between them that is sufficient for doing the relevant computation.
        * The key facts about transformer that make this framework work is that the output of each layer is the sum of the output of each component, and the input to each layer (the residual stream) is the sum of the output of every previous layer and thus the sum of the output of every previous component.
          * Note: This means that there is an edge into a component from __every __component in earlier layers
        * And because the inputs are the __sum __of the output of each component, we can often cleanly consider subsets of nodes and edges - this is linear and it’s easy to see the effect of adding and removing terms.
        * The differences with the features framing are somewhat subtle - see this [comment thread for details](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing?commentId=rGA23WRHgnrb3A3Yx). A key difference is that [causal scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc) allows you to rewrite the model’s computational graph to anything that leads to equivalent computation (eg, we could separate the query, key and value inputs to a head, into drawing from 3 different residual stream inputs, where by default these are equal)
        * This distinction is more important in a transformer than other models. In, eg, a ConvNet, each node is a neuron and (hopefully) represents a feature, but it’s less obvious how to think about an attention head as “representing a feature”. In some intuitive sense heads are “larger” than neurons - eg their output space lies in a rank d_head subspace, rather than just being a direction. And they do not have a **privileged basis** so there is not a clear, principled way to decompose them into lower dimensional objects. The computational subgraph framing side-steps this, by making an entire head or layer a valid node.
      * [Causal Scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc) is an algorithm developed by Redwood Research built upon this framing, described in the techniques section of this glossary
  * Machine Learning
    A whirlwind tour of basic ideas in machine learning, and my intuitions for how to think about them, and __why__ they matter. Not mechanistic interpretability specific, but if you want to reverse engineer an ML system, it's really important to have a clear intuition for what's going on and why! If you get lost, my favourite introduction to basic deep learning is Michael Nielson’s book [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
    * Basic Concepts
      Basic ideas in ML that you should know, with a focus on __why__ these concepts matter, and how to think about them
      * A **tensor **is a generalisation of vectors. A **rank n **tensor corresponds to a grid of numbers with n axes.
        * Eg vectors are rank 1 tensors, a rank 2 tensor is a grid of numbers (like a black and white image with a shade per pixel, or a sequence of vectors), a rank 3 tensor is a cube of numbers (like a batch of black and white images, or a single RGB image, with 3 numbers per pixel), etc
      * **Activations **are the intermediate values computed when running a network - eg the outputs of each layer. By convention, activation normally does not mean the inputs or outputs of the network. Activations are always tensors (and normally vectors).
        * **Activation space **is the vector space that the activations live inside. It often makes sense to refer to regions or directions in activation space.
      * Network **weights **or **parameters **are the learned numbers that determine the function the network applies to an arbitrary input. Parameters are always tensors
        * Note - weights and activations are both represented internally as tensors, and may even have the same rank, but they are conceptually distinct objects.
      * A **MLP **or **Multi-Layered Perceptron **is the classic neural network architecture. Each layer is a linear map from the previous layer’s output followed by a non-linear **activation function**. A **hidden layer **refers to one of the internal activations
        * Note - transformers contain **MLP Layers**, which are a 2 layer MLP (with a single activation function, in the middle).
        * Confusingly, in an MLP, **weights **are the matrices that form linear maps, **biases **are vectors that are added to the output of a linear map, but these are both **parameters **and thus sometimes both referred to as **weights**.
      * **Activation functions** are the non-linearity applied after a linear layer to produce **neuron activations**. Activation functions normally act elementwise, ie the ith element of the output is __just __a function of the ith element of the input vector.
        * **[ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)**or **Rectified Linear Unit **is the classic one, x -> max(x, 0)
        * **[GELU](https://paperswithcode.com/method/gelu)**or **Gaussian Error Linear Unit **is essentially a smoother ReLU (I have no idea why it works, but it’s the main activation used nowadays!)
      * A **neuron **means the part of an MLP hidden layer corresponding to a single element of the activation tensor - ie a 5-dimensional hidden layer consists of 5 neurons.
        * Importantly, neurons have a meaning in the standard basis - if we apply a change of basis to the output space of the layer, we do __not __get 5 new neurons. The activation function (eg ReLU) is applied independently to each neuron, and this stops being true if we apply a change of basis.
      * **Softmax **is a function on n-dimensional vectors that maps $$x_i \to \frac{e^{x_i}}{\sum_j e^{x_j}}$$. We call the term before the softmax the **logits**.
        * **Log_softmax **is the log of this, and maps logits to **log probs**. $$\textrm{logsoftmax}(x)_i = x_i - \textrm{logsumexp}(x) = x_i - \log \sum_j e^{x_j}$$ - importantly the second term is independent of i!
      * Massive tangent on why softmax is motivated:
        * Intuitively, softmax maps arbitrary vectors to probability distributions. A classic use case is MNIST, where the model is trained to classify a picture of a digit into the 10 possible classes. The model outputs a 10-dimensional vector of logits for each picture, and the softmax maps this to a probability distribution.
        * Intuitively, the logits represent the log of the ratio of probabilities, and the denominator is just a normalisation factor to make things add up to 1.
          * Log probs are __also __the log of the ratio of probabilities - they differ by a constant, which corresponds to scaling the probability ratio, which doesn’t change the ratio.
          * Intuition: log probability ratio is the right way to think about probabilities because Bayes theorem says that given some hypothesis A (and its complement not A) and some evidence E; P(A|E):P(-A|E) = (P(A):P(-A)) * (P(E|A):P(E|-A))
            * This generalises to ratios of probabilities over n mutually exclusive classes - A and -A (ie not A) are the n=2 case
          * This corresponds to adding vectors of log probabilities. log(P(A|E):P(-A|E)) = log(P(A):P(-A)) + log(P(E|A):P(E|-A))
            * The first term is the vector of logits post update, the second term is the original vector of logits
          * Neural networks are very good at linear algebra and so very good at adding things, but not very good at multiplying things, so log odds are the natural way to do things, because it can eg have different sub-modules which look for different bits of evidence, and add all the resulting vectors of logits together.
      * Neural networks are always trained to map some **input x** to some **output label y**. The **loss function **is some function scoring how close the predicted y is from the **true **or **ground truth **y
        * The main loss function used in **classification tasks** (which are approximately all tasks modern language models are trained for) is **[cross-entropy loss](https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514)**. For a classification task, the output label y is an integer corresponding to one of a fixed finite set of n output classes. And a model trained with cross-entropy loss outputs a vector of **logits **whose length is the number of classes.** **The vector of logits is mapped to a probability distribution over classes.
          * The loss function is **the average correct log prob**. That is, for each input x, we map the vector of logits to a vector of log probs, and take the element of that vector corresponding to the correct label.
          * Tangent: The intuition is that this is **negative log likelihood**, treating the neural network as an approximation to the discrete distribution over possible class labels y. So a neural network that gets low cross-entropy loss is approximating the maximum likelihood estimator in traditional stats language
          * See the tangent about softmax for more intuition
        * The main loss function used in **regression tasks **(which honestly I don’t see much in interpretability work) is **MSE Loss **aka **mean-squared error **aka **quadratic loss**. Here the label y is a float, and the model outputs a float, and we take the squared difference.
          * This is often averaged over a batch
          * Sometimes the outputs and labels are a vector or tensor of floats, and we __sum __the elementwise squared difference
          * This is easy to mess up so be careful! We sum up squared difference over a single input, and average over a batch.
      * **Gaussian **aka **normal **aka **bell curve** [distributions](https://www.wikiwand.com/en/Normal_distribution)
        * **Standard Gaussian (in 1D) **means a mean 0 and variance 1 normal
        * **Standard Gaussian in n dimensions** means the random distribution over n dimensional vectors, where each coordinate is an independent standard Gaussian in 1D.
          * Key fact - if you apply a rotation (orthonormal change of basis) to a standard Gaussian in n dimensions, you get another standard Gaussian in n dimensions - ie each element remains mean 0 and variance 1 and independent
            * If it’s not orthonormal, it remains mean 0 but not variance 1
    * Training Concepts
      Basic concepts in how ML systems are trained and surrounding intuitions
      * **Training distribution**: The distribution of data that the model is trained on - means the joint distribution of the inputs and the labels
        * Often used non-rigorously - technically all models are trained on the distribution that is just “the finite set of available data”, but this is normally taken to mean a theoretical distribution of data __like __that, where we could imagine the actual training data being drawn from it.
        * **In distribution data **is data like the training data - the test and validation set are usually in distribution
        * **Out of distribution data/OOD **is data from a different distribution, eg we train the model to classify pictures of cats vs dogs, then give it a picture of a gerbil.
      * **SGD **or **stochastic gradient descent** is the classic optimizer used for neural networks. We give the model a batch of data, measure the loss, look at the gradient of each parameter with respect to the loss, and update each parameter with its gradient times a constant called the **learning rate**.
        * Inputs at each training step are normally given as a **batch**, ie a list of several inputs. These are normally stacked into a tensor with a batch axis, and the loss is calculated independently for each element of the batch and averaged over the batch. This is more efficient because it can be parallelised, and gives a better estimate of the gradient.
          * Intuition: because the loss is averaged over the batch, which is linear, the gradient of the loss with respect to each parameter is also the average of the gradient for each batch element. In theory, we want to be doing real gradient descent, where we take our updates according to the expected gradient of the loss across the __entire __training distribution. But, the gradient for any particular input will on __average __have the expected gradient across the training distribution (by definition). Taking the average gradient across a batch still has the average as the expected gradient, but lowers the noise, while being far cheaper than evaluating the gradient across the entire training distribution.
      * **Weight decay** aka **L2 regularization** is when every gradient update, we also decrease each weight by a constant scale factor (scale factor $$1-a$$ is close to 1)
        * This is equivalent to subtracting the weights times a small constant $$a$$ (close to zero)
        * This is equivalent to adding $$a/2$$ times the sum of squared weights to the loss (this is the standard framing, but IMO less intuitive)
        * Tangent: In linear regression this is equivalent to putting a Gaussian prior over weights.
      * **[Adam](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/#:~:text=Adam%20optimizer%20involves%20a%20combination,minima%20in%20a%20faster%20pace.) **is the main optimizer to train modern ML models, a fancier version of SGD (Stochastic Gradient Descent).
        * Adam tracks an exponentially weighted moving average of the gradient, and of the elementwise squared gradient, and the gradient updates are the elementwise
          * **EWMA **or **exponentially weighted moving average **is a way to calculate a moving average as you scan through a sequence $$s_n$$. There is a fixed parameter b, and the nth average $$x_n$$ is $$x_n = b \times x_{n-1} + (1-b) \times s_n$$.
            * This is useful because it generalises easily to tensors, and doesn’t require you to store any memory (unlike a normal moving average)
            * Intuitively, this expands to $$\frac{x_n}{1-b} = s_n + b \times s_{n-1} + b^2 \times s_{n-2} + …$$
          * Typical Adam learning rates for transformers are 1e-3 to 1e-4 for pre-training and 1e-5 for fine-tuning
        * This is convoluted - intuitively, the squared gradient tracks the “variance” or “noise level” of the gradient, and dividing by it gives noisier parameters a lower learning rate. This is useful because if a parameter has noisy gradients, then you want to take smaller steps, because your gradient information is less trustworthy
        * **[AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)** is a variant of Adam with **weight decay**.
          * Importantly, this is __not __the same as using the Adam optimizer and setting the weight_decay parameter to non-zero, never do that.
          * Tangent: The difference is that the EWMAs are only calculated from the normal gradients, and weight decay is applied before the averaging (ie weight decay updates on batch n __only __includes information about the parameters at batch n, not updates from past parameters). See the PyTorch page for pseudocode, which links to a paper explaining why this matters.
    * Training Dynamics
      Clarifying some terms are training dynamics of a system - how it develops during training and why
      * **Memorization**: When the model learns to do well on the training set but not the test set. Intuitively, it doesn’t learn any structure of the data (ie circuits that generalise between data points), but learns a **lookup table**, which separately maps each data point to its label
        * **Generalisation**, in contrast, is when the model performs well on both the training and test distribution
        * This is similar to **overfitting **in classical statistics, but a model can eg also memorise data with random labels, where there is no structure to learn. In general, there is a spectrum from learning the data well to overfitting, and a spectrum between generalising to memorising
        * Intuitively, a model should find it __exactly as hard__ to memorise data with randomly shuffled labels, as it does to memorise the actual training data
        * A key intuition is that memorisation becomes more complex the larger the training set is (because you learn a larger lookup table), while generalisation is exactly as hard for any training set size, since you’re learning the underlying structure
      * **Phase transition** aka **phase change**: When the model suddenly develops some capability during a brief period of training. Examples are [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) (where models develop the circuit of induction heads and the capability of in-context learning) and [grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)
        * Sometimes called **S-shaped (loss) curves**, as there’s a plateau, rapid increase or decrease, and then another plateau.
        * Phase transitions can occur over **training, **over **dataset size** and over **model size/scale** (sometimes called **training-wise, data-wise, **and** model-wise)**
          * [Induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY) and grokking are training-wise.
          * An example of a model-wise transition is arithmetic. GPT-3 can (fairly) reliably do 3 digit addition, but smaller models are basically terrible. There’s just a sudden jump in ability.
            * This phenomena as we scale up is sometimes called [emergent phenomena](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html)
        * Adam Jermyn has [a good post](https://www.alignmentforum.org/posts/RKDQCB6smLWgs2Mhr/multi-component-learning-and-s-curves) on why this should be an inherent feature of model learning
      * **Grokking**: A special type of phase transition, where the model __first __memorises the training data (ie has good train loss and bad test loss) and then suddenly learns to generalise (so test loss suddenly becomes good). Ie, initially train and test loss diverge, and then there’s a sudden decrease in test loss that leads them to converge.
        * Notably, [this was exhibited](https://arxiv.org/abs/2201.02177) when training small transformers on a range of algorithmic tasks
        * Grokking tends to require **regularisation **and **limited training data**, and seems to an intermediate phase between the model immediately generalising (a lot of data) and the model immediately memorising and never grokking (low training data)
          * Intuitively, the model has a trade-off between learning the generalising solution, and the memorising solution, which are both valid circuits. Memorisation is more complex with more data, generalising is exactly as complex, and regularisation incentivises simplicity. So there’s some critical mass of training data where memorisation is more complex, and the model prefers to generalise.
          * We get the weird __grokking __behaviour, when some feature of the problem makes memorisation “easier to reach”. The model wants to memorise and generalise, but mildly prefers to generalises, but memorisation is easier so it gets there first. But it ultimately prefers to generalise, and moves slowly towards that in the background.
        * In my work, [A Mechanistic Interpretability Analysis of Grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking), I showed that when grokking modular addition, the model learns a clean, interpretable algorithm using trig identities. And that training can be broken down into 3 phases:
          * **Memorisation**: The model memorises the training data and does poorly on the test data. We call the current algorithm learned the **memorisation circuit**
          * **Circuit formation**: The model slowly forms a __separate __trig-based circuit to do the general problem of modular addition, and it slowly transitions from the memorising circuit to the generalising circuit. Throughout this period it maintains good train performance, and has poor test performance (even a weak memorisation circuit adds a lot of noise to the training data)
          * **Clean-up**: The model gets good enough at generalising that it no longer needs the memorisation circuit, and the regularisation incentivises it to clean it up. It’s __already__ formed generalising circuit, but the clean-up gets rid of the memorisation noise, and this leads to the sudden phase transition.
      * **[Bias-variance trade-off](https://www.wikipedia.org/en/Bias%E2%80%93variance_tradeoff)****: **A key result in classical statistics that there is a trade-off between variance in model error (ie the average __squared__ error) and bias in model error (ie the average error).
        * Intuitively, more complex and expressive models have lower bias (they can learn the structure of the data well) but higher variance (they can overfit to noise in the problem - they have more capacity to learn structure, for good and for bad!)
      * **[(Deep) Double descent](https://openai.com/blog/deep-double-descent/): **A phenomena where, as the size of a model increases, test loss first decreases, then increases, then __decreases again__. The increase is predicted in standard statistical theory - the model has enough parameters to overfit to noise - but the second decrease is a weird mystery!
        * This is likely part of why the conventional wisdom in classical statistics is not to make your models too big, while in ML the conventional wisdom is that bigger = better
        * This is also exhibited as you increase the amount of data in the model! That is, there are situations where having more data will make your model __worse__ at the task!
      * **Path dependence / Path Independence: **Whether the final trained model depends on the specific details of the path taking during training (path dependence), vs just being a function of the problem setup and training data, but where the model will always end up with a similar final model (path independence)
        * For example, if there’s only one way to solve a problem, we would expect things to be path independent. But if there are many equally good ways, there might be some random path dependence to where it ends up.
        * This matters, because it suggests that techniques to influence the training dynamics (certain kinds of regularisation or curriculums) may be ways to influence the final model to have a solution we prefer to one we don’t (eg, give us the right answer because you care about being helpful, rather than because it’s what we want to hear)
    * Misc
      * **Machine Learning** aka **ML** vs **Deep Learning** aka **DL** vs **Neural Networks** etc form a mess of terms that can be confusing when entering the field. This is particularly messy because all of the above "sound cool" and are often used as flashy buzzwords in technically incorrect contexts. Here's my attempted breakdown (I'm not sure there's an objective truth here though!):
        * **Neural networks** are a family of techniques to model data. They can be broken down into a composition of a series of layers, where each layer is a simple function that roughly consists of a big stack of matrix multiplications and non-linear functions. There are many, many variations on this, so it's hard to really be precise here, but they revolve around this kind of core building block.
        * **Deep neural networks** are networks with many layers, and **deep learning** refers to machine learning that uses deep neural networks. This is the technical definition, but in practice it's just used as a buzzword nowadays, since most interesting machine learning uses deep networks. 
          * Even work that __does__ study tiny, shallow networks, tends to refer to this as deep learning nowadays
        * **Machine Learning** is the general study of training models to fit to data. Importantly, the model has parameters, and is fed a bunch of data, and learns parameters that are good at predicting that data. But, in practice, this is mostly used as synonymous with **Deep Learning** nowadays, since that's where the hype and majority of the interesting work in the field is happening. 
        * **Classic machine learning** aka **traditional machine learning** aka **statistical learning** is the study of non-neural network techniques for training models to fit data. Linear regression, logistic regression, support vector machines, random forests, etc. 
          * I often see people new to deep learning doing old fashioned machine learning courses that spend a bunch of time on these ideas first. In my opinion these are __not__ on the critical path for learning deep learning, though are worth learning about at some point.
      * **Log space **and **linear space** are fuzzy concepts to describe a space of numbers - in linear space, the distance between two numbers is their difference, in log space the distance is their ratio. Intuitively, you would refer to things being close in log space, or movement in log space, when considering numbers across a wide scale, and use linear space by default.
        * A key example is that logits and log probs are in log space, while probabilities are in linear space.
        * This is a fuzzy concept, but it’s a term people sometimes use, and worth being able to recognise.
      * **[Scaling Laws](https://arxiv.org/pdf/2001.08361.pdf)**: A very important result in ML that, as you scale up the amount of data, amount of compute or number of parameters of models, the performance improves according to an extremely smooth power law. This holds up across over 7 orders of magnitude of model compute.
        * This is significant, in part because it’s just extremely surprising! Things do not fit to smooth curves so well across so much data by accident.
          * Often analogised to statistical mechanics - weird randomness and chaos on small scales averaging into an extremely smooth trend at large scale.
        * It has practical significance as well, because it suggests that companies continuing to invest larger and larger amounts of money into training large systems will continue to produce more capable systems. It further allows them to predict __how __to do this (how much data to train on and how big the model should be
          * [**Chinchilla**](https://arxiv.org/abs/2203.15556)** **is a large language model from DeepMind. It is notable, because it was trained on 1.4T tokens and has 70B parameters (for contrast, GPT-3 was trained on 300B tokens and has 175B parameters). The key result of the paper was that the original scaling laws work miscalculated the exponents in the scaling law, and that it is optimal to use smaller models trained on far more data.
        * This is further significant, because there seem to be scaling laws for many other aspects of models - [how data efficient transfer learning is](https://arxiv.org/abs/2102.01293), [how well different alignment techniques work](https://arxiv.org/abs/2112.00861), etc. If these are true and robustly hold up, this seems to suggest some deep principles of how models work, and how we might be able to predict their future capabilities.
          * Conversely, these are often __not __robust trends that hold universally. [**Emergent phenomena**](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html)** **is a notable current area of research studying capabilities (like arithmetic or chain-of-thought prompting)
      * **Reinforcement Learning** aka **RL**: A major subfield of machine learning that focuses on training **agents** to maximise a **reward**.
        * A standard setup is that we think of the model as an **agent** acting in some **environment**. The model outputs an **action**, which changes the **state** of the environment, and the environment reacts by giving the agent an **observation**. The model also receives a **reward**, which is a function of the actions, observations and states. The core goal of RL is to effectively train a model to take actions that maximise expected reward.
          * **Expected reward** means the expected value of the reward, over any inherent uncertainty in the environment and reward function
        * An important difference between RL and regular ML is that the reward function is __not__ differentiable. But all of ML is highly dependent on being able to differentiate your function to apply gradient descent. There are a variety of hacks and approaches to deal with this fundamental difficulty. 
        * There are a __lot__ of important ideas in RL that are not within scope for this explainer! Further reading:
          * [Lilian Weng has a great post](https://lilianweng.github.io/posts/2018-02-19-rl-overview/) explaining RL and common algorithms
          * [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/) is a more in-depth tutorial that guides you in understanding and implementing important algorithms
            * **Deep RL** is the subfield of Reinforcement Learning where the model in question is a (deep) neural network. In my (biased!) opinion, this is the only part of RL that I'm interested in.
  * Transformers
    A study of how transformer language models work, how to think about them, what's going on inside, and all the key terms. This is pretty long and in depth, even though it's not mechanistic interpretability specific, because to do mechanistic interpretability on a system you __really__ want a rich model of how they work, and all of the moving parts inside. 
    * __For more background, see: [my transformer overview](https://neelnanda.io/transformer-tutorial), and [my guide to implementing GPT-2](https://neelnanda.io/transformer-tutorial-2), with accompanying [template code](https://neelnanda.io/transformer-template) and [solution code](https://neelnanda.io/transformer-solution) for you to code it yourself. See also [Jay Alammar's Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) for an alternate take__
    * __There's fairly inconsistent notation for some of these things. I try to align the terminology with how it's used in my [TransformerLens](https://github.com/neelnanda-io/TransformerLens/) library for mechanistic interpretability of transformers.__
      * __By convention, all weight matrices are written so that they multiply on the right (ie $$x \to x M$$ not $$Mx$$), which is the convention in code but not maths - sorry!__
    * Transformer Basics
      Basic concepts and terms when thinking about transformers
      * __This is mostly a description of how GPT-2 works - there are many variants, but they’re all essentially the same thing, even GPT-3, Gopher, Chinchilla and PaLM__
      * ![The Transformer Architecture](https://lh5.googleusercontent.com/oMRBmT0Do5V4mSh_oYMq7HgTqEIt7tz6rhs_oLdfwWIY1pFUcjj95P7AU7SKdoM8ZOTLlqw_J2nAj_iCsqUi00KXJAmHZcf_h2c1ul-zLf4BgERRU6hVBa3IMeceP9juFNhBz2FybwDF6_714msbDUAk4pBTgcxUpax_qE5Ar9b9o0LWbKJA-jmqVtR5)
      * The **transformer** is the neural network architecture used for modern language models (and a bunch of other models, like CLIP, Whisper and DALL-E 1!).
      * Fundamentally, the transformer is a **sequence modelling model**. The input is a sequence of **tokens **(roughly, sub-words) which are mapped to the output, a tensor of **logits**, which are mapped by a **softmax **to a probability distribution over possible next tokens**.**
        * We call the input sequence of tokens the **context**
      * A transformer consists of an **embedding layer**, followed by n **transformer blocks**/**transformer layers**, and finally a linear **unembed **layer which maps the model’s activations to the **output logits**
        * Confusingly, a **transformer layer **actually contains two layers, an **attention layer **and an **MLP layer**.
      * Internally, the central object of a transformer is the **residual stream. **The residual stream after layer n is the sum of the embedding and the outputs of all layers up to layer n, and is the input to layer n+1.
        * In the standard framing of a neural network, we think of the output of layer n being fed into the input of layer n+1. The residual stream can fit into this framing by thinking of it as a series of **skip connections** - an identity map around the layer, so output_n = output_layer_n + skip_connection_n = output_layer_n + input_layer_n.
          * I think this is a less helpful way to think about things though, as in practice the skip connection conveys far more information than the output of any individual layer, and information output by layer n is often only used by layers n+2 and beyond.
        * The residual stream can be thought of as a **shared bandwidth/shared memory **of the transformer - it’s the only way that information can move between layers, and so the model needs to find a way to store all relevant information in there. 
          * We might expect to find different subspaces dedicated to different kinds of information - eg "the current token is", "the subject of the sentence is", "the previous token is", "the position of the start of the line is", etc
        * **Memory management** is the phenomena when part of the model are used to manage this shared memory. Eg if one head/neuron's output is used by a certain layer and then no longer needed, a later layer neuron may delete it. 
          * One motif that can indicate this is a neuron with significant negative cosine similarity between its input and output vector
      * More details on how the residual stream works:
        * At the start of the model, the residual stream consists of the **embedded tokens **plus the **positional embedding**, this is a vector at each token position (or equivalently a position by d_model rank 2 tensor)
        * This is then input into the first attention layer, and the layer’s output is added to the residual stream
        * The new residual stream is then input into the first MLP layer, and the layer’s output is added to the residual stream
        * Etc
        * Finally, the **unembedding** is a linear map which maps the final residual stream to a tensor of logits (one number for each element of the vocabulary)
        * Importantly, this means that the input and output to each layer has the same dimension and lives in the same space
      * Importantly, there is a __separate __residual stream for each position in the sequence. The model’s processing is applied in parallel (ie with the same parameters) to the residual stream at each position.
        * Intuitively, the attention layers move information between the residual streams at different positions, and the MLP layers apply non-linear processing to that information, once it’s been moved to the right place
        * Attention layers are the __only __layers that can move information between token positions - MLP layers, LayerNorm, Embedding, Positional Embedding, Unembed, cannot.
      * The tensor of logits (position by vocabulary size) gives a probability distribution over next tokens. Importantly, there is a vector of logits for __each __position in the sequence - so an input of n tokens makes n predictions for the next token. The logits at position k predict the token at position k+1
        * GPT-2 uses **causal attention**, meaning that information can only move forwards (equivalently, attention layers can only look backwards), which means that the residual stream (and thus vector of logits) at position k is __only __a function of the first k tokens (so it can’t trivially cheat and look at the next token).
      * GPT-2 is trained with **next token prediction loss**, ie, its loss function is the cross-entropy loss for predicting the next token, averaged over the **context** (ie all tokens in the sequence) and over the batch.
      * GPT-2 is a **generative model**. By feeding in text and repeatedly sampling a next token and appending that to the end of the input, it can generate text.
      * Key hyperparameters: (names are the convention in TransformerLens, other code bases or papers may vary)
        * **d_model **is the width of the residual stream (768 in GPT-2 Small)
          * Aka **embedding_size **or **hidden_size**
        * **d_mlp **is the number of neurons in the MLP layer (3072 in GPT-2 Small)
          * Ie, the MLP layer consists of a linear map W_in from R^d_model to R^d_mlp, a non-linear activation function, and a linear map W_out from R^d_mlp to R^d_model
          * Almost always set to 4 * d_model (for some reason)
        * **d_head **is the internal dimension of the attention heads, ie each head’s queries, keys and values have length d_head (64 in GPT-2 Small)
          * Often defaults to 64
        * **n_heads **is the number of attention heads per head layer (12 in GPT-2 Small)
          * By convention, n_heads * d_head == d_model
        * **n_layers **is the number of layers of the transformer. Note that each “layer” contains 1 Attention __and __1 MLP layer. Does not include embedding, layernorms, or unembedding (12 in GPT-2 Small)
          * **2L Transformer **is equivalent to 
          * Sometimes called the number of **transformer blocks**
        * **d_vocab** is the size of vocabulary, ie the total number of possible tokens
        * **n_ctx **is the maximum context length, ie the longest sequence of tokens a model can be run on
          * By convention, during pretraining the model is run on batches of sequences of full length (ie n_ctx)
          * If a model has absolute positional embeddings it cannot even be run on longer sequences, relative positional embeddings (eg rotary) can be. (But it won’t necessarily be __good __at modelling them, since it’s out of distribution)
    * Transformer Components
      What the types of layer and component in a transformer are, their parameters and hyperparameters, and how to think about them mechanistically
      * **Tokenization**: The process of converting arbitrary natural language to a sequence of elements in a fixed, discrete vocabulary. This is done with a **tokenizer**, which has a fixed vocabulary of tokens (essentially, sub-words), and applies an algorithm to deterministically break down the text into a sequence of tokens that are elements of a fixed finite **vocabulary** (normally about 50,000 tokens). This is equivalent to converting the text into a sequence of integers.
        * This is a deterministic algorithm, and we can **de-tokenize **to uniquely(ish) recover the input text.
        * I recommend playing around with [OpenAI’s tokenization tool](https://beta.openai.com/tokenizer) to get a feel for this. Tokenization is weird!
        * Note: Tokens do __not __all have the same length. Intuitively, the goal is to have common substrings become few tokens, and rare substrings become many. (Which is why it’s better than eg using characters)
          * Eg “ ant|idis|establishment|arian|ism” is 5 tokens, “af|j|d|hs|bs|dh|fb|df|sh|bd|isf|h|bis|df|ds” is 15
            * There isn’t a consistent convention for writing tokens that I know of, I use pipes (|) to show token boundaries, since it’s a rare token used in text.
          * The algorithm commonly used is called **Byte-Pair Encodings (BPE)**. You start with a fixed vocab of tokens, eg the 256 ASCII tokens. You tokenize a bunch of text. Then you identify the most common pair of tokens, and make that a new token. Repeat this 50,000 times.
            * Notably, this will give a different tokenization for different text datasets!
        * Subtleties: Tokenization gets __super __messy when you dig into it. Having a preceding space or capital changes the tokenization of a word.
          * Eg numbers are not tokenized with a consistent number of digits per token, like |1|000000| +| 87|65|43|
        * Special tokens:
          * **BOS (Beginning of Sequence) Token: **A special token that goes at the start of the context. Some models are trained with this, others are not.
            * This is useful because it gives attention heads a **resting position **- attention patterns are probability distributions that add up to one, and so looking at a BOS token can mean that the head is off.
            * OPT, and all of the toy and SoLU models in TransformerLens were trained with this, GPT-2 and GPT-Neo were not.
          * **EOS (End of Sequence) Token: **A special token, normally used to separate different texts when they are concatenated together in the same context.
            * This is used in pre-training, because for efficiency reasons we want the training data to be in full batches of max context length (n_ctx) sequences of tokens. Each text normally won’t be a multiple of n_ctx, so by concatenating them we can fill out each batch.
          * **PAD Token: **A special token, when a tokenizer tokenizes a sequence with n tokens, and wants to output m>n tokens, it appends m-n padding tokens to the end.
            * This isn’t very relevant to generative models - padding is at the end, and heads can only attend backwards, so you just ignore the padding, and it doesn’t matter what the model does with it. I believe it’s not used at all during training, but may be used at inference.
      * **Embedding**: The first layer of the model, which converts the token (an integer) at each position to a vector (of length d_model), the starting vector of the residual stream. It does this with a **lookup table**, **W_E (**shape d_vocab x d_model), mapping each element of the vocabulary to a different learned vector.
        * Note - lookup tables are equivalent to applying a **one-hot encoding **to the token, and then multiplying by W_E. A one-hot encoding is when you map an integer k between 0 and n-1 to a vector of zeros of length n, with a 1 in the kth position. IMO it is approximately never useful to think about one-hot encodings rather than lookup tables.
      * **Positional information/embedding/encoding**: By default, each position in the sequence looks the same to the transformer, as attention looks at each pair of positions independently, and doesn’t care about where in the sequence they are. This is obviously bad, because position contains key information! There are a range of solutions to this:
        * GPT-2 does with **learned, absolute positional embeddings** - there is a learned lookup table mapping each position to a vector of length d_model that is added in to the residual stream.
          * **Absolute **means that position k contains information
        * **[Shortformer positional embeddings](https://arxiv.org/abs/2012.15832)** are a variant of absolute positional embeddings where the positional embedding is added in to the input to the query and key computation of each attention layer but __not __to the value vector, and __not __to the residual stream. 
          * The original paper used this on top of sinusoidal, [my TransformerLens library](https://github.com/neelnanda-io/Easy-Transformer) uses it on top of learned absolute embeddings.
        * **[Sinusoidal embeddings](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)**are **fixed, absolute** - there’s a fixed lookup table mapping each position to a series of sine and cosine waves of different frequencies (ie, the ith element of each lookup vector forms a wave across the context).
        * **[Rotary/RoPE](https://blog.eleuther.ai/rotary-embeddings/)** is a popular method today (and I hate it), it’s a **relative **method and doesn’t add anything in to the residual stream. Instead it intervenes on the query key dot product to make it a function of the relative position.
          * Tangential intuition: If d_head was 2, key and queries are both 2D. If we apply a rotation by n degrees to the queries and keys at position n, then the dot product of key m and query n is __just __a function of n-m. Intuitively, rotating __everything __by n degrees preserves dot products (because it’s a rotation), so this is equivalent to fixing the query and rotating the key back by m-n. This is efficient because rotating query and key n by n degrees can be done very cheaply for arbitrary context length, and the rest of the code is the same.
            * To do queries and keys longer than 2, they pair up adjacent elements of the query and key, and rotate each pair by n times a different fixed angle.
            * This is used in GPT-J and GPT-NeoX, which in practice only do rotary on the first ¼ of the dimensions of the keys and queries, and leave the final ¾ unchanged
      * **LayerNorm**: A normalisation layer used in a transformer, used whenever the residual stream is input into a layer (ie before the attention, MLP and unembedding layers). It acts independently on the residual stream vector at each position. Roughly, it sets the vector of the residual stream to have mean 0 and norm 1, and then gives each element a new scale and mean (as learned, per-element parameters)
        * LayerNorm has 4 steps: (in my terminology, this is not standard)
          * It first subtracts the mean of the vector (**centering**)
          * It then divides by the standard deviation (**normalising**)
          * It then **scales** (elementwise multiplies with some **learned scale weights (w))**
          * It then **translates **(adds on a **learned bias vector (b)**)
        * Somewhat analogous to **[BatchNorm](https://www.wikipedia.org/en/Batch_normalization)**, but it doesn’t need any averaging over the batch
        * Intuitively, it makes residual stream vectors __consistent__ - mapping them to the same size and range, in a way that makes things more stable for the layer using them
          * But in practice, people use it because it works, and this kind of intuition can easily be total BS.
        * Note - each element of the residual stream has a __separate __scale and translation parameter. There are 2*d_model learned parameters per LayerNorm layer
        * Aside: Note that LayerNorm changes the functional form of the model. In particular, the “divide by the standard deviation” step is non-linear (but the other 3 are linear).
      * **Unembedding**: The final layer of the model, applies a linear map (**W_U, **d_model by d_vocab) to the final residual stream at each position to produce a tensor of logits (position by d_vocab)
        * A LayerNorm is applied between reading the residual stream and applying the unembed map.
        * **Tied embeddings**: Often models will **tie **their embed and unembed. This means setting $$W_U = W_E^T$$. This doesn’t significantly affect performance, but in my opinion is wildly unprincipled and annoying.
          * Fuzzy intuition for why this is bad: the set of __input __tokens and the set of __output __tokens are different spaces with a different structure - “ The” is likely to follow “.” but not vice versa, while tying the embeddings imposes symmetry on them.
          *  A concrete example is a zero layer transformer - where we remove everything __apart__ from the embedding and unembedding. There is no attention, so the logits at position $$k$$ is just a function of token $$k$$, which is the $$k$$th row of $$W_E W_U$$. This is trying to predict what __follows__ $$k$$, which is just the distribution of bigrams - which tokens follow $$k$$ most often in the training data? 
            * If $$W_U = W_E^T$$ then this is a symmetric matrix, but bigrams are __not__ symmetric. Donald -> Trump, but Trump -/> Donald
          * A further reason is that the model uses the [residual stream as shared bandwidth](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=C4SK_El5WKWCmowYGCvYMUBy). Every component reads and writes to it, and the model may want to specialise different subspaces for different information. 
            * If $$W_U = W_E^T$$ then it must use the __same__ subspace for "what was the input token" and "what do I think the next token was". 
            * But, intuitively, these are different kinds of information! If the current token is $$Harry$$ and a previous component guesses that the next token will be $$Potter$$, we don't want later components to think that the __current__ token is $$Potter$$!
          * GPT-2 does this, GPT-Neo does not, GPT-3 does not, my interpretability friendly models do not.
      * **Attention Layer** aka **attn**: The first part of each block, this layer moves information between positions. An attention layer consists of several **attention heads**, these each have their own parameters and act independently and in parallel (the heads do not interact) and the output of the attention layer is the sum of the output of each head.
        * Attention is notoriously confusing! [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/#self-attention-in-detail) is a good resource here for an overview. See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [my walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) for a __much __deeper dive into attention.
          * See the **Attention Heads** and the **A Mathematical Framework** sections for many, many more details on attention
        * GPT-2 uses **causal **or **masked attention**, where each head can only look backwards, and move information forwards. Other models like BERT use **bidirectional attention** without this constraint.
      * **MLP Layer: **The second type of transformer layer. It has a linear map from the residual stream to an **internal MLP space** (**W_in**, d_model by d_mlp), applies a non-linear activation function (normally **[GELU](https://paperswithcode.com/method/gelu)**, an elementwise function), followed by a linear map from **MLP-space** back down to residual stream space (**W_out**, d_mlp by d_model)
        * Intuitively, MLP layers are used to process information at a position. They act in parallel at all sequence positions, using the same parameters, and do not move information between positions.
        * A LayerNorm is applied when reading from the residual stream
        * **MLP** stands for **Multi-Layered Perceptron**, the original kind of neural network. In this case, it’s just a two layer network (or one hidden layer network)
        * Note that there is only one activation function, between the two linear maps, and not one at the start or the end.
      * A** transformer neuron** refers to a single one of the internal activations in an MLP layer. The neuron has a **vector of input weights **(the relevant column of W_in) and a **vector of output weights **(the relevant row of W_out).
        * Note that both of these are d_model length vectors, and correspond to directions in the residual stream.
        * We refer to the neurons as a direction in the **standard basis**, ie the basis in which the MLP activations are represented.
        * Each neuron has a GELU applied to it, and there’s no dependence between them, which means that we can think of the MLP layer as being the sum of many neurons acting independently and in parallel.
        * Importantly, this is a __different __usage of the word neuron to in classic neural networks or ConvNets, where a neuron’s output is the scalar representing it in neuron activation space. Because the MLP layer has both an output and input linear map and the central space of the model is the residual stream, the neuron is associated with __both __its input and its output weights. So we interpret the neuron output as its contribution to the residual stream, not as a scalar in neuron activation space.
        * Note also that neuron doesn’t refer to any other bits of the transformer - there’s no such thing as a residual stream neuron, head neuron, key neuron, query neuron, value neuron, etc. See later discussion of **privileged bases **for more intuition.
    * Attention Heads
      A more in the weeds dive into key concepts in attention heads and how to think about them
      * __This can be pretty dense - I recommend checking out __[__the code in my clean transformer implementation__](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Attention)__ (note that this is vectorised code doing all heads in parallel) and __[__the contents on self-attention in the Illustrated Transformer__](https://jalammar.github.io/illustrated-transformer/#self-attention-at-a-high-level)
      * **Attention Head**: An attention head is the one component in a transformer which can move information between token positions. An attention layer consists of several attention heads which act in parallel - each head chooses what information to move to the current token independently of the others, and the output of the layer is the sum of the output of each head.
        * Note - attention heads move information between __positions__, not __tokens__. A specific position corresponds to a specific input token, but after a few layers it will contain significant information about the rest of the sequence. The attention head can only move the information about the rest of the sequence, or about the token’s relationship to the rest of the sequence, if it chooses.
          * Eg, it could move the feature “this token position contains the final token of the subject of the sentence”, but not __what __subject or __what __token.
      * A key intuition is that an attention head consists of two computations - how much information to move from each source position to each destination position, and __what __information to move. These are computed with separate-ish activations and parameters.
      * **Step 1:**  Where to move information from and to.
        * In particular, for each pair of tokens (an earlier **source token** and a later **destination token**), the head moves some information from the source token to the destination token. The amount of information moved is determined by a scalar activation for each pair of positions, this number is called an **attention weight** and together are called the **attention pattern**. (Intuitively, “how much attention to pay”)
          * These are sometimes called **src**, **source position**, **source **and **dst, dest, destination position, destination** respectively
        * For each position, we compute a **query** vector (**q **in TransformerLens) and a **key **vector (**k** in TransformerLens).
          * Intuitively, a query represents “what information am I looking for?” and a key represents “what information do I have to offer?”
          * These are linear maps from the residual stream. They are calculated by the weights and biases **W_Q **([d_model, d_head]) and **b_Q **([d_head]), and **W_K **and **b_K **respectively.
        * For every __pair __of source positions and destination positions (where the source is before the destination), we take the dot product of the source key and the destination query. This gives us a [position, position] tensor of **attention scores **(aka **attention logits**, called **attn_scores** in TransformerLens). A high attention score means “that key is relevant to this query”
          * By convention, this tensor is in the order [destination_position, source_position] (though these are normally the same number).
        * We then do a **row-wise softmax**. That is, for each destination token, we take a softmax over the attention scores to all prior source tokens (including the current destination token) to produce a probability distribution of **attention patterns** **(**aka **attention weights **aka **self-attention pattern **aka **attention**, called **pattern** in TransformerLens.
          * The probability distribution part isn’t __that __important. What matters is that they are all positive reals, and add up to 1 (and anything similar is also fine)
      * **Step 2: **Decide __what __information to move from each source token, and aggregate it weighted by the attention pattern.
        * For each source token, the model calculates a **value **vector (**v **in TransformerLens). Intuitively this represents “the information I have to share”
          * This is another linear map from the source residual stream **W_V **([d_model, d_head]) and **b_V **([d_head])
        * The model __averages __the value vectors across all source tokens, using the attention pattern, to give a **mixed value **(**z **in TransformerLens).
          * We’ve moved from having a value vector at each source position to a mixed value vector at each destination position
        * It then maps the mixed value to a **result **vector, which it adds to the residual stream. The result is the output of the head, and the output of the layer is the sum of result vectors.
          * This is computed by a linear map determined by **W_O** [d_head, d_model] - note the different shape from the other 3, because this broadcasts __up __from small, internal head space to the bigger residual stream space.
          * There is a single bias vector shared by all heads **b_O **[d_model] - it is shared because the outputs all add together so the biases all add together, there’s no need to have a separate one.
      * **Information routing: **The act of moving information between token positions. This is one of the main functions of an attention head, and it is often useful.
        * This is a key task that transformers need to perform, as each token position has a separate residual stream. When a feature is computed, it lies in just the residual stream at that position. In order to use it for computation at another position, it must be moved to the correct position.
        * Note that the key task here is __moving __a feature, and detecting which features to move and to where. The residual stream has limited **bandwidth **(determined by the number of dimensions), so this is a non-trivial and important task! You only want the most essential features to be stored in any given token position, otherwise the model struggles. But it is a different task from actually __computing __that feature, which is more often done by the MLP layers.
      * Example: We could have an attention head used for factual recall, to answer questions like “The Eiffel Tower is in” with “ Paris”. The head looks at the final token position in the most recent proper noun (“ Tower”), and copies the information about its location to the “ in” token position.
        * Note that we are specifically analysing the head’s ability to do information routing here - moving information about the Eiffel Tower and its location from the Tower token position to the in token position. It is not the head’s job to __compute __this information, just to move it.
        * A query might be “I am looking for the final token in a proper noun” and a key might be “I am the final token in a proper noun”. These align, which give it a high attention score
          * Note that the key is a function of the __context__ of the “ Tower” token, not the token __value__. It is a feature generated by combining “ Tower” and “ Eiffel” and does not contain information about __which __proper noun
          * After the softmax, this is much bigger than the rest, so “ in” __only __looks at “ Tower”
        * A value might be “I am located in Paris”. This has been computed by earlier layers in the network via some kind of lookup circuit.
          * Again, this is a feature in the residual stream at the “ Tower” position, __not __the raw embedding of the “ Tower” token.
        * The attention pattern purely looks at the “ Tower” token position, and so it copies the value vector from there. The mixed value is (approx) the same as the value. The output weights (W_O) converts it to the result, containing the feature “make the output token ‘ Paris’”, which the unembed maps to a high “ Paris” logit.
      * **Implementation details** - the above is an accurate description of the computation inside an attention head, but there are several mathematically equivalent ways to do it, and implementations differ:
        * In TransformerLens, the computation for each head is done in parallel in a vectorised way. This is implemented by giving each parameter an extra head_index axis (length n_heads) and each activation an extra head_index axis
          * Vector activations, like queries keys and values have shape [batch, position, head_index, d_head], attention patterns have shape [batch, head_index, destination_position, source_position]
        * Standard implementations of transformers don’t bother with a separate head_index axis. All weights are [d_head * n_heads, d_model] (and confusingly, d_head * n_heads is normally equal to d_model), and sometimes weights are transposed
          * In GPT-2, query, key and value weights are concatenated into an enormous [3*d_head*n_heads, d_model] matrix, it’s horrifying.
        * Standard implementations of attention concatenate the mixed values across all heads, and then multiply them all by one big concatenated output matrix W_O ((d_head * n_heads) by d_model)
          * This is mathematically equivalent to multiplying each head’s mixed value by that head’s W_O matrix (d_head by d_model) and adding up the resulting head output vectors.
            * Analogous to how, 2 * 3 + 3 * 4 = (2, 3) @ (3, 4)
        * Attention pattern is normally calculated by finding the dot product of __all __pairs of keys and queries, including keys from positions after the query. Then we apply a mask, where any score from a key after the query is replaced with -inf. Then we apply a row-wise softmax to get the pattern.
          * Due to vagaries of how tensors are implemented, this is way more efficient than just __not __calculating the useless scores.
      * **Cross attention**: A form of attention that moves information from one sequence of residual streams to another sequence of residual streams. That is, the source token is in sequence 1 and produces a key, the destination token is in sequence 2 and produces a query, and we take the dot product of these for each pair of source and destination tokens.
        * Normal attention is sometimes called **self-attention**
        * Cross-attention is used in **encoder-decoder **transformers
    * Misc transformer words
      Common transformer jargon
      * **Attention-only transformers** aka attn-only aka attention-only models: A transformer variant studied in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), with just attention layers (ie, no MLP layers - it still has an embed, unembed, LayerNorm before attention + unembed, and residual stream.
        * These aren’t very __good __at the task of predicting the next token, and aren’t used in practice. These were trained as simple toy models to practice interpretability, and to look for circuits that may generalise to larger models. Most notably, we found **[induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)**, which are an important circuit that occur in much larger models
          * Intuitively, attention is used to move information between positions, MLP to process it. So attention-only models are useful to study the circuits by which information is moved around inside a model
        * There are [1, 2, 3 and 4 layer attention-only models](https://github.com/neelnanda-io/TransformerLens/blob/main/easy_transformer/model_properties_table.md) in TransformerLens! (Called attn-only-1l etc)
        * Normal transformers may be called **full transformers **or **with MLPs** to contrast, but you should assume that any reference to a transformer is to a normal transformer unless explicitly stated otherwise.
      * **[RMSNorm](https://arxiv.org/abs/1910.07467#:~:text=Biao%20Zhang%2C%20Rico%20Sennrich,both%20inputs%20and%20weight%20matrix.)** aka Root Mean Square Norm: A variant of LayerNorm which sets the vector to have __norm__ 1 and then scales each element by learned weights
        * Ie, without the centering and translating - LayerNorm sets the __mean __to 0, and then the standard deviation to be 1 (which is equivalent to setting the norm to be 1, after setting the mean to be zero).
        * This is used by [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (DeepMind’s version of GPT-3) instead of LayerNorm, though LayerNorm is standard
        * As far as I’m aware, this is just a simpler (and thus easier to interpret) version of LayerNorm that works as well.
      * **Decoder-only** transformers are GPT-2 style transformers, which have causal attention. This means that attention heads can only look backwards (so attention patterns look lower triangular). Equivalently, information can only move forwards within the model, not backwards - the residual stream at token k can only be a function of the first k tokens.
        * These models are used to **generate text** - mapping a sequence to a next token, sampling from this, and appending the new token to the end of the input.
          * We call the input that starts the generation the **prompt**. **Prompt engineering **is the study of
        * Because the residual stream at position k is __only __a function of prior tokens, when adding a generated token at position n+1, we can cache all previous residual streams, and __only __compute the new residual streams at position n+1, which is much, much faster.
        * Basically all generative transformers are decoder-only - GPT-2, GPT-3, Gopher, PaLM, Chinchilla, etc
      * **Encoder-only** transformers are BERT-style transformers, which have bidirectional attention, attention heads can look forwards or backwards, so the residual stream at any position can be a function of all tokens.
        * These models are often used for **classification tasks**, eg sentiment analysis
        * Vision transformers are normally encoder-only
        * Note that BERT, confusingly, takes in two input sequences, but is __not __an encoder-decoder transformer. The sequences are tokenized and then concatenated into a single input sequence
      * **Encoder-decoder **transformers are the original style of transformers. These first have an **encoder**, which takes in one input sequence and outputs a set of final residual streams, and then a separate **decoder**, which generates an output sequence. Each decoder block has a self-attention layer, then a **cross-attention layer** and then an MLP layer. The cross-attention layer attends to the final residual stream of the encoder (so to the same tensor for each decoder block)
        * Intuitively, this is used for **conditional text generation**, where we want to input a sequence, and to generate a __qualitatively different __sequence.
          * In theory, we could always just use a decoder only transformer, and concatenate the input sequence and the generated sequence into one - this a different kind of conditional generation. Encoder-decoder only really makes sense when we want to hard-code that the two sequences are fundamentally different.
        * Examples:
          * Translating French to English - we input a French text and use this to conditionally generate an English text (the output)
          * Transcribing audio ([Whisper](https://openai.com/blog/whisper/)) - we input audio (tokenized into a sequence in a convoluted way) and it conditionally generates text (the output)
            * Note that Whisper has several modes, eg transcribing, translating then transcribing, adding timestamps, etc and achieves this by also including a prefix to the generated text with special symbols indicating the task. This is totally valid, we just
          * Question answering ([T5](https://arxiv.org/abs/1910.10683)) - we input a question, and it generates an answer conditioned on the question (T5 also does a lot of other stuff)
        * The original transformer paper used encoder-decoder, but the current fashion is decoder-only, since it’s not __necessary__ to have encoder-decoder and it can make things much more complicated (to train, to run and to interpret)
          * [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) specifically explains encoder-decoder transformers - this confused me a lot when I used it as a guide to GPT-2!
      * **Few-shot learning** - When the prompt to a generative model contains several examples of a task, and the model generates text to answer a new example of the task. Eg prompting GPT-3 to do addition by giving it several correct addition problems beforehand.
        * Importantly, pre-trained models like GPT-3 can often use few-shot learning to achieve good performance on a variety of tasks, despite not being explicitly trained for them. It’s not clear how much it’s actually doing any “learning” vs just being cued to use its existing capabilities on the task at hand.
          * Aside: Turns out that some models contain [few-shot learning heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#pattern-matching), which can be given a completely novel task, and which attend back to the previous examples most relevant to the current task (eg with the same answer).
        * Only really becomes possible in larger models - GPT-3 can do it, I haven’t checked GPT-J or GPT-NeoX
      * **In-Context Learning:** When a model is capable of using tokens far back in the context to predict the next token. This can be measured by comparing model performance at predicting the next token around eg token 1000 vs around eg token 50. This seems to be driven by **[induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)**
        * Few-shot learning is a specific instance of the general phenomena of in-context learning
        * Other works may use this term in different ways.
      * **Post-LayerNorm models **apply LayerNorm when __writing __to the residual stream (ie at layer output) not reading (ie at layer input). The version described above (LayerNorm at layer input) is called **pre-LayerNorm**
        * I basically don’t see this done anymore, but BERT did it.
        * For some arcane reason, OPT-350M uses post-LayerNorm, but all other OPT models use pre-LayerNorm
      * A **Prompt **is the input given to a generative model like GPT-3, from which it will generate an output.
      * **Chain-of-thought prompting **is a prompting technique
    * Training
      Jargon around how transformers are trained
      * **Pre-training: **The initial stage of model training, where the model is trained on a large amount of data. This tends to use the vast majority of the total compute of training.
        * Pre-training tasks are designed to be easy to get tons of data for and to not require laborious human labelling, and to incentivise the model to learn the structure of the data distribution, even if the task itself isn’t intrinsically interesting.
        * **Next-token prediction:** GPT-2 is pretrained by giving it arbitrary internet text (posts with >3 karma on Reddit) and training it to predict the next token.
          * Subtlety: Because the attention is causal, it actually outputs a prediction for __every __token - information can only move forwards, so the residual stream at position k (and thus the logits) are only a function of the first k tokens
        * **Masked-token prediction**: BERT is pretrained by giving it arbitrary text, “masking” random tokens (ie replacing with a special masked token) and training it to predict what the masked token is.
          * Because BERT has bidirectional attention, each position’s residual stream can be a function of tokens before __or __after, so next-token prediction doesn’t work.
      * **Fine-tuning: **Taking a pretrained model, and training it on a more specific task. Normally used in contrast to pre-training. By convention, fine-tuning uses far less compute than pre-training.
        * Eg, fine-tuning GPT-2 to answer trivia questions, to make it better at that specific task.
      * **Compute **is ML jargon for computing power used to train models (it’s a noun, not a verb)
      * **Dropout**: The standard technique of randomly (and independently) setting each element of an activation to zero during training. Two notable types:
        * **Residual dropout**: Just before the output of a layer is added to the residual stream, dropout is applied
        * **Attention dropout**: Dropout is applied on the attention pattern.
          * That is, for any pair of source and destination tokens, there's some (normally 10%) chance that no information can be moved between them (!!)
        * Notably, neither type applies dropout to the MLP layers! I'm not sure why.
        * Dropout is used in some transformers (notably GPT-2), though seems to be going out of fashion. Allegedly it doesn't really improve performance
      * **[Reinforcement Learning from Human Feedback](https://openai.com/blog/learning-to-summarize-with-human-feedback/) (RLHF)** is a special form of fine-tuning large, pre-trained language models. The model produces outputs, these are shown to a human rater who gives feedback, and the model uses reinforcement learning (normally the Proximal Policy Optimisation algorithm
        * Human feedback is extremely expensive, so there are various hacks for efficiency. Notably, there is a learned **reward model **(which predicts the reward for a given output) and the model trains to optimise the reward according to the reward model. And it asks for human feedback on questions that would most help update the reward model, and to ensure the model remains in sync and it doesn’t overfit to a flawed model.
  * Transformer Circuits
    Key concepts and findings from the [mechanistic interpretability of language models](https://transformer-circuits.pub/)
    * __If you aren’t familiar with transformers, I recommend referring to the section on Transformers, especially Attention Heads__
    * Language Modelling
      Basic concepts in language modelling, not much to do with transformers
      * **Unigram**: Predicting the next token based on its frequency in the training data. This is the trivial function that totally ignores the input, and is a good baseline to compare a model’s abilities to.
        * Eg, “ the” is a very common token, and so should be predicted as more likely than eg “RandomRedditor” (yes, that is a token in the GPT-2 Tokenizer…)
      * **Bigram**: Predicting the next token based on what most frequently follows the current token. Another useful baseline to compare a model’s abilities to
        * Eg “ Barack” -> “ Obama
      * **N-Gram**: A generalisation of the above - what token is most likely to follow the previous $$N-1$$? In practice, the number grows exponentially in $$N$$, so models likely only learn the most common/important ones.
      * **Skip Trigram: **A pattern of the form “A … B -> C”. The model predicts that token C follows token B if token A appears __anywhere __in the prior context. Notable for being easy to implement with a single attention head, in a 1L **attention-only model **(described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html#interpretation-as-skip-trigrams))
        * Eg “keep … in -> mind”, because the phrase “keep in mind” is common. This is strictly worse than the trigram “keep in -> mind” as it can easily misfire, but is often easier to implement
        * **[Skip Trigram Bugs](https://transformer-circuits.pub/2021/framework/index.html#skip-trigram-bugs)**: An interesting phenomena where if a single head implements two skip trigrams  and  then it must also implement  and . This is because the destination token __only __determines which source tokens the head attends to (The [QK-Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=n_Lc0Z5N9HMhAYcycDda-UEB)) but not what information it copies __once __it knows where to attend (done by the [OV-Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS)).
          * For example, “one … two -> three” and “one … four -> five” are reasonable skip trigrams (they suggest that the text is counting upwards), but also force “one … four -> three”!
          * The model can get around this by using __different __heads for the two skip trigrams (with different OV circuits!)
      * **Bigram/trigram munging**: An informal term for “the kind of messy task that is probably done with a lot of vague and weak statistical correlations in the data”, that will likely take the form of many bigrams and trigrams. For example, identifying gender likely emerges from many small correlations and cues. This is, because these tasks are unlikely to be **localised **and are likely much harder to interpret and not the outcome of any specific circuit.
        * Sometimes this can be ambiguous! Eg, “The Eiffel Tower is in” -> “ Paris” could be implemented by a sophisticated factual recall circuit, which looks up the fact, realises that it wants a location word at “ in”, and moves this info from “ Tower” to “ in”. Alternately, it could be the bigram that “ in” is likely to be followed by a location word (boosting __all __city names) and observing that “ Eiffel” appears in the context, and having the skip trigram “ Eiffel… in -> Paris”
          * Note that this latter algorithm also fires on “The Eiffel Tower is in Paris. The Colosseum is in”, and so isn’t a great solution!
    * A Mathematical Framework for Transformer Circuits
      An overview of A Mathematical Framework, with a focus on the reframing of attention heads, why they matter, and how to think about everything
      * __[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) introduces several useful reframings of attention heads - I’ll review them here, but check out the paper or [my video walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) for details__
      * **Low-rank factorized matrix** - a linear algebra concept for when a matrix M ([big, big]) breaks down into a product M = AB where A is [big, small] and B is [small, big]
        * Eg, if a 100 x 100 matrix M is the product AB for A 100 x 2 and B 2 x 100.
        * These turn up to come up a bunch in transformers, because this is an efficient way to represent these matrices.
          * It takes fewer parameters to represent. Eg We simulate 10,000 parameters with just 400.
          * It is faster to compute products - calculating $$u=Mv$$ is slow, while $$w=Bv$$ and then $$u=Av$$ is fast. We call $$w$$ the **intermediate product**
        * Any matrix can be approximated by a low rank factorized matrix (eg, by taking the singular value decomposition, and setting all but the k largest singular values to zero). Often, it is more useful to __think __about the function of the low-rank factorized matrix by studying M, rather than A and B on their own.
        * A key mental move here will be noticing that two parameter matrices multiply each other with no non-linearity in between them, and so can be thought of .
        * Another insight: The **intermediate product** in a low-rank factorized product (ie, $$w=Bv$$ when calculating $$u=Aw=A(Bv)$$) has **no privileged basis**. That is, from the perspective of the network, M is all that matters. We can freely change A and B (Eg doubling every element of A and halving every element of B) so long as the product AB remains the same.
          * This means we can apply an arbitrary invertible matrix in the middle: $$A’ = AR$$ and $$B’ = R^{-1}B$$ have $$A’B’=AB$$. This changes $$w’=R^{-1}w$$ but not $$u’=u$$, so $$w$$ is arbitrary and we have no reason to think that the coordinates are meaningful.
      * **Direct Path term: **The contribution to the logits from the original token embeddings. Equal to $$W_E W_U$$ (ignoring LayerNorm).
        * This is the part of the residual that goes **direct**ly from the input token embeddings to the unembed, via the skip connections around each layer
        * Because the residual stream is a sum of all layer outputs, and the logits are a linear map from the residual stream, there is a clear term
        * Importantly, the direct path term is __only __a function of the current token and not of earlier tokens. This means that the best it can do is to represent bigrams.
      * **QK-Circuit** aka **QK-Matrix**: The calculation of the attention pattern is actually solely determined by the low-rank factorized matrix $$W_{QK} = W_Q^T W_K$$. Queries and keys are just intermediate states and so unlikely to be directly interpretable. We call this matrix the **QK-circuit**
        * Sketch proof: $$x$$ with shape [position, d_model] is the residual stream. $$k=xW_K$$, $$q=xW_Q$$, $$\textrm{scores}=q \cdot k = q k^T = x^T (W_Q^T W_K) x$$
          * Aside: Note that this is a bilinear form, ie a function that takes in two vectors (k and q) and returns a scalar (the attention scores)
        * If we look at just the contribution to the attention scores from the input tokens, we get the **full QK-Circuit **$$W_E^T W_Q^T W_K W_E$$. This is a rank d_head matrix that’s d_vocab by d_vocab (typical values - a rank 64 matrix that is 50,000 by 50,000)
          * Note - in A Mathematical framework QK-Circuit refers to full QK-circuit, and they have no term for what I call the QK-circuit. I prefer my notation, but this could easily get confusing.
      * **OV-Circuit**: Similar idea - the calculation of the attention head output only depends on the low-rank factorised matrix $$W_OV = W_V W_O$$. We call this the **OV-Circuit **and values are just an intermediate state and so not likely to be directly interpretable.
        * (Irritatingly, the paper uses left multiplying matrices as is standard in maths, while my and their code uses the right multiplying convention as this is more convenient in code. Sorry!)
        * Sketch proof - if $$x$$ with shape [position, d_model] is the residual stream, $$\textrm{result} = (\textrm{mixedvalue}) W_O = (Av)W_O = AxW_VW_O=AxW_{OV}$$
        * A key intuition of this result is that attention multiply on the **position**__ __axis, $$W_OV$$ multiplies on the **residual** axis, and these are independent operations that can be applied in either order. Conceptually, this says that the destination token can only choose __where __to copy information from, but once the attention pattern is set then __what __to copy is purely determined by the OV-Circuit and source residual stream, and not a function of the destination token. This gives rise to issues, like **skip-trigram bugs**
        * Aside: I use **copy **to refer to the function of the $$W_{OV}$$ matrix of selecting information to move from the source residual stream. This is a fuzzy and informal use of copying. $$W_{OV}$$ is a low rank linear map, and can in some sense be thought of as identifying a d_head dimensional subspace in the source residual stream and “copying” it to a d_head dimensional subspace in the destination residual stream. But it’s an arbitrary linear map, and can be thought of in many other ways!
      * Linear algebra aside: Note that the QK-Circuit is a **bilinear form **(two vectors are input and a scalar is output) and the OV-Circuit is an **endomorphism** (a vector is input and another vector is output (in the same space)). These are both represented by a d_model by d_model matrix, but are fundamentally different operations
      * **Freezing attention patterns**: A technique where a change is made to the network (eg ablating a head) and we recalculate all outputs __but __hold the attention patterns the same.
        * This is significant because, if we do this, then the heads are a purely linear function $$x\to AxW_{OV}$$. An attention-only model with frozen attention patterns is just a linear function!
      * **Copying**: A common operation of heads, where they learn to map inputs to a similar output.
        * For example, a head which attends to copies of the same token has a **copying full QK-Circuit**
        * Or a head which predicts that whatever source token it attends to will be the next token, this has a **copying full OV-Circuit**
      * **Composition: **When the output of a model component is a significant part of the input of another component. This is a somewhat fuzzy concept - every model component’s input is the residual stream, which is the sum of the outputs of every previous component, so in some sense everything composes. But composition informally means “this is the part of the input that __matters __and is actually used”.
        * When I refer to a **component** I basically mean any part of a model that takes an input from the residual stream and outputs to it (or the embedding or unembed). So attention & MLP layers, heads, and even neurons
          * An MLP layer can be decomposed into a sum of individual neurons - mathematically, a MLP layer is $$GELU(xW_{in})W_{out}$$. If $$n=GELU(xW_{in})$$ is a d_mlp dimensional vector, then $$n W_{out} = \sum_i n_i (W_{out,i})$$, each term here is a separate neuron output.
        * We can think of a component as **reading in **its input from the residual stream and **writing out **its output to the residual stream.
          * Note that these operations are __not __opposites!
          * Reading is a **projection**, the residual stream is dotted with each input vector of the relevant matrix. This means that __every __vector in the residual stream contributes (unless it’s orthogonal), though some contribute much more than others.
          * Writing is an **embedding**, it adds on its output. This can only go to a specific low rank subspace, spanned by the output vectors of the matrix.
        * Heads are interesting because they have three inputs - queries, keys and values (determined by $$W_Q$$, $$W_K$$, and $$W_V$$ respectively). This gives rise to 3 kinds of composition depending on which input the earlier component’s output composes with: **Q-Composition**,** K-Composition **and **V-Composition **
          * Often outputs will compose with multiple inputs
          * This does slightly contradict my earlier claim that queries, keys and values are uninterpretable intermediate products. The reasoning is that there are two parts of a head that __are __meaningful - the attention pattern and the result. The attention pattern comes from $$x^T W_{QK} x$$, which has an input on the query side and on the key side, and we can attribute its structure to which input
          * The claim that heads have “three inputs” is somewhat fuzzy - after all, all three inputs are the residual stream! The Q input comes from the destination__ __residual stream, and K and V from the source, which is importantly different. We can further distinguish the three with the observation that, because $$W_Q,W_K,W_V$$ are low-rank, they read in their inputs from a low rank subspace of the residual stream. We can thus think of composition as “the second part significantly reads in information from the subspace that the first part wrote to”
        * Intuitive examples:
          * Q-Composition is when we use contextual information about a destination token to figure out where to move information __to__.
            * Eg we see the “ in” in “The Eiffel Tower is located in”. An earlier component notices “ located” comes before and computes the feature “expect a location next”, which is part of the query
          * K-Composition is when we use contextual information about a source token to figure out where to move information __to__.
            * Eg, we use the fact that “ Tower” is the final token of “Eiffel Tower” to compute the feature “is the final token in a proper noun” and uses this to tell future tokens that it has important information
          * V-Composition is when we move contextual information __from __the source position to the destination position
            * Eg we move the feature “is located in Paris” from the “ Tower” token to the “ in” token.
          * Note: by contextual information I mean “literally anything that is not literally the model purely using the original token embeddings at this position”
    * Induction Circuits
      How induction heads/circuits work, why they matter, and a lot of surrounding intuition
      * __Induction Heads/Circuits are one of the best understood and studied circuits in language models. First described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) and enough of a big deal that we then wrote [an entire other paper on them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html). Check out [my walkthrough with Charles Frye](https://www.youtube.com/watch?v=dCkQQYwPxdM) on that paper for a more accessible introduction.__
      * The **induction behaviour/task**: The task of detecting and continuing repeated subsequences in text, by finding patterns of the form .
        * For example, if you want to predict what comes after a token like “ James”, and you see that “ James Bond” has appeared in the past, you predict that “ Bond” will be the next token.
          * An important note is that this is __not __a task about memorising patterns in the data, like that “ James” often comes before “ Bond”. “ James” often comes before many other tokens (“ Cameron”, “ Brown”, etc), and pure pattern recognition can’t get beyond that just suggesting one of those. The induction task involves noticing that “ Bond” occurred earlier in the text, immediately after “ James” (the current token) and thus concluding that “ Bond” is likely to come next.
        * This is sometimes called **strict induction**, in contrast to more general forms of induction like check whether the last k tokens have a match earlier in the prompt.
          * Often text will have a single token repeated that is not part of a long repeated sequence, so additional cues like longer repeated strings or further repetitions are helpful. Thus, in practice, models will learn to do these more general forms of induction in addition to strict induction.
        * Notably, this is an algorithmic task, that we want to work for __any __pairs of tokens A and B, not just tokens that naturally occur near each other.
      * **Previous token head: **An attention head that attends to the previous token.
        * This is solely a statement about the attention pattern (and thus the QK circuit), and nothing about what the output of the head is (ie it says nothing about the OV circuit).
        * This is a somewhat fuzzy definition, because most previous token heads don’t always attend to the previous token, or only mostly attend to it (eg 0.8 attention to the previous token, 0.2 to the current token). Informally, it means a head whose main behaviour seems to be attending to the previous token.
        * [Diagram](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)
          * ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2FTG2OVD3vPT.png?alt=media&token=64ed0d38-18a3-4f31-9048-1835f98f613a)
      * **Induction head: **A head which implements the induction behaviour. They attend to the token immediately __after __an earlier copy of the current token, and then predicts that the token attended to will come next. (ie, in  it attends from A2 to B, and predicts that B will come next)
        * That is, in the string “ Harry_1 Potter … Harry_2”, the induction head attends from Harry_2 to Potter (because Potter is after Harry_1), and then predicts that Potter will come next
        * Note that this is both a statement about the QK-Circuit (attention pattern goes to the token after the previous copy) __and __about the OV-Circuit (it predicts that that token will come next)
        * By “predicts that the token attended to will come next”, we typically mean that it __directly __increases the B logit (the direct logit attribution is high), rather than (or in addition to) indirectly by feeding into other layers.
        * If there are multiple previous copies of A, it typically attends diffusely over the tokens that come after each copy of A, but this isn’t specifiexd as part of the definition
          * Eg if it attended sharply to just the token after the most recent copy, that would also count.
        * **Anti-induction heads: **A variant that has the same attention pattern, but __suppresses __the token attended to.
          * These seem to happen, and we aren’t sure why!
        * [Diagram](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)
          * ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2F2fUHHJsKfO.png?alt=media&token=f2caa99f-be18-4b4e-9408-6538c142f1c4)
      * **Induction circuit: **A two head circuit, which use **K-composition** to produce the induction behaviour. The first head is a previous token head, and the second head is an induction head.
        * ![](https://lh4.googleusercontent.com/1VGBweSfpChM1QGKhCZi3d74QXy-1FlvdxRaw7BWwlcPirrXPifS-yQW7-lpCQXDlopydjN51HhBcZj4gy0S0d_EbVaSJ0BSks2Yk8dvPLlolNxeTYgrMuXAnN1IZZ3HYom4CKqPBRuCbTx9k9-v5tJzjiye_bPnfPYYqsad4rCmDXabheuAsFS0got7)
          * This is a great illustrated diagram from [a blog post from Callum McDougall](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated), you should check it out!
        * How does the circuit work? __Note: This is pretty involved! Check out the sources at the start of the section if you’re confused__
          * It acts on the sequence 
            * A1 & A2 and B1 & B2 are two copies of token A and B respectively, distinguished so that we can refer to the token with position A1 and token value A
          * The previous token head attends from position B1 to position A1. This is solely computed from the **positional embeddings**, and doesn’t depend on the token values. (This is the Previous token head **QK-Circuit**)
          * Once it’s decided where to attend to, the previous token head **reads** in the information that position A1 has token value A, and **writes** to the B1 residual stream the feature “the token __before __me is A” (This is the Previous token head **OV-Circuit**)
          * The induction head’s query identifies that position A2 has token value A. It then searches for source tokens with the feature “the token __before __me is A”, and attends to those. It notices this on position B1, and so attends from position A2 to position B1 (This is the Induction head **QK-Circuit**)
            * Note, this does not involve using the fact that the token __value __at position B1 is B. It __solely __involves looking at the output that the previous token head wrote to the residual stream at position B1
          * Once it’s decided where to attend to, the induction head **reads **in the information that position B1 has token value B, and then **writes **to the A2 residual stream “predict that the next token is B” (This is the Induction head **OV-Circuit**)
        * Some key takeaways from this:
          * This inherently involves two heads composing across layers, and so cannot be done in a 1 layer model. We only observe induction heads in a two layer model
            * The core intuition here is that __attention operates on pairs of tokens__. For each separate pair of source & destination token positions we calculate an attention score. And at the start of the model, each token position __only __contains the positional information and token value information. So position B1 fundamentally cannot tell a first layer head that it comes immediately after an A.
            * This means that the circuit inherently requires using __contextual __information about position B1
          * This composition happens via the residual stream, supporting the idea that it’s used as **shared bandwidth. **The attention of the induction head looked at the output of the previous token head on position B1, not the token value. And this output was communicated via the residual stream.
            * We could (and do) easily have had the induction head be in layer 4 and the previous token head in layer 1 - the residual stream means that they don’t need to be in adjacent layers.
          * This was an example of **K-Composition**. Each attention head has 3 inputs - query, key and value. Each can read from different subspaces of the residual stream, and so can compose with different outputs of previous heads. The query determines where to move information __to__, the key determines where to move it __from__, and the value determines __what __information to move.
            * The hard part of the induction circuit was figuring out that source position B1 was the right place to move information __from__, so this is an example of **K-Composition**
            * Note further that the key and value picked up on totally different features in the source token.
          * Each head has two, fairly distinct behaviours - figure out __which __source tokens to move information from (**QK-Circuit**), and figure out __what __information to move from them to the destination token (**OV-Circuit**). These are best understood as separate mechanisms in the head (though may only make sense in the context of the other)
          * This circuit is about how information is routed through the network - all of the real effort went into figuring out where to attend to, and once the induction head attention pattern was computed, the head just needed to copy the token value.
            * This is an important difference between transformers and earlier sequence modelling networks! (Convolutional networks, recurrent networks, etc). Transformers get to allocate ⅙ of their parameters figuring out __how __to move information between elements of the sequence (the query and key weights), and this can represent significant computation.
            * Further, because it was about routing information, it makes sense that it was a circuit of attention heads, since this is their main purpose of the network.
          * The circuit is best understood as being the composition of two __heads__, rather than two layers. This is evidence of the claim that an attention head is the right unit of analysis to understand an attention layer, and that heads can be thought of mostly independently.
        * Note that this behaviour fundamentally requires two heads composing to do (and thus at least two attention layers).
      * **Duplicate token heads**: A head which attends to earlier copies of the current token.
        * Notably, it has a skew towards attending to tokens that are __not __the current token. This is surprisingly hard! “What token is this” and “what position is this in” are represented as separate directions in the residual stream. The obvious way to implement a duplicate token head is to check whether “what token is this” is aligned between the two tokens, which fires just as well on the current token as on earlier copies. The head must __also __have a “these are __not __the same position” calculation, and add them together.
      * **Pointer arithmetic**: A term for circuits that involve moving positional embeddings between positions, and using these to later figure out where to attend to.
        * [Diagram](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)
          * ![](https://lh6.googleusercontent.com/RwHUalPTuqOWuWWXv75He286b64Cp4szxGBu-wK5lHupFV144SYrJs3XXF-AialWIUUU3giShwnUbHEXzWBw9rALXld-N97OFrLlB96doBmOUq8VoIDUuYXRYqeSSh6h0RNu_8zbZhRqESNiH-DjPAY6SzHHoqXMHnH7Rv9y7bXltBde1pPjLgSbIz_j)
        * Analogous to pointers in programming languages like C, where we can get a variable representing where another variable is stored in memory.
        * This gives an alternate way to create an induction circuit:
          * The first head is a **duplicate token head**, which attends from A2 to A1 (QK), and copies the __positional embedding __for position A1 (OV)
          * The second head acts like an induction head (attending from A2 to B1, and predicting that token B will come next).
            * But it is computed with **Q-Composition** - its query takes in the feature “a previous copy of A is at position A1”, __rotates __it to position “A1 + 1”, and attends to position “A1 + 1” (ie “B1”)
            * Positional embeddings are often sinusoidal (eg, $$n \to (\sin(\omega n), \cos(\omega n))$$. Mapping this to $$n+1$$ is just a linear map (technically a rotation by angle $$\omega$$), so it can be done with the QK-Circuit
        * This only works in models with **absolute positional embeddings**, since relative positional embeddings are not in the residual stream
          * Aside: In theory, models with relative positional embeddings (eg **rotary**) can also re-derive absolute positional embeddings, using the causal attention mask (see [a preliminary exploration](https://www.youtube.com/watch?v=yo4QvDn-vsU) I did of this in a toy model). Even though we don’t explicitly give it to them. So, in theory, a model like GPT-J trained with rotary could just rederive these and implement a pointer arithmetic circuit?
        * Empirically, models with learned absolute positional embeddings (eg GPT-2) have a harder time forming induction heads. My guess is that it’s because there are two distinct ways to form them, and it reduces the gradient signal to learn either.
      * That was a lot of words on a simple sounding circuit. Why does any of this matter? It turns that induction heads are a __much __bigger deal than they seem at first glance (so much so that [we wrote a whole paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) on them!):
        * Induction heads seem to be **universal**. They occur in every model (above one layer) that has been looked at, up to at least 13 billion parameters (see [a heatmap I made of induction heads in 41 open source models](https://www.neelnanda.io/mosaic))
        * Notably, this is an algorithm that is run __at inference time__. This is not a task that can be done by simply memorising statistical patterns in the data (like that token B is in general likely to come after A), the model needs to be doing some actual computation on the input data (basically a very simple form of search).
          * In my opinion, this is pretty conclusive proof that neural networks can’t __just __be a cloud of statistical correlations (though you could argue that it is just pattern matching)
        * Induction heads appear as a **phase change**. There is a narrow period of training where we go from no induction heads to fully formed induction heads.
          * Further, induction heads are __so __important for language modelling that there is a visible bump in the loss curve when they form. (That is, for a brief period, the model’s loss improves faster than it does before or after, forming a non convex bump on the loss curve)
          * ![](https://lh6.googleusercontent.com/lPR8XybVsEz-lLzz9S6DOpcYVLkO6OHC5T7mAiKzjLSbB2YrSRmpFj20DyWpjywqliR8czXME_-5SD-tTNRgbdEeIMm2Wpv4pQZFndFhfRyMHnjo7tUmR8Tsh8bZkVnyo_APJTLruJ8AkS-uMsLTuqbM6VfsPGSfxG2A0g44di4OJ81VU5ZoTv42BEwx)
        * Induction heads are crucial to **in-context learning **- the fact that transformers can use tokens far back in the context to predict the next token. This can be measured by seeing that, on average, the loss per token  gets better the later in the context that you go.
          * ![](https://lh3.googleusercontent.com/GZQOhUlQLvstv6xYTp3UQ2-3_gPBhCvlNTJfp1b8_ANB8vaiJ_XcvYhQkPLLP7Ah4aJj-PjV26VxZthN1s8xZL2cXiCwAgi67ngRZNwxTCT8wrhY9NJ1qhb-crmqVpOQKApCiIwTm3CTNjr3ABApfAtzuBd8RcrVS1H5k67_QGwTExXIZ0z3AE-ZiAy1)
          * It’s actually kind of surprising that models can do this! GPT-3 has a context length of 2048 tokens (about 1,000 words, or about 2-3 pages of dense text), and it is better at predicting tokens towards the end than the middle. That is, it has learned to detect relationships between tokens over a page apart! 
          * This is one important way that transformers are more effective than earlier language models (RNNs or LSTMs). These both struggle to track long-range dependencies, because they recursively look at each token, and so need to carry long-range data in their memory the whole way, rather than being able to attend far back. And, empirically, early model loss per token position plateaus about 100 tokens into the context
        * Induction heads turn out to also sometimes do much more complex behaviour!
          * **Translation heads**:** **Heads that attend to the token after an earlier copy of the current token __in another language__ ([example that can attend between French, English and German](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#pattern-matching))
            * [Some example translation heads in GPT-J](http://neelnanda.io/gptj-translation-heads)
          * **Few-Shot learning heads **aka **pattern matching heads**: When given a few-shot learning set up (ie many copies of a similar problem, in a “question -> answer” setup), few-shot learning heads attend from the final token of a question to the answer in earlier examples __that are more relevant __to the current question. ([example on an artificial pattern matching task](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#pattern-matching))
          * Notably, these heads are __also __induction heads!
        * My current best guess for what’s going on with all of the above is that, fundamentally, there is one basic algorithm for tracking long-range dependencies:
          * Position doesn’t matter, so we need to learn some meaningful key and query - a key indicating some information that’s useful no matter how far back it is, and a query indicating what long-range information could be useful.
          * There are now two options - this operation can be symmetric or asymmetric.
            * If it’s asymmetric, the model needs to learn some lookup table of “if I am in context X, then check whether context Y happened earlier”, but if it is in context X, it does __not __want to check whether context Y happened.
              * **Skip-trigrams **are a good example -  is useful,  is not.
            * If its symmetric, in contrast, it __also __wants to know whether context X happened if it is context Y
              * Induction heads are a good example - here both contexts are just “is token A present”, plus an offset saying “look at the thing after token A”
          * Implementing an asymmetric operation is hard, it basically needs to be done as a massive lookup table. But __symmetric __operations are way easier - we can just embed context X and context Y into some shared latent space, where they map to the same (or similar) vectors. This means that the attention score will be high!
            * Further, you can easily learn relationships with a third context Z, by just mapping it to a similar vector too (etc for arbitrarily large clusters)
          * As an illustrative example, let’s look at translation heads. The heads above seem to be able to translate English -> French, French -> German, English -> German (and vice versa). Learning a lookup table for 6 pairs is expensive, but learning to map a token in any large to some shared __semantic __space according to the meaning of that token is cheap, and scales easily to many languages.
          * A consequence of this is that if __exactly __the same context comes up before, it will be mapped to exactly the same meaning, and so have a high dot product. Thus these heads will double as induction heads! Further, likely the easiest way to learn a head like this is by learning an induction head first, and then refining exactly what “the token before me is the same as the current token” means
            * Analogously, a translation head implemented as described above will be equally good as an English -> English head!
          * The induction (rather than just duplicate token) comes in because the model wants to predict the __next __token, so it likely wants to also learn some kind of offset - look at the token immediately __after __the text similar to the current text.
        * A final takeaway is that we discovered induction heads by [studying two layer attention-only models](https://transformer-circuits.pub/2021/framework/index.html#induction-heads)! These are a tiny setting, much easier to analyse than a real language model, but we discovered a deep principle of transformers that turned out to generalise. I’m very excited to see more work studying toy models (especially with MLPs) and looking for what we can learn!
          * To enable this, I’ve open sourced a scan of toy models in [my TransformerLens library](https://github.com/neelnanda-io/TransformerLens/)
    * SoLU
      The SoLU activation function, which seems to make neurons more interpretable, and some surrounding intuitions and terms around neuron interpretability
      * __A description of __[__Anthropic’s SoLU paper__](https://transformer-circuits.pub/2022/solu/index.html)__, and some key concepts - check out the paper for full details!__
      * **SoLU**: An activation function for transformer MLPs studied by Anthropic, which seems to make neurons more interpretable. It seems to make neurons more **monosemantic **and thus reduces the amount of **neuron** **superposition** in the model**. **Used as a replacement for existing activation functions like **GELU** or **ReLU**
        * A SoLU MLP layer consists of a linear map () from the residual stream to MLP space (size ), followed by multiplying every neuron by a softmax across all neurons. This is then followed by a LayerNorm across all neurons, and mapped back to the residual stream with a linear map (). ([See technical details in the paper](https://transformer-circuits.pub/2022/solu/index.html#section-4-2)) Mathematically:
          * $$x_0$$ is the residual stream (after a LayerNorm), size `d_model`
          * $$x_1 = x_0W_{in} + b_{in}$$, size `d_mlp`, called `pre` in TransformerLens
          * $$x_2 = SoLU(x_1)=x_1 * \textrm{softmax}(x_1)$$, size `d_mlp` ($$*$$ is elementwise multiplication), called `mid` in TransformerLens
            * Notably, we take a softmax over the __neurons__.
          * $$x_3 = LN(x_2)=LN(SoLU(x_1))$$, size `d_mlp`. Note that it now has mean zero and norm 1. Called  `post` in TransformerLens
          * $$x_4 = x_3 W_{out}+ b_{out}$$, size `d_model`, called `mlp_out` in TransformerLens.
          * Aside: because the LayerNorm scales things to have norm 1, this is actually equivalent to $$x_2 = x_1 * \exp(x_1)$$
        * Generally, we use **SoLU **to refer to $$x \to x * \textrm{softmax}(x)$$ and **SoLU+LN **to refer to the full activation $$x \to LN(SoLU(x))$$
      * **Activation Sparsity: **The property that any given neuron fires sparsely (ie, most of the time is not firing). This inhibits superposition, because most features are sparse (ie aren’t present in most inputs). So a monosemantic neuron should fire sparsely. But if a neuron is polysemantic, it will fire for many features, and so be less sparse. So if we make this harder we encourage monosemanticity.
        * This is induced by **SoLU**, as softmax tends to extremise things, ie be high for a few neurons and low for everything else. (though less so by **SoLU+LN**, see below).
        * From the paper: “superposition requires a gap between the sparsity of the underlying features and the sparsity of the neurons representing them”
      * **Lateral Inhibition**:** **A property of a neuron activation function, where one neuron firing makes other neurons less likely to fire/fire less. SoLU exhibits lateral inhibition.
        * Eg, if neuron 0’s pre-SoLU activation is 1000, and every other neuron is 1, then the softmax sends all other neurons to approximately zero. But if neuron 0’s pre-SoLU activation is 0, then this doesn’t happen!
        * Also, if neuron 0’s activation before the LayerNorm is big, it __also __makes all other neurons have smaller activations, because LayerNorm scales everything down.
        * This is in contrast with other activations like GELU or ReLU, where the activation function acts independently on each neuron. The activation post SoLU+LayerNorm is affected by other neuron activations.
        * This creates an incentive for the model to have monosemantic neurons (ie for a feature direction to correspond to a single neuron, rather than many neurons). If a feature direction corresponds to many neurons, then any given neuron likely fires for many features. And so the model will need the other neurons firing to disambiguate __which__ feature that neuron fires on. Lateral inhibition makes this harder, as there are just fewer other neurons to help out!
        * This is related to, but stronger than activation sparsity - not only does each neuron fire sparsely, but they also fire at __different __times.
          * Sparsity without lateral inhibition could be achieved by eg having every neuron fire on one in a thousand inputs.
      * [Check out the paper](https://transformer-circuits.pub/2022/solu/index.html#section-4-1) for further discussion of **activation sparsity** and **lateral inhibition**, and other ways to reduce polysemanticity in models.
      * The main metric used to evaluate **monosemanticity** is by looking at the **max activating dataset examples **for each neuron. This shows a [moderate but significant improvement relative to GELU](https://transformer-circuits.pub/2022/solu/index.html#section-6-2).
        * An example of an all-caps neuron:
          * ![](https://lh5.googleusercontent.com/zF1_hxmjqCXCT96nqA23bO1yxONvav-ypkdsT2Nb5pnlQ33ugJu1zCdkkbWJozDaryGKdoH9JTdHSYQJITtjYtlWe0svFj2At5EyhlEHzgzUkpT1ZnACNW63-xtwY_2yN4S9oTFs-Bk-yvo-BO2ILx4_AEXliN_ePk3aOF6tWYwYNtzgY4U9NtBq7_N_)
        * You can see the max activating examples for a range of my open source SoLU models on [my Neuroscope website](https://neuroscope.io/). Check out the paper for some of their (higher quality!) examples
        * But this metric has significant criticism, as it may only show that the __largest __part of the neuron activation range represents a single concept, while intermediate activations are used for a range of other things.
        * Indeed, the paper finds that without the LayerNorm, performance tanks. They speculate (and find suggestive evidence) that the model is somehow using the LayerNorm to “smuggle through” non neuron basis aligned features. The hypothesis is that the model wants to use **superposition **(ie representing more features than it has dimensions), and so wants to be able to represent non neuron basis aligned features. The lateral inhibition stops this. But by having the neuron activation magnitudes be really small, the effects of lateral inhibition are less bad. And then the LayerNorm scales things up to have norm 1, so the size doesn’t matter! 
        * A possible implication of this (fleshed out in [A Toy Model of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)) is that superposition is actually an inherent part of how models do well on a task, and that an architecture which truly prevents superposition will significantly reduce model performance.
          * One of the main ways that SoLU is notable is that it has [approximately the same performance as GELU](https://transformer-circuits.pub/2022/solu/index.html#section-5). This is a concrete example of an architecture change that increases interpretability while preserving performance. My personal guess is that SoLU __is__ an interpretability improvement, but not that significant a one, and still has substantial superposition. But that it’s a proof of concept and suggests that there may be even better ones out there!
      * One fun part of the SoLU paper is their [qualitative explorations and description of different kinds of neurons](https://transformer-circuits.pub/2022/solu/index.html#section-6-3), I highly recommend skimming it! Some notable categories:
        * **[Detokenization](https://transformer-circuits.pub/2022/solu/index.html#section-6-3-2)**: Early layer neurons which take sets of the token that the model __wants __to think of as one concept, and fire when those tokens occur together.
          * Eg firing on " Donald| Trump", " book| club", "\|left" (in LaTeX), etc.
          * A particularly fun example is neurons that fire on a token like  when it appears in different languages (German vs English vs Dutch vs Afrikaans) - it means very different things in those contexts, despite being the same neuron!
          * I often think of these as **sensory neurons**. They take the raw input data of the model and convert it into a more usable format, like how our sensory neurons contain the raw data of our retina into something more comprehensible.
          * In section 5.1 of [finding neurons in a haystack](https://arxiv.org/pdf/2305.01610.pdf) we studied detokenization neurons further
        * **[Retokenization](https://transformer-circuits.pub/2022/solu/index.html#section-6-3-3)**: Late layer neurons, which fire when outputting the first token of multi-token outputs.
          * Eg, on the "n" of " n|app|ies", or the " st" of " st|rag|glers"
          * Intuitively, the model has computed the feature “the next output is nappies”, and is now converting that to the actual output tokens.
          * I often think of these as **motor neurons**. The model has converted things to some conceptual space, reasoned through it to figure out a sensible output, and now needs to convert them to the precise output format (of predict the next token) that it’s optimized for.
        * [In the middle layers](https://transformer-circuits.pub/2022/solu/index.html#section-6-3-4) (especially in larger models), there’s the broad range of the actually interesting conceptual neurons where the model does actual processing, once it’s extracted concepts from the raw tokens.
          * The paper has a bunch of fun examples, like “numbers that correspond to groups of people” or “A/B/C/D when used as a grade” or “this describes text written on an object”
        * In light of the above results about polysemanticity being smuggled through, my interpretation of these results is less “there exists a neuron that __purely __represents this feature” and more “these are features that the model cares about enough to devote some parameters to representing, even if it’s not an entire neuron”. But these are still useful insights into models!
      * [My Neuroscope website](http://neuroscope.io/) contains a bunch more examples for open source SoLU models, I recommend going and exploring! (Note: website still very much in development!)
    * The Indirect Object Identification Circuit
      The Indirect Object Identification circuit in GPT-2 Small, why it matters, limitations, and how to think about it
      * __This is an overview of some excellent work done by Redwood Research in the paper Interpretability in the Wild ([Wang et al](https://arxiv.org/abs/2211.00593)). My [paper walkthrough with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo) is a good gentler overview ([Part 2 here](https://www.youtube.com/watch?v=b9xfYBKIaX4)). If this is confusing, go read the paper or watch the walkthrough__
      * **Indirect Object Identification / IOI**: The [grammatical task](https://www.grammarly.com/blog/indirect-object/#:~:text=You%20can%20find%20an%20indirect,and%20before%20the%20direct%20object.) of identifying that a sentence like “When John and Mary went to the store, John gave the bag to” ends with Mary, rather than John.
        * Grammatically, in the second clause, John is the subject, bag is the direct object (being given) and Mary is the indirect object (being given to)
        * Concretely, the task is to identify that the two names in the first half (John and Mary) will be repeated in the second half. But because John is the subject (the one giving), it is __not __the object (the one receiving) and so this must be Mary.
      * **IOI Circuit: **The circuit learned by GPT-2 Small to compute the IOI task. This consists of 7 classes of heads (26 total), and at least 3 levels of composition - see the diagram below. Note that, unlike induction heads, we don't yet have much data on whether this circuit generalises.
        * ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2FNqtyVNVdHr.png?alt=media&token=bf4c4e94-f301-4bb1-bfe9-ec61cedf7522)
        * First, we introduce some notation:
          * **S1**:** **(Subject 1) the token position of the first subject (the first John)
          * **IO**: (Indirect Object)** **The token position of the indirect object (the occurrence of Mary)
          * **S2**: (Subject 2) The token position of the second occurrence of the subject (the second John)
          * **END**: The final token (“ to”) - this is the position where the model needs to output the prediction that the __next __token is Mary
        * The algorithm is roughly:
          * Identify which names in the sentence are repeated
            * Done by **duplicate token heads** and **[induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)**
          * Inhibit the names that are repeated
            * Done by the **S-Inhibition heads**
          * Attend to any names that are __not__ repeated, and predict that these come next
            * Done by the **name mover heads**
        * Note that this circuit is studied specifically in GPT-2 Small (an 85M parameter model + 35M embedding parameters), and we specifically look for a circuit that explains the **[logit difference](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=5Z-8RDn4JFf7wLtKosyDTNXA)** between Mary and John (ie IO and S1)
          * The logit difference is significant, because it controls for eg identifying that the answer should be a name at all, rather than another type of word or punctuation.
      * **1** Identifying repeated names: The first step of the circuit is to identify which names are repeated. Concretely, there are heads at S2, which detect that it is repeated
        * The simplest heads here are **[duplicate token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo)**. These notice that the S2 token occurs in the past, attend from S2 to S1, and output the feature “the current token is repeated” to the S2 residual stream.
          * Note: The diagram implies that induction heads and duplicate token heads compose. This is incorrect, they just both write their output to the residual stream and are read.
        * Addition complications:
          * There also seem to be **[induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)** which output the “is repeated” feature to the S2 residual stream (which compose with **[previous token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc)** on the token after S1+1). This is surprising! Intuitively, they aren’t __necessary __for the task.
      * **2** Inhibit repeated names: There are later heads (name movers) that, by default, will copy all early names to END. So the circuit needs to __inhibit__ these.
        * There are two steps here - moving the feature "John is repeated" from S2 to END (**[information routing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jNE3AI9RMCddy9qJ1MygxF_g)**) and __using__ that feature to inhibit the name movers from attending to S1.
        * Both of these are implemented by **S-Inhibition Heads** which attend from END to S2.
          * They route the feature "John is repeated" to END. This is gained via **[V-Composition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=1HxGaFmHemMNDoINKzNb-wRA)** with the **induction heads** and **duplicate token heads**, which have already computed the “John is repeated” feature, but store it at the S2 token position, not END.
          * The feature "John is repeated" is used to inhibit the attention paid by **name movers** from END to S1. This is done by **[Q-Composition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=1HxGaFmHemMNDoINKzNb-wRA)** with the various name mover heads.
            * It's not clear to me whether to think of the inhibition as being "done" by the S-Inhibition heads (despite the name!) or name mover heads. In some sense, it's done by a circuit of the pair - the S-Inhibition heads write to a subspace that the name mover heads read from, and this requires both heads to coordinate. 
      * **3** Predict that the names that are __not__ repeated come next: This is done the **name mover heads**, which attend from END to IO (and __not__ S1 or S2) and predict that the token that they attend to will come after END
        * They know to __not __attend to S1 (or S2) because of **Q-Composition **with the S-Inhibition Heads. By default they attend to all names, and the S-Inhibition Heads inhibit this.
        * The hard part is calculating the attention (ie the **[QK-Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=n_Lc0Z5N9HMhAYcycDda-UEB)**). The **[OV-Circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS)** is pretty simple, and just needs to learn to copy. 
      * **Weird complications:**
        * There seem to be two ways to implement the “inhibit repeated names behaviour”.
          * One is by detecting that S2’s token __value __is repeated, copying both that and the token value to END, and using this to inhibit a name mover’s attention to S1.
          * Another is by detecting that the __positional embedding __of S1 is the position of a repeated token, routing this from S2 to END via the **S-Inhibition Heads** and telling the name movers to inhibit attention to that __position__. This is an example of **[pointer arithmetic](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=Kmaa-ZrES4T3yzCGy8q0Z2jC)**.
          * The circuit seems to implement both, and the paper has some great analysis disentangling this in [Appendix A](https://arxiv.org/pdf/2211.00593.pdf#page=14). The key idea is to take a sentence with S1 and IO swapped (John and Mary vs Mary and John) and to patch **S-Inhibition heads** between them. This preserves the token __value__ of IO, but not the position (and similar setups can be made the other way round)
        * **Negative Name Mover Heads: **Heads which attend from END to IO or to S1 and predict that the token at the IO position does __not __come next.
          * That is, they still do Q-Composition with the S-Inhibition Heads, but they use this information to actively favour John relative to Mary, literally making the task harder. Because they systematically favour the __wrong __answer, this shows that the model is actively using the information about the task!
        * **Backup Name Mover Heads: **Heads which attend from END to IO and predict that the token at the IO position comes next, but which __only__ do this if the main name mover is [ablated](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx)
          * It's very weird and interesting that the model learns this redundancy! Note that backup name movers tend to already be __weakly__ doing the task, and just substantially improve if the main head is ablated. Further, a single name mover head tends to strongly affect a handful of other heads, and only slightly affect others
            * Ablating a name mover can also cause a negative name mover to significantly decrease it's level of inhibition, as another form of redundancy
          * My personal guess is that this is a general circuit in GPT-2 to counteract the use of [attention dropout](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UyTrZH6Kxi_slXqcnJAyyie), though allegedly they appear somewhat in GPT-Neo which was not trained with dropout. I'd love for someone to investigate this!
      * **Standards of Evidence:**
        * The main techniques used to reverse engineer this circuit were causal intervention based techniques, notably **activation patching **and **path patching **(see [the paper](https://arxiv.org/abs/2211.00593) for technical details). The focus was on making surgical interventions on model internals and how they connected together and observing how it responded. 
          * These have since been refined into the more general technique of **[causal scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc)** 
        * This is in contrast to [prior](https://distill.pub/2020/circuits/curve-circuits/) [circuits](https://distill.pub/2020/circuits/early-vision/) [work](https://transformer-circuits.pub/2021/framework/index.html) that focuses more on explicitly reverse engineering the weights and actually understanding the underlying algorithms.
        * My personal take is that actually reverse engineering the weights is the gold-standard of evidence, but pretty labour intensive and likely much less scalable. The causal intervention techniques are a bit less rigorous, but much more scalable to the messiness of real models, and in my opinion still pretty great evidence when done well (which the paper does!). 
          * Redwood has since done great work developing this into the idea of **[causal scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc)**
        * As a concrete example of a limitation, their techniques reliably show that there is __some__ feature that inhibits the attention of the name movers via **[Q-Composition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=1HxGaFmHemMNDoINKzNb-wRA)**, and that this is a result of the output of the S-Inhibition Head, but not __what__ this feature is. 
          * More generally, this technique gives much better insight into how the parts fit together into forming the overall end-to-end algorithm, than they do into the exact internals of what features are represented by different parts. The latter is more subjective and harder to operationalise, though [causal scrubbing](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=KfagbOQ29EYq3FA_OGaxZaoc) makes a valiant attempt.
      * This can seem like a pretty boring, toy problem at first, but I think that this is an extremely cool interpretability paper. **Why does this matter?**
        * Firstly, there just aren't that many examples of concrete, well understood circuits! I think this statement is just true in general, though this case is also notable for interpreting a circuit in a real(ish) model, that wasn't trained specifically to be a nice, toy model. 
        * It illustrates some interesting motifs!
          * It was not obvious to me in advance that the way to implement this kind of circuit is to have a name mover head that attends to __all__ names by default, and to have a separate circuit to __inhibit__ repeated names. My prediction was that it'd learn to boost the correct name!
          * The redundancy (backup name mover heads and to a lesser degree negative name movers) was super interesting and unexpected, and I'm very interested in how much it generalises.
          * Reinforcing the idea from induction heads that transformers put meaningful computation into how to route information through the model.
        * It's a good case study to build off of, and there are many interesting questions it can help answer:
          * How [universal](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=yXHKJqH-hJybWah3LTuGfQat) are the circuits found here?
          * What techniques work to identify these circuits? The paper was a significant advance to me in how it refined [activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)
          * How automatable is interpretability? Having __any__ kind of test set is a big improvement (though, obviously, I'd rather have way more!)
      * **Streetlight interpretability**: A criticism of this line of work is that it's an incredibly cherry-picked, toy problem that was chosen for being easy to interpret (ie, looking for circuits under the [proverbial streetlight](https://www.wikipedia.org/en/Streetlight_effect))
        * My personal take is that this is a fairly legitimate criticism - it __is__ a cherry-picked task, and I expect others to be notably harder. But also, it was not obvious to me that even this task would be tractable, nor that we'd learn so much from it! And I expect that there's further low-hanging fruit to pluck from this kind of toy-ish problem and building our knowledge further. Things could obviously still hit a wall, but I think that there are many other limits to mechanistic interpretability that I __did__ predict that turned out to be totally wrong (eg, that circuits wouldn't work for language models because they didn't have a continuous input space to analyse!)
      * It's worth dwelling on the various ways the task was unusually tractable (both to flesh out the criticism, and as useful tips for identifying future tasks!):
        * It was an algorithmic-ish task, so it was plausible that there would be a comprehensible, logical solution (rather than a total mess!)
          * Further, it was easy to generate a ton of synthetic data (fitting different names and other nouns into a handful of templates)
        * There was a clear way to compare two similar options. This let us study the **[logit difference](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=5Z-8RDn4JFf7wLtKosyDTNXA)**, which is a much nicer metric! It lets us control for a bunch of irrelevant stuff (like identifying that the answer should be a name) __and__ is equal to the difference in log prob. This means that it's both a linear function of the residual stream __and__ much closer to the model's true objective (low loss)
        * There was a clear __counterfactual__ (swapping S2 from John to Mary, and thus swapping the answer from Mary to John). This let us easily set up patching, and allows us to study the __average__ logit difference. This controls for irrelevant behaviour like generally learning that John is a more common name than Mary.
        * This is a __common__ grammatical structure. Models will devote parameters to a task in proportion to how much it reduces their loss, and so common grammatical structures are worth a lot of parameters! (Compared to, say, weird niche tasks like addition)
  * Techniques
    A whirlwind tour of the techniques used to actually understand models
    * Mechanistic Interpretability Techniques
      Core techniques in Mechanistic Interpretability, how to think about them, where to use them, and accompanying code so you can use them yourself!
      * __The core techniques worth knowing for mechanistic interpretability of transformers. If this section feels overwhelming, my favourites are [direct logit attribution](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ) and [activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)__
        * __I recommend implementing these with [my TransformerLens library](https://github.com/neelnanda-io/TransformerLens/), for accessing and editing internal activations in GPT-2 style transformers - start with [the main demo](https://neelnanda.io/transformer-lens-demo) to learn how the library works, and [the exploratory analysis demo](https://neelnanda.io/exploratory-analysis-demo) to see some of these techniques in practice. Where possible I link to more in-depth explanations and just give a sketch.__
        * __The [Interpretability in the Wild/Indirect Object Identification Paper](https://arxiv.org/abs/2211.00593) demonstrates many of these well, see also [my walkthrough with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo).__
      * **Ablation** aka **ablate** aka **Knockout**: Conceptually, an ablation deletes one activation of the network, and analyses how performance on a task changes. If it changes a lot then that activation was important, if it doesn’t change much then it wasn’t important. One subtlety is exactly what deleting means:
        * **Zero Ablation **aka **pruning**: Deletion = set that part’s output to zero. This is the standard way to do it.
          * Note that pruning is sometimes used to mean deleting a __weight __rather than an activation.
          * This is arguably unprincipled, because the model may be using that part as a bias term (eg if that activation is always in the range [100, 102], then setting it to zero may break __everything __on all tasks)
        * **Mean Ablation**: Deletion = set that part’s output to its average (on some data distribution, normally the training distribution)
          * This somewhat addresses the bias term concern above, but imperfectly, as it may still throw the model off the manifold of “normal activations”. Eg activations are always points on a circle, and their mean is the origin which totally breaks things
        * **Random Ablation **aka **resampling**
          * Replacing the activation with the same activation on a randomly chosen other data point
            * The data point is sampled from some distribution. Normally the training distribution, but it may be something more specific, eg a randomly selected fact.
        * These may all be unreliable on models trained with dropout, as dropout automatically applies zero ablations to random activations at training time, so it trains the model to be robust to this kind of intervention.
          * For example, the [Indirect Object Identification](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa) task in GPT-2 Small, if a name mover head is zero or mean ablated, a backup name mover head in a later layer will change its behaviour to compensate. I haven’t checked random ablation
      * **Linearizing LayerNorm **aka **Ignoring LayerNorm **aka **Fixing LayerNorm**: We run the model on some input, cache all of the LayerNorm scaling factors, and treat them as constant even as we vary other parts of the network (eg ablating things, or doing a linear decomposition).
        * Context: **LayerNorm** is an extremely annoying part of a transformer. Every time a layer reads in the residual stream as an input, it goes through a LayerNorm. Fortunately, LayerNorm is __almost __linear. The centering, scaling and normalising are literally linear, while the normalisation is not (intuitive test - what happens as we double the input vector).
        * [**Folding LayerNorm**](https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln)**: **A technique where we edit the weights of our model to merge the learned LayerNorm parameters to scale and translate into the weights and bias of the linear layer immediately after the LayerNorm, leaving LayerNorm as just centering and normalising.
          * Mathematically, if $x$ is the normalised and centered intermediate term in LayerNorm, the output after the next linear layer $(x \otimes w_l + b_l) W + b = x (w \otimes W) + (b_l W + b)$, so we can create the merged weights ($\otimes$ meaning elementwise addition)
          * If the  flag is True in  or  (defaults to True) in **TransformerLens**, then weights and biases are automatically folded in. They get folded into any layer that reads from the residual stream - $W_K, W_Q, W_V$ for attention, $W_{in}$ for MLPs and $W_U$ for the unembed.
      * **Logit Difference: **The difference between the **logits** for two possible next tokens. Sometimes used as a metric for model performance ([Example Code in TransformerLens](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=AZGHMwxABJqU))
        * In my opinion, this is a really good metric to judge model performance with various interpretability-inspired interventions and techniques (eg direct logit attribution or activation patching). It tends to be best used with the difference between a correct and plausible but incorrect next token (eg “he” vs “she” or “True” vs “False” or “Paris” vs “Rome”)
        * Intuition for why this is a nice metric, with the concrete example of **Indirect Object Identification**, where in a sentence like “After John and Mary went to the store, John handed a bottle of milk to” we take the difference between the “ Mary” logit and the “ John” logit.:
          * The **logits** are much nicer and easier to understand than **log probs**, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged. So studying the logit for the correct next token can be limited.
          * But , and so  - the logit difference is actually the log prob difference, because the ability to add an arbitrary constant cancels out!
          * Further, the metric helps us isolate the precise capability we care about - figuring out __which__ name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.
        * It works even better if you can take the __average __logit difference with another prompt with the answers the other way round. Eg the average logit difference between Paris and Rome in “The Eiffel Tower is in” and between Rome and Paris in “The Colosseum is in”. This controls for things even better, as sometimes the model has memorised “Rome occurs more often than Paris”, and this averaging will cancel that out.
      * **Per-token loss **aka **per-token log prob**: The log prob of the correct next token. (Often in practice it’s the negative log prob). This has a value for each token of the input prompt, and the cross-entropy loss
        * Sometimes the per-token loss of token $k$ is the log prob from position $k-1$ for predicting token $k$, and sometimes it is the log prob at position $k$ for predicting token $k+1$. Sorry!
          * Note - for a prompt of length $n$ there are $n-1$ per-token losses, as there is no next token for the final token. This means that either the first or last token has no per-token loss, depending on the convention used.
        * This only makes sense because the model is trained to predict the next token at __every __position. Casual attention means the logit at position $k$ is only a function of the first $k$ tokens, so it can’t cheat.
        * Sometimes **per-token logit **is used - it’s analogous, but for the logit of the correct next token, not the log prob.
      * **Direct Logit Attribution**: Looking at the direct contribution of the output of some component (head, neuron, layer, etc) to the logit for the true next token.
        * Transformer Lens has utilities to do this easily: [Example code](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=Direct_Logit_Attribution)
        * **Background**: The central object of a transformer is the residual stream. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).
        * The logits of a model are `unembed(layer_norm_final(residual_stream_final))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called direct logit attribution.
        * Intuition: If we are looking at the direct logit attribution for a single position then this is equivalent to __projecting __onto a direction in residual stream space. This is a tool for partially interpreting the residual stream
          * This is because the logits are a linear map from the residual stream, and the correct next logit is a single element of the output of this linear map. If you work through the algebra, a single element of the output of a linear map is equivalent to dot producting with a single input vector of the matrix, which is just a direction
          * **Projecting** means “map $x \to x \cdot v$ for some fixed vector $v$. Projecting is sometimes specifically used when $v$ is a unit vector.
        * Note that we look at logits, __not __log_probs. , which is not a linear function and can’t easily be linearized.
          * Intuition: We could also try something like “subtract each component in turn from the final residual stream and look at how much this decreases the log prob”. This can give notably different results! If we are really, really confident in the correct answer (eg the logit is 100 and the next highest logit is 0), then the log prob is very very close to zero. If 100 model components each contribute 1 to the logits, then their marginal impact on the log prob is tiny, even though their aggregate impact is large, so arguably direct logit attribution is a more meaningful technique.
          * This is also much more expensive - direct logit attribution is a linear map and can be easily vectorized which makes it very fast to apply.
        * Direct logit attribution is often more powerful when used with the **logit difference**, since the logit difference is equal to the log prob difference.
      * **[Logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)**: A technique where we take the residual stream after layer $$k$$ and apply the unembed to it directly. Essentially zero ablating all subsequent layers
        * This is similar to direct logit attribution. The main difference is that it specifically applies to truncating the final few blocks, and we normally apply the final layer norm, and unembed, and log_softmax, rather than linearizing anything.
        * The key finding is that, often, the model has become confident in the correct next token before the final layer, and each layer incrementally improves and refines that confidence.
          * This is evidence for thinking about transformers as having the central object of the **residual stream**, that each layer incrementally updates. This is in contrast to the standard view of networks where the output of layer $n$ is __only __relevant to layer $n+1$, and thus we would expect that final tokens are computed only in the final layer.
        * Here layer normally means block, not attention/MLP layer, though the same idea works.
        * This is equivalent to zero ablating layers $k+1…n_{layers}-1$
        * [Example code for TransformerLens](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=Logit_Lens), which has utilities to do this easily.
        * [A library from Nostalgebraist for this](https://github.com/nostalgebraist/transformer-utils)
        * [Analyzing Transformers in Embedding Space](https://arxiv.org/abs/2209.02535) builds somewhat on this technique
        * Intuition: Direct logit attribution lets us identify the __end __of a circuit. This is often the easiest part, because the output logits are directly interpretable, and it’s easiest to interpret things close to an interpretable thing.
      * **[Activation Patching](https://neelnanda.io/exploratory-analysis-demo): **A technique introduced in [the ROME paper](http://rome.baulab.info), which uses a causal intervention to identify which activations in a model matter for producing some output. It runs the model on input A, replaces (patches) an activation with that same activation on input B, and sees how much that shifts the answer from A to B.
        * More details: The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then intervene on a specific activation and patch in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.
          * We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to localise which activations matter.
          * A key detail is that we move a single activation __from__ the clean run __to __the corrupted run. So if this changes the answer from incorrect to correct, we can be confident that the activation moved was important
        * Intuition: The ability to **localise** is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent. And, empirically, on at least some tasks activation patching tends to find that computation is extremely localised
          * Intuition: This technique helps us precisely identify which parts of the model matter for a certain part of a task. Eg, answering “The Eiffel Tower is in” with “Paris” requires figuring out that the Eiffel Tower is in Paris, and that it’s a factual recall task and that the output is a location. Patching to “The Colosseum is in” controls for everything other than the “Eiffel Tower is located in Paris” feature.
          * It helps a lot if the corrupted prompt has the same number of tokens
        * Intuition: This, unlike direct logit attribution, can identify meaningful parts of a circuit from anywhere within the model, rather than just the end.
        * [A concrete example + code in TransformerLens](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=Activation_Patching)
          * We can patch in any set of activations that we want - the residual stream, a layer output, a head output, a neuron output, the attention pattern for a head, etc.
      * **Causal Tracing**: The version of activation patching focused on in [the ROME paper](https://rome.baulab.info/). Here, the corrupted run has the same prompt as the clean run, we intervene on certain tokens, and add significant noise to their token embeddings.
        * Eg, to measure the activations in “The Eiffel Tower is in” -> Paris that contain the feature “is located in Paris”, we can corrupt the “ Eiffel Tower” tokens, but not the “ is in” tokens, analogous to patching to the corrupted prompt “The Colosseum is in”
        * It’s not obvious to me whether this is better or worse than activation patching from a corrupted prompt - finding a corrupted prompt is often much more effort, but it better controls for things like identifying that this is a tourist location, identifying that it’s something associated commonly with cities, identifying that it’s a famous landmark, etc. And the noise may take the model off of the distribution of standard activations, in a way that breaks things unexpectedly.
        * [Demo notebook from the ROME paper](https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb)
        * Often causal tracing and activation patching are used interchangeably, sorry! The ROME paper uses causal tracing to refer to both corrupted token embeddings and corrupted prompts, but focuses on corrupting token embeddings, and the IOI paper uses just patching. This is my best attempt to disentangle the notation
      * **Path Patching**: A variant of activation patching, introduced in the Interpretability in the Wild paper ([Sec 3.1](https://arxiv.org/pdf/2211.00593.pdf#page=5)) that studies which __connections __between components matter. For a pair of components, we patch in the clean output of component 1, but __only __along paths that affect the input of component 2.
        * This is in contrast to activation patching, which just replaces the output of component 1, such that __every __subsequent component is affected. This tests whether component 1 changes model behaviour __via __affecting component 2.
        * The technique is fairly convoluted in the paper, and considers all paths between component 1 and component 2 via skip connections and MLP layers, but __not __via other attention heads. See the paper for technical details and [their codebase](https://github.com/redwoodresearch/Easy-Transformer) for implementation details.
        * **Direct Path Patching: **A simpler variant where we only patch in the component of the input to component 2 that __directly __comes from component 1
          * Formally, we break down the residual stream as input to component 2 into the sum of the output of each previous component. We subtract off the corrupted value of component 1’s output and patch in the clean value of component 1’s output.
          * This is equivalent to considering the path from component 1 to component 2 via the residual stream.
      * **[Eigenvalue score](https://transformer-circuits.pub/2021/framework/index.html#summarizing-ovqk-matrices): **A metric for how much a head’s **OV-Circuit** or **QK-Circuit **is copying, as introduced in A Mathematical Framework. We take the eigenvalues of the matrix, and measure how much they are dominated by positive reals.
        * Formally, we take $\frac{\sum \lambda_i}{\sum |\lambda_i|}$, which is $1$ if they’re all positive reals, $-1$ if they’re all negative reals, and otherwise somewhere in $(-1, 1)$
        * Intuitively, if a matrix $M$ has positive real eigenvectors, then $v \cdot Mv$ is likely to be big.
          * Note: There exist pathological examples to this, and this is not a mathematically precise statement, more of a fuzzy intuition.
        * Empirically, a significant number of heads appear to be copying, and this coincides with scoring well on this score.
        * Can be applied to either the **OV-Circuit **(d_model by d_model - the map from the residual stream to itself) or the **full OV-Circuit **(d_vocab by d_vocab - the map from the input tokens to the output logits). Ditto for QK
      * **[Composition Score](https://transformer-circuits.pub/2021/framework/index.html#analyzing-a-two-layer-model)**: A metric for how much two heads may compose by looking at the product of the output and input matrices, as introduced in A Mathematical Framework. Formally, if the output matrix is O and the input matrix is I, the score is $$\frac{|OI|}{|O||I|}$$, where $$|\cdot|$$ is the [Frobenius norm](https://mathworld.wolfram.com/FrobeniusNorm.html) (ie the square root of the sum of squared elements)
        * $O$ is always $W_{OV}$ of the first head. For Q-Composition it’s $W_{QK}$, for K-Composition it’s $W_{QK}^T$ for V-Composition it’s $W_{OV}$
          * It’s not just $W_Q$ etc because those are semi-arbitrary terms in a low rank factorisation, and the overall matrix is what matters.
          * In theory, this can be expanded to any pair of components (MLP layer inputs are $$W_{in}$$ and outputs are $$W_{out}$$, neurons have the corresponding vector in $$W_{in}$$ or $$W_{out}$$, embedding has output $$W_E$$ and unembedding as input $$W_U$$)
        * This is initially described in A Mathematical Framework as **virtual weights** - I find that that as explained is a fuzzy and confusing concept and recommend not focusing on it.
        * The exact motivations for why this is a sensible metric are somewhat convoluted and I recommend not digging into this very hard, since I’m not confident this metric works well
          * The key insight is that $$|M|$$ is the norm of the vector of singular values (because $$U$$ and $$V$$ are rotations and do not change norm), and if you work through the algebra it looks surprisingly reasonable
        * Anecdotally, this works well for [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY) in toy models, but doesn’t work very well at identifying the composition in the **[Indirect Object Identification](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa) **circuit. I expect it to work best for pairs of heads where __all__ or at least __most __of what they do is composing, and poorly for pairs of heads that do many things in different contexts and only want to compose some of the time.
      * **Max Activating Dataset Examples** aka **dataset examples** aka **max examples**: A simple technique for neuron interpretability. The model is run over a lot of data points, and we select the top K data points by how much they activate that neuron.
        * Sometimes there is a clear pattern in these inputs (eg they’re all pictures of boats), and this is (weak!) evidence that the neuron detects that pattern. Sometimes there are multiple clusters of inputs according to different patterns, which suggests that the neuron is **polysemantic**
        * This is a very simple and dumb technique, and has faced criticism, see eg [The Interpretability Illusion](https://arxiv.org/abs/2104.07143) which found that different datasets gave different sets of examples, each of which had a __different __clear pattern.
        * See outputs of this for image models in [OpenAI Microscope](https://microscope.openai.com/) and language models in [Neuroscope](https://neuroscope.io/)
      * **[Feature Visualization](https://distill.pub/2017/feature-visualization/)**: A technique for neuron interpretability in image models. A synthetic image is generated to optimise that neuron, and this often represents a coherent concept and suggests that the neuron detects that concept (eg curves, dogs, car windows, arches on buildings)
        * Formally, that neuron is a scalar value, and we can think of the whole model as a function mapping an input image to that neuron. This is a __differentiable __function, and so we can optimise the image to maximally activate that neuron with gradient descent. Note that, unlike standard backpropagation, here we are taking gradients with respect to the __input __not with respect to the __parameters__
          * This is hard to do well and there are a range of hacks to make it work better! Check out the paper and accompanying code for details.
        * This doesn’t seem to work well with language models, as they have discrete inputs that are hard to optimise over. [Here’s a good write-up of an unsuccessful attempt](https://pair-code.github.io/interpretability/text-dream/blogpost/)
      * **[Causal scrubbing](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)**: An algorithm being developed by Redwood Research that tries to create an automated metric for deciding whether a computational subgraph corresponds to a circuit.
        * (The following is my attempt at a summary - if you get confused, go check out their 100 page doc…)
        * The exact algorithm is pretty involved and convoluted, but the key idea is to think of an interpretability hypothesis as saying which parts of a model __don’t __matter for a computation.
          * The null hypothesis is that __everything __matters (ie, the state of knowing nothing about a model).
          * Let’s take the running example of an **induction circuit**, which predicts repeated subsequences. We take a sequence … A B … A (A, B arbitrary tokens) and output B as the next token. Our hypothesis is that this is done by a **previous token head**, which notices that A1 is before B, and then an **induction head**, which looks from the destination token A2 to source tokens who’s __previous __token is A (ie B), and predicts that the value of whatever token it’s looking at (ie B) will come next.
        * If a part of a model doesn’t matter, we should be able to change it without changing the model output. Their favoured tool for doing this is a **random ablation**, ie replacing the output of that model component with its output on a different, randomly chosen input. (See later for motivation).
        * The next step is that we can be specific about which parts of the input matter for __each__ relevant component.
          * So, eg, we should be able to replace the output of the previous token head with __any __sequence with an A in that position, if we think that that’s all it depends on. And this sequence can be different from the input sequence that the input head sees, so long as the first A token agrees.
        * There are various ways to make this even more specific that they discuss, eg separately editing the key, value and query inputs to a head.
        * The final step is to take a metric for circuit quality - they use the **expected loss recovered**, ie “what fraction of the expected loss on the subproblem we’re studying does our scrubbed circuit recover, compared to the original model with no edits”
      * **Prefix-Matching** aka **Induction Score**: A specific technique used to automatically identify [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY). You input a sequence of repeated random tokens, and look at the average attention paid from a token in the second half to the token __after__ its first copy. 
        * `induction_score = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len).mean()`
        * Example input: 9 3 8 2 7 9 3 8 2 7, and we study attention from 9_2 -> 3_1, 3_2 -> 8_1, etc (`_1` means first occurence)
        * The attention pattern is notable for having an obvious stripe when visualised (called an **induction stripe**) - if the sequence has length $$2n$$, the model will always attend to the token $$n-1$$ positions back.
        * Sometimes a BOS (beginning of sequence) token is put at the start of the sequence
        * This can also be used to identify previous token or duplicate token heads
    * Non-MI Techniques
      An incomplete overview non Mechanistic Interpretability techniques that it might be useful to know about
      * __Other techniques maybe worth knowing about, but less central. This is not my main area of expertise, so there are likely errors, some explanations are pretty sparse, and this is definitely not complete! __[__Rauker et al__](https://arxiv.org/abs/2207.13243)__ is a good survey of interpretability techniques__
      * **Dimensionality Reduction: **A technique which can take a large set of vectors and maps each vector to a smaller dimensional vector.
        * __This is not my area of expertise, and I welcome corrections! Don’t trust me on these__
        * There are many of these! I recommend focusing on learning + understanding SVD first. It's very useful and easy to reason about
          * **[PCA](https://www.wikipedia.org/en/Principal_component_analysis)** aka **Principal Component Analysis **is a linear dimensionality reduction technique that acts on a set of vectors. It finds an orthonormal basis of **principal components**, ordered by importance, where the first principal component “explains as much variance of the vectors as possible”, the second principal component explains as much variance as possible after removing the first component, etc.   Each component has a **principal value**, a scalar describing how much variance it explains
            * A vector $$v$$ **explaining **$$y\%$$** of the variance **means that the average squared norm of our vectors after removing the component in the $$v$$ direction, ie $$x\to x - \frac{x \cdot v}{|v|}$$”, divided by the original squared norm, is $$1-\frac{y}{100}$$
          * **[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)** aka **Singular Value Decomposition: **A linear dimensionality reduction technique that acts on matrices. It breaks the matrix down to $$M=USV$$, where $$U$$ has orthogonal rows, $$V$$ has orthogonal columns and $$S$$ is diagonal. The entries of $$S$$ are called **singular values**.
            * This is essentially doing PCA on both rows and columns. $$V$$ is the principal components of the columns and $$S$$ the principal values, and vice versa for rows.
            * SVD and PCA are nice, because we can just take the first $$k$$ principal components to get a kD reduction, as they’re ordered by importance
        * These are useful parts of an interpretability toolkit because one of the key challenges of interpreting networks is that they’re extremely high dimensional objects.
        * A key distinction is between **linear techniques **(applying a linear map and taking the first few coordinates) and non-linear techniques (applying a non-linear map - generally more expressive but more confusing and harder to interpret)
          * My first tactic is using SVD on everything, since it’s easy and simple to reason about.
          * Techniques that map things to 2D or 3D are great because they can be directly visualized.
        * Dimensionality reduction techniques are sometimes best thought of as acting on a set of vectors, and sometimes best thought of as acting on the matrix of stacked vectors (where each column is one vector of that set)
        * There are many ones, here’s a list of other popular examples:
          * **NMF** aka **Non-negative Matrix Factorization**: A linear dimensionality reduction technique that acts on __non-negative __matrices, by decomposing $$M\approx AB$$, where $$M,A,B$$ have all elements non-negative.
            * Has a reputation for giving unusually interpretable components, because the components tend to be sparse. **Intuition**: 
              * We're trying to find a factorisation $$M\approx AB$$. This is a continuous operation on $$A,B$$, and there's no reason their coordinates __need__ to be non-negative - if we did a singular value decomposition, then $$U,V$$ probably wouldn't have all elements non-negative!
              * When we incrementally improve the approximation, we'll normally be following some gradient direction in matrix space. The constaint that $$A,B$$ have non-negative elements is constraining us to the equivalent of the top right quadrant of matrix space (but in way more dimensions!) - ie we are optimising in a region with a **boundary**. 
              * So, as we're following some direction in matrix space, we'll probably hit the boundary along many dimensions, and being on the boundary means that that element of the matrix is zero! So we should expect that solution to NMF are highly sparse.
            * Seems best used on outputs of ReLU neurons, since the matrix needs to have every element non-negative. I'm not sure it's relevant to GELU activations, but have seen hacks - eg ignoring negative activations, or having two dimensions per neuron, one for the positive part and one for the negative part - I'm not sure how well these work!
            * Eg used in [Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/#ActivationGroups) with ReLU activations
              * [Jay Alammar explores using it for GELU neurons in language models](https://jalammar.github.io/explaining-transformers/)
            * [A good explainer](https://arxiv.org/pdf/1401.5226.pdf)
          * **[t-SNE](https://www.wikipedia.org/en/T-distributed_stochastic_neighbor_embedding)** aka **t-distributed stochastic neighbor embedding**: A non-linear dimensionality reduction technique, acts on vectors
            * [A good paper for intuition building](https://distill.pub/2016/misread-tsne/)
          * **[UMAP](https://umap-learn.readthedocs.io/en/latest/)** aka **Uniform Manifold Approximation and Projection for Dimension Reduction** is a non-linear dimensionality reduction, similar to t-SNE
          * **Eigenvalue Decomposition: **A linear dimensionality reduction technique for square, complex matrices (real matrices can always be treated as complex). $$M = V^\dagger\Lambda V$$, where $$V$$ is the basis of eigenvectors and $$\Lambda$$ is the diagonal matrix of eigenvalues
            * Some matrices are pathological and do not have a basis of eigenvalues, eg ((0, 1), (0, 0)),
          * **[The Grand Tour](https://distill.pub/2020/grand-tour/)**: A linear dimensionality reduction method, which project high dimensions to 2D and animates it so that every possible view is eventually presented
      * **[Saliency Mapping](https://en.wikipedia.org/wiki/Saliency_map?oldformat=true)**: A family of techniques in image interpretability that creates a highlighted annotation to each pixel of the input (a **saliency map**) trying to estimate how important it is for producing the output of a network. Eg highlighting which parts of an X-Ray image an image classification models looks at to diagnose cancer vs no cancer.
        * This is a significant area of research and there are many variants!
      * **[ROME](https://rome.baulab.info/): **A technique for editing the memory of a network so that it outputs an incorrect answer for a specific fact (eg “The Eiffel Tower is in” -> “ Rome”), but which tries to keep everything else the same.
        * Stands for **Rank One Model Editing **because it adds a rank one matrix to the output weights of a specific MLP layer
        * [**MEMIT**](https://memit.baulab.info/)** **is a follow-up paper with a technique capable of simultaneously editing many facts (largest experiments went up to 10,000)
      * 
      * **Attribution methods** are ways of “attributing” to different scalar activations how important they are for producing some metric (eg model loss or probability of the correct classification). 
        * Intuitively, this is about credit allocation - giving each scalar a score such that we account for its unique contribution without double-counting. 
        * The focus is normally on attributing each scalar in the input to the output, but it can be easily extended to intermediate activations like neuron activations. 
      * **[Integrated Gradients](https://arxiv.org/abs/1703.01365)** is a particularly well-known and principled attribution method. The goal is to understand how each element of the input affects some metric (eg the loss). 
        * The **Integrated Gradient** is $$IG(i) = K_i \int _0 ^ 1  \frac{\partial L(\alpha I)}{\partial K_i} d \alpha$$
        * The technique is to approximate the resulting integral by taking average of the gradients over evenly spaced values of $$\alpha \in [0, 1]$$
          * Notably, we can get the gradient with respect to __every__ pixel on a single forward and backwards pass
          * Important note - the technique only makes sense when varying __many__ scalars at once, and attributing part of the overall effect to each of the scalars
        * Derivation:
          * Let’s imagine that we want to attribute each pixel in an input to the output loss. The key idea is to imagine following a path through input space, going linearly from zero to the input image $$K$$
            * Let $$L$$ denote the map from an input image $$K \in \mathbb{R}^{3 \times n \times n}$$ to the scalar loss
            * The path function is now $$\rho: \alpha \in [0, 1] \to L(\alpha K)$$ 
          * At each point along this path we can measure the gradient of the loss with respect to a specific pixel, which intuitively measures how much that pixel matters at this point - if we varied that pixel infinitessimally, how much would it change the loss?
            * For pixel $$K_i$$ (ie with index $$i$$), we can study $$\partial \rho_i(\alpha) = \frac{\partial L(\alpha K)}{\partial K_i}$$
          * The integral of this gradient $$\int _0 ^ 1 \partial \rho_i(\alpha) d \alpha$$ is thus an aggregate score for how much this pixel matters.
        * This is a principled metric, because: 
          * $$L(K) - L(0) = \rho(1) - \rho(0) = \int^1 _ 0 \frac {d\rho}{d\alpha} d\alpha$$. 
          * $$\frac{d\rho}{d\alpha}$$ $$ =  K \cdot \nabla_K L(\alpha K)$$ is the directional derivative with respect to the input $$K$$
          * The i th coordinate of $$\nabla_K L(\alpha K)$$ is $$\frac{\partial L(\alpha I)}{\partial K_i} = \partial \rho_i(\alpha)$$
          * And so $$L(K) - L(0) = \sum_i K_i IG(i)$$
        * This can easily be extended to move between two inputs $$K \to K'$$ by linearly moving between them
        * This can easily be extended to attribute any activation to any scalar metric, eg a set of neurons to a specific attention weight in a subsequence layer, neurons to the next token loss, etc.
        * [Implemented in the Captum library](https://captum.ai/docs/extension/integrated_gradients)
      * **[Probing](https://arxiv.org/pdf/1610.01644.pdf)**: A technique for identifying directions in network activation space that correspond to a concept/feature.
        * In spirit, you give the network a bunch of inputs with that feature, and a bunch without it. You train a linear map on a specific activation (eg the output of layer 5) which distinguishes these two sets, giving a 1D linear map (a **probe**), corresponding to a direction in activation space, which likely corresponds to that feature
        * There are __many __variations of this technique! [A literature review](https://arxiv.org/abs/2102.12452)
      * **Disentanglement: **([overview](https://arxiv.org/pdf/2207.13243.pdf#page=6)) The study of how to train models where we can easily extract the features from a model’s internal activations. The focus is normally to ensure that neurons represent a single feature, rather than being **polysemantic**.** **
      * **[TCAV](https://arxiv.org/pdf/1711.11279.pdf)** aka **Testing with Concept Activation Vectors: **A technique to measure how important a concept is for a model output on a specific input
        * Eg, how important the “is striped” feature is to concluding that an image contains a zebra
        * It first defines a **Concept Activation Vector**, essentially a **probe**. The user defines a concept (eg “is striped”), gives positive and negative examples of it, and a linear map is trained to distinguish them, using the activation from some layer of the network. This gives a direction corresponding to the concept.
        * It then takes an input, and looks at the derivative of the model’s output (the zebra log prob) with respect to that layer’s activation. This derivative is another direction in activation space, which intuitively represents “the direction to change the activation in to maximally increase the probability of zebra”. Looking at either it’s cosine similarity or dot product with the **Concept Activation Vector **for “is striped” thus gives a metric of how important that concept is for producing that model output.
    * Tooling
      Key tooling for Mechanistic Interpretability, and where to learn more
      * **[TransformerLens](https://github.com/neelnanda-io/Transformer-Lens/) **is a library for mechanistic interpretability of transformers. It lets you load in open source language models like GPT-2, cache all of their activations, and intervene on them, among a bunch of other helpful features. Start with **[the main demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/v2/Main_Demo.ipynb)** to learn how the library works
        * Heavily inspired by [Anthropic’s Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html) for interpreting models (especially models too large to fit on one GPU)
      * **[CircuitsVis](https://github.com/alan-cooney/CircuitsVis)** is a library being developed by Alan Cooney to create interactive visualisations within Python, which is extremely useful. (Currently in development, but maintained and usable!)
        * A fork of [PySvelte](https://github.com/neelnanda-io/PySvelte), an unmaintained library from Anthropic
      * **[OpenAI Microscope](https://microscope.openai.com/)**, a website giving information about each neuron in image models - the feature visualization (a generated image to maximally activate the neuron) and **maximum activating dataset examples**
      * **[Neuroscope](http://neuroscope.io/)**, a (substantially worse) version of Microscope for language models that I made, which shows the **maximum activating dataset examples** for each neuron in several language models
        * Technical details:
          * To calculate the max activating dataset examples, I run the model across a bunch of 1024 token strings, and take the 20 maximum activating strings.
            * For SoLU models, I use the hook_mid activation - after the SoLU but before the LayerNorm
            * For GELU models I use hook_post, the activation after the GELU
            * For OpenWebText models I run it over 9B tokens, for the Pile over 2B tokens, and for the C4 + Python code models over 1.4B tokens of C4 and .3B tokens of Python Code
              * For the C4 + Python code, I concatenate the two datasets (C4 first) when giving the dataset index, the division is at 1359845
          * Each page shows a truncated text for each string by default - the 50 tokens before and 10 tokens after the max activated token. I did not run the model again on the truncated string, I just took the activations from the full string, and displaying them on the relevant substring
            * If you click the red toggle (__above__ the truncated title), the full text appears __below__ the truncation (sorry! I couldn't get the toggle to be immediately above)
          * The URL format is neuroscope.io/MODEL_NAME/LAYER_INDEX/NEURON_INDEX.html - edit at will. Eg https://neuroscope.io/solu-2l/0/687.html
            * WEBSITE/MODEL_NAME/random.html or WEBSITE/MODEL_NAME/LAYER/random.html will direct you to a random neuron of that model/layer
          * All models were run with autocast from float32 to bfloat16, which likely introduces minor errors
            * Possibly because of the autocasting, there are minor differences between the max activation I recorded when parsing the whole dataset, and what I got when I re-ran on the specific text string for a page. The 20 strings on a page are sorted in order of the former, but the actual values displayed are the latter, which means they're occasionally in a weird order.
          * My toy language models and my SoLU models (on C4 + Code) are trained with a variant of the NeoX tokenizer, where every number gets tokenized as individual digits (everything else is the same)
            * The GPT-2 models are trained with the GPT-2 tokenizer, and the Pile models are trained with the GPT-NeoX tokenizer (which deals much better with code and whitespace)
          * All tokens are coloured relative to the highest activation across __all__ of the text. Negative activations are shown, but also relative to the highest positive activation so they rarely show up.
          * All models studied can be loaded in [TransformerLens](https://github.com/neelnanda-io/TransformerLens/), the URL contains the alias needed to load them
          * 
      * **[Lucid](https://github.com/tensorflow/lucid/)** - a library for feature visualization in image models.
        * Note - this was written in Tensorflowv1 which no one in their right mind should use. Instead, use [Lucent](https://github.com/greentfrapp/lucent), a port to PyTorch
  * Notable Models
    A highly incomplete mish-mash of models I think you should know about
    * Open Source GPT-Style Models (in [TransformerLens](https://github.com/neelnanda-io/TransformerLens/))xfx
      The models supported by my TransformerLens library for mechanistic interpretability of generative language models
      * Each model can be loaded with `model = HookedTransformer.from_pretrained(MODEL_NAME)` and used + interpreted. [A full list of models in the library](https://github.com/neelnanda-io/Easy-Transformer/blob/main/easy_transformer/model_properties_table.md) (incl names + hyper-params)
      * **GPT-2** - the classic generative pre-trained models from OpenAI
        * Sizes Small (85M), Medium (300M), Large (700M) and XL (1.5B).
        * Trained on ~22B tokens of internet text. ([Open source replication](https://huggingface.co/datasets/openwebtext) of the dataset)
        * Trained with dropout on the residual stream (on the output of each layer before it’s added to the residual stream) and on the attention patterns 
          * That is, for every pair of source and destination tokens and each head, there’s an independent 10% chance that that attention weight gets set to zero, and no information is moved. I have no idea why this is used!!
        * Trained with tied embeddings (ie embedding and unembedding weights are the same)
      * **GPT-Neo** - Eleuther's replication of GPT-2
        * Sizes 125M, 1.3B, 2.7B
        * Trained on 300B(ish?) tokens of [the Pile](https://pile.eleuther.ai/) a large and diverse dataset including a bunch of code (and weird stuff)
        * Trained with untied embeddings and no dropout
        * Uses local attention in every other layer (ie in every other layer, heads can only attend 256 tokens back and no further)
      * [OPT](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) - Meta AI's series of open source models
        * Trained on 180B tokens of diverse text.
        * 125M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B
          * There’s also a 350M, but it uses post-LayerNorm (for some reason?!) and is not supported.
          * There’s a 175B available on request
        * Uses ReLU embeddings
        * Trained with a Beginning of Sequence (BOS) token
      * **GPT-J** - Eleuther's 6B parameter model, trained on the Pile
        * Uses rotary positional embeddings
        * Unlike every other model in this list, its d_head is 256, not 64
        * Attention and MLP layers are in parallel (ie, `resid_post = resid_pre + Attn(resid_pre) + MLP(resid_pre)`, not `resid_mid = resid_pre + Attn(resid_pre)` and `resid_post = resid_mid + MLP(resid_mid)`
      * **GPT-NeoX** - Eleuther's 20B parameter model, trained on the Pile
        * Untied embeddings
        * No dropout
        * Rotary positional embeddings
          * Aside - rotary involves pairing up elements in a head’s query and key vectors. GPT-J pairs them up like (1, 2), (3, 4), …, but GPT-NeoX pairs them up like (1, n+1), (2, n+2), (3, n+3), …. It is infuriating.
        * Note: TransformerLens is not currently designed to support models split across multiple GPUs, which will make working with GPT-NeoX hard. Please reach out if this feature is important to you!
      * **Stanford CRFM models** - a replication of GPT-2 Small and GPT-2 Medium, trained on 5 different random seeds.
        * Notably, 600 checkpoints were taken during training per model, and these are available in the library with eg `HookedTransformer.from_pretrained("stanford-gpt2-small-a", checkpoint_index=265)`
        * Trained during the development of [their Mistral library](https://github.com/stanford-crfm/mistral)
      * **Pythia** - a scan of checkpointed models trained by Eleuther to enable interpretability of training dynamics, up to 13B parameters. 
        * Trained on 299B tokens of the Pile. Each model is trained on exactly the same data in the same order
          * There is a second scan (dedup) trained on 1.5 epochs of a deduplicated version of the Pile
        * Models have 143 checkpoints linearly spaced during training
        * 19M, 125M, 350M, 800M, 1.3B, 2.7B, 6.7B, 13B
        * Trained on exactly the same architecture as GPT-NeoX - no dropout, parallel attention & MLPs, untied embeddings
        * Load checkpoint N with `HookedTransformer.from_pretrained(“pythia-19m”, checkpoint_index=N)`
    * My Interpretability-Friendly Models (in [TransformerLens](https://github.com/neelnanda-io/TransformerLens/))
      Details on some interpretability-friendly toy language models and SoLU models that I trained and open sources
      * __A collection of models I trained and open sourced specifically for interpretability__
      * **Toy Models**: Inspired by [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), I've trained 12 tiny language models, of 1-4L and each of width 512. I think that interpreting these is likely to be far more tractable than larger models, and both serve as good practice and will likely contain motifs and circuits that generalise to far larger models (like [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY)):
        * Attention-Only models (ie without MLPs): attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l
        * GELU models (ie with MLP, and the standard GELU activations): gelu-1l, gelu-2l, gelu-3l, gelu-4l
        * SoLU models (ie with MLP, and [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), designed to make MLP neurons more interpretable): solu-1l, solu-2l, solu-3l, solu-4l
        * All models are trained on 22B tokens of data, 80% from C4 (web text) and 20% from Python Code
        * Models of the same layer size were trained with the same weight initialization and data shuffle, to more directly compare the effect of different activation functions.
      * **SoLU models**: A larger scan of models trained with [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), in the hopes that it makes the MLP neuron interpretability easier.
        * A scan up to GPT-2 Medium size, trained on 30B tokens of the same data as toy models, 80% from C4 and 20% from Python code.
          * solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)
        * An older scan up to GPT-2 Medium size, trained on 15B tokens of [the Pile](https://pile.eleuther.ai/)
          * solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M), solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile (200M), solu-12l-pile (340M)
      * **Finetuned Toy Models**: Versions of the toy models `solu-1l` and `solu-4l` that were then fine-tuned on 4.8B tokens of Wikipedia. Load with `solu-1l-wiki` or `solu-4l-wiki`. 
        * I'm very curious about how fine-tuning affects the circuits in a model, and am excited to see what it's done here! I'm hoping that the models are sufficiently small and toy that you can get some real traction on what's going on here.
        * 5B is a lot of tokens, so there are 163 checkpoints, taken roughly exponentially spaced over training. 
      * Notes:
        * Each of these models has about ~200 checkpoints taken during training that can also be loaded from TransformerLens, with the `checkpoint_index` argument to `from_pretrained`.
        * Note that all models are trained with a Beginning of Sequence token, and will likely break if given inputs without that!
    * Other Open Source Models
      An incomplete list of other notable open source models
      * **OpenAI’s [CLIP](https://openai.com/blog/clip/)**, a multimodal model that takes in a text and an image and outputs how related the text and image are (ie, could the text be a caption to this image). 
        * Trained with a contrastive loss function: it has an image half and a text half, and each half maps their input to a shared latent space. It is then trained on a set of image, caption pairs to have the latent from a caption align the most with the latent from the correct image over any others in the batch.
      * **StabilityAI’s [Stable Diffusion](https://github.com/CompVis/stable-diffusion)**, a [diffusion model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models) which generates images from a text description (an open source version of [DALL-E 2](https://openai.com/dall-e-2/))
        * Note that DALL-E 1 is not a diffusion model.
      * **Google’s [BERT](https://arxiv.org/abs/1810.04805)**, an encoder-only transformer, which takes in two sequences and can be fine-tuned to output many possible classification tasks, eg 
        * BERT is pre-trained with a masked language modelling loss
        * Note - it takes in two inputs, but these are concatenated into a single sequence, separated by a special token, and self-attention is used. This is unlike an encoder-decoder model like T5 where the two inputs form separate sequences and cross attention attends from one to the other.
      * **Google Brain’s T5**, an encoder-decoder transformer used to generate text conditioned on an input text, eg generating an answer given the question, 
      * **[OpenAI’s Whisper](https://openai.com/blog/whisper/)**, an encoder-decoder transformer that takes in audio and outputs text - a transcription, a translated transcription, with time stamps, etc.
  * Please cite this work as ```@misc{nanda_2022, title={A Comprehensive Mechanistic Interpretability Explainer & Glossary}, url={https://neelnanda.io/glossary}, author={Nanda, Neel}, year={2022}, month={Dec}}```

Directory Structure:

└── ./
    ├── docs
    │   ├── source
    │   │   ├── documentation
    │   │   │   ├── intervention.rst
    │   │   │   ├── modeling.rst
    │   │   │   ├── schema.rst
    │   │   │   ├── tracing.rst
    │   │   │   └── util.rst
    │   │   ├── about.rst
    │   │   ├── documentation.rst
    │   │   ├── features.rst
    │   │   ├── index.rst
    │   │   ├── start.rst
    │   │   ├── status.rst
    │   │   └── tutorials.rst
    │   ├── sourcelatex
    │   │   ├── documentation
    │   │   │   ├── contexts.rst
    │   │   │   ├── intervention.rst
    │   │   │   ├── models.rst
    │   │   │   ├── module.rst
    │   │   │   ├── patching.rst
    │   │   │   ├── tracing.rst
    │   │   │   └── util.rst
    │   │   └── index.rst
    │   └── contributing.md
    ├── CHANGELOG.md
    ├── CODE_OF_CONDUCT.md
    └── README.md



---
File: /docs/source/documentation/intervention.rst
---

nnsight.intervention
--------------------

.. automodule:: nnsight.intervention
   :members:

.. automodule:: nnsight.intervention.protocols
   :members:

.. automodule:: nnsight.intervention.protocols.entrypoint
   :members:

.. automodule:: nnsight.intervention.protocols.grad
   :members:

.. automodule:: nnsight.intervention.protocols.intervention
   :members:

.. automodule:: nnsight.intervention.protocols.module
   :members:

.. automodule:: nnsight.intervention.protocols.swap
   :members:

.. automodule:: nnsight.intervention.contexts
   :members:

.. automodule:: nnsight.intervention.contexts.editing
   :members:

.. automodule:: nnsight.intervention.contexts.globals
   :members:

.. automodule:: nnsight.intervention.contexts.interleaving
   :members:

.. automodule:: nnsight.intervention.contexts.invoker
   :members:

.. automodule:: nnsight.intervention.contexts.local
   :members:

.. automodule:: nnsight.intervention.contexts.session
   :members:

.. automodule:: nnsight.intervention.contexts.tracer
   :members:

.. automodule:: nnsight.intervention.backends
   :members:

.. automodule:: nnsight.intervention.backends.editing
   :members:

.. automodule:: nnsight.intervention.backends.remote
   :members:

.. automodule:: nnsight.intervention.graph
   :members:

.. automodule:: nnsight.intervention.graph.graph
   :members:

.. automodule:: nnsight.intervention.graph.node
   :members:

.. automodule:: nnsight.intervention.graph.proxy
   :members:

.. automodule:: nnsight.intervention.base
   :members:

.. automodule:: nnsight.intervention.envoy
   :members:

.. automodule:: nnsight.intervention.interleaver
   :members:


---
File: /docs/source/documentation/modeling.rst
---

nnsight.modeling
----------------


.. automodule:: nnsight.modeling
   :members:

.. automodule:: nnsight.modeling.language
   :members:

.. automodule:: nnsight.modeling.diffusion
   :members:

.. automodule:: nnsight.modeling.vllm.vllm
   :members:

.. automodule:: nnsight.modeling.vllm.sampling
   :members:




---
File: /docs/source/documentation/schema.rst
---

nnsight.schema
---------------

.. automodule:: nnsight.schema
   :members:

.. automodule:: nnsight.schema.Config
   :members:

.. automodule:: nnsight.schema.Request
   :members:

.. automodule:: nnsight.schema.Response
   :members:

.. automodule:: nnsight.schema.format.functions
   :members:

.. automodule:: nnsight.schema.format.types
   :members:





---
File: /docs/source/documentation/tracing.rst
---

nnsight.tracing
---------------

.. automodule:: nnsight.tracing
   :members:

.. automodule:: nnsight.tracing.graph.graph
   :members:

.. automodule:: nnsight.tracing.graph.node
   :members:

.. automodule:: nnsight.tracing.graph.proxy
   :members:

.. automodule:: nnsight.tracing.protocols.base
   :members:

.. automodule:: nnsight.tracing.protocols.lock
   :members:

.. automodule:: nnsight.tracing.protocols.stop
   :members:

.. automodule:: nnsight.tracing.protocols.variable
   :members:

.. automodule:: nnsight.tracing.contexts.base
   :members:

.. automodule:: nnsight.tracing.contexts.conditional
   :members:

.. automodule:: nnsight.tracing.contexts.globals
   :members:

.. automodule:: nnsight.tracing.contexts.iterator
   :members:

.. automodule:: nnsight.tracing.contexts.tracer
   :members:


---
File: /docs/source/documentation/util.rst
---

nnsight.util
------------


.. automodule:: nnsight.util
   :members:


---
File: /docs/source/about.rst
---

.. raw:: html

    <style>
        .accordion-header {
            margin: 0 !important;
        }
    </style>

    <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('figure.align-default').forEach(el => {
                el.style.marginBottom = "0px";
            });
        });
    </script>

About NNsight
=============

An API for transparent science on black-box AI.
-----------------------------------------------

.. card:: How can you study the internals of a deep network that is too large for you to run?

    In this era of large-scale deep learning, the most interesting AI models are massive black boxes
    that are hard to run. Ordinary commercial inference service APIs let you interact with huge
    models, but they do not let you see model internals.

    The NNsight library is different: it gives you full access to all the neural network internals.
    When used together with a remote service like the `National Deep Inference Fabric <https://ndif.us/>`_ (NDIF),
    it lets you run experiments on huge open models easily, with full transparent access. 
    NNsight is also terrific for studying smaller local models.

.. figure:: _static/images/interleaved.png


.. card::
    
    An overview of the NNsight/NDIF pipeline. Researchers write simple Python code to run along with the neural network locally or remotely. Unlike commercial inference, the experiment code can read or write any of the internal states of the neural networks being studied.  This code creates a computation graph that can be sent to the remote service and interleaved with the execution of the neural network.

How do I use NNsight?
---------------------

NNsight is built on PyTorch.

Running inference on a huge remote model with NNsight is very similar to running a neural network locally on your own workstation.  In fact, with NNsight, the same code for running experiments locally on small models can also be used on large models just by changing a few arguments.

The difference between NNsight and normal inference is that when you use NNsight, you do not treat the model as an opaque black box.
Instead, you set up a Python ``with`` context that enables you to get direct access to model internals while the neural network runs.
Here is how it looks:

.. code-block:: python
    :linenos:

    from nnsight import LanguageModel
    model = LanguageModel('meta-llama/Meta-Llama-3.1-70B')
    with model.trace('The Eiffel Tower is in the city of ', remote=True):
        hidden_state = model.layers[10].input[0].save()  # save one hidden state
        model.layers[11].mlp.output = 0  # change one MLP module output
        output = model.output.save() # save model output
    print('The model predicts', output)
    print('The internal state was', hidden_state)

The library is easy to use. Any HuggingFace language model can be loaded into a ``LanguageModel`` object, as you can see on line 2.  
Notice we are loading a 70-billion parameter model, which is ordinarily pretty difficult to load on a regular workstation since it would take 140-280 gigabytes of GPU RAM just to store the parameters. 

The trick that lets us work with this huge model is on line 3.  We set the flag ``remote=True`` to indicate that we want to actually run the network on the remote service. 
By default the remote service will be NDIF.  If we want to just run a smaller model locally on our machine, we could leave it as ``remote=False``.

Then when we trace and invoke the model on line 3, we do not just call it as a function. Instead, we access it using a ``with`` context manager. 
This allows NNsight to open up black box neural network models, providing direct access to model internals.

You can see what simple direct access looks like on lines 4-6. 
On line 4, we grab a hidden state at layer 10, and on line 5, we change the output of an MLP module inside the transformer at layer 11.

When you run this ``with``-block code on lines 3 through 6 on your local workstation, it creates a computation graph storing all your requested calculations on the model.  
When the ``with`` block is completed, all the defined calculations are sent to the remote server and executed there. 
Then when model execution is completed, saved results can be accessed on your local workstation, as shown on line 7 and 8.

What happens behind the scenes?
-------------------------------
When using NNsight, it is helpful to understand that the operations are not executed immediately but instead adds to an intervention graph that is executed alongside the model's computation graph upon exit of the with block.

An example of one such intervention graph can be seen below:

.. figure:: _static/images/execution.png

.. card::
    
    An example of an intervention graph. Operations in research code create nodes in the graph which depend on module inputs and outputs as well as other nodes. Then, this intervention graph is interleaved with the normal computation graph of the chosen model, and requested inputs and outputs are injected into the intervention graph for execution. 

Basic access to model internals can give you a lot of insight about what is going on inside a large model as it runs.  For example, you can use the `logit lens <https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens>`_ to read internal hidden states as text.  
And use can use `causal tracing <https://rome.baulab.info/>`_ or `path patching <https://arxiv.org/abs/2304.05969>`_ or `other circuit discovery methods <https://arxiv.org/abs/2310.10348>`_ to locate the layers and components within the network that play a decisive role in making a decision.

And with NNsight, you can use these methods on large models like Llama-3.1-70b or Llama-3.1-405b.

The NNsight library also provides full access to gradients and optimization methods, out of order module applications, cross prompt interventions, and many more features.

Next Steps
----------

See the :doc:`start` and :doc:`features` pages for more information on NNsight's functionality.

Join our forum https://discuss.ndif.us/ for updates, feature requests, bug reports, and opportunities to help with our efforts. 
If you'd like to report an issue or give our project a star, check out our GitHub: https://github.com/ndif-team/nnsight. 
We also welcome you to join the `NDIF Discord <https://discord.gg/6uFJmCSwW7>`_ for real-time discussions and community support.



---
File: /docs/source/documentation.rst
---

Documentation
=============

.. toctree::
   :titlesonly:

   documentation/modeling
   documentation/tracing
   documentation/intervention
   documentation/schema
   documentation/util


Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ for debugging any issues and discussing features.


---
File: /docs/source/features.rst
---

Features
=========

.. raw:: html

   <script>
   document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('h5.card-title').forEach(el => {
      el.style.margin = '0';
      });
   });
   </script>

   <style>
      .toctree-wrapper {
         display: none !important;
      }
      h5 {
         margin-top: 0 !important;
      }
   </style>

.. grid:: 1 1 2 2
   :gutter: 3

   .. grid-item-card:: 
      :link: notebooks/features/getting.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-wrench fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Getting</h5>
               <p class="card-text">Access values</p>
            </div>
         </div>

   .. grid-item-card::
      :link: notebooks/features/setting.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-pull-request fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Setting</h5>
               <p class="card-text">Intervene on values</p>
            </div>
         </div>

   .. grid-item-card::
      :link: notebooks/features/scan_validate.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-binoculars fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Scan and Validate</h5>
               <p class="card-text">Debug tensor shapes</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/features/operations.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-glasses fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Operations</h5>
               <p class="card-text">Edit values</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/modules.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-cubes fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Modules</h5>
               <p class="card-text">Apply modules</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/custom_functions.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-atom fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Custom Functions</h5>
               <p class="card-text">Add thing to the Intervention Graph</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/gradients.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-backward fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Gradients</h5>
               <p class="card-text">Intervene on gradients</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/early_stopping.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-circle-stop fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Early Stopping</h5>
               <p class="card-text">Save computation time</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/conditionals.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-branch fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Conditional Interventions</h5>
               <p class="card-text">Use If Needed</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/cross_prompt.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-shuffle fa-2x"></i>
            </div>
            <div>
               <h5 class="card-title">Cross Prompts</h5>
               <p class="card-text">Edit in one pass</p>
            </div>
         </div>
   
   .. grid-item-card:: 
      :link: notebooks/features/multiple_token.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-gears fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Generation</h5>
               <p class="card-text">Generate multiple tokens</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/model_editing.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-pen-to-square fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Model Editing</h5>
               <p class="card-text">Add persistent interventions</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/features/remote_execution.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-satellite-dish fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Remote Execution</h5>
               <p class="card-text">Use our servers</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/sessions.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-bars fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Sessions</h5>
               <p class="card-text">Do many traces in one request</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/streaming.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-paper-plane fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Streaming</h5>
               <p class="card-text">Send remote values to local</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/iterator.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-arrow-rotate-left fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Iterative Interventions</h5>
               <p class="card-text">Make loops</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/lora_training.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-tower-broadcast fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">LORA</h5>
               <p class="card-text">Train one</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/vllm_support.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-stopwatch fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">vLLM Support</h5>
               <p class="card-text">Fast inference</p>
            </div>
         </div>
   

Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ to ask questions about our features or suggest new ones.

.. toctree::
   :glob:
   :maxdepth: 1
   
   notebooks/features/*




---
File: /docs/source/index.rst
---

:html_theme.sidebar_secondary.remove:
:sd_hide_title:

nnsight
=======

.. toctree::
  :maxdepth: 1
  :hidden:

  start
  documentation
  features
  tutorials
  About <about>

.. raw:: html
   :file: pages/home.html


---
File: /docs/source/start.rst
---

Getting Started
===============

**NNsight** (/ɛn.saɪt/) is a package for the interpreting and manipulating the internals of deep learning models.

.. _installation:

Installation
------------

To get started with NNsight, install it with ``pip``. 

.. code-block:: console

   pip install nnsight

Please give the project a :ndif:`star on Github` to support the project. NNsight is open-source and you can read and clone the full source at https://github.com/ndif-team/nnsight.

Remote Model Access
-------------------

To remotely access LLMs through NDIF, you must sign up for an NDIF API key.

:bdg-link-primary:`NDIF API Key Registration <https://login.ndif.us/>`

NDIF hosts multiple LLMs, including various sizes of the Llama 3.1 models and DeepSeek-R1 models. 
All of our models are open for public use, but you need to apply for access to the Llama-3.1-405B models. 
You can view the full list of hosted models at https://nnsight.net/status/.

If you have a clear research need for Llama-3.1-405B and would like more details about applying for access, 
please refer to our  `405B pilot program application <https://ndif.us/405b.html>`_.

Access LLM Internals
--------------------

Now that you have your NDIF API key, you can start exploring LLM internals with NDIF and NNsight. 
We've put together a Colab notebook to help you get started.

:bdg-link-primary:`Open Colab <https://colab.research.google.com/github/ndif-team/ndif-website/blob/onboarding-fixes/public/notebooks/NDIFGetStarted.ipynb>`

This notebook will walk you through the following steps:

#. Installing NNsight
#. Setting up your NDIF API key
#. Loading a LLM in NNsight
#. Accessing and altering LLM internals remotely


Next Steps
-----------

.. grid:: 2 2 2 2 
   :gutter: 2

   .. grid-item-card:: Walkthrough
      :link: notebooks/tutorials/walkthrough.ipynb

      Walk through the basic functionality of the package.

   .. grid-item-card:: Remote Access
      :link: notebooks/features/remote_execution.ipynb

      Configure API access for remote model execution.

   .. grid-item-card:: Features
      :link: features
      :link-type: doc

      Check out the basic features provided by :bdg-primary:`nnsight`.

   .. grid-item-card:: Tutorials
      :link: tutorials
      :link-type: doc

      See :bdg-primary:`nnsight` implementations of common interpretability techniques.

   .. grid-item-card:: Forum
      :link: https://discuss.ndif.us/

      Discuss :bdg-primary:`nnsight`, NDIF, and more!






---
File: /docs/source/status.rst
---

:html_theme.sidebar_secondary.remove:
:sd_hide_title:

.. raw:: html

    <style>
        .accordion-header {
            margin: 0 !important;
        }

        /* Custom accordion styles */
        .custom-accordion-header {
            background-color: var(--pst-color-surface);
            /* Default state background color */
            color: var(--pst-color-text-base);
            /* Text color */
            border-bottom: var(--pst-color-border);
            /* Border color */
        }

        .custom-accordion-header {
            background-color: var(--pst-color-surface);
            /* Default state background color */
            color: var(--pst-color-text-base);
            /* Text color */
            border-bottom: var(--pst-color-border);
            /* Border color */
        }

        .accordion {
            --bs-accordion-btn-icon: none;
            --bs-accordion-btn-active-icon: none;
        }

        .custom-accordion-header.collapsed {
            background-color: var(--pst-color-on-background);
            /* Collapsed state background color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .custom-accordion-header:not(.collapsed) {
            background-color: var(--pst-color-surface);
            /* Active/Expanded state background color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .custom-accordion-body {
            background-color: var(--pst-color-on-background);
            /* Body background color */
            border-color: var(--pst-color-border);
            /* Border color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .sd-card {
            border-radius: 0 !important;
        }

        #loader {
            width: 120px;
            height: 120px;
            display: inline-block;
            position: relative;
        }
        #loader::after,
        #loader::before {
            content: '';  
            box-sizing: border-box;
            width:120px;
            height: 120px;
            border-radius: 50%;
            background: #FFF;
            position: absolute;
            left: 0;
            top: 0;
            animation: animloader 2s linear infinite;
        }
        #loader::after {
            animation-delay: 1s;
        }
        
        @keyframes animloader {
            0% {
                transform: scale(0);
                opacity: 1;
            }
            100% {
                transform: scale(1);
                opacity: 0;
            }
        }

    </style>


    <script>

        let ndif_url = "https://ndif.dev"
        let error_color = "#7e0000"
        let success_color = "#66800b"
        let warning_color = "#7d7106"

        function autoFormatJsonString(jsonString) {
            // Parse the JSON string into an object
            let jsonObject = JSON.parse(jsonString);

            // Convert the object back into a string with indentation
            let prettyPrintedJson = JSON.stringify(jsonObject, null, 2);

            // Replace keys in the JSON string with styled spans
            prettyPrintedJson = prettyPrintedJson.replace(/"([^"]+)":/g, '<span style="background-color: lightgrey;">"$1":</span>');

            // Set the formatted JSON string as the innerHTML of the element
            document.getElementById('jsonContainer').innerHTML = `<pre>${prettyPrintedJson}</pre>`;
        };

        function update(message, color) {
            document.querySelectorAll('div.sd-card-body.status-container').forEach(el => {
                el.style.backgroundColor = color;
                el.querySelectorAll('p.sd-card-text').forEach(el => {
                    el.textContent = message;
                });
            });
        }

        function loading(flag) {
            document.getElementById("loader").style.display = flag ? "block" : "none";
        }

        document.addEventListener('DOMContentLoaded', function() {

            loading(true);

            update("Fetching NDIF status...", warning_color);

            fetch(ndif_url + "/ping")

                .then((response) => {
                    if (response.status == 200) {

                        update("NDIF is up. Fetching model status...", warning_color);

                        console.log('Ping success');
                        // Nested fetch to ndif.dev/stats
                        fetch(ndif_url + "/stats")
                            .then((statsResponse) => {

                                loading(false);

                                if (statsResponse.status == 200) {
                                    statsResponse.json().then((parsed) => {
                                        // Initialize an empty string to accumulate information
                                        let infoString = '';

                                        let index = 0;

                                        let modelSummary = {};

                                        if (parsed.length === 0) {

                                            update("NDIF is up but there are no models deployed. Seems unintentional.", error_color);

                                            return
                                        }


                                        update("NDIF is operational.", success_color);

                                        Object.values(parsed).forEach((value) => {
                                            // Create a unique key for each model-config combination
                                            let modelConfigKey = `${value.repo_id}`;

                                            // Check if this model-config combination already exists in the summary
                                            if (modelSummary[modelConfigKey]) {
                                                // Increment the count if it does
                                                modelSummary[modelConfigKey].number_of_copies += 1;
                                            } else {
                                                // Otherwise, add a new entry
                                                modelSummary[modelConfigKey] = {
                                                    number_of_copies: 1,
                                                    config_string: value.config_json_string
                                                };
                                            }
                                        });

                                        // Now modelSummary contains the consolidated information
                                        console.log(modelSummary);

                                        // Iterate through the JSON dictionary and append information
                                        // Iterate through the modelSummary dictionary and append information
                                        Object.keys(modelSummary).forEach((key) => {
                                            var headingId = 'heading' + (index + 1);
                                            var collapseId = 'collapse' + (index + 1);

                                            const summaryItem = modelSummary[key];
                                            const configJsonString = summaryItem.config_string;

                                            let jsonObject = JSON.parse(configJsonString);

                                            // Convert the object back into a string with indentation
                                            let prettyPrintedJson = JSON.stringify(jsonObject, null, 4);

                                            prettyPrintedJson = prettyPrintedJson.replace(/"([^"]+)":/g, '"<b>$1</b>":');
                                            let huggingFaceLink = `<a href="http://huggingface.co/${key}" target="_blank">HuggingFace Model Repository ↗</a>`;

                                            infoString += `<div class="accordion-item">
                                                    <h2 class="accordion-header" id="${headingId}">
                                                        <button class="accordion-button custom-accordion-header collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#${collapseId}" aria-expanded="false" aria-controls="${collapseId}">
                                                            (${summaryItem.number_of_copies}x) ${key}
                                                        </button>
                                                    </h2>
                                                    <div id="${collapseId}" class="accordion-collapse collapse" aria-labelledby="${headingId}" data-bs-parent="#accordionExample">
                                                        <div class="accordion-body custom-accordion-body">${huggingFaceLink}<pre>${prettyPrintedJson}</pre></div>
                                                    </div>
                                                </div>`;


                                            index++;
                                        });

                                        var elm = document.getElementById("accordionHook");

                                        elm.innerHTML = infoString;


                                        console.log('Stats success');
                                    }).catch((jsonError) => {
                                        console.log('JSON parsing error:', jsonError);
                                    });
                                } else {
                                    update("Unable to get NDIF status.", error_color);

                                }
                            })
                            .catch((statsError) => {
                                update("Unable to get NDIF status.", error_color);
                                loading(false);

                                console.log('Stats error', statsError);
                            });
                    } else {
                        update("NDIF is unavailable", error_color);
                        loading(false);
                        console.log('Ping error');
                    }
                })
                .catch((pingError) => {
                    update("NDIF is unavailable", error_color);
                    loading(false);
                    console.error('Ping fetch failed:', pingError);
                });

        }, false);
    </script>


Status
======

.. card::
    :class-body: status-container
    :shadow: none

    Getting Status

.. card::
    :shadow: none
    
    The library can be used to run local models without requiring a key. However, running experiments on remote models requires a free server API key. To obtain a key, register for an `NDIF account <https://login.ndif.us>`_ which allows you to manage and generate keys.
    For information on API key configuration and remote system limits, please refer to our `Remote Execution Tutorial <https://nnsight.net/notebooks/features/remote_execution/>`_.

    We currently have engineers on call Monday to Friday from 9 AM to 5 PM ET to assist with any connectivity issues for our remote models. Please reach out to us on `Discord <https://discord.com/invite/6uFJmCSwW7>`_ or at mailto:info@ndif.us.

.. raw:: html

    <div style="
        width:100%;
        display: flex;
        justify-content: center;
        ">
        <div id="loader"></div>
    </div>
    


    <div class="accordion accordion-flush" id="accordionHook">
    </div>


---
File: /docs/source/tutorials.rst
---

.. role:: raw-html(raw)
   :format: html

.. raw:: html

   <script>
   document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('h5.card-title').forEach(el => {
      el.style.margin = '0';
      });
   });
   </script>

   <style>
      .toctree-wrapper {
         display: none !important;
      }
      h5 {
         margin-top: 0 !important;
      }
   </style>


Tutorials
=========

.. grid:: 1 1 2 2
   :class-container: tutorial-card-section
   :gutter: 3

   .. grid-item-card:: 
      :link: notebooks/tutorials/walkthrough.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-person-walking fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Walkthrough</h5>
               <p class="card-text">Learn the basics</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/tutorials/start_remote_access.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-satellite-dish fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Access LLMs</h5>
               <p class="card-text">Use our hosted models</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/activation_patching.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-pull-request fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Activation Patching</h5>
               <p class="card-text">Causal intervention</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/attribution_patching.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-diagram-project fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Attribution Patching</h5>
               <p class="card-text">Approximate patching</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/boundless_DAS.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-magnifying-glass fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Boundless DAS</h5>
               <p class="card-text">Identifying causal mechanisms</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/dict_learning.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-book-open fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Dictionary Learning</h5>
               <p class="card-text">Sparse autoencoders</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/diffusion_lens.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-camera-retro fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Diffusion Lens</h5>
               <p class="card-text">Explore diffusion model text embedding</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/function_vectors.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-dharmachakra fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Function Vectors</h5>
               <p class="card-text">Steer model behavior</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/logit_lens.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-arrow-down-a-z fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Logit Lens</h5>
               <p class="card-text">Decode activations</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/LoRA_tutorial.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-sliders fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">LoRA</h5>
               <p class="card-text">Fine tuning for sentiment analysis</p>
            </div>
         </div>      

Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ to ask questions about our tutorials, share your projects in NNsight, or request new tutorials.

.. toctree::
   :glob:
   :maxdepth: 1

   notebooks/tutorials/*





---
File: /docs/sourcelatex/documentation/contexts.rst
---

nnsight.contexts
-----------------


.. automodule:: nnsight.contexts
   :members:

.. automodule:: nnsight.contexts.Runner
   :members:

.. automodule:: nnsight.contexts.Tracer
   :members:

.. automodule:: nnsight.contexts.Invoker
   :members:




---
File: /docs/sourcelatex/documentation/intervention.rst
---

nnsight.intervention
--------------------

.. automodule:: nnsight.intervention
   :members:


---
File: /docs/sourcelatex/documentation/models.rst
---

nnsight.models
--------------


.. automodule:: nnsight.models
   :members:


.. automodule:: nnsight.models.NNsightModel
   :members:


.. automodule:: nnsight.models.LanguageModel
   :members:


.. automodule:: nnsight.models.DiffuserModel
   :members:


---
File: /docs/sourcelatex/documentation/module.rst
---

nnsight.module
--------------

.. automodule:: nnsight.Module
   :members:


---
File: /docs/sourcelatex/documentation/patching.rst
---

nnsight.patching
----------------


.. automodule:: nnsight.patching
   :members:


---
File: /docs/sourcelatex/documentation/tracing.rst
---

nnsight.tracing
---------------

.. automodule:: nnsight.tracing
   :members:

.. automodule:: nnsight.tracing.Graph
   :members:

.. automodule:: nnsight.tracing.Node
   :members:

.. automodule:: nnsight.tracing.Proxy
   :members:



---
File: /docs/sourcelatex/documentation/util.rst
---

nnsight.util
------------


.. automodule:: nnsight.util
   :members:


---
File: /docs/sourcelatex/index.rst
---

Documentation
=============

.. toctree::
   :titlesonly:

   documentation/models
   documentation/module
   documentation/contexts
   documentation/util
   documentation/intervention
   documentation/tracing
   documentation/patching



---
File: /docs/contributing.md
---

# Installation

Run `pip install -r requirements.txt` while in the `nnsight/docs` directory.

Optionally, run `pip install -e nnsight` from the root directory to use the working dir

If you haven't already, you should also install pandoc. With brew run: `brew install pandoc`

# Adding Tutorials

Tutorials are written as Jupyter Notebooks in `.ipynb` format, and automatically converted to html by the `nbsphinx` extension.

The only requirement of `nbsphinx` is that you add a header to the notebook to act as the rendered title on Sphinx docs. To do this, create a markdown cell at the top of your notebook with a header `#`.

Then, just add your notebook to the `nnsight/docs/source/notebooks/tutorials` directory.

If you're adding a new notebook, navigate to `nnsight/docs/source/tutorials.rst`. At the bottom of the page, add the path to your notebook under the `toctree`.

```
.. toctree::
   :maxdepth: 1

   notebooks/tutorials/walkthrough.ipynb
   ...
   <YOUR NOTEBOOK PATH>

```

# Compiling Sphinx to HTML

Run `make dirhtml` from the `nnsight/docs` directory. The build is located in `nnsight/docs/build/dirhtml`. You can run `python3 -m http.server <PORT>` from that directory, and see the site at `http://localhost:<PORT>`.



---
File: /CHANGELOG.md
---

# Changelog

## `0.3.0`

_released: 2024-08-29_

We are excited to announce the release of `nnsight0.3`.

This version significantly enhances the library's remote execution capabilities. It improves the integration experience with the [NDIF](https://ndif.us) backend and allows users to define and execute optimized training loop workflows directly on the remote server, including LoRA and other PEFT methods.

### Breaking Changes

-  Module `input` access has a syntactic change:
    - Old: `nnsight.Envoy.input`
    - New: `nnsight.Envoy.inputs`
    - Note: `nnsight.Envoy.input` now provides access to the first positional argument of the module's input.

- `scan` & `validate` are set to `False` by default in the `Tracer` context.

### New Features

- [<ins>Session context</ins>](https://nnsight.net/notebooks/features/sessions/): efficiently package multi-tracing experiments into a single request, enabling faster, more scalable remote experimentation.

- [<ins>Iterator context</ins>](https://nnsight.net/notebooks/features/iterator/): define an intervention loop for iterative execution.

- [<ins>Model editing</ins>](nnsight.net/notebooks/features/model_editing/): alter a model by setting default edits and interventions in an editing context, applied before each forward pass.

- [<ins>Early stopping</ins>](https://nnsight.net/notebooks/features/early_stopping/): interrup a model's forward pass at a chosen module before execution completes. 

- [<ins>Conditional context</ins>](https://nnsight.net/notebooks/features/conditionals/): define interventions within a Conditional context, executed only when the specified condition evaluates to be True.

- [<ins>Scanning context</ins>](https://nnsight.net/notebooks/features/scan_validate/): perform exclusive model scanning to gather important insights.

- <ins>`nnsight` builtins</ins>: define traceable `Python` builtins as part of the intervention graph.

- <ins>Proxy update</ins>: assign new values to existing proxies. 
     
- <ins>In-Trace logging</ins>: add log statements to be called during the intervention graph execution.

- [<ins>Traceable function calls</ins>](https://nnsight.net/notebooks/features/custom_functions/): make unsupported functions traceable by the intervention graph. Note that [<ins>all pytorch functions are now traceable</ins>](https://nnsight.net/notebooks/features/operations/) by `nnsight` by default.



---
File: /CODE_OF_CONDUCT.md
---


# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official email address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at j.bell@northeastern.edu .
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations



---
File: /README.md
---

<img src="./docs/source/_static/images/nnsight_logo.svg" alt="drawing" style="width:200px;float:left"/>

# nnsight 

<a href="https://arxiv.org/abs/2407.14561"><img src="https://img.shields.io/badge/READ%20THE%20PAPER%20HERE!-orange" style="transform: scale(3);"></a>

<a href="https://www.nnsight.net"><img src="https://img.shields.io/badge/-Read%20the%20Docs%20Here-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white"></img></a> <a href="https://discord.gg/6uFJmCSwW7"><img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white"></a>

The `nnsight`  package enables interpreting and manipulating the internals of deep learned models. Read our [paper!](https://arxiv.org/abs/2407.14561)

#### Installation

Install this package through pip by running:

`pip install nnsight`

#### Examples

Here is a simple example where we run the nnsight API locally on gpt2 and save the hidden states of the last layer:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace('The Eiffel Tower is in the city of') as tracer:

      hidden_states = model.transformer.h[-1].output[0].save()

      output = model.output.save()
```

Lets go over this piece by piece.

We import the `LanguageModel` object from the `nnsight` module and create a gpt2 model using the huggingface repo ID for gpt2, `'openai-community/gpt2'`. This accepts arguments to create the model including `device_map` to specify which device to run on.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2',device_map='auto')
```

Then, we create a tracing context block by calling `.trace(...)` on the model object. This denotes we want to run the model with our prompt.


```python
with model.trace('The Eiffel Tower is in the city of') as tracer:
```

Now calling `.trace(...)` does not actually initialize or run the model. Only after the tracing` block is exited, is the actual model loaded and ran. All operations in the block are "proxies" which essentially creates a graph of operations we wish to carry out later.

Within this context, all operations/interventions will be applied to the processing of the given prompt.

```python
hidden_states = model.transformer.h[-1].output[0].save()
```

On this line were saying, access the last layer of the transformer `model.transformer.h[-1]`, access its output `.output`, index it at 0 `.output[0]`, and save it `.save()`

A few things, we can see the module tree of the model by printing the model. This allows us to know what attributes to access to get to the module we need.
Running `print(model)` results in:

```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

`.output` returns a proxy for the output of this module. This essentially means were saying, when we get to the output of this module during inference, grab it and perform any operations we define on it (which also become proxies). There are two operational proxies here, one for getting the 0th index of the output, and one for saving the output. We take the 0th index because the output of gpt2 transformer layers are a tuple where the first index are the actual hidden states (last two indicies are from attention). We can call `.shape` on any proxies to get what shape the value will eventually be. 
Running `print(model.transformer.h[-1].output.shape)` returns `(torch.Size([1, 10, 768]), (torch.Size([1, 12, 10, 64]), torch.Size([1, 12, 10, 64])))`

During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed. However calling `.save()` on the proxy informs the computation graph to save the value of this proxy and never destroy it, allowing us to access to value after generation.

After exiting the generator context, the model is ran with the specified arguments and intervention graph. `output` is populated with the actual output and `hidden_states` will contain the hidden value.

```python
print(output)
print(hidden_states)
```

returns:

```
tensor([[ 464,  412,  733,  417, 8765,  318,  287,  262, 1748,  286, 6342]],
       device='cuda:0')
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
```



---

###### Operations

Most basic operations and torch operations work on proxies and are added to the computation graph. 

```python
from nnsight import LanguageModel
import torch 

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.trace('The Eiffel Tower is in the city of'):

  hidden_states_pre = model.transformer.h[-1].output[0].save()

  hs_sum = torch.sum(hidden_states_pre).save()

  hs_edited = hidden_states_pre + hs_sum

  hs_edited = hs_edited.save()

print(hidden_states_pre)
print(hs_sum)
print(hs_edited)
```

In this example we get the sum of the hidden states and add them to the hidden_states themselves (for whatever reason). By saving the various steps, we can see how the values change.

```
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
tensor(501.2957, device='cuda:0')
tensor([[[501.3461, 501.1229, 501.1267,  ..., 500.2860, 501.4237, 500.2270],
         [510.0451, 504.2014, 506.5981,  ..., 493.2538, 502.5920, 498.4279],
         [501.5916, 505.9643, 497.6315,  ..., 501.5348, 498.6892, 504.5219],
         ...,
         [503.4493, 508.1874, 505.1607,  ..., 501.3545, 499.3091, 507.2145],
         [500.8496, 508.7242, 491.9892,  ..., 503.3485, 498.5010, 501.8512],
         [507.9242, 503.0215, 506.0926,  ..., 508.9671, 504.3639, 503.3438]]],
       device='cuda:0')
       
```

---
###### Setting

We often not only want to see whats happening during computation, but intervene and edit the flow of information. 

```python
from nnsight import LanguageModel
import torch 

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.trace('The Eiffel Tower is in the city of') as tracer:

  hidden_states_pre = model.transformer.h[-1].mlp.output.clone().save()

  noise = (0.001**0.5)*torch.randn(hidden_states_pre.shape)

  model.transformer.h[-1].mlp.output = hidden_states_pre + noise

  hidden_states_post = model.transformer.h[-1].mlp.output.save()

print(hidden_states_pre)
print(hidden_states_post)
```
In this example, we create a tensor of noise to add to the hidden states. We then add it, use the assigment `=` operator to update the value of `.output` with these new noised activations. 

We can see the change in the results:

```
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
tensor([[[ 0.0674, -0.1741, -0.1771,  ..., -0.9811,  0.1972, -1.0645],
         [ 8.7080,  2.9067,  5.2924,  ..., -8.0253,  1.2729, -2.8419],
         [ 0.2611,  4.6911, -3.6434,  ...,  0.2295, -2.6007,  3.2635],
         ...,
         [ 2.1859,  6.9242,  3.8666,  ...,  0.0556, -2.0282,  5.8863],
         [-0.4568,  7.4101, -9.3698,  ...,  2.0630, -2.7971,  0.5522],
         [ 6.6764,  1.7416,  4.8027,  ...,  7.6507,  3.0754,  2.0218]]],
       device='cuda:0')
```

---
###### Multiple Token Generation

When generating more than one token, use `.generate(...) ` and `.next()`  on the module you want to get the next value of to denote following interventions should be applied to the subsequent generations.

Here we again generate using gpt2, but generate three tokens and save the hidden states of the last layer for each one:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate('The Eiffel Tower is in the city of', max_new_tokens=3) as tracer:
 
  hidden_states1 = model.transformer.h[-1].output[0].save()

  invoker.next()

  hidden_states2 = model.transformer.h[-1].next().output[0].save()

  invoker.next()

  hidden_states3 = model.transformer.h[-1].next().output[0].save()

```
---

###### Cross Prompt Intervention


Intervention operations work cross prompt! Use two invocations within the same generation block and operations can work between them.

You can do this by not passing a prompt into `.trace`/`.generate`, but by calling `.invoke(...)` on the created tracer object.

In this case, we grab the token embeddings coming from the first prompt, `"Madison square garden is located in the city of New"` and replace the embeddings of the second prompt with them.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate(max_new_tokens=3) as tracer:
    
    with tracer.invoke("Madison square garden is located in the city of New"):

        embeddings = model.transformer.wte.output

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):

        model.transformer.wte.output = embeddings

        output = model.generator.output.save()

print(model.tokenizer.decode(output[0]))
print(model.tokenizer.decode(output[1]))
```

This results in:

```
Madison square garden is located in the city of New York City.
_ _ _ _ _ _ _ _ _ _ York City.
```

We also could have entered a pre-saved embedding tensor as shown here:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate(max_new_tokens=3) as tracer:
    
    with tracer.invoke("Madison square garden is located in the city of New") as invoker:

        embeddings = model.transformer.wte.output.save()

with model.generate(max_new_tokens=3) as tracer:

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _") as invoker:

        model.transformer.wte.output = embeddings.value

```
---

###### Ad-hoc Module

Another thing we can do is apply modules in the model's module tree at any point during computation, even if it's out of order.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel("openai-community/gpt2", device_map='cuda')

with model.generate('The Eiffel Tower is in the city of') as generator:

  hidden_states = model.transformer.h[-1].output[0]
  hidden_states = model.lm_head(model.transformer.ln_f(hidden_states)).save()
  tokens = torch.softmax(hidden_states, dim=2).argmax(dim=2).save()
        
print(hidden_states)
print(tokens)
print(model.tokenizer.decode(tokens[0]))

```

Here we get the hidden states of the last layer like usual. We also chain apply `model.transformer.ln_f` and `model.lm_head` in order to "decode" the hidden states into vocabularly space.
Applying softmax and then argmax allows us to then transform the vocabulary space hidden states into actually tokens which we can then use the tokenizer to decode.

The output looks like:

```
tensor([[[ -36.2874,  -35.0114,  -38.0793,  ...,  -40.5163,  -41.3759,
           -34.9193],
         [ -68.8886,  -70.1562,  -71.8408,  ...,  -80.4195,  -78.2552,
           -71.1206],
         [ -82.2950,  -81.6519,  -83.9941,  ...,  -94.4878,  -94.5194,
           -85.6998],
         ...,
         [-113.8675, -111.8628, -113.6634,  ..., -116.7652, -114.8267,
          -112.3621],
         [ -81.8531,  -83.3006,  -91.8192,  ...,  -92.9943,  -89.8382,
           -85.6898],
         [-103.9307, -102.5054, -105.1563,  ..., -109.3099, -110.4195,
          -103.1395]]], device='cuda:0')
tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]],
       device='cuda:0')

-el Tower is a the middle centre Paris
```

---

More examples can be found at [nnsight.net](https://www.nnsight.net)

### Citation

If you use `nnsight` in your research, please cite using the following

```bibtex
@article{fiottokaufman2024nnsightndifdemocratizingaccess,
      title={NNsight and NDIF: Democratizing Access to Foundation Model Internals}, 
      author={Jaden Fiotto-Kaufman and Alexander R Loftus and Eric Todd and Jannik Brinkmann and Caden Juang and Koyena Pal and Can Rager and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Michael Ripa and Adam Belfki and Nikhil Prakash and Sumeet Multani and Carla Brodley and Arjun Guha and Jonathan Bell and Byron Wallace and David Bau},
      year={2024},
      eprint={2407.14561},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.14561}, 
}
``````



Directory Structure:

└── ./
    ├── docs
    │   └── source
    │       ├── _static
    │       │   └── model_properties_table_interactive.html
    │       └── content
    │           ├── news
    │           │   └── release-2.0.md
    │           ├── citation.md
    │           ├── contributing.md
    │           ├── gallery.md
    │           ├── getting_started_mech_interp.md
    │           ├── getting_started.md
    │           ├── special_cases.md
    │           └── tutorials.md
    ├── further_comments.md
    └── README.md



---
File: /docs/source/_static/model_properties_table_interactive.html
---

<!DOCTYPE html>
<html>

<head>
	<title>TransformerLens models</title>
	<script src="https://cdn.jsdelivr.net/npm/ag-grid-community/dist/ag-grid-community.min.js"></script>
	<style>
		header {
			background-color: #f4f4f4;
			padding: 10px;
			text-align: left;
		}

		body,
		html {
			margin: 0;
			padding: 0;
			width: 100%;
			/* Ensure the body takes up full viewport width */
		}

		.ag-theme-alpine {
			height: 80%;
			width: 100%;
			/* This should already keep the grid within the page width */
			box-sizing: border-box;
			/* Ensure padding and borders are included in the width */
		}

		.ag-paging-panel {
			/* Aligns children (the pagination buttons) to the start of the container, i.e., left side */
			justify-content: flex-start;
			align-items: center;
		}
	</style>
</head>

<body>
	<header>
		Model table for <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a>. Source code: <a
			href="https://github.com/mivanit/transformerlens-model-table">github.com/mivanit/transformerlens-model-table</a>.
		Hover a cell to view full text, left click to copy to clipboard, right click to open contents in new tab.
	</header>

	<div id="modelTable" class="ag-theme-alpine"></div>

	<script>
		const CLICK_TO_COPY_EMOJI = String.fromCodePoint(0x1F446) + String.fromCodePoint(0x1F4CB);
		// read a jsonl file
		async function fetchJsonlData(url) {
			const response = await fetch(url);
			const text = await response.text();
			return text.trim().split('\n').map(line => JSON.parse(line));
		}

		async function fetchVersion() {
			try {
				const response = await fetch('model_table.version');
				// load the contents of the file as json
				return response.json().then(json => json.version);
			} catch (error) {
				console.error('Could not fetch version: ', error);
				return 'unknown';
			}
		}

		// fancy cell rendering -- hover/copy/open the data, make it emojis if its too long
		function longCellRenderer(params) {
			// Create the div element
			var div = document.createElement('div');
			div.title = params.value; // Set the title text to the full text
			div.style.cursor = 'pointer'; // Ensure the cursor style is set
			// If its too long, make it emojis
			if (params.value !== null) {
				if (params.value.length > 50) {
					div.textContent = CLICK_TO_COPY_EMOJI;
					div.style.cssText = 'font-size: 20px; display: flex; justify-content: center; align-items: center; background-color: #f4f4f4; border: 1px solid #d4d4d4; border-radius: 5px; height: 30px; width: 60px; cursor: pointer;';
				}
			}

			// Add click event listener to copy text to the clipboard
			div.onclick = function () {
				navigator.clipboard.writeText(params.value).then(function () {
					console.log('Successfully copied to clipboard');
				}).catch(function (err) {
					console.error('Could not copy text to clipboard: ', err);
				});
			};

			// On right click, open a new plain text tab whose contents are the cell's value
			div.oncontextmenu = function (event) {
				event.preventDefault(); // Prevent the default context menu from appearing
				const newWindowName = params.node.data['name.default_alias'] + ' : ' + params.colDef.headerName;
				const newWindow = window.open('', newWindowName);
				// Set the title of the page to the rows "name.default_alias" and the column's header
				newWindow.document.write('<title>' + newWindowName + '</title>');
				// Set the contents of the new window to the cell's value
				newWindow.document.write('<pre>' + params.value + '</pre>');
				// newWindow.document.close();
				return false; // Prevent the default context menu from appearing
			};

			// Return the div as the cell's DOM
			return div;
		}

		// huggingface model name cell renderer -- add a link to the model page
		function hf_link_cell_renderer(params) {
			// Create the div element
			var div = document.createElement('div');
			div.style.cursor = 'pointer'; // Ensure the cursor style is set
			// If its too long, make it emojis
			if (params.value !== null) {
				// make a link with the text of the value, pointing to https://huggingface.co/{x}
				// make it open in a new tab
				div.innerHTML = `<a href="https://huggingface.co/${params.value}" target="_blank" rel="noopener noreferrer">${params.value}</a>`;
				div.style.cssText = 'cursor: pointer;';
			}
			// Return the div as the cell's DOM
			return div;
		}

		function value_formatter(params) {
			if (params.value === null) {
				return '';
			}
			return typeof params.value === 'object' ? JSON.stringify(params.value) : params.value;
		}

		async function setupGrid() {
			const version = await fetchVersion();
			document.querySelector('header').innerHTML = `Interactive Model table for <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a> v${version}.  Hover a cell with [${CLICK_TO_COPY_EMOJI}] to view full text, left click to copy to clipboard, right click to open contents in new tab.`;

			// read the data
			const rowData = await fetchJsonlData('model_table/data.jsonl');
			const columnGroups = {};

			// create the column definitions
			Object.keys(rowData[0]).forEach(key => {
				// if key ends with "__", then ignore it (raw tensor shapes)
				if (key.endsWith('__')) {
					return;
				}
				// treat dot separated keys as column groups
				const keyParts = key.split('.');
				if (keyParts.length === 2) {
					// column in a group
					const groupName = keyParts[0];
					const fieldName = keyParts[1];
					// init an empty group if it doesn't exist
					if (!columnGroups[groupName]) {
						columnGroups[groupName] = {
							headerName: groupName,
							children: [],
						};
					}
					// add the column to the group
					const columnDef = {
						headerName: fieldName,
						field: key,
						// if it's an object, stringify it
						valueFormatter: value_formatter,
						// only show the first child if there are many (we modify this later in some special cases)
						columnGroupShow: columnGroups[groupName].children.length < 1 ? null : 'open',
						// hacky width calculation (doesn't work great)
						width: Math.min(Math.max(130, key.length * 5), 500),
						// numeric if it's a number
						type: typeof rowData[0][key] === 'number' ? 'numericColumn' : 'textColumn',
					};

					// special renderer for tensor shapes
					if (groupName === 'tensor_shapes') {
						columnDef.cellRenderer = longCellRenderer;
					}

					// special renderer for huggingface model name
					if (groupName === 'name' && fieldName === 'huggingface') {
						columnDef.cellRenderer = hf_link_cell_renderer;
					}

					columnGroups[groupName].children.push(columnDef);
				} else {
					// solo column
					const columnDef = {
						headerName: key,
						field: key,
						valueFormatter: value_formatter,
					};
					// special renderer for full cfg
					if (key === 'config') {
						columnDef.cellRenderer = longCellRenderer;
					}
					columnGroups[key] = columnDef;
				}
			});

			// special modifications
			columnGroups['model_type'].width = 130;
			columnGroups['config'].width = 100;
			columnGroups['tensor_shapes'].width = 200;
			// open these groups by default
			columnGroups['tensor_shapes'].openByDefault = true;
			columnGroups['cfg'].openByDefault = true;

			const columnDefs = Object.values(columnGroups);

			// create the grid
			const gridOptions = {
				columnDefs: columnDefs,
				rowData: rowData,
				rowSelection: 'multiple',
				// customize pagination
				pagination: true,
				paginationPageSize: 500,
				paginationPageSizeSelector: [10, 25, 50, 100, 500, 1000],
				enableCellTextSelection: true,
				enableBrowserTooltips: true,
				// default column options
				defaultColDef: {
					resizable: true,
					filter: true,
					// always show the floating filter
					floatingFilter: true,
					// disable filter hamburger menu (for space)
					menuTabs: [],
				},
				// we assume dots are groups, don't treat them as field notation
				suppressFieldDotNotation: true,
				// define column type to avoid warning
				columnTypes: {
					textColumn: {},
				},
				// this disables animations, but makes the horizontal scrolling not painfully slow
				domLayout: 'print',
			};

			// create the grid
			new agGrid.createGrid(document.getElementById('modelTable'), gridOptions);
		}

		setupGrid();
	</script>
</body>

</html>


---
File: /docs/source/content/news/release-2.0.md
---

# TransformerLens 2.0

I am very happy to announce TransformerLens now has a 2.0 release! If you have been using recent versions of TransformerLens, then the good news is that not much has changed at all. The primary motivation behind this jump is to transition the project to strictly following semantic versioning as described [here](https://semver.org/). At the last minute we did also remove the recently added HookedSAE, so if you had been using that, I would direct you to Joseph Bloom’s [SAELens](http://github.com/jbloomAus/SAELens). Bundled with this major version change are also a handful of internal modifications that only affect contributors.

## First, an introduction

My name is Bryce Meyer. I am a software engineer, with just a little under 15 years of professional experience with a wide range of expertise from embedded computing to API design. Within the last couple years I have gotten more and more involved in ML, and especially AI safety within the last nine months. At the end of March, I was chatting a bit with Joseph Bloom, and he asked me if I might be interested in taking on the role as primary maintainer of TransformerLens. I have been doing so on a part time basis since the beginning of April, and so far I am pretty happy with the progress that has been made.

In this first month, with the help of the many kind contributors to TransformerLens we have managed to address every single pull request, many of which had been awaiting reply for quite some time. In total around 30 pull requests have been merged with contributions from around 20 people. Within those PRs a number of features were added, including, but not limited to support for Mixtral, LLaMA 3, 4-bit Quantized LLaMA, and HookedSAETransformer, a brand new class to splice sparse autoencoders into HookedTransformer.

I have two primary immediate goals for my time as primary maintainer of this project. The first is to position TransformerLens in a way where it is approachable to as many people as possible, while also remaining powerful for people who are pushing the limits of the field. My second goal is to find ways to make the code base for this project easier to manage for the future, as the development and availability of LLMs continues to accelerate.

I feel that this project has a massive amount of momentum at the moment, and I am hoping to carry that over into the future with new features. I have a software engineering background, not research, and I know I need to talk a lot with users to ensure the library is meeting their needs. I have personally spoken with around a dozen people in the community on their experience with TransformerLens, and what they may want to see happen in the future. If you have not spoken with me, but you would like to, please [make an appointment](https://calendly.com/bryce-c7e/30min). I am curious to hear from anyone who is using this tool, from absolute beginner to complete experts.

## Adopting Semantic Versioning

In the last month, a lot has changed with TransformerLens. Not only have a lot of changes been made to the code, but ideas of how the project will be managed are also evolving. The biggest change in the management of the project is the previously mentioned adoption of Semantic Versioning. Previously, the project had not been officially managed under Semantic Versioning, and there were instances where compatibility was not maintained through the 1.x branch. Going forward, API changes will be managed strictly, and in a way that maintains compatibility through major versions. If you are starting a project today using TransformerLens 2.0, then you can be rest assured that you will be able to upgrade your code all the way through the 2.x branch without worrying about needing to make changes to your code. For full details of how Semantic Versioning will affect TransformerLens, please see the appendix.

## Deprecations

There are right now two deprecations in the code base. The parameter `move_model` in `ActivationCache.to`, and the function `cache_all` in `hook_points`. To keep things simple for the change to semantic versioning, they will be remaining. However, if you are using them, then make sure to adapt your code right away. They will be removed in 3.0. Along with that, anything new that is marked deprecated in 2.x will also be removed when the next major version comes around.

Whenever something new does become deprecated, it will also be prominently noted in the release notes to make sure these sorts of things do not slip by. In my previously mentioned scenario where a key was renamed, in the future a situation like this will be handled by changing the code to the new key, but then persisting the deprecated old key pointing at the new key. This will allow anyone relying on that key to continue to do so without interruption.

However, I would still encourage everyone to periodically check release notes to make sure they are keeping an eye out for when things become deprecated. I don’t imagine it will happen often, but it will happen. Keeping an eye on it will save a lot of trouble in the future when moving from one major release to the next.

## Roadmap

There are three primary timeframes that I am approaching the current development plans of TransformerLens.

### Immediate - within the next month

At the moment, TransformerLens is in a state where all pull requests are being addressed quickly, but the issue tracker is still full of items that have not been addressed. The first thing to be done now is to go through all issues, categorize them, and address anything that is easy to address. Once that is done, both issues and pull requests will be up to date, and should remain that way going forward.

### Mid-term - within the next 3 months

Please note that the below is a draft roadmap. We are very happy to change our prioritization if user feedback surfaces other issues.

#### Performance

One of the first new items to be improved in TransformerLens is general performance. This will be achieved in a couple ways. The first involves diagnosing various areas in the code where memory leaks may be occurring. It seems like there are a number of places where references are not properly being released, thus causing garbage collection to not run correctly. If we can identify these places, and find proper ways to run garbage collection, we should be able to improve overall performance a lot, especially when dealing with larger models.

The second performance task will be exploring ways to improve the ability to batch processes. I have already had code shared with myself that seems to work well at batching just about anything together in a very general way. There is also a separate volunteer working on going through said code and finding a good implementation to add to TransformerLens.

#### Streamlining Adding New Models

Improving the model submission process is another item to be addressed in the near future. It may be a good use of time to have some open discussions on this, but I do think this needs to be improved. In my discussions these last few weeks, I have found two primary problems involving models. The first is a general confusion among a good number of people on how they would go about adding a model to TransformerLens. The second problem is ensuring that the logits calculated within TransformerLens match HuggingFace. The goal is to solve both of these problems by systematizing the submission process (e.g. with good tutorials), and requiring that all new models submitted to the project have calculated logits to show that the models they are submitting match HuggingFace. This requirement at the moment would be quite a bit to ask for contributors. In order to alleviate that problem, we will build a little tool that will automatically calculate these logits, and spit out a table of said logits to be submitted with the PR. This would also give us the ability to snapshot said logits, store them in the repo, and periodically regenerate them to make sure cumulative changes to the code base have not affected these values.

### Long-term - within the next year

#### Model Testing

Finding a way to create more robust tests with models integrated is a pretty big item that has been discussed. This is already implemented for the smaller models in each family, but is hard for model families like LLaMA where even the smallest is 7B. A lot of thoughts have been thrown around on this topic, but our best guess for a reasonable solution is to create an untrained small version of the model on HuggingFace (eg randomly initialized weights) and to verify that we can load that in. The resulting tests would not be accurate in the sense that using the full model would be, but it would allow testing for consistency on a smaller sample size of the larger model, and thus allow for the ability to test code against those more bite sized models. If we can find a successful way to go about handling this, then this could turn into a resource available for a lot of other projects to allow people to write efficient integration tests.

#### Model Integration

Making it easier for TransformerLens to handle a large range of models is another long term project that is at the moment a very hard problem to solve. Doing so, however, will be key to ensuring that the library is more future-proof. There have been many ideas put out of how to solve this, and this seems to be a topic that a lot of people have very strong opinions on. Most likely, there will need to be a handful of roundtable discussions on this to find the best solution. 

One of the ideas is to have a generalized wrapper that can take in a model from HuggingFace. Another idea is to create a way to allow TransformerLens to have plugins, so addition of models can be handled outside of the main project, and people can publish compatibility with new models themselves without having to put them into the main project. Finally, there is an idea to keep the submission within TransformerLens as is, but to overhaul the way they are configured so that code can be shared more across models, and something like configuration composition can then be utilized to allow for common cases to be tested in isolation, and thus more rapidly allowing new settings to be accepted, and for people to attempt to configure models themselves without having to have the configuration itself in the repo. 

It is very likely that none of these current ideas will end up being the solution, but we do need to come up with a solution. In my discussions with the community, one of the most common pain points was not having model compatibility. Up to now, it has been managed relatively well, but we still end up taking some time to accept new models, and there are a lot of models that TransformerLens does not support. This problem is only going to grow exponentially as the amount of available models grows as the whole field explodes.

## Contributors

This next section is only relevant to contributors, so if anyone is reading this who is only using TransformerLens as a tool, then you can skip this section. 

### New Dev Branches

There have been two new branches setup. One with the last release of 1.x, and another that will act as the current active development branch. The vast majority of pull requests should be made to the new dev branch. The reason for doing this is due to a potential mismatch between docs and the last release. Previously, all pull requests were put into the main branch, which caused the docs to be generated This meant that there were quite a few instances where the docs referenced features and functions that had not yet been released to people installing via pip. 

From now on, dev will represent the most up to date version of the code with the vast majority of pull requests going to dev. The old main branch will only be updated when a release is ready to be sent out. Releases will be sent out when there are enough changes in dev to justify a release, and when bugs are found in the current release with PRs fixing those bugs going directly into main with an immediate release following.

### Integration Tests

The existing unit tests have been split out into two main groups. Those groups are now called integration tests, and, once again, unit tests. A lot of the existing unit tests were testing various pieces of the code base, and outside resources working together. Traditionally, unit tests should test everything in absolute isolation. That means that if the code is working with other parts of the code base, or outside resources, those pieces should be mocked out, and spied on to absolutely control all input and output of the functions, including side effects of said functions. The goal of this is to be able to be absolutely certain that the logic in the tested functions is being tested in absolute isolation, so that bugs can be entirely ruled out within that functions logic.

A lot of the tests that would be categorized as integration, are still incredibly useful, but they are useful in a different way. To make both of them more useful in general, it makes sense to separate them. Unit tests are the first place to look when fixing a bug in a code base, but if they are dealing with outside resources, you cannot be absolutely certain that the bug does not originate from the outside resource. If the unit tests all test code in isolation, then if all of your unit tests pass, but your integration tests do not, then you can immediately rule out a whole bunch of places where bugs may be possible, and start looking in different places for bugs. Being able to separate the two test styles is going to make everyones lives a lot easier when it comes to maintaining the project.

### Test Coverage
 
The CI has recently been updated to publish a test coverage report. The overall coverage of the project should be improved. If anyone would like to begin improving the coverage, that could be a great way to start getting involved. There are quite a few parts of the code base that have no coverage. Reviewing that report, and finding a place to write a good, meaningful test is a great way to get started, and it should be easier than ever to do so. I have a handful of unit tests that I would like to have written that will improve coverage substantially, so if anyone would like to volunteer to take on some of those, let me know and I will be happy to point you in the right direction. 

### Components Refactor

This is the biggest change to the actual code base in this shift to 2.0. The whole components.py file has been removed in favor of a full module with individual files for each class that was previously in that file. The file was approaching 3000 lines with 18 distinct classes within it. There was no order to it either, since a lot of components were interdependent on each other, and it thus ended up being ordered from least dependent to most dependent. Now, everything has its own file, and every class is really easy to find in case of needing to reference anything. The refactor has been done in a way where no code needed to be changed anywhere else in the project, or outside the project. If you have been doing something like `import MLP from transformer_lens.components`, that will work, and continue to work exactly the same.

## Conclusion

Thank you very much for taking the time to read through this. I am very excited to be able to take on maintaining this project, and I am hoping to be able to work on this full time for at least the next year. I am not a researcher, and I am approaching all of this from a software engineering standpoint. This does give me a bit of an outsider's perspective on this in comparison to a lot of the people that have put in so much work to make this tool worth using. 

I really think this tool is incredibly important, and it is enabling research that is going to have a huge impact on the world. My hope is that I can bring my expertise in software engineering to this project, so that the researchers using this tool can be more efficient, and so they can get to the more important tasks they need to complete. I will do my best to make that a reality.

## Appendix

### Semantic Versioning

One of the main goals of semantic versioning (semver) is to communicate to people using the software if a new version is going to be completely compatible with an older version, or if they may need to check a change log to see if a feature they are using has changed. In the case of something like TransformerLens, the full API itself is what we base our version changes on. That includes classes, functions, parameters to functions, and data returned from said functions, including all exposed object properties or keys. When anything is added, then the minor version number gets a bump. When the API remains the same, but a bug has been fixed, the patch number gets a bump. Finally, when anything is removed whatsoever, then that requires a major version change. 

With TransformerLens, the reason why it is necessary in this transition to bump to 2.0 is simply due to the fact that things from 1.0 have been removed along the 1.x branch, thus breaking the exposed API. In most cases, the breaking changes were simple things like exposed keys being renamed. I discovered a handful of these cases within the last month while bringing demos up from earlier versions of TransformerLens into more recent versions of 1.x. 

I do not know exactly what the full extent of these changes were, and doing an exploration of this is probably not a great use of time. Regardless, the point stands that if you have a project using TransformerLens 1.0, you cannot reliably upgrade to 1.17 without possibly needing to change your code due to these exposed changes.The easiest thing to do in this situation while adopting semver is to simply bump the major version, and start fresh.

Going forward, code that you write using TransformerLens will work, and will consume the same minimal API for all minor and patch releases in the 2.x branch.



---
File: /docs/source/content/citation.md
---


# Citation

Please cite this library as:

```BibTeX
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}
```



---
File: /docs/source/content/contributing.md
---

# Contributing

## Setup

### DevContainer

For a one-click setup of your development environment, this project includes a
[DevContainer](https://containers.dev/). It can be used locally with [VS
Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) or
with [GitHub Codespaces](https://github.com/features/codespaces).

### Manual Setup

This project uses [Poetry](https://python-poetry.org/docs/#installation) for package management.
Install as follows (this will also setup your virtual environment):

```bash
poetry config virtualenvs.in-project true
poetry install --with dev,docs,jupyter
```

## Testing

If adding a feature, please add unit tests for it. If you need a model, please use one of the ones
that are cached by GitHub Actions (so that it runs quickly on the CD). These are `gpt2`,
`attn-only-1l`, `attn-only-2l`, `attn-only-3l`, `attn-only-4l`, `tiny-stories-1M`. Note `gpt2` is
quite slow (as we only have CPU actions) so the smaller models like `attn-only-1l` and
`tiny-stories-1M` are preferred if possible.

### Running the tests

- Unit tests only via `make unit-test`
- Acceptance tests only via `make acceptance-test`
- Docstring tests only via `make docstring-test`
- Notebook tests only via `make notebook-test`
- Run all test suites mentioned `make test`

## Formatting

This project uses `pycln`, `isort` and `black` for formatting, pull requests are checked in github
actions.

- Format all files via `make format`
- Only check the formatting via `make check-format`

Note that `black` line length is set to 100 in `pyproject.toml` (instead of the default 88).

## Documentation

Please make sure to add thorough documentation for any features you add. You should do this directly
in the docstring, and this will then automatically generate the API docs when merged into `main`.
They will also be automatically checked with [pytest](https://docs.pytest.org/) (via
[doctest](https://docs.python.org/3/library/doctest.html)).

If you want to view your documentation changes, run `poetry run docs-hot-reload`. This will give you
hot-reloading docs (they change in real time as you edit docstrings).

### Docstring Style Guide

We follow the Google Python Docstring Style for writing docstrings, with some added features from
reStructuredText (reST).

#### Sections and Order

You should follow this order:

```python
"""Title In Title Case.

A description of what the function/class does, including as much detail as is necessary to fully understand it.

Warning:

Any warnings to the user (e.g. common pitfalls).

Examples:

Include any examples here. They will be checked with doctest.

  >>> print(1 + 2)
  3

Args:
    param_without_type_signature:
        Each description should be indented once more.
    param_2:
        Another example parameter.

Returns:
    Returns description without type signature.

Raises:
    Information about the error it may raise (if any).
"""
```

#### Supported Sphinx Properties

##### References to Other Functions/Classes

You can reference other parts of the codebase using
[cross-referencing](https://www.sphinx-doc.org/en/master/usage/domains/python.html#cross-referencing-python-objects)
(noting that you can omit the full path if it is in the same file).

```reStructuredText
:mod:transformer_lens # Function or module

:const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES`

:class:`transformer_lens.HookedTransformer`

:meth:`transformer_lens.HookedTransformer.from_pretrained`

:attr:`transformer_lens.HookedTransformer.cfg`
```

##### Maths

You can use LaTeX, but note that as you're placing this in python strings the backwards slash (`\`)
must be repeated (i.e. `\\`). You can write LaTeX inline, or in "display mode".

```reStructuredText
:math:`(a + b)^2 = a^2 + 2ab + b^2`
```

```reStructuredText
.. math::
   :nowrap:

   \\begin{eqnarray}
      y    & = & ax^2 + bx + c \\
      f(x) & = & x^2 + 2xy + y^2
   \\end{eqnarray}
```

#### Markup

- Italics - `*text*`
- Bold - `**text**`
- Code - ` ``code`` `
- List items - `*item`
- Numbered items - `1. Item`
- Quotes - indent one level
- External links = ``` `Link text <https://domain.invalid/>` ```



---
File: /docs/source/content/gallery.md
---

# Gallery

Research done involving TransformerLens:

- [Progress Measures for Grokking via Mechanistic
  Interpretability](https://arxiv.org/abs/2301.05217) (ICLR Spotlight, 2023) by Neel Nanda, Lawrence
  Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt
- [Finding Neurons in a Haystack: Case Studies with Sparse
  Probing](https://arxiv.org/abs/2305.01610) by Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
  Harvey, Dmitrii Troitskii, Dimitris Bertsimas
- [Towards Automated Circuit Discovery for Mechanistic
  Interpretability](https://arxiv.org/abs/2304.14997) by Arthur Conmy, Augustine N. Mavor-Parker,
  Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso
- [Actually, Othello-GPT Has A Linear Emergent World Representation](https://neelnanda.io/othello)
  by Neel Nanda
- [A circuit for Python docstrings in a 4-layer attention-only
  transformer](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)
  by Stefan Heimersheim and Jett Janiak
- [A Toy Model of Universality](https://arxiv.org/abs/2302.03025) (ICML, 2023) by Bilal Chughtai,
  Lawrence Chan, Neel Nanda
- [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language
  Models](https://openreview.net/forum?id=ZB6bK6MTYq) (2023, ICLR Workshop RTML) by Alex Foote, Neel
  Nanda, Esben Kran, Ioannis Konstas, Fazl Barez
- [Eliciting Latent Predictions from Transformers with the Tuned
  Lens](https://arxiv.org/abs/2303.08112) by Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
  Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt

User contributed examples of the library being used in action:

- [Induction Heads Phase Change
  Replication](https://colab.research.google.com/github/ckkissane/induction-heads-transformer-lens/blob/main/Induction_Heads_Phase_Change.ipynb):
  A partial replication of [In-Context Learning and Induction
  Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
  from Connor Kissane
- [Decision Transformer
  Interpretability](https://github.com/jbloomAus/DecisionTransformerInterpretability): A set of
  scripts for training decision transformers which uses transformer lens to view intermediate
  activations, perform attribution and ablations. A write up of the initial work can be found
  [here](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability).


---
File: /docs/source/content/getting_started_mech_interp.md
---

# Getting Started in Mechanistic Interpretability

Mechanistic interpretability is a very young and small field, and there are a _lot_ of open
problems. This means there's both a lot of low-hanging fruit, and that the bar for entry is low - if
you would like to help, please try working on one! The standard answer to "why has no one done this
yet" is just that there aren't enough people! Key resources:

- [A Guide to Getting Started in Mechanistic Interpretability](https://neelnanda.io/getting-started)
- [ARENA Mechanistic Interpretability Tutorials](https://arena-ch1-transformers.streamlit.app/) from
  Callum McDougall. A comprehensive practical introduction to mech interp, written in
  TransformerLens - full of snippets to copy and they come with exercises and solutions! Notable
  tutorials:
  - [Coding GPT-2 from
    scratch](https://arena-ch1-transformers.streamlit.app/[1.1]_Transformer_from_Scratch), with
    accompanying video tutorial from me ([1](https://neelnanda.io/transformer-tutorial)
    [2](https://neelnanda.io/transformer-tutorial-2)) - a good introduction to transformers
  - [Introduction to Mech Interp and
    TransformerLens](https://arena-ch1-transformers.streamlit.app/[1.2]_Intro_to_Mech_Interp): An
    introduction to TransformerLens and mech interp via studying induction heads. Covers the
    foundational concepts of the library
  - [Indirect Object
    Identification](https://arena-ch1-transformers.streamlit.app/[1.3]_Indirect_Object_Identification):
    a replication of interpretability in the wild, that covers standard techniques in mech interp
    such as [direct logit
    attribution](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ),
    [activation patching and path
    patching](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)
- [Mech Interp Paper Reading List](https://neelnanda.io/paper-list)
- [200 Concrete Open Problems in Mechanistic
  Interpretability](https://neelnanda.io/concrete-open-problems)
- [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): To look
  up all the jargon and unfamiliar terms you're going to come across!
- [Neel Nanda's Youtube channel](https://www.youtube.com/channel/UCBMJ0D-omcRay8dh4QT0doQ): A range
  of mech interp video content, including [paper
  walkthroughs](https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&index=1),
  and [walkthroughs of doing
  research](https://www.youtube.com/watch?v=yo4QvDn-vsU&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T)


---
File: /docs/source/content/getting_started.md
---

# Getting Started

**Start with the [main demo](https://neelnanda.io/transformer-lens-demo) to learn how the library works, and the basic features**.

To see what using it for exploratory analysis in practice looks like, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!

Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **Check out my [list of concrete open problems](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit) to figure out where to start.**. It begins with advice on skilling up, and key resources to check out. 

If you're new to transformers, check out my [what is a transformer tutorial](https://neelnanda.io/transformer-tutorial) and [tutorial on coding GPT-2 from scratch](https://neelnanda.io/transformer-tutorial-2) (with [an accompanying template](https://neelnanda.io/transformer-template) to write one yourself!

## Advice for Reading the Code

One significant design decision made was to have a single transformer implementation that could support a range of subtly different GPT-style models. This has the upside of interpretability code just working for arbitrary models when you change the model name in `HookedTransformer.from_pretrained`! But it has the significant downside that the code implementing the model (in `HookedTransformer.py` and `components.py`) can be difficult to read. I recommend starting with my [Clean Transformer Demo](https://neelnanda.io/transformer-solution), which is a clean, minimal implementation of GPT-2 with the same internal architecture and activation names as HookedTransformer, but is significantly clearer and better documented.

## Installation

`pip install git+https://github.com/TransformerLensOrg/TransformerLens`

Import the library with `import transformer_lens`

(Note: This library used to be known as EasyTransformer, and some breaking changes have been made since the rename. If you need to use the old version with some legacy code, run `pip install git+https://github.com/TransformerLensOrg/TransformerLens@v1`.)

## Huggingface Gated Access

Some of the models available in TransformerLens require gated access to be used. Luckily TransformerLens provides a way to access those models via the configuration of an environmental variable. Simply configure your access token found [here](https://huggingface.co/settings/tokens) as `HF_TOKEN` in your environment.

You will need to make sure you accept the agreements for any gated models, but once you do, the models will work with TransformerLens without issue. If you attempt to ues one of these models before you have accepted any related agreements, the console output will be very helpful and point you to the URL where you need to accept an agreement. As of 23/4/24, the current list of gated models supported by TransformerLens is as follows.

* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1
* https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
* https://huggingface.co/mistralai/Mistral-7B-v0.1



---
File: /docs/source/content/special_cases.md
---

# Special Cases

## Mixture of Experts error rates
Due to the Top-K gating performed in the hidden layer of Mixture of Experts models, small errors can be amplified 
greatly in cases where a different expert is selected, which leads to a higher than normal variance in the error rate
of the final logits. In testing done on Mixtral running in half precision, the standard deviation of the absolute error 
rate of the logits compared to those from the default model was found to be around 2e-3.

There are two main ways to mitigate this:
1. Disable preprocessing options by using `HookedTransformer.from_pretrained_no_processing` instead of `HookedTransformer.from_pretrained`
2. Increase the precision of the data type used in the model



---
File: /docs/source/content/tutorials.md
---

# Tutorials

- **Start with the [main demo](https://neelnanda.io/transformer-lens-demo) to learn how the library works, and the basic features**.

## Where To Start

- To see what using it for exploratory analysis in practice looks like, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!

- [What is a Transformer tutorial](https://neelnanda.io/transformer-tutorial)

## Demos

- [**Activation Patching in TransformerLens**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Activation_Patching_in_TL_Demo.ipynb) - Accompanies the [Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory Analysis Demo.ipynb). This demo explains how to use [Activation Patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx) in TransformerLens, a mechanistic interpretability technique that uses causal intervention to identify which activations in a model matter for producing an output.

- [**Attribution Patching**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb) - [Attribution Patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) is an incomplete project that uses gradients to take a linear approximation to activation patching. It's a good approximation when patching in small activations like the outputs of individual attention heads, and bad when patching in large activations like a residual stream.

- [**Exploratory Analysis**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb) - Probably the best place to start, after the Main Demo. Demonstrates how to use TransformerLens to perform exploratory analysis - focuses less on rigor and more on getting a grasp of what's going on quickly. Uses a lot of useful interpretability techniques like logit attribution and activation patching. Steal liberally from this!

- [**Grokking**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb) - "Grokking" is a phenomenon where a model can learn to memorise the training data (minimising training loss) but then, if trained for a lot longer, can learn to generalise, leading to a sharp decrease in test loss as well. This demo shows training a model on the task of modular addition, verifying that it groks, and doing analysis. The demo is light on explanation, so you'll probably want to pair it with [Neel's video series](https://www.youtube.com/watch?v=ob4vuiqG2Go) on the paper it's based on.

- [**Head Detector**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb) - Shows how to use TransformerLens to automatically detect several common types of attention head, as well as create your own custom detection algorithms to find your own!

- [**Interactive Neuroscope**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Interactive_Neuroscope.ipynb) - Very hacky demo, but this is a feature, not a bug. Shows how to quickly create useful web-based visualisations of data, even if you're not a professional front-end developer. This demo creates an interactive Neuroscope - a visualization of a neuron's activations on text that will dynamically update as you edit the text.

- [**LLaMA**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb) - Converts Meta's LLaMA model (7B parameter version for now until multi-GPU support is added) to TransformerLens.

- [**Main Demo**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) - The main demo. This is where to start if you're new to TransformerLens. Shows a lot of great features for getting started, including available models, how to access model activations, and generally useful features you should know about.

- [**No Position Experiment**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb) - The accompanying notebook to Neel's [real-time research video](https://www.youtube.com/watch?v=yo4QvDn-vsU). Trains a model with no positional embeddings to predict the previous token, and makes a start at analysing what's going on there!

- [**Othello-GPT**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Othello_GPT.ipynb) - This is a demo notebook porting the weights of the Othello-GPT Model from the excellent [Emergent World Representations](https://arxiv.org/pdf/2210.13382.pdf) paper to TransformerLens. Neel's [sequence on investigating this](https://www.lesswrong.com/s/nhGNHyJHbrofpPbRG) is also well worth reading if you're interested in this topic!

- [**SVD Interpreter Demo**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/SVD_Interpreter_demo.ipynb) - Based on the [Conjecture post](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight#Directly_editing_SVD_representations) about how the singular value decompositions of transformer matrices are surprisingly interpretable, this demo shows how to use TransformerLens to reproduce this and investigate further.

- [**Tracr to TransformerLens**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Tracr_to_Transformer_Lens_Demo.ipynb) - [Tracr](https://github.com/deepmind/tracr) is a cool new DeepMind tool that compiles a written program in [RASP](https://arxiv.org/abs/2106.06981) to transformer weights.This is a (hacky!) script to convert Tracr weights from the JAX form to a TransformerLens HookedTransformer in PyTorch.



---
File: /further_comments.md
---

# Further Details on Config Options
## Shortformer Attention (`positional_embeddings_type == "shortformer"`)
Shortformer style models are a variant on GPT-2 style positional embeddings, which do not add positional embeddings into the residual stream but instead add it in to the queries and keys immediately before multiplying by W_Q and W_K, and NOT having it around for the values or MLPs. It's otherwise the same - the positional embeddings are absolute, and are learned. The positional embeddings are NOT added to the residual stream in the standard way, and instead the queries and keys are calculated as W_Q(res_stream + pos_embed) and W_K(res_stream + pos_embed). The values and MLPs are calculated as W_V(res_stream) and W_MLP(res_stream) and so don't have access to positional information. This is otherwise the same as GPT-2 style positional embeddings. This is a variant on the Shortformer model from the paper [Shortformer: The Benefits of Shorter Sequences in Language Modeling](https://arxiv.org/abs/2012.15832). It's morally similar to rotary, which also only gives keys & queries access to positional info

The original intention was to use this to do more efficient caching: caching is hard with absolute positional embeddings, since you can't translate the context window without recomputing the entire thing, but easier if the prior values and residual stream terms are the same. I've mostly implemented it because it makes it easier for models to form induction heads. I'm not entirely sure why, though hypothesise that it's because there's two ways for induction heads to form with positional embeddings in the residual stream and only one with shortformer style positional embeddings.
    
# Weight Processing
## What is LayerNorm Folding? (`fold_ln`)
[LayerNorm](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) is a common regularisation technique used in transformers. Annoyingly, unlike eg BatchNorm, it can't be turned off at inference time, it's a meaningful change to the mathematical function implemented by the transformer. From an interpretability perspective, this is a headache! And it's easy to shoot yourself in the foot by naively ignoring it - eg, making the mistake of saying neuron_pre = resid_mid @ W_in, rather than LayerNorm(resid_mid) @ W_in. This mistake is an OK approximation, but by folding in the LayerNorm we can do much better!

TLDR: If we have LayerNorm (weights w_ln and b_ln) followed by a linear layer (W+b), we can reduce the LayerNorm to LayerNormPre (just centering & normalising) and follow it by a linear layer with `W_eff = w[:, None] * W` (element-wise multiplication) and `b_eff = b + b_ln @ W`. This is computationally equivalent, and it never makes sense to think of W and w_ln as separate objects, so HookedTransformer handles it for you when loading pre-trained weights - set fold_ln = False when loading a state dict if you want to turn this off
        
Mathematically, LayerNorm is the following:
```
x1 = x0 - x0.mean()
x2 = x1 / ((x1**2).mean()).sqrt()
x3 = x2 * w
x4 = x3 + b
```
        
Apart from dividing by the norm, these are all pretty straightforwards operations from a linear algebra perspective. And from an interpretability perspective, if anything is linear, it's really easy and you can mostly ignore it (everything breaks up into sums, you can freely change basis, don't need to track interference between terms, etc) - the hard part is engaging with non-linearities!
        
A key thing to bear in mind is that EVERY time we read from the residual stream, we apply a LayerNorm - this gives us a lot of leverage to reason about it!
        
So let's translate this into linear algebra notation.
        `x0` is a vector in `R^n`

```
x1 = x0 - x0.mean()
   = x0 - (x0.mean()) * ones (broadcasting, ones=torch.ones(n))
   = x0 - (x0 @ ones/sqrt(n)) * ones/sqrt(n).
```

ones has norm sqrt(n), so ones/sqrt(n) is the unit vector in the diagonal direction. We're just projecting x0 onto this (fixed) vector and subtracting that value off. Alternately, we're projecting onto the n-1 dimensional subspace orthogonal to ones.
            
Since LayerNorm is applied EVERY time we read from the stream, the model just never uses the ones direction of the residual stream, so it's essentially just decreasing d_model by one. We can simulate this by just centering all matrices writing to the residual stream.

Why is removing this dimension useful? I have no idea! I'm not convinced it is...
        
```
x2 = x1 / ((x1**2).mean()).sqrt() (Ignoring eps)
   = (x1 / x1.norm()) * sqrt(n)
```

This is a projection onto the unit sphere (well, sphere of radius sqrt(n) - the norm of ones). This is fundamentally non-linear, eg doubling the input keeps the output exactly the same.

This is by far the most irritating part of LayerNorm. I THINK it's mostly useful for numerical stability reasons and not used to do useful computation by the model, but I could easily be wrong! And interpreting a circuit containing LayerNorm sounds like a nightmare...

In practice, you can mostly get aware with ignore this and treating the scaling factor as a constant, since it does apply across the entire residual stream for each token - this makes it a "global" property of the model's calculation, so for any specific question it hopefully doesn't matter that much. But when you're considering a sufficiently important circuit that it's a good fraction of the norm of the residual stream, it's probably worth thinking about.

```
x3 = x2 * w
   = x2 @ W_ln
```

(`W_ln` is a diagonal matrix with the weights of the LayerNorm - this is equivalent to element-wise multiplication)
This is really easy to deal with - we're about to be input to a linear layer, and can say `(x2 @ W_ln) @ W = x2 @ (W_ln @ W) = x2 @ W_eff` - we can just fold the LayerNorm weights into the linear layer weights.
        
`x4 = x3 + b` is similarly easy - `x4 @ W + B = x2 @ W_eff + B_eff`, where `W_eff = W_ln @ W` and `B_eff = B + b @ W`
        
This function is calculating `W_eff` and `B_eff` for each layer reading from the residual stream and replacing W and B with those.

A final optimisation we can make is to **center the reading weights**. x2 has mean 0, which means it's orthogonal to the vector of all ones (`x2 @ ones = x2.sum() = len(x2) * x2.mean()`). This means that the component of `W_eff` that's parallel to `ones` is irrelevant, and we can set that to zero. In code, this means `W_eff -= W_eff.mean(dim=0, keepdim=True)`. This doesn't change the computation but makes things a bit simpler.
        
See this for more: https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization

## Centering Writing Weights (`center_writing_weight`)

A related idea to folding layernorm - *every* component reading an input from the residual stream is preceded by a LayerNorm, which means that the mean of a residual stream vector (ie the component in the direction of all ones) never matters. This means we can remove the all ones component of weights and biases whose output *writes* to the residual stream. Mathematically, `W_writing -= W_writing.mean(dim=1, keepdim=True)`

## Centering Unembed (`center_unembed`)

The logits are fed into a softmax. Softmax is translation invariant (eg, adding 1 to every logit doesn't change the output), so we can simplify things by setting the mean of the logits to be zero. This is equivalent to setting the mean of every output vector of `W_U` to zero. In code, `W_U -= W_U.mean(dim=-1, keepdim=True)`

## Fold Value Biases (`fold_value_biases`)

Each attention head has a value bias. Values are averaged to create mixed values (`z`), weighted by the attention pattern, but as the bias is constant, its contribution to `z` is exactly the same. The output of a head is `z @ W_O`, and so the value bias just linearly adds to the output of the head. This means that the value bias of a head has *nothing to do with the head*, and is just a constant added to the attention layer outputs. We can take the sum across these and `b_O` to get an "effective bias" for the layer. In code, we set `b_V=0.` and `b_O = (b_V @ W_O).sum(dim=0) + b_O`

<details><summary>Technical derivation</summary>

`v = residual @ W_V[h] + broadcast_b_V[h]` for each head `h` (where `b_V` is broadcast up from shape `d_head` to shape `[position, d_head]`). And `z = pattern[h] @ v = pattern[h] @ residual @ W_V[h] + pattern[h] @ broadcast_b_V[h]`. Because `pattern[h]` is `[destination_position, source_position]` and `broadcast_b_V` is *constant* along the `(source_)position` dimension, we're basically just multiplying it by the sum of the pattern across the `source_position` dimension, which is just 1. So it remains exactly the same, and so is just brodcast across the destination positions. 
</details>



---
File: /README.md
---

# TransformerLens

<!-- Status Icons -->
[![Pypi](https://img.shields.io/pypi/v/transformer-lens?color=blue)](https://pypi.org/project/transformer-lens/)
![Pypi Total Downloads](https://img.shields.io/pepy/dt/transformer_lens?color=blue) ![PyPI -
License](https://img.shields.io/pypi/l/transformer_lens?color=blue) [![Release
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml)
[![Tests
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml)
[![Docs
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment)

A Library for Mechanistic Interpretability of Generative Language Models. Maintained by [Bryce Meyer](https://github.com/bryce13950) and created by [Neel Nanda](https://neelnanda.io/about)

[![Read the Docs
Here](https://img.shields.io/badge/-Read%20the%20Docs%20Here-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white&link=https://TransformerLensOrg.github.io/TransformerLens/)](https://TransformerLensOrg.github.io/TransformerLens/)

This is a library for doing [mechanistic
interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models. The
goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms
the model learned during training from its weights.

TransformerLens lets you load in 50+ different open source language models, and exposes the internal
activations of the model to you. You can cache any internal activation in the model, and add in
functions to edit, remove or replace these activations as the model runs.

## Quick Start

### Install

```shell
pip install transformer_lens
```

### Use

```python
import transformer_lens

# Load a model (eg GPT-2 Small)
model = transformer_lens.HookedTransformer.from_pretrained("gpt2-small")

# Run the model and get logits and activations
logits, activations = model.run_with_cache("Hello World")
```

## Key Tutorials

* [Introduction to the Library and Mech
  Interp](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)
* [Demo of Main TransformerLens Features](https://neelnanda.io/transformer-lens-demo)

## Gallery

Research done involving TransformerLens:

<!-- If you change this also change docs/source/content/gallery.md -->
* [Progress Measures for Grokking via Mechanistic
  Interpretability](https://arxiv.org/abs/2301.05217) (ICLR Spotlight, 2023) by Neel Nanda, Lawrence
  Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt
* [Finding Neurons in a Haystack: Case Studies with Sparse
  Probing](https://arxiv.org/abs/2305.01610) by Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
  Harvey, Dmitrii Troitskii, Dimitris Bertsimas
* [Towards Automated Circuit Discovery for Mechanistic
  Interpretability](https://arxiv.org/abs/2304.14997) by Arthur Conmy, Augustine N. Mavor-Parker,
  Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso
* [Actually, Othello-GPT Has A Linear Emergent World Representation](https://neelnanda.io/othello)
  by Neel Nanda
* [A circuit for Python docstrings in a 4-layer attention-only
  transformer](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)
  by Stefan Heimersheim and Jett Janiak
* [A Toy Model of Universality](https://arxiv.org/abs/2302.03025) (ICML, 2023) by Bilal Chughtai,
  Lawrence Chan, Neel Nanda
* [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language
  Models](https://openreview.net/forum?id=ZB6bK6MTYq) (2023, ICLR Workshop RTML) by Alex Foote, Neel
  Nanda, Esben Kran, Ioannis Konstas, Fazl Barez
* [Eliciting Latent Predictions from Transformers with the Tuned
  Lens](https://arxiv.org/abs/2303.08112) by Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
  Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt

User contributed examples of the library being used in action:

* [Induction Heads Phase Change
  Replication](https://colab.research.google.com/github/ckkissane/induction-heads-transformer-lens/blob/main/Induction_Heads_Phase_Change.ipynb):
  A partial replication of [In-Context Learning and Induction
  Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
  from Connor Kissane
* [Decision Transformer
  Interpretability](https://github.com/jbloomAus/DecisionTransformerInterpretability): A set of
  scripts for training decision transformers which uses transformer lens to view intermediate
  activations, perform attribution and ablations. A write up of the initial work can be found
  [here](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability).

Check out [our demos folder](https://github.com/TransformerLensOrg/TransformerLens/tree/main/demos) for
more examples of TransformerLens in practice

## Getting Started in Mechanistic Interpretability

Mechanistic interpretability is a very young and small field, and there are a _lot_ of open
problems. This means there's both a lot of low-hanging fruit, and that the bar for entry is low - if
you would like to help, please try working on one! The standard answer to "why has no one done this
yet" is just that there aren't enough people! Key resources:

* [A Guide to Getting Started in Mechanistic Interpretability](https://neelnanda.io/getting-started)
* [ARENA Mechanistic Interpretability Tutorials](https://arena3-chapter1-transformer-interp.streamlit.app/) from
  Callum McDougall. A comprehensive practical introduction to mech interp, written in
  TransformerLens - full of snippets to copy and they come with exercises and solutions! Notable
  tutorials:
  * [Coding GPT-2 from
    scratch](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch), with
    accompanying video tutorial from me ([1](https://neelnanda.io/transformer-tutorial)
    [2](https://neelnanda.io/transformer-tutorial-2)) - a good introduction to transformers
  * [Introduction to Mech Interp and
    TransformerLens](https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp): An
    introduction to TransformerLens and mech interp via studying induction heads. Covers the
    foundational concepts of the library
  * [Indirect Object
    Identification](https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification):
    a replication of interpretability in the wild, that covers standard techniques in mech interp
    such as [direct logit
    attribution](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ),
    [activation patching and path
    patching](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)
* [Mech Interp Paper Reading List](https://neelnanda.io/paper-list)
* [200 Concrete Open Problems in Mechanistic
  Interpretability](https://neelnanda.io/concrete-open-problems)
* [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): To look
  up all the jargon and unfamiliar terms you're going to come across!
* [Neel Nanda's Youtube channel](https://www.youtube.com/channel/UCBMJ0D-omcRay8dh4QT0doQ): A range
  of mech interp video content, including [paper
  walkthroughs](https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&index=1),
  and [walkthroughs of doing
  research](https://www.youtube.com/watch?v=yo4QvDn-vsU&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T)

## Support & Community

[![Contributing
Guide](https://img.shields.io/badge/-Contributing%20Guide-blue?style=for-the-badge&logo=GitHub&logoColor=white)](https://TransformerLensOrg.github.io/TransformerLens/content/contributing.html)

If you have issues, questions, feature requests or bug reports, please search the issues to check if
it's already been answered, and if not please raise an issue!

You're also welcome to join the open source mech interp community on
[Slack](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2n26nfoh1-TzMHrzyW6HiOsmCESxXtyw).
Please use issues for concrete discussions about the package, and Slack for higher bandwidth
discussions about eg supporting important new use cases, or if you want to make substantial
contributions to the library and want a maintainer's opinion. We'd also love for you to come and
share your projects on the Slack!

| :exclamation:  HookedSAETransformer Removed   |
|-----------------------------------------------|

Hooked SAE has been removed from TransformerLens in version 2.0. The functionality is being moved to
[SAELens](http://github.com/jbloomAus/SAELens). For more information on this release, please see the
accompanying
[announcement](https://transformerlensorg.github.io/TransformerLens/content/news/release-2.0.html)
for details on what's new, and the future of TransformerLens.

## Credits

This library was created by **[Neel Nanda](https://neelnanda.io)** and is maintained by **[Bryce Meyer](https://github.com/bryce13950)**.

The core features of TransformerLens were heavily inspired by the interface to [Anthropic's
excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson
Elhage and Chris Olah for building Garcon and showing the value of good infrastructure for enabling
exploratory research!

### Creator's Note (Neel Nanda)

I (Neel Nanda) used to work for the [Anthropic interpretability team](transformer-circuits.pub), and
I wrote this library because after I left and tried doing independent research, I got extremely
frustrated by the state of open source tooling. There's a lot of excellent infrastructure like
HuggingFace and DeepSpeed to _use_ or _train_ models, but very little to dig into their internals
and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get
into the field even if you don't work at an industry org with real infrastructure! One of the great
things about mechanistic interpretability is that you don't need large models or tons of compute.
There are lots of important open problems that can be solved with a small model in a Colab notebook!

### Citation

Please cite this library as:

```BibTeX
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}
```

# ARENA_Content.ipynb

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
IN_GITHUB = True
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False

if not IN_GITHUB and not IN_COLAB:
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_GITHUB or IN_COLAB:
    %pip install torch
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git@dev

from transformer_lens import HookedTransformer, HookedTransformerConfig
import torch as t

device = t.device("cuda" if t.cuda.is_available() else "cpu")
```

```python
# NBVAL_IGNORE_OUTPUT

reference_gpt2 = HookedTransformer.from_pretrained(
    "gpt2-small",
    fold_ln=False,
    center_unembed=False,
    center_writing_weights=False,
    device=device,
)
```

```python

# [1.1] Transformer From Scratch
# 1️⃣ UNDERSTANDING INPUTS & OUTPUTS OF A TRANSFORMER

sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])
first_vocab = sorted_vocab[0]
assert isinstance(first_vocab, tuple)
assert isinstance(first_vocab[0], str)
first_vocab[1]
```

```python
reference_gpt2.to_str_tokens("Ralph")
```

```python
reference_gpt2.to_str_tokens(" Ralph")
```

```python

reference_gpt2.to_str_tokens(" ralph")

```

```python
reference_gpt2.to_str_tokens("ralph")
```

```python

reference_text = "I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!"
tokens = reference_gpt2.to_tokens(reference_text)
tokens.shape

```

```python

logits, cache = reference_gpt2.run_with_cache(tokens, device=device)
logits.shape

```

```python

most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])
most_likely_next_tokens[-1]

```

```python
# 2️⃣ CLEAN TRANSFORMER IMPLEMENTATION

layer_0_hooks = [
    (name, tuple(tensor.shape)) for name, tensor in cache.items() if ".0." in name
]
non_layer_hooks = [
    (name, tuple(tensor.shape)) for name, tensor in cache.items() if "blocks" not in name
]

sorted(non_layer_hooks, key=lambda x: x[0])

```

```python

sorted(layer_0_hooks, key=lambda x: x[0])
```

```python
# NBVAL_IGNORE_OUTPUT
# [1.2] Intro to mech interp
# 2️⃣ FINDING INDUCTION HEADS

cfg = HookedTransformerConfig(
    d_model=768,
    d_head=64,
    n_heads=12,
    n_layers=2,
    n_ctx=2048,
    d_vocab=50278,
    attention_dir="causal",
    attn_only=True, # defaults to False
    tokenizer_name="EleutherAI/gpt-neox-20b",
    seed=398,
    use_attn_result=True,
    normalization_type=None, # defaults to "LN", i.e. layernorm with weights & biases
    positional_embedding_type="shortformer"
)
model = HookedTransformer(cfg)
```

```python

text = "We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this."

logits, cache = model.run_with_cache(text, remove_batch_dim=True)

logits.shape
```

```python
cache["embed"].ndim
```

---

# Activation_Patching_in_TL_Demo.ipynb


---

# Attribution_Patching_Demo.ipynb


---

# BERT.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/BERT.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# BERT in TransformerLens
This demo shows how to use BERT in TransformerLens for the Masked Language Modelling and Next Sentence Prediction task.

# Setup
(No need to read)

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False

if not IN_GITHUB and not IN_COLAB:
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB:
    %pip install transformer_lens
    %pip install circuitsvis
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
import circuitsvis as cv

# Testing that the library works
cv.examples.hello("Neel")
```

```python
# Import stuff
import torch

from transformers import AutoTokenizer

from transformer_lens import HookedEncoder, BertNextSentencePrediction
```

```python
torch.set_grad_enabled(False)
```

# BERT

In this section, we will load a pretrained BERT model and use it for the Masked Language Modelling and Next Sentence Prediction task

```python
# NBVAL_IGNORE_OUTPUT
bert = HookedEncoder.from_pretrained("bert-base-cased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

## Masked Language Modelling
Use the "[MASK]" token to mask any tokens which you would like the model to predict.
When specifying return_type="predictions" the prediction of the model is returned, alternatively (and by default) the function returns logits.
You can also specify None as return type for which nothing is returned

```python
prompt = "The [MASK] is bright today."

prediction = bert(prompt, return_type="predictions")

print(f"Prompt: {prompt}")
print(f'Prediction: "{prediction}"')
```

You can also input a list of prompts:

```python
prompts = ["The [MASK] is bright today.", "She [MASK] to the store.", "The dog [MASK] the ball."]

predictions = bert(prompts, return_type="predictions")

print(f"Prompt: {prompts}")
print(f'Prediction: "{predictions}"')
```

## Next Sentence Prediction
To carry out Next Sentence Prediction, you have to use the class BertNextSentencePrediction, and pass a HookedEncoder in its constructor.
Then, create a list with the two sentences you want to perform NSP on as elements and use that as input to the forward function.
The model will then predict the probability of the sentence at position 1 following (i.e. being the next sentence) to the sentence at position 0.

```python
nsp = BertNextSentencePrediction(bert)
sentence_a = "A man walked into a grocery store."
sentence_b = "He bought an apple."

input = [sentence_a, sentence_b]

predictions = nsp(input, return_type="predictions")

print(f"Sentence A: {sentence_a}")
print(f"Sentence B: {sentence_b}")
print(f'Prediction: "{predictions}"')
```

# Inputting tokens directly
You can also input tokens instead of a string or a list of strings into the model, which could look something like this

```python
prompt = "The [MASK] is bright today."

tokens = tokenizer(prompt, return_tensors="pt")["input_ids"]
logits = bert(tokens) # Since we are not specifying return_type, we get the logits
logprobs = logits[tokens == tokenizer.mask_token_id].log_softmax(dim=-1)
prediction = tokenizer.decode(logprobs.argmax(dim=-1).item())

print(f"Prompt: {prompt}")
print(f'Prediction: "{prediction}"')
```

Well done, BERT!

---

# Colab_Compatibility.ipynb

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"

try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    # %pip install sentencepiece # Llama tokenizer requires sentencepiece
    %pip install transformers>=4.31.0 # Llama requires transformers>=4.31.0 and transformers in turn requires Python 3.8
    %pip install torch
    %pip install tiktoken
    # %pip install transformer_lens
    %pip install transformers_stream_generator
    # !huggingface-cli login --token NEEL'S TOKEN
```

```python
import torch
from transformer_lens import HookedTransformer, HookedEncoderDecoder, HookedEncoder, BertNextSentencePrediction, loading
from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer
from typing import List
import gc

untested_models = []
untested_models.extend(loading.OFFICIAL_MODEL_NAMES)

print("TransformerLens currently supports " + str(len(untested_models)) + " models out of the box.")

GENERATE = True
# Fill this in if you have llama weights uploaded, and you with to test those models
LLAMA_MODEL_PATH = ""
```

```python
def mark_models_as_tested(model_set: List[str]) -> None:
    for model in model_set:
        untested_models.remove(model)

def run_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tl_model = HookedTransformer.from_pretrained_no_processing(model, device=device)
        if GENERATE:
            print(tl_model.generate("Hello my name is"))
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_llama_set(model_set: List[str], weight_root: str, device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        # to run this, make sure weight root is the root that contains all models with the
        # sub directories sharing the same name as the model in the list of models
        tokenizer = LlamaTokenizer.from_pretrained(weight_root + model)
        hf_model = LlamaForCausalLM.from_pretrained(weight_root + model, low_cpu_mem_usage=True)
        tl_model = HookedTransformer.from_pretrained_no_processing(
            model,
            hf_model=hf_model,
            device=device,
            fold_ln=False,
            center_writing_weights=False,
            center_unembed=False,
            tokenizer=tokenizer,
        )
        if GENERATE:
            print(tl_model.generate("Hello my name is"))
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_encoder_decoder_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tokenizer = AutoTokenizer.from_pretrained(model)
        tl_model = HookedEncoderDecoder.from_pretrained(model, device=device)
        if GENERATE:
            # Originally from the t5 demo
            prompt = "Hello, how are you? "
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"]
            attention_mask = inputs["attention_mask"]
            decoder_input_ids = torch.tensor([[tl_model.cfg.decoder_start_token_id]]).to(input_ids.device)

            while True:
                logits = tl_model.forward(input=input_ids, one_zero_attention_mask=attention_mask, decoder_input=decoder_input_ids)
                # logits.shape == (batch_size (1), predicted_pos, vocab_size)

                token_idx = torch.argmax(logits[0, -1, :]).item()
                print("generated token: \"", tokenizer.decode(token_idx), "\", token id: ", token_idx, sep="")

                # append token to decoder_input_ids
                decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[token_idx]]).to(input_ids.device)], dim=-1)

                # break if End-Of-Sequence token generated
                if token_idx == tokenizer.eos_token_id:
                    break
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_encoder_only_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tl_model = HookedEncoder.from_pretrained(model, device=device)
        tl_model_nsp = NextSentencePrediction.from_pretrained(model, device=device)

        if GENERATE:
            print("Testing Masked Language Modelling:")
            # Slightly adapted version of the BERT demo
            prompt = "The capital of France is [MASK]."

            prediction = tl_model(prompt, return_type="predictions")

            print(f"Prompt: {prompt}")
            print(f'Prediction: "{prediction}"')

            print("Testing Next Sentence Prediction:")
            sentence_a = "She went to the grocery store."
            sentence_b = "She bought some milk."

            prediction = tl_model_nsp([sentence_a, sentence_b], return_type="predictions")

            print(f"Sentence A: {sentence_a}")
            print(f"Sentence B: {sentence_b}")
            print(f"Prediction: {prediction}")

        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*
```

```python
# The following models can run in the T4 free environment
free_compatible = [
    "ai-forever/mGPT",
    "ArthurConmy/redwood_attn_2l",
    "bigcode/santacoder",
    "bigscience/bloom-1b1",
    "bigscience/bloom-560m",
    "distilgpt2",
    "EleutherAI/gpt-neo-1.3B",
    "EleutherAI/gpt-neo-125M",
    "EleutherAI/gpt-neo-2.7B",
    "EleutherAI/pythia-1.4b",
    "EleutherAI/pythia-1.4b-deduped",
    "EleutherAI/pythia-1.4b-deduped-v0",
    "EleutherAI/pythia-1.4b-v0",
    "EleutherAI/pythia-14m",
    "EleutherAI/pythia-160m",
    "EleutherAI/pythia-160m-deduped",
    "EleutherAI/pythia-160m-deduped-v0",
    "EleutherAI/pythia-160m-seed1",
    "EleutherAI/pythia-160m-seed2",
    "EleutherAI/pythia-160m-seed3",
    "EleutherAI/pythia-160m-v0",
    "EleutherAI/pythia-1b",
    "EleutherAI/pythia-1b-deduped",
    "EleutherAI/pythia-1b-deduped-v0",
    "EleutherAI/pythia-1b-v0",
    "EleutherAI/pythia-31m",
    "EleutherAI/pythia-410m",
    "EleutherAI/pythia-410m-deduped",
    "EleutherAI/pythia-410m-deduped-v0",
    "EleutherAI/pythia-410m-v0",
    "EleutherAI/pythia-70m",
    "EleutherAI/pythia-70m-deduped",
    "EleutherAI/pythia-70m-deduped-v0",
    "EleutherAI/pythia-70m-v0",
    "facebook/opt-1.3b",
    "facebook/opt-125m",
    "gpt2",
    "gpt2-large",
    "gpt2-medium",
    "gpt2-xl",
    "meta-llama/Llama-3.2-1B",
    "meta-llama/Llama-3.2-1B-Instruct",
    "microsoft/phi-1",
    "microsoft/phi-1_5",
    "NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr",
    "NeelNanda/Attn_Only_1L512W_C4_Code",
    "NeelNanda/Attn_Only_2L512W_C4_Code",
    "NeelNanda/Attn_Only_3L512W_C4_Code",
    "NeelNanda/Attn_Only_4L512W_C4_Code",
    "NeelNanda/GELU_1L512W_C4_Code",
    "NeelNanda/GELU_2L512W_C4_Code",
    "NeelNanda/GELU_3L512W_C4_Code",
    "NeelNanda/GELU_4L512W_C4_Code",
    "NeelNanda/SoLU_10L1280W_C4_Code",
    "NeelNanda/SoLU_10L_v22_old",
    "NeelNanda/SoLU_12L1536W_C4_Code",
    "NeelNanda/SoLU_12L_v23_old",
    "NeelNanda/SoLU_1L512W_C4_Code",
    "NeelNanda/SoLU_1L512W_Wiki_Finetune",
    "NeelNanda/SoLU_1L_v9_old",
    "NeelNanda/SoLU_2L512W_C4_Code",
    "NeelNanda/SoLU_2L_v10_old",
    "NeelNanda/SoLU_3L512W_C4_Code",
    "NeelNanda/SoLU_4L512W_C4_Code",
    "NeelNanda/SoLU_4L512W_Wiki_Finetune",
    "NeelNanda/SoLU_4L_v11_old",
    "NeelNanda/SoLU_6L768W_C4_Code",
    "NeelNanda/SoLU_6L_v13_old",
    "NeelNanda/SoLU_8L1024W_C4_Code",
    "NeelNanda/SoLU_8L_v21_old",
    "Qwen/Qwen-1_8B",
    "Qwen/Qwen-1_8B-Chat",
    "Qwen/Qwen1.5-0.5B",
    "Qwen/Qwen1.5-0.5B-Chat",
    "Qwen/Qwen1.5-1.8B",
    "Qwen/Qwen1.5-1.8B-Chat",
    "Qwen/Qwen2-0.5B",
    "Qwen/Qwen2-0.5B-Instruct",
    "Qwen/Qwen2-1.5B",
    "Qwen/Qwen2-1.5B-Instruct",
    "Qwen/Qwen2.5-0.5B",
    "Qwen/Qwen2.5-0.5B-Instruct",
    "Qwen/Qwen2.5-1.5B",
    "Qwen/Qwen2.5-1.5B-Instruct",
    "Qwen/Qwen3-0.6B",
    "Qwen/Qwen3-1.7B",
    "roneneldan/TinyStories-1Layer-21M",
    "roneneldan/TinyStories-1M",
    "roneneldan/TinyStories-28M",
    "roneneldan/TinyStories-2Layers-33M",
    "roneneldan/TinyStories-33M",
    "roneneldan/TinyStories-3M",
    "roneneldan/TinyStories-8M",
    "roneneldan/TinyStories-Instruct-1M",
    "roneneldan/TinyStories-Instruct-28M",
    "roneneldan/TinyStories-Instruct-2Layers-33M",
    "roneneldan/TinyStories-Instruct-33M",
    "roneneldan/TinyStories-Instruct-3M",
    "roneneldan/TinyStories-Instruct-8M",
    "roneneldan/TinyStories-Instuct-1Layer-21M",
    "stanford-crfm/alias-gpt2-small-x21",
    "stanford-crfm/arwen-gpt2-medium-x21",
    "stanford-crfm/battlestar-gpt2-small-x49",
    "stanford-crfm/beren-gpt2-medium-x49",
    "stanford-crfm/caprica-gpt2-small-x81",
    "stanford-crfm/celebrimbor-gpt2-medium-x81",
    "stanford-crfm/darkmatter-gpt2-small-x343",
    "stanford-crfm/durin-gpt2-medium-x343",
    "stanford-crfm/eowyn-gpt2-medium-x777",
    "stanford-crfm/expanse-gpt2-small-x777",
]

if IN_COLAB:
    run_set(free_compatible)

mark_models_as_tested(free_compatible)
```

```python
paid_gpu_models = [
    "01-ai/Yi-6B",
    "01-ai/Yi-6B-Chat",
    "bigscience/bloom-1b7",
    "bigscience/bloom-3b",
    "bigscience/bloom-7b1",
    "codellama/CodeLlama-7b-hf",
    "codellama/CodeLlama-7b-Instruct-hf",
    "codellama/CodeLlama-7b-Python-hf",
    "EleutherAI/pythia-2.8b",
    "EleutherAI/pythia-2.8b-deduped",
    "EleutherAI/pythia-2.8b-deduped-v0",
    "EleutherAI/pythia-2.8b-v0",
    "EleutherAI/pythia-6.9b",
    "EleutherAI/pythia-6.9b-deduped",
    "EleutherAI/pythia-6.9b-deduped-v0",
    "EleutherAI/pythia-6.9b-v0",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "google/gemma-2-2b",
    "google/gemma-2-2b-it",
    "google/gemma-2b",
    "google/gemma-2b-it",
    "google/gemma-7b",
    "google/gemma-7b-it",
    "meta-llama/Llama-2-7b-chat-hf",
    "meta-llama/Llama-2-7b-hf",
    "meta-llama/Llama-3.1-8B",
    "meta-llama/Llama-3.1-8B-Instruct",
    "meta-llama/Llama-3.2-3B",
    "meta-llama/Llama-3.2-3B-Instruct",
    "meta-llama/Meta-Llama-3-8B",
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "microsoft/phi-2",
    "microsoft/Phi-3-mini-4k-instruct",
    "mistralai/Mistral-7B-Instruct-v0.1",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mistral-Nemo-Base-2407",
    "mistralai/Mistral-Small-24B-Base-2501",
    "Qwen/Qwen-7B",
    "Qwen/Qwen-7B-Chat",
    "Qwen/Qwen1.5-4B",
    "Qwen/Qwen1.5-4B-Chat",
    "Qwen/Qwen1.5-7B",
    "Qwen/Qwen1.5-7B-Chat",
    "Qwen/Qwen2-7B",
    "Qwen/Qwen2-7B-Instruct",
    "Qwen/Qwen2.5-3B",
    "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-7B",
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-8B",
    "stabilityai/stablelm-base-alpha-3b",
    "stabilityai/stablelm-base-alpha-7b",
    "stabilityai/stablelm-tuned-alpha-3b",
    "stabilityai/stablelm-tuned-alpha-7b",
]

if IN_COLAB:
    run_set(paid_gpu_models)

mark_models_as_tested(paid_gpu_models)
```

```python
paid_cpu_models = [
    "EleutherAI/gpt-j-6B",
    "EleutherAI/gpt-neox-20b",
    "EleutherAI/pythia-12b",
    "EleutherAI/pythia-12b-deduped",
    "EleutherAI/pythia-12b-deduped-v0",
    "EleutherAI/pythia-12b-v0",
    "facebook/opt-13b",
    "google/gemma-2-9b",
    "google/gemma-2-9b-it",
    "meta-llama/Llama-2-13b-chat-hf",
    "meta-llama/Llama-2-13b-hf",
    "microsoft/phi-4",
    "Qwen/Qwen-14B",
    "Qwen/Qwen-14B-Chat",
    "Qwen/Qwen1.5-14B",
    "Qwen/Qwen1.5-14B-Chat",
    "Qwen/Qwen2.5-14B",
    "Qwen/Qwen2.5-14B-Instruct",
]

if IN_COLAB:
    run_set(paid_cpu_models, "cpu")

mark_models_as_tested(paid_cpu_models)
```

```python
incompatible_models = [
    "01-ai/Yi-34B",
    "01-ai/Yi-34B-Chat",
    "facebook/opt-30b",
    "facebook/opt-66b",
    "google/gemma-2-27b",
    "google/gemma-2-27b-it",
    "meta-llama/Llama-2-70b-chat-hf",
    "meta-llama/Llama-3.1-70B",
    "meta-llama/Llama-3.1-70B-Instruct",
    "meta-llama/Llama-3.3-70B-Instruct",
    "meta-llama/Meta-Llama-3-70B",
    "meta-llama/Meta-Llama-3-70B-Instruct",
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "mistralai/Mixtral-8x7B-v0.1",
    "Qwen/Qwen2.5-32B",
    "Qwen/Qwen2.5-32B-Instruct",
    "Qwen/Qwen2.5-72B",
    "Qwen/Qwen2.5-72B-Instruct",
    "Qwen/Qwen3-14B",
    "Qwen/QwQ-32B-Preview",
]

mark_models_as_tested(incompatible_models)
```

```python
# The following models take a few extra steps to function. Check the official demo for more
# information on how to use. 7b and 13b will work in the paid environment. 30b and 65b will not work
# in Colab
not_hosted_models = [
    "llama-7b-hf",
    "llama-13b-hf",
    "llama-30b-hf",
    "llama-65b-hf",
]

if LLAMA_MODEL_PATH:
    run_llama_set(not_hosted_models, LLAMA_MODEL_PATH)

mark_models_as_tested(not_hosted_models)
```

```python
# These all work on the free version of Colab
encoder_decoders = [
    "google-t5/t5-base",
    "google-t5/t5-large",
    "google-t5/t5-small",
]
if IN_COLAB:
    run_encoder_decoder_set(encoder_decoders)

mark_models_as_tested(encoder_decoders)
```

```python
# This model works on the free version of Colab
encoder_only_models = [
    "google-bert/bert-base-cased",
    "google-bert/bert-base-uncased",
    "google-bert/bert-large-cased",
    "google-bert/bert-large-uncased",
]

if IN_COLAB:
    run_encoder_only_set(encoder_only_models)

mark_models_as_tested(encoder_only_models)
```

```python
broken_models = [
    "Baidicoot/Othello-GPT-Transformer-Lens",
]
```

```python
# Any models listed in the cell below have not been tested. This should always remain blank. If your
# PR fails due to this notebook, most likely you need to check any new model changes to ensure that
# this notebook is up to date.
print(*untested_models, sep="\n")
```

---

# Config_Overhaul.ipynb

# Overview

The current way configuration is designed in TransformerLens has a lot of limitations. It does not
allow for outside people to pass through configurations that are not officially supported, and it
is very bug prone with something as simple as typo potentially giving you a massive headache. There
are also a number of hidden rules that are not clearly documented, which can go hidden until
different pieces of TransformerLens are activated. Allowing to pass in an optional object of configuration
with no further changes does solve a couple of these problems, but it does not solve the bigger
issues. It also introduces new problems with users potentially passing in architectures that are not
supported without having a clear way to inform the user what isn't supported.

My proposal for how all of these problems can be resolved is to fundamentally revamp the
configuration to allow for something that I like to call configuration composition. From a technical
perspective, this involves creating a centralized class that describes all supported configurations
by TransformerLens. This class would then be used to construct specific configurations for all models
that are currently supported, and it would then allow anyone to easily see in a single place all
configuration features supported by TransformerLens while also being able to read the code to
understand how they can create their own configurations for the purpose of either submitting new
models into TransformerLens, or configuring an unofficially supported model by TransformerLens,
when TransformerLens already happens to support all of the architectural pieces separately.

This could simple be an overhaul of the existing HookedTransformerConfig. Everything I am
describing here could be made compatible with that class to give it a more usable interface that is
then directly interacted with by the end user. At the moment, that class is not really built to be
interacted with, and is instead used as a wrapper around spreading configured anonymous objects.
Overhauling this class to do what I am about to describe is a viable path, but keeping it as it is,
and making a new class as something meant to be used by the end user would be a way to maintain
compatibility, avoid refactors, and keep model configuration only focused on putting together
configuration for models, as opposed to configuring full settings needed by HookedTransformer, which
includes checking the available environment.

A very unscientific basic example of how this would look in code by the end user can be seen
immediately below. I will delve into details of each piece in this document.

```python
config = ModelConfig(
    d_model=4096,
    d_head=8192 // 64,
    n_heads=64,
    act_fn="silu"
    # Other universally required properties across all models go here in the constructor
)
# Enabling specific features not universal among all models
config.enabled_gated_mlp()
# Customizing optional attributes
config.set_positional_embedding_type("alibi")

# and so on, until the full configuration is set

```

## The constructor

The first piece of this I want to talk about is what will be injected into the constructor. It
should basically take everything absolutely required by all models. This keeps the code easy for
someone to understand, without adding too much clutter. All fields should be required, and if there
is ever an idea that a field should be in the constructor as an option, then that is probably an
indication that there is a good case to add a function to configure that variable in a different
point in the class. An example of what this would look like can be seen below...

```python
# make it easy for someone to see what activation functions are supported, this would be moved from
# HookedTransformerConfig
ActivationFunction = "silu" | "gelu"

class ModelConfig:
    def __init__(
        self,
        d_model: int,
        eps: int,
        act_fn: ActivationFunction,
        remaining_required_attributes,
    ):
        self.d_model = d_model
        self.eps = eps
        self.act_fn = act_fn
        # Set defaults for any remaining supported attributes that are not required here
        self.gated_mlp = False

```

## Boolean Variables

Within TransformerLens config, anything that is a boolean variable is essentially a feature flag.
This means that all features at the time of construction would have default values, most likely set
to false. They then get toggled on with an `enable_feature` function call on the config object.
Having these functions will make very clear for someone less familiar with TransformerLens what
features are available. It also allows us to decorate these calls, which is very important. There
are some instances where if a boolean is true, a different one cannot be true, but this requirement
is not clear anywhere without analyzing code. Decorating these functions allows us to make sure
these sort of bugs are not possible. I will use `gated_mlp` as an example here, but it is not
meant to be a real implementation.

```python
def enabled_gated_mlp(self: ModelConfig) -> ModelConfig:
    self.gated_mlp = True
    # Configure any side effects caused by enabling of a feature
    self.another_feature = False
    # Returning self allows someone to chain together config calls
    return self

ModelConfig.enabled_gated_mlp = enabled_gated_mlp
```

## Additional Options

Any other options would similarly have their own functions to configure. This allows for similar
decoration as with feature flags, and it also in a way documents the architectural capabilities of
TransformerLens in a single place. If there are groups of options that are also always required
together, this then gives us a way to require all of those options as opposed to having them all be
configured at the root level. This also allows us to make changes to other attributes that may be
affected as a side affect of having some values set, which again makes it both harder for people to
introduce bugs, and also creates code that documents itself. Another off the cuff example of
something like this can be seen below.

```python
def set_rotary_dim(self: ModelConfig, rotary_dim: int) -> ModelConfig:
    self.rotary_dim = rotary_dim
    # Additional settings that seem to be present whenever rotary_dim is set
    self.positional_embedding_type = "rotary"
    self.rotary_adjacent_pairs = False
    return self

ModelConfig.set_rotary_dim = set_rotary_dim
```

## Config Final Thoughts

The best way to describe this idea is configuration composition. The reason being is that the user is
essentially composing a model configuration by setting the base, and then combining various options
from predefined functions. Doing it like this has a lot of advantages. One of those advantages being
that there would need to be a lot less memorization on how architectures should be combined. e.g.
maybe it's not that hard to remember that `rotary_adjacent_pairs` should be False when `rotary_dim`
is set, but these sorts of combinations accumulate. Having it interfaced out gives everyone a
place to look to see how parts of configuration work in isolation without the need to memorize a
large amount of rules.

This would also allow us to more easily mock out fake configurations and enable specific features in
order to test that functionality in isolation. This also should make it easier for someone to at a
glance understand all model compatibilities with TransformerLens, since there would be a single file
where they would all be listed out and documented. It will also allow for people to see
compatibility limitations at a glance.

As for compatibility, this change would be 100% compatible with the existing structure. The objects
I am suggesting are abstractions of the existing configuration dictionaries for the purpose of
communication and ease of use. This means that they can be passed around just like the current
anonymous dictionaries.

## Further Changes

With this, there are a number of changes that I would like to make to the actual
`loading_from_pretrained` file in order to revise it to be ready for the possibility of rapidly
supporting new models. The biggest change in this respect would be to break out what is now a
configuration dictionary for every model into having its own module where one of these configuration
objects would be constructed. That object would then be exposed, so that it can be imported into
`loading_from_pretrained`. We would then create a dictionary where the official name of the
model would have the configuration object as its value, thus completely eliminating that big giant
if else statement, and replacing it with a simple return from the dictionary. The configurations
themselves would then live in a directory structure like so...

config/ <- where the ModelConfig file lives
config/meta-llama/ <- directory for all models from the group
config/meta-llama/Llama-2-13b.py <- name matching hugging face to make it really easy to find the
                                    configuration

## Impact on Testing

This change, would allow us to directly interact with these configuration objects to allow us to
more easily assert that configurations are set properly, and to also allow us to more easily access
these configurations in tests for the purposes of writing better unit tests.

## Summary

This change should solve a lot of problems. It may be a big change at first from what currently
exists, but in time I think most people will find it more elegant, and easier to understand.

```python

```

---

# Exploratory_Analysis_Demo.ipynb

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb)

# Exploratory Analysis Demo

This notebook demonstrates how to use the
[TransformerLens](https://github.com/TransformerLensOrg/TransformerLens/) library to perform exploratory
analysis. The notebook tries to replicate the analysis of the Indirect Object Identification circuit
in the [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper.

## Tips for Reading This

* If running in Google Colab, go to Runtime > Change Runtime Type and select GPU as the hardware
accelerator.
* Look up unfamiliar terms in [the mech interp explainer](https://neelnanda.io/glossary)
* You can run all this code for yourself
* The graphs are interactive
* Use the table of contents pane in the sidebar to navigate (in Colab) or VSCode's "Outline" in the
  explorer tab.
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar (with Colab) not CTRL+F

## Setup

### Environment Setup (ignore)

**You can ignore this part:** It's just for use internally to setup the tutorial in different
environments. You can delete this section if using in your own repo.

```python

# Detect if we're running in Google Colab
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False

# Install if in Colab
if IN_COLAB:
    %pip install transformer_lens
    %pip install circuitsvis
    # Install a faster Node version
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa

# Hot reload in development mode & not running on the CD
if not IN_COLAB:
    from IPython import get_ipython
    ip = get_ipython()
    if not ip.extension_manager.loaded:
        ip.extension_manager.load('autoreload')
        %autoreload 2

```

### Imports

```python
from functools import partial
from typing import List, Optional, Union

import einops
import numpy as np
import plotly.express as px
import plotly.io as pio
import torch
from circuitsvis.attention import attention_heads
from fancy_einsum import einsum
from IPython.display import HTML, IFrame
from jaxtyping import Float

import transformer_lens.utils as utils
from transformer_lens import ActivationCache, HookedTransformer
```

### PyTorch Setup

We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training.

```python
torch.set_grad_enabled(False)
print("Disabled automatic differentiation")
```

### Plotting Helper Functions (ignore)

Some plotting helper functions are included here (for simplicity).

```python
def imshow(tensor, **kwargs):
    px.imshow(
        utils.to_numpy(tensor),
        color_continuous_midpoint=0.0,
        color_continuous_scale="RdBu",
        **kwargs,
    ).show()

def line(tensor, **kwargs):
    px.line(
        y=utils.to_numpy(tensor),
        **kwargs,
    ).show()

def scatter(x, y, xaxis="", yaxis="", caxis="", **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(
        y=y,
        x=x,
        labels={"x": xaxis, "y": yaxis, "color": caxis},
        **kwargs,
    ).show()
```

## Introduction

This is a demo notebook for [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens), a library for mechanistic interpretability of GPT-2 style transformer language models. A core design principle of the library is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state.

The goal of this notebook is to demonstrate what exploratory analysis looks like in practice with the library. I use my standard toolkit of basic mechanistic interpretability techniques to try interpreting a real circuit in GPT-2 small. Check out [the main demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) for an introduction to the library and how to use it.

Stylistically, I will go fairly slowly and explain in detail what I'm doing and why, aiming to help convey how to do this kind of research yourself! But the code itself is written to be simple and generic, and easy to copy and paste into your own projects for different tasks and models.

Details tags contain asides, flavour + interpretability intuitions. These are more in the weeds and you don't need to read them or understand them, but they're helpful if you want to learn how to do mechanistic interpretability yourself! I star the ones I think are most important.
<details><summary>(*) Example details tag</summary>Example aside!</details>

### Indirect Object Identification

The first step when trying to reverse engineer a circuit in a model is to identify *what* capability
I want to reverse engineer. Indirect Object Identification is a task studied in Redwood Research's
excellent [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper (see [my interview
with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo) or [Kevin Wang's Twitter
thread](https://threadreaderapp.com/thread/1587601532639494146.html) for an overview). The task is
to complete sentences like "After John and Mary went to the shops, John gave a bottle of milk to"
with " Mary" rather than " John".

In the paper they rigorously reverse engineer a 26 head circuit, with 7 separate categories of heads
used to perform this capability. Their rigorous methods are fairly involved, so in this notebook,
I'm going to skimp on rigour and instead try to speed run the process of finding suggestive evidence
for this circuit!

The circuit they found roughly breaks down into three parts:
1. Identify what names are in the sentence
2. Identify which names are duplicated
3. Predict the name that is *not* duplicated

The first step is to load in our model, GPT-2 Small, a 12 layer and 80M parameter transformer with `HookedTransformer.from_pretrained`. The various flags are simplifications that preserve the model's output but simplify its internals.

```python
# NBVAL_IGNORE_OUTPUT
model = HookedTransformer.from_pretrained(
    "gpt2-small",
    center_unembed=True,
    center_writing_weights=True,
    fold_ln=True,
    refactor_factored_attn_matrices=True,
)

# Get the default device used
device: torch.device = utils.get_device()
```

The next step is to verify that the model can *actually* do the task! Here we use `utils.test_prompt`, and see that the model is significantly better at predicting Mary than John!

<details><summary>Asides:</summary>

Note: If we were being careful, we'd want to run the model on a range of prompts and find the average performance

`prepend_bos` is a flag to add a BOS (beginning of sequence) to the start of the prompt. GPT-2 was not trained with this, but I find that it often makes model behaviour more stable, as the first token is treated weirdly.
</details>

```python
example_prompt = "After John and Mary went to the store, John gave a bottle of milk to"
example_answer = " Mary"
utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)
```

We now want to find a reference prompt to run the model on. Even though our ultimate goal is to reverse engineer how this behaviour is done in general, often the best way to start out in mechanistic interpretability is by zooming in on a concrete example and understanding it in detail, and only *then* zooming out and verifying that our analysis generalises.

We'll run the model on 4 instances of this task, each prompt given twice - one with the first name as the indirect object, one with the second name. To make our lives easier, we'll carefully choose prompts with single token names and the corresponding names in the same token positions.

<details> <summary>(*) <b>Aside on tokenization</b></summary>

We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency!

Tokens are a *massive* headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`

**Exercise:** I recommend using `model.to_str_tokens` to explore how the model tokenizes different strings. In particular, try adding or removing spaces at the start, or changing capitalization - these change tokenization!</details>

```python
prompt_format = [
    "When John and Mary went to the shops,{} gave the bag to",
    "When Tom and James went to the park,{} gave the ball to",
    "When Dan and Sid went to the shops,{} gave an apple to",
    "After Martin and Amy went to the park,{} gave a drink to",
]
names = [
    (" Mary", " John"),
    (" Tom", " James"),
    (" Dan", " Sid"),
    (" Martin", " Amy"),
]
# List of prompts
prompts = []
# List of answers, in the format (correct, incorrect)
answers = []
# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)
answer_tokens = []
for i in range(len(prompt_format)):
    for j in range(2):
        answers.append((names[i][j], names[i][1 - j]))
        answer_tokens.append(
            (
                model.to_single_token(answers[-1][0]),
                model.to_single_token(answers[-1][1]),
            )
        )
        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.
        prompts.append(prompt_format[i].format(answers[-1][1]))
answer_tokens = torch.tensor(answer_tokens).to(device)
print(prompts)
print(answers)
```

**Gotcha**: It's important that all of your prompts have the same number of tokens. If they're different lengths, then the position of the "final" logit where you can check logit difference will differ between prompts, and this will break the below code. The easiest solution is just to choose your prompts carefully to have the same number of tokens (you can eg add filler words like The, or newlines to start).

There's a range of other ways of solving this, eg you can index more intelligently to get the final logit. A better way is to just use left padding by setting `model.tokenizer.padding_side = 'left'` before tokenizing the inputs and running the model; this way, you can use something like `logits[:, -1, :]` to easily access the final token outputs without complicated indexing. TransformerLens checks the value of `padding_side` of the tokenizer internally, and if the flag is set to be `'left'`, it adjusts the calculation of absolute position embedding and causal masking accordingly.

In this demo, though, we stick to using the prompts of the same number of tokens because we want to show some visualisations aggregated along the batch dimension later in the demo.

```python
for prompt in prompts:
    str_tokens = model.to_str_tokens(prompt)
    print("Prompt length:", len(str_tokens))
    print("Prompt as tokens:", str_tokens)
```

We now run the model on these prompts and use `run_with_cache` to get both the logits and a cache of all internal activations for later analysis

```python
tokens = model.to_tokens(prompts, prepend_bos=True)

# Run the model and cache all activations
original_logits, cache = model.run_with_cache(tokens)
```

We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the indirect object's name and the subject's name (eg, `logit(Mary)-logit(John)`).

```python
def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):
    # Only the final logits are relevant for the answer
    final_logits = logits[:, -1, :]
    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)
    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]
    if per_prompt:
        return answer_logit_diff
    else:
        return answer_logit_diff.mean()

print(
    "Per prompt logit difference:",
    logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)
    .detach()
    .cpu()
    .round(decimals=3),
)
original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)
print(
    "Average logit difference:",
    round(logits_to_ave_logit_diff(original_logits, answer_tokens).item(), 3),
)
```

We see that the average logit difference is 3.5 - for context, this represents putting an $e^{3.5}\approx 33\times$ higher probability on the correct answer.

## Brainstorm What's Actually Going On (Optional)

Before diving into running experiments, it's often useful to spend some time actually reasoning about how the behaviour in question could be implemented in the transformer. **This is optional, and you'll likely get the most out of engaging with this section if you have a decent understanding already of what a transformer is and how it works!**

You don't have to do this and forming hypotheses after exploration is also reasonable, but I think it's often easier to explore and interpret results with some grounding in what you might find. In this particular case, I'm cheating somewhat, since I know the answer, but I'm trying to simulate the process of reasoning about it!

Note that often your hypothesis will be wrong in some ways and often be completely off. We're doing science here, and the goal is to understand how the model *actually* works, and to form true beliefs! There are two separate traps here at two extremes that it's worth tracking:
* Confusion: Having no hypotheses at all, getting a lot of data and not knowing what to do with it, and just floundering around
* Dogmatism: Being overconfident in an incorrect hypothesis and being unwilling to let go of it when reality contradicts you, or flinching away from running the experiments that might disconfirm it.

**Exercise:** Spend some time thinking through how you might imagine this behaviour being implemented in a transformer. Try to think through this for yourself before reading through my thoughts!

<details> <summary>(*) <b>My reasoning</b></summary>

<h3>Brainstorming:</h3>

So, what's hard about the task? Let's focus on the concrete example of the first prompt, "When John and Mary went to the shops, John gave the bag to" -> " Mary".

A good starting point is thinking though whether a tiny model could do this, eg a <a href="https://transformer-circuits.pub/2021/framework/index.html">1L Attn-Only model</a>. I'm pretty sure the answer is no! Attention is really good at the primitive operations of looking nearby, or copying information. I can believe a tiny model could figure out that at `to` it should look for names and predict that those names came next (eg the skip trigram " John...to -> John"). But it's much harder to tell how <i>many</i> of each previous name there are - attending 0.3 to each copy of John will look exactly the same as attending 0.6 to a single John token. So this will be pretty hard to figure out on the " to" token!

The natural place to break this symmetry is on the second " John" token - telling whether there is an earlier copy of the <i>current</i> token should be a much easier task. So I might expect there to be a head which detects duplicate tokens on the second " John" token, and then another head which moves that information from the second " John" token to the " to" token.

The model then needs to learn to predict " Mary" and <i>not</i> " John". I can see two natural ways to do this:
1. Detect all preceding names and move this information to " to" and then delete the any name corresponding to the duplicate token feature. This feels easier done with a non-linearity, since precisely cancelling out vectors is hard, so I'd imagine an MLP layer deletes the " John" direction of the residual stream
2. Have a head which attends to all previous names, but where the duplicate token features <i>inhibit</i> it from attending to specific names. So this only attends to Mary. And then the output of this head maps to the logits.

(Spoiler: It's the second one).

<h3>Experiment Ideas</h3>

A test that could distinguish these two is to look at which components of the model add directly to the logits - if it's mostly attention heads which attend to " Mary" and to neither " John" it's probably hypothesis 2, if it's mostly MLPs it's probably hypothesis 1.

And we should be able to identify duplicate token heads by finding ones which attend from " John" to " John", and whose outputs are then moved to the " to" token by V-Composition with another head (Spoiler: It's more complicated than that!)

Note that all of the above reasoning is very simplistic and could easily break in a real model! There'll be significant parts of the model that figure out whether to use this circuit at all (we don't want to inhibit duplicated names when, eg, figuring out what goes at the start of the <i>next</i> sentence), and may be parts towards the end of the model that do "post-processing" just before the final output. But it's a good starting point for thinking about what's going on.

## Direct Logit Attribution

*Look up unfamiliar terms in the [mech interp explainer](https://neelnanda.io/glossary)*

Further, the easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards. The main technique used to do this is called **direct logit attribution**

**Background:** The central object of a transformer is the **residual stream**. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).

The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called **direct logit attribution**. Here we look at the direct attribution to the logit difference!

<details> <summary>(*) <b>Background and motivation of the logit difference</b></summary>

Logit difference is actually a *really* nice and elegant metric and is a particularly nice aspect of the setup of Indirect Object Identification. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities).

The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged.

But `log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)`, and so `log_probs(" Mary") - log_probs(" John") = logits(" Mary") - logits(" John")` - the ability to add an arbitrary constant cancels out!

Further, the metric helps us isolate the precise capability we care about - figuring out *which* name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.

Our metric is further refined, because each prompt is repeated twice, for each possible indirect object. This controls for irrelevant behaviour such as the model learning that John is a more frequent token than Mary (this actually happens! The final layernorm bias increases the John logit by 1 relative to the Mary logit)

</details>

<details> <summary>Ignoring LayerNorm</summary>

LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is *almost* a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts).

But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of *all* components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the `cache` to apply the global layernorm scaling factor to each constant. See [my clean GPT-2 implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Clean_Transformer_Implementation) for more on LayerNorm.
</details>

Getting an output logit is equivalent to projecting onto a direction in the residual stream. We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch

```python
answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)
print("Answer residual directions shape:", answer_residual_directions.shape)
logit_diff_directions = (
    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]
)
print("Logit difference directions shape:", logit_diff_directions.shape)
```

To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer.

<details> <summary>Technical details</summary>

`logits = Unembed(LayerNorm(final_residual_stream))`, so we technically need to account for the centering, and then learned translation and scaling of the layernorm, not just the variance 1 scaling.

The centering is accounted for with the preprocessing flag `center_writing_weights` which ensures that every weight matrix writing to the residual stream has mean zero.

The learned scaling is folded into the unembedding weights `model.unembed.W_U` via `W_U_fold = layer_norm.weights[:, None] * unembed.W_U`

The learned translation is folded to `model.unembed.b_U`, a bias added to the logits (note that GPT-2 is not trained with an existing `b_U`). This roughly represents unigram statistics. But we can ignore this because each prompt occurs twice with names in the opposite order, so this perfectly cancels out.

Note that rather than using layernorm scaling we could just study cache["ln_final.hook_normalised"]

</details>

```python
# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].
final_residual_stream = cache["resid_post", -1]
print("Final residual stream shape:", final_residual_stream.shape)
final_token_residual_stream = final_residual_stream[:, -1, :]
# Apply LayerNorm scaling
# pos_slice is the subset of the positions we take - here the final token of each prompt
scaled_final_token_residual_stream = cache.apply_ln_to_stack(
    final_token_residual_stream, layer=-1, pos_slice=-1
)

average_logit_diff = einsum(
    "batch d_model, batch d_model -> ",
    scaled_final_token_residual_stream,
    logit_diff_directions,
) / len(prompts)
print("Calculated average logit diff:", round(average_logit_diff.item(), 3))
print("Original logit difference:", round(original_average_logit_diff.item(), 3))
```

### Logit Lens

We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers.

```python
def residual_stack_to_logit_diff(
    residual_stack: Float[torch.Tensor, "components batch d_model"],
    cache: ActivationCache,
) -> float:
    scaled_residual_stack = cache.apply_ln_to_stack(
        residual_stack, layer=-1, pos_slice=-1
    )
    return einsum(
        "... batch d_model, batch d_model -> ...",
        scaled_residual_stack,
        logit_diff_directions,
    ) / len(prompts)
```

Fascinatingly, we see that the model is utterly unable to do the task until layer 7, almost all performance comes from attention layer 9, and performance actually *decreases* from there.

**Note:** Hover over each data point to see what residual stream position it's from!

<details> <summary>Details on `accumulated_resid`</summary>
**Key:** `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)

* `layer` is the layer for which we input the residual stream (this is used to identify *which* layer norm scaling factor we want)
* `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention & before MLP
* `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.
* return_labels is whether to return the labels for each component returned (useful for plotting)
</details>

```python
accumulated_residual, labels = cache.accumulated_resid(
    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True
)
logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)
line(
    logit_lens_logit_diffs,
    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,
    hover_name=labels,
    title="Logit Difference From Accumulate Residual Stream",
)
```

### Layer Attribution

We can repeat the above analysis but for each layer (this is equivalent to the differences between adjacent residual streams)

Note: Annoying terminology overload - layer k of a transformer means the kth **transformer block**, but each block consists of an **attention layer** (to move information around) *and* an **MLP layer** (to process information).

We see that only attention layers matter, which makes sense! The IOI task is about moving information around (ie moving the correct name and not the incorrect name), and less about processing it. And again we note that attention layer 9 improves things a lot, while attention 10 and attention 11 *decrease* performance

```python
per_layer_residual, labels = cache.decompose_resid(
    layer=-1, pos_slice=-1, return_labels=True
)
per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)
line(per_layer_logit_diffs, hover_name=labels, title="Logit Difference From Each Layer")
```

## Head Attribution

We can further break down the output of each attention layer into the sum of the outputs of each attention head. Each attention layer consists of 12 heads, which each act independently and additively.

<details> <summary>Decomposing attention output into sums of heads</summary>
The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer)
</details>

We see that only a few heads really matter - heads L9H6 and L9H9 contribute a lot positively (explaining why attention layer 9 is so important), while heads L10H7 and L11H10 contribute a lot negatively (explaining why attention layer 10 and layer 11 are actively harmful). These correspond to (some of) the name movers and negative name movers discussed in the paper. There are also several heads that matter positively or negatively but less strongly (other name movers and backup name movers)

There are a few meta observations worth making here - our model has 144 heads, yet we could localise this behaviour to a handful of specific heads, using straightforward, general techniques. This supports the claim in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) that attention heads are the right level of abstraction to understand attention. It also really surprising that there are *negative* heads - eg L10H7 makes the incorrect logit 7x *more* likely. I'm not sure what's going on there, though the paper discusses some possibilities.

```python
per_head_residual, labels = cache.stack_head_results(
    layer=-1, pos_slice=-1, return_labels=True
)
per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)
per_head_logit_diffs = einops.rearrange(
    per_head_logit_diffs,
    "(layer head_index) -> layer head_index",
    layer=model.cfg.n_layers,
    head_index=model.cfg.n_heads,
)
imshow(
    per_head_logit_diffs,
    labels={"x": "Head", "y": "Layer"},
    title="Logit Difference From Each Head",
)
```

## Attention Analysis

Attention heads are particularly easy to study because we can look directly at their attention patterns and study from what positions they move information from and two. This is particularly easy here as we're looking at the direct effect on the logits so we need only look at the attention patterns from the final token.

We use Alan Cooney's circuitsvis library to visualize the attention patterns! We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt (as an illustration).

<details> <summary>Interpreting Attention Patterns</summary>
An easy mistake to make when looking at attention patterns is thinking that they must convey information about the <i>token</i> looked at (maybe accounting for the context of the token). But actually, all we can confidently say is that it moves information from the *residual stream position* corresponding to that input token. Especially later on in the model, there may be components in the residual stream that are nothing to do with the input token! Eg the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in ".", "!" or "?"
</details>

```python
def visualize_attention_patterns(
    heads: Union[List[int], int, Float[torch.Tensor, "heads"]],
    local_cache: ActivationCache,
    local_tokens: torch.Tensor,
    title: Optional[str] = "",
    max_width: Optional[int] = 700,
) -> str:
    # If a single head is given, convert to a list
    if isinstance(heads, int):
        heads = [heads]

    # Create the plotting data
    labels: List[str] = []
    patterns: List[Float[torch.Tensor, "dest_pos src_pos"]] = []

    # Assume we have a single batch item
    batch_index = 0

    for head in heads:
        # Set the label
        layer = head // model.cfg.n_heads
        head_index = head % model.cfg.n_heads
        labels.append(f"L{layer}H{head_index}")

        # Get the attention patterns for the head
        # Attention patterns have shape [batch, head_index, query_pos, key_pos]
        patterns.append(local_cache["attn", layer][batch_index, head_index])

    # Convert the tokens to strings (for the axis labels)
    str_tokens = model.to_str_tokens(local_tokens)

    # Combine the patterns into a single tensor
    patterns: Float[torch.Tensor, "head_index dest_pos src_pos"] = torch.stack(
        patterns, dim=0
    )

    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)
    plot = attention_heads(
        attention=patterns, tokens=str_tokens, attention_head_names=labels
    ).show_code()

    # Display the title
    title_html = f"<h2>{title}</h2><br/>"

    # Return the visualisation as raw code
    return f"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>"
```

Inspecting the patterns, we can see that both types of name movers attend to the indirect object - this suggests they're simply copying the name attended to (with the OV circuit) and that the interesting part is the circuit behind the attention pattern that calculates *where* to move information from (the QK circuit)

```python
top_k = 3

top_positive_logit_attr_heads = torch.topk(
    per_head_logit_diffs.flatten(), k=top_k
).indices

positive_html = visualize_attention_patterns(
    top_positive_logit_attr_heads,
    cache,
    tokens[0],
    f"Top {top_k} Positive Logit Attribution Heads",
)

top_negative_logit_attr_heads = torch.topk(
    -per_head_logit_diffs.flatten(), k=top_k
).indices

negative_html = visualize_attention_patterns(
    top_negative_logit_attr_heads,
    cache,
    tokens[0],
    title=f"Top {top_k} Negative Logit Attribution Heads",
)

HTML(positive_html + negative_html)
```

## Activation Patching

**This section explains how to do activation patching conceptually by implementing it from scratch. To use it in practice with TransformerLens, see [this demonstration instead](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Activation_Patching_in_TL_Demo.ipynb)**.

The obvious limitation to the techniques used above is that they only look at the very end of the circuit - the parts that directly affect the logits. Clearly this is not sufficient to understand the circuit! We want to understand how things compose together to produce this final output, and ideally to produce an end-to-end circuit fully explaining this behaviour.

The technique we'll use to investigate this is called **activation patching**. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.

The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.

We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to *localise* which activations matter.

The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.

Here's an animation from the ROME paper demonstrating this technique (they studied factual recall, and use stars to represent corruption applied to the subject of the sentence, but the same principles apply):

![CT Animation](https://rome.baulab.info/images/small-ct-animation.gif)

See also [the explanation in a mech interp explainer](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx) and [this piece](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching#how-to-think-about-activation-patching) describing how to think about patching on a conceptual level

The above was all fairly abstract, so let's zoom in and lay out a concrete example to understand Indirect Object Identification.

Here our clean input will be eg "After John and Mary went to the store, **John** gave a bottle of milk to" and our corrupted input will be eg "After John and Mary went to the store, **Mary** gave a bottle of milk to". These prompts are identical except for the name of the indirect object, and so patching is a causal intervention which will allow us to understand precisely which parts of the network are identifying the indirect object.

One natural thing to patch in is the residual stream at a specific layer and specific position. For example, the model is likely initially doing some processing on the second subject token to realise that it's a duplicate, but then uses attention to move that information to the " to" token. So patching in the residual stream at the " to" token will likely matter a lot in later layers but not at all in early layers.

We can zoom in much further and patch in specific activations from specific layers. For example, we think that the output of head L9H9 on the final token is significant for directly connecting to the logits

We can patch in specific activations, and can zoom in as far as seems reasonable. For example, if we patch in the output of head L9H9 on the final token, we would predict that it will significantly affect performance.

Note that this technique does *not* tell us how the components of the circuit connect up, just what they are.

<details> <summary>Technical details</summary>
The choice of clean and corrupted prompt has both pros and cons. By carefully setting up the counterfactual, that <i>only</i> differs in the second subject, we avoid detecting the parts of the model doing irrelevant computation like detecting that the indirect object task is relevant at all or that it should be outputting a name rather than an article or pronoun. Or even context like that John and Mary are names at all.

However, it *also* bakes in some details that *are* relevant to the task. Such as finding the location of the second subject, and of the names in the first clause. Or that the name mover heads have learned to copy whatever they look at.

Some of these could be patched by also changing up the order of the names in the original sentence - patching in "After <b>John and Mary</b> went to the store, John gave a bottle of milk to" vs "After <b>Mary and John</b> went to the store, John gave a bottle of milk to".

In the ROME paper they take a different tack. Rather than carefully setting up counterfactuals between two different but related inputs, they **corrupt** the clean input by adding Gaussian noise to the token embedding for the subject. This is in some ways much lower effort (you don't need to set up a similar but different prompt) but can also introduce some issues, such as ways this noise might break things. In practice, you should take care about how you choose your counterfactuals and try out several. Try to reason beforehand about what they will and will not tell you, and compare the results between different counterfactuals.

I discuss some of these limitations and how the author's solved them with much more refined usage of these techniques <a href="https://www.youtube.com/watch?v=gzwj0jWbvbo">in our interview</a>
</details>

## Residual Stream

Lets begin by patching in the residual stream at the start of each layer and for each token position.

We first create a set of corrupted tokens - where we swap each pair of prompts to have the opposite answer.

```python
corrupted_prompts = []
for i in range(0, len(prompts), 2):
    corrupted_prompts.append(prompts[i + 1])
    corrupted_prompts.append(prompts[i])
corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)
corrupted_logits, corrupted_cache = model.run_with_cache(
    corrupted_tokens, return_type="logits"
)
corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)
print("Corrupted Average Logit Diff", round(corrupted_average_logit_diff.item(), 2))
print("Clean Average Logit Diff", round(original_average_logit_diff.item(), 2))
```

```python
model.to_string(corrupted_tokens)
```

We now intervene on the corrupted run and patch in the clean residual stream at a specific layer and position.

We do the intervention using TransformerLens's `HookPoint` feature. We can design a hook function that takes in a specific activation and returns an edited copy, and temporarily add it in with `model.run_with_hooks`.

```python
def patch_residual_component(
    corrupted_residual_component: Float[torch.Tensor, "batch pos d_model"],
    hook,
    pos,
    clean_cache,
):
    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]
    return corrupted_residual_component

def normalize_patched_logit_diff(patched_logit_diff):
    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise
    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance
    return (patched_logit_diff - corrupted_average_logit_diff) / (
        original_average_logit_diff - corrupted_average_logit_diff
    )

patched_residual_stream_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for position in range(tokens.shape[1]):
        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("resid_pre", layer), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can immediately see that, exactly as predicted, originally all relevant computation happens on the second subject token, and at layers 7 and 8, the information is moved to the final token. Moving the residual stream at the correct position near *exactly* recovers performance!

For reference, tokens and their index from the first prompt are on the x-axis. In an abuse of notation, note that the difference here is averaged over *all* 8 prompts, while the labels only come from the *first* prompt.

To be easier to interpret, we normalise the logit difference, by subtracting the corrupted logit difference, and dividing by the total improvement from clean to corrupted to normalise
0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance

```python
prompt_position_labels = [
    f"{tok}_{i}" for i, tok in enumerate(model.to_str_tokens(tokens[0]))
]
imshow(
    patched_residual_stream_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched Residual Stream",
    labels={"x": "Position", "y": "Layer"},
)
```

## Layers

We can apply exactly the same idea, but this time patching in attention or MLP layers. These are also residual components with identical shapes to the residual stream terms, so we can reuse the same hooks.

```python
patched_attn_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
patched_mlp_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for position in range(tokens.shape[1]):
        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)
        patched_attn_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("attn_out", layer), hook_fn)],
            return_type="logits",
        )
        patched_attn_logit_diff = logits_to_ave_logit_diff(
            patched_attn_logits, answer_tokens
        )
        patched_mlp_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("mlp_out", layer), hook_fn)],
            return_type="logits",
        )
        patched_mlp_logit_diff = logits_to_ave_logit_diff(
            patched_mlp_logits, answer_tokens
        )

        patched_attn_diff[layer, position] = normalize_patched_logit_diff(
            patched_attn_logit_diff
        )
        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(
            patched_mlp_logit_diff
        )
```

We see that several attention layers are significant but that, matching the residual stream results, early layers matter on the second subject token, and later layers matter on the final token, and layers essentially don't matter on any other token. Extremely localised! As with direct logit attribution, layer 9 is positive and layers 10 and 11 are not, suggesting that the late layers only matter for direct logit effects, but we also see that layers 7 and 8 matter significantly. Presumably these are the heads that move information about which name is duplicated from the second subject token to the final token.

```python
imshow(
    patched_attn_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched Attention Layer",
    labels={"x": "Position", "y": "Layer"},
)
```

In contrast, the MLP layers do not matter much. This makes sense, since this is more a task about moving information than about processing it, and the MLP layers specialise in processing information.

The one exception is MLP 0, which matters a lot, but I think this is misleading and just a generally true statement about MLP 0 rather than being about the circuit on this task.

<details> <summary>My takes on MLP0</summary>
It's often observed on GPT-2 Small that MLP0 matters a lot, and that ablating it utterly destroys performance. My current best guess is that the first MLP layer is essentially acting as an extension of the embedding (for whatever reason) and that when later layers want to access the input tokens they mostly read in the output of the first MLP layer, rather than the token embeddings. Within this frame, the first attention layer doesn't do much.

In this framing, it makes sense that MLP0 matters on the second subject token, because that's the one position with a different input token!

I'm not entirely sure why this happens, but I would guess that it's because the embedding and unembedding matrices in GPT-2 Small are the same. This is pretty unprincipled, as the tasks of embedding and unembedding tokens are <i>not</i> inverses, but this is common practice, and plausibly models want to dedicate some parameters to overcoming this.

I only have suggestive evidence of this, and would love to see someone look into this properly!
</details>

```python
imshow(
    patched_mlp_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched MLP Layer",
    labels={"x": "Position", "y": "Layer"},
)
```

## Heads

We can refine the above analysis by patching in individual heads! This is somewhat more annoying, because there are now three dimensions (head_index, position and layer), so for now lets patch in a head's output across all positions.

The easiest way to do this is to patch in the activation `z`, the "mixed value" of the attention head. That is, the average of all previous values weighted by the attention pattern, ie the activation that is then multiplied by `W_O`, the output weights.

```python
def patch_head_vector(
    corrupted_head_vector: Float[torch.Tensor, "batch pos head_index d_head"],
    hook,
    head_index,
    clean_cache,
):
    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][
        :, :, head_index, :
    ]
    return corrupted_head_vector

patched_head_z_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("z", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can now see that, in addition to the name mover heads identified before, in mid-late layers the heads L8H6, L8H10, L7H9 matter and are presumably responsible for moving information from the second subject to the final token. And heads L5H5, L6H9, L3H0 also matter a lot, and are presumably involved in detecting duplicated tokens.

```python
imshow(
    patched_head_z_diff,
    title="Logit Difference From Patched Head Output",
    labels={"x": "Head", "y": "Layer"},
)
```

## Decomposing Heads

Decomposing attention layers into patching in individual heads has already helped us localise the behaviour a lot. But we can understand it further by decomposing heads. An attention head consists of two semi-independent operations - calculating *where* to move information from and to (represented by the attention pattern and implemented via the QK-circuit) and calculating *what* information to move (represented by the value vectors and implemented by the OV circuit). We can disentangle which of these is important by patching in just the attention pattern *or* the value vectors. (See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [my walkthrough video](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition. If you're not familiar with the details of how attention is implemented, I recommend checking out [my clean transformer implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=3Pb0NYbZ900e) to see how the code works))

First let's patch in the value vectors, to measure when figuring out what to move is important. . This has the same shape as z ([batch, pos, head_index, d_head]) so we can reuse the same hook.

```python
patched_head_v_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("v", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can plot this as a heatmap and it's initially hard to interpret.

```python
imshow(
    patched_head_v_diff,
    title="Logit Difference From Patched Head Value",
    labels={"x": "Head", "y": "Layer"},
)
```

But it's very easy to interpret if we plot a scatter plot against patching head outputs. Here we see that the earlier heads (L5H5, L6H9, L3H0) and late name movers (L9H9, L10H7, L11H10) don't matter at all now, while the mid-late heads (L8H6, L8H10, L7H9) do.

Meta lesson: Plot things early, often and in diverse ways as you explore a model's internals!

```python
head_labels = [
    f"L{l}H{h}" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)
]
scatter(
    x=utils.to_numpy(patched_head_v_diff.flatten()),
    y=utils.to_numpy(patched_head_z_diff.flatten()),
    xaxis="Value Patch",
    yaxis="Output Patch",
    caxis="Layer",
    hover_name=head_labels,
    color=einops.repeat(
        np.arange(model.cfg.n_layers), "layer -> (layer head)", head=model.cfg.n_heads
    ),
    range_x=(-0.5, 0.5),
    range_y=(-0.5, 0.5),
    title="Scatter plot of output patching vs value patching",
)
```

When we patch in attention patterns, we see the opposite effect - early and late heads matter a lot, middle heads don't. (In fact, the sum of value patching and pattern patching is approx the same as output patching)

```python
def patch_head_pattern(
    corrupted_head_pattern: Float[torch.Tensor, "batch head_index query_pos d_head"],
    hook,
    head_index,
    clean_cache,
):
    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][
        :, head_index, :, :
    ]
    return corrupted_head_pattern

patched_head_attn_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("attn", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

```python
imshow(
    patched_head_attn_diff,
    title="Logit Difference From Patched Head Pattern",
    labels={"x": "Head", "y": "Layer"},
)
head_labels = [
    f"L{l}H{h}" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)
]
scatter(
    x=utils.to_numpy(patched_head_attn_diff.flatten()),
    y=utils.to_numpy(patched_head_z_diff.flatten()),
    hover_name=head_labels,
    xaxis="Attention Patch",
    yaxis="Output Patch",
    title="Scatter plot of output patching vs attention patching",
)
```

## Consolidating Understanding

OK, let's zoom out and reconsolidate. At a high-level, we find that all the action is on the second subject token until layer 7 and then transitions to the final token. And that attention layers matter a lot, MLP layers not so much (apart from MLP0, likely as an extended embedding).

We've further localised important behaviour to several categories of heads. We've found 3 categories of heads that matter a lot - early heads (L5H5, L6H9, L3H0) whose output matters on the second subject and whose behaviour is determined by their attention patterns, mid-late heads (L8H6, L8H10, L7H9, L7H3) whose output matters on the final token and whose behaviour is determined by their value vectors, and late heads (L9H9, L10H7, L11H10) whose output matters on the final token and whose behaviour is determined by their attention patterns.

A natural speculation is that early heads detect both that the second subject is a repeated token and *which* is repeated (ie the " John" token is repeated), middle heads compose with this and move this duplicated token information from the second subject token to the final token, and the late heads compose with this to *inhibit* their attention to the duplicated token, and then attend to the correct indirect object name and copy that directly to the logits.

### Visualizing Attention Patterns

We can validate this by looking at the attention patterns of these heads! Let's take the top 10 heads by output patching (in absolute value) and split it into early, middle and late.

We see that middle heads attend from the final token to the second subject, and late heads attend from the final token to the indirect object, which is completely consistent with the above speculation! But weirdly, while *one* early head attends from the second subject to its first copy, the other two mysteriously attend to the word *after* the first copy.

```python
top_k = 10
top_heads_by_output_patch = torch.topk(
    patched_head_z_diff.abs().flatten(), k=top_k
).indices
first_mid_layer = 7
first_late_layer = 9
early_heads = top_heads_by_output_patch[
    top_heads_by_output_patch < model.cfg.n_heads * first_mid_layer
]
mid_heads = top_heads_by_output_patch[
    torch.logical_and(
        model.cfg.n_heads * first_mid_layer <= top_heads_by_output_patch,
        top_heads_by_output_patch < model.cfg.n_heads * first_late_layer,
    )
]
late_heads = top_heads_by_output_patch[
    model.cfg.n_heads * first_late_layer <= top_heads_by_output_patch
]

early = visualize_attention_patterns(
    early_heads, cache, tokens[0], title=f"Top Early Heads"
)
mid = visualize_attention_patterns(
    mid_heads, cache, tokens[0], title=f"Top Middle Heads"
)
late = visualize_attention_patterns(
    late_heads, cache, tokens[0], title=f"Top Late Heads"
)

HTML(early + mid + late)
```

### Comparing to the Paper

We can now refer to the (far, far more rigorous and detailed) analysis in the paper to compare our results! Here's the diagram they give of their results.

![IOI1](https://pbs.twimg.com/media/FghGkTAWAAAmkhm.jpg)

(Head 1.2 in their notation is L1H2 in my notation etc. And note - in the [latest version of the paper](https://arxiv.org/pdf/2211.00593.pdf) they add 9.0 as a backup name mover, and remove 11.3)

The heads form three categories corresponding to the early, middle and late categories we found and we did fairly well! Definitely not perfect, but with some fairly generic techniques and some a priori reasoning, we found the broad strokes of the circuit and what it looks like. We focused on the most important heads, so we didn't find all relevant heads in each category (especially not the heads in brackets, which are more minor), but this serves as a good base for doing more rigorous and involved analysis, especially for finding the *complete* circuit (ie all of the parts of the model which participate in this behaviour) rather than just a partial and suggestive circuit. Go check out [their paper](https://arxiv.org/abs/2211.00593) or [our interview](https://www.youtube.com/watch?v=gzwj0jWbvbo) to learn more about what they did and what they found!

Breaking down their categories:

* Early: The duplicate token heads, previous token heads and induction heads. These serve the purpose of detecting that the second subject is duplicated and which earlier name is the duplicate.
    * We found a direct duplicate token head which behaves exactly as expected, L3H0. Heads L5H0 and L6H9 are induction heads, which explains why they don't attend directly to the earlier copy of John!
    * Note that the duplicate token heads and induction heads do not compose with each other - both directly add to the S-Inhibition heads. The diagram is somewhat misleading.
* Middle: They call these S-Inhibition heads - they copy the information about the duplicate token from the second subject to the to token, and their output is used to *inhibit* the attention paid from the name movers to the first subject copy. We found all these heads, and had a decent guess for what they did.
    * In either case they attend to the second subject, so the patch that mattered was their value vectors!
* Late: They call these name movers, and we found some of them. They attend from the final token to the indirect object name and copy that to the logits, using the S-Inhibition heads to inhibit attention to the first copy of the subject token.
    * We did find their surprising result of *negative* name movers - name movers that inhibit the correct answer!
    * They have an entire category of heads we missed called backup name movers - we'll get to these later.

So, now, let's dig into the two anomalies we missed - induction heads and backup name mover heads

## Bonus: Exploring Anomalies

### Early Heads are Induction Heads(?!)

A really weird observation is that some of the early heads detecting duplicated tokens are induction heads, not just direct duplicate token heads. This is very weird! What's up with that?

First off, what's an induction head? An induction head is an important type of attention head that can detect and continue repeated sequences. It is the second head in a two head induction circuit, which looks for previous copies of the current token and attends to the token *after* it, and then copies that to the current position and predicts that it will come next. They're enough of a big deal that [we wrote a whole paper on them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).

![Move image demo](https://pbs.twimg.com/media/FNWAzXjVEAEOGRe.jpg)

Second, why is it surprising that they come up here? It's surprising because it feels like overkill. The model doesn't care about *what* token comes after the first copy of the subject, just that it's duplicated. And it already has simpler duplicate token heads. My best guess is that it just already had induction heads around and that, in addition to their main function, they *also* only activate on duplicated tokens. So it was useful to repurpose this existing machinery.

This suggests that as we look for circuits in larger models life may get more and more complicated, as components in simpler circuits get repurposed and built upon.

We can verify that these are induction heads by running the model on repeated text and plotting the heads.

```python
example_text = "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components."
example_repeated_text = example_text + example_text
example_repeated_tokens = model.to_tokens(example_repeated_text, prepend_bos=True)
example_repeated_logits, example_repeated_cache = model.run_with_cache(
    example_repeated_tokens
)
induction_head_labels = [81, 65]
```

```python
code = visualize_attention_patterns(
    induction_head_labels,
    example_repeated_cache,
    example_repeated_tokens,
    title="Induction Heads",
    max_width=800,
)
HTML(code)
```

#### Implications

One implication of this is that it's useful to categories heads according to whether they occur in
simpler circuits, so that as we look for more complex circuits we can easily look for them. This is
easy to do here! An interesting fact about induction heads is that they work on a sequence of
repeated random tokens - notable for being wildly off distribution from the natural language GPT-2
was trained on. Being able to predict a model's behaviour off distribution is a good mark of success
for mechanistic interpretability! This is a good sanity check for whether a head is an induction
head or not.

We can characterise an induction head by just giving a sequence of random tokens repeated once, and
measuring the average attention paid from the second copy of a token to the token after the first
copy. At the same time, we can also measure the average attention paid from the second copy of a
token to the first copy of the token, which is the attention that the induction head would pay if it
were a duplicate token head, and the average attention paid to the previous token to find previous
token heads.

Note that this is a superficial study of whether something is an induction head - we totally ignore
the question of whether it actually does boost the correct token or whether it composes with a
single previous head and how. In particular, we sometimes get anti-induction heads which suppress
the induction-y token (no clue why!), and this technique will find those too . But given the
previous rigorous analysis, we can be pretty confident that this picks up on some true signal about
induction heads.

<details> <summary>Technical Implementation Details</summary>
We can do this again by using hooks, this time just to access the attention patterns rather than to intervene on them.

Our hook function acts on the attention pattern activation. This has the name
"blocks.{layer}.{layer_type}.hook_{activation_name}" in general, here it's
"blocks.{layer}.attn.hook_attn". And it has shape [batch, head_index, query_pos, token_pos]. Our
hook function takes in the attention pattern activation, calculates the score for the relevant type
of head, and write it to an external cache.

We add in hooks using `model.run_with_hooks(tokens, fwd_hooks=[(names_filter, hook_fn)])` to
temporarily add in the hooks and run the model, getting the resulting output. Previously
names_filter was the name of the activation, but here it's a boolean function mapping activation
names to whether we want to hook them or not. Here it's just whether the name ends with hook_attn.
hook_fn must take in the two inputs activation (the activation tensor) and hook (the HookPoint
object, which contains the name of the activation and some metadata such as the current layer).

Internally our hooks use the function `tensor.diagonal`, this takes the diagonal between two
dimensions, and allows an arbitrary offset - offset by 1 to get previous tokens, seq_len to get
duplicate tokens (the distance to earlier copies) and seq_len-1 to get induction heads (the distance
to the token *after* earlier copies). Different offsets give a different length of output tensor,
and we can now just average to get a score in [0, 1] for each head
</details>

```python
seq_len = 100
batch_size = 2

prev_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device)

def prev_token_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=1, dim1=-1, dim2=-2)
    # print(diagonal)
    # print(pattern)
    prev_token_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

duplicate_token_scores = torch.zeros(
    (model.cfg.n_layers, model.cfg.n_heads), device=device
)

def duplicate_token_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=seq_len, dim1=-1, dim2=-2)
    duplicate_token_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

induction_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device)

def induction_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=seq_len - 1, dim1=-1, dim2=-2)
    induction_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

torch.manual_seed(0)
original_tokens = torch.randint(
    100, 20000, size=(batch_size, seq_len), device="cpu"
).to(device)
repeated_tokens = einops.repeat(
    original_tokens, "batch seq_len -> batch (2 seq_len)"
).to(device)

pattern_filter = lambda act_name: act_name.endswith("hook_pattern")

loss = model.run_with_hooks(
    repeated_tokens,
    return_type="loss",
    fwd_hooks=[
        (pattern_filter, prev_token_hook),
        (pattern_filter, duplicate_token_hook),
        (pattern_filter, induction_hook),
    ],
)
print(torch.round(utils.get_corner(prev_token_scores).detach().cpu(), decimals=3))
print(torch.round(utils.get_corner(duplicate_token_scores).detach().cpu(), decimals=3))
print(torch.round(utils.get_corner(induction_scores).detach().cpu(), decimals=3))
```

We can now plot the head scores, and instantly see that the relevant early heads are induction heads or duplicate token heads (though also that there's a lot of induction heads that are *not* use - I have no idea why!).

```python
imshow(
    prev_token_scores, labels={"x": "Head", "y": "Layer"}, title="Previous Token Scores"
)
imshow(
    duplicate_token_scores,
    labels={"x": "Head", "y": "Layer"},
    title="Duplicate Token Scores",
)
imshow(
    induction_scores, labels={"x": "Head", "y": "Layer"}, title="Induction Head Scores"
)
```

The above suggests that it would be a useful bit of infrastructure to have a "wiki" for the heads of a model, giving their scores according to some metrics re head functions, like the ones we've seen here. TransformerLens makes this easy to make, as just changing the name input to `HookedTransformer.from_pretrained` gives a different model but in the same architecture, so the same code should work. If you want to make this, I'd love to see it!

As a proof of concept, [I made a mosaic of all induction heads across the 40 models then in TransformerLens](https://www.neelnanda.io/mosaic).

![induction scores as proof of concept](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2F5vtuFmdzt_.png?alt=media&token=4d613de4-9d14-48d6-ba9d-e591c562d429)

### Backup Name Mover Heads

Another fascinating anomaly is that of the **backup name mover heads**. A standard technique to apply when interpreting model internals is ablations, or knock-out. If we run the model but intervene to set a specific head to zero, what happens? If the model is robust to this intervention, then naively we can be confident that the head is not doing anything important, and conversely if the model is much worse at the task this suggests that head was important. There are several conceptual flaws with this approach, making the evidence only suggestive, eg that the average output of the head may be far from zero and so the knockout may send it far from expected activations, breaking internals on *any* task. But it's still an easy technique to apply to give some data.

But a wild finding in the paper is that models have **built in redundancy**. If we knock out one of the name movers, then there are some backup name movers in later layers that *change their behaviour* and do (some of) the job of the original name mover head. This means that naive knock-out will significantly underestimate the importance of the name movers.

Let's test this! Let's ablate the most important name mover (head L9H9) on just the final token using a custom ablation hook and then cache all new activations and compared performance. We focus on the final position because we want to specifically ablate the direct logit effect. When we do this, we see that naively, removing the top name mover should reduce the logit diff massively, from 3.55 to 0.57. **But actually, it only goes down to 2.99!**

<details> <summary>Implementation Details</summary>
Ablating heads is really easy in TransformerLens! We can just define a hook on the z activation in the relevant attention layer (recall, z is the mixed values, and comes immediately before multiplying by the output weights $W_O$). z has a head_index axis, so we can set the component for the relevant head and for position -1 to zero, and return it. (Technically we could just edit in place without returning it, but by convention we always return an edited activation).

We now want to compare all internal activations with a hook, which is hard to do with the nice `run_with_hooks` API. So we can directly access the hook on the z activation with `model.blocks[layer].attn.hook_z` and call its `add_hook` method. This adds in the hook to the *global state* of the model. We can now use run_with_cache, and don't need to care about the global state, because run_with_cache internally adds a bunch of caching hooks, and then removes all hooks after the run, *including* the previously added ablation hook. This can be disabled with the reset_hooks_end flag, but here it's useful!
</details>

```python
top_name_mover = per_head_logit_diffs.flatten().argmax().item()
top_name_mover_layer = top_name_mover // model.cfg.n_heads
top_name_mover_head = top_name_mover % model.cfg.n_heads
print(f"Top Name Mover to ablate: L{top_name_mover_layer}H{top_name_mover_head}")

def ablate_top_head_hook(z: Float[torch.Tensor, "batch pos head_index d_head"], hook):
    z[:, -1, top_name_mover_head, :] = 0
    return z

# Adds a hook into global model state
model.blocks[top_name_mover_layer].attn.hook_z.add_hook(ablate_top_head_hook)
# Runs the model, temporarily adds caching hooks and then removes *all* hooks after running, including the ablation hook.
ablated_logits, ablated_cache = model.run_with_cache(tokens)
print(f"Original logit diff: {original_average_logit_diff:.2f}")
print(
    f"Post ablation logit diff: {logits_to_ave_logit_diff(ablated_logits, answer_tokens).item():.2f}"
)
print(
    f"Direct Logit Attribution of top name mover head: {per_head_logit_diffs.flatten()[top_name_mover].item():.2f}"
)
print(
    f"Naive prediction of post ablation logit diff: {original_average_logit_diff - per_head_logit_diffs.flatten()[top_name_mover].item():.2f}"
)
```

So what's up with this? As before, we can look at the direct logit attribution of each head to see what's going on. It's easiest to interpret if plotted as a scatter plot against the initial per head logit difference.

And we can see a *really* big difference in a few heads! (Hover to see labels) In particular the negative name mover L10H7 decreases its negative effect a lot, adding +1 to the logit diff, and the backup name mover L10H10 adjusts its effect to be more positive, adding +0.8 to the logit diff (with several other marginal changes). (And obviously the ablated head has gone down to zero!)

```python
per_head_ablated_residual, labels = ablated_cache.stack_head_results(
    layer=-1, pos_slice=-1, return_labels=True
)
per_head_ablated_logit_diffs = residual_stack_to_logit_diff(
    per_head_ablated_residual, ablated_cache
)
per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(
    model.cfg.n_layers, model.cfg.n_heads
)
imshow(per_head_ablated_logit_diffs, labels={"x": "Head", "y": "Layer"})
scatter(
    y=per_head_logit_diffs.flatten(),
    x=per_head_ablated_logit_diffs.flatten(),
    hover_name=head_labels,
    range_x=(-3, 3),
    range_y=(-3, 3),
    xaxis="Ablated",
    yaxis="Original",
    title="Original vs Post-Ablation Direct Logit Attribution of Heads",
)
```

One natural hypothesis is that this is because the final LayerNorm scaling has changed, which can scale up or down the final residual stream. This is slightly true, and we can see that the typical head is a bit off from the x=y line. But the average LN scaling ratio is 1.04, and this should uniformly change *all* heads by the same factor, so this can't be sufficient

```python
print(
    "Average LN scaling ratio:",
    round(
        (
            cache["ln_final.hook_scale"][:, -1]
            / ablated_cache["ln_final.hook_scale"][:, -1]
        )
        .mean()
        .item(),
        3,
    ),
)
print(
    "Ablation LN scale",
    ablated_cache["ln_final.hook_scale"][:, -1].detach().cpu().round(decimals=2),
)
print(
    "Original LN scale",
    cache["ln_final.hook_scale"][:, -1].detach().cpu().round(decimals=2),
)
```

**Exercise to the reader:** Can you finish off this analysis? What's going on here? Why are the backup name movers changing their behaviour? Why is one negative name mover becoming significantly less important?

---

# Grokking_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Grokking Demo Notebook

<b style="color: red">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>

# Setup
(No need to read)

```python
TRAIN_MODEL = True
```

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install circuitsvis
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio
if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
pio.templates['plotly'].layout.xaxis.title.font.size = 20
pio.templates['plotly'].layout.yaxis.title.font.size = 20
pio.templates['plotly'].layout.title.font.size = 30
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import os
import tqdm.auto as tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from typing import List, Union, Optional
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
```

```python
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache

device = "cuda" if torch.cuda.is_available() else "cpu"
```

Plotting helper functions:

```python
def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(y=y, x=x, labels={"x":xaxis, "y":yaxis, "color":caxis}, **kwargs).show(renderer)
```

```python
# Define the location to save the model, using a relative path
PTH_LOCATION = "workspace/_scratch/grokking_demo.pth"

# Create the directory if it does not exist
os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)
```

# Model Training

## Config

```python
p = 113
frac_train = 0.3

# Optimizer config
lr = 1e-3
wd = 1.
betas = (0.9, 0.98)

num_epochs = 25000
checkpoint_every = 100

DATA_SEED = 598
```

## Define Task
* Define modular addition
* Define the dataset & labels

Input format:
|a|b|=|

```python
a_vector = einops.repeat(torch.arange(p), "i -> (i j)", j=p)
b_vector = einops.repeat(torch.arange(p), "j -> (i j)", i=p)
equals_vector = einops.repeat(torch.tensor(113), " -> (i j)", i=p, j=p)

```

```python
dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)
print(dataset[:5])
print(dataset.shape)
```

```python
labels = (dataset[:, 0] + dataset[:, 1]) % p
print(labels.shape)
print(labels[:5])
```

Convert this to a train + test set - 30% in the training set

```python
torch.manual_seed(DATA_SEED)
indices = torch.randperm(p*p)
cutoff = int(p*p*frac_train)
train_indices = indices[:cutoff]
test_indices = indices[cutoff:]

train_data = dataset[train_indices]
train_labels = labels[train_indices]
test_data = dataset[test_indices]
test_labels = labels[test_indices]
print(train_data[:5])
print(train_labels[:5])
print(train_data.shape)
print(test_data[:5])
print(test_labels[:5])
print(test_data.shape)
```

## Define Model

```python

cfg = HookedTransformerConfig(
    n_layers = 1,
    n_heads = 4,
    d_model = 128,
    d_head = 32,
    d_mlp = 512,
    act_fn = "relu",
    normalization_type=None,
    d_vocab=p+1,
    d_vocab_out=p,
    n_ctx=3,
    init_weights=True,
    device=device,
    seed = 999,
)
```

```python
model = HookedTransformer(cfg)
```

Disable the biases, as we don't need them for this task and it makes things easier to interpret.

```python
for name, param in model.named_parameters():
    if "b_" in name:
        param.requires_grad = False

```

## Define Optimizer + Loss

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)
```

```python
def loss_fn(logits, labels):
    if len(logits.shape)==3:
        logits = logits[:, -1]
    logits = logits.to(torch.float64)
    log_probs = logits.log_softmax(dim=-1)
    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]
    return -correct_log_probs.mean()
train_logits = model(train_data)
train_loss = loss_fn(train_logits, train_labels)
print(train_loss)
test_logits = model(test_data)
test_loss = loss_fn(test_logits, test_labels)
print(test_loss)
```

```python
print("Uniform loss:")
print(np.log(p))
```

## Actually Train

**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots.

```python
train_losses = []
test_losses = []
model_checkpoints = []
checkpoint_epochs = []
if TRAIN_MODEL:
    for epoch in tqdm.tqdm(range(num_epochs)):
        train_logits = model(train_data)
        train_loss = loss_fn(train_logits, train_labels)
        train_loss.backward()
        train_losses.append(train_loss.item())

        optimizer.step()
        optimizer.zero_grad()

        with torch.inference_mode():
            test_logits = model(test_data)
            test_loss = loss_fn(test_logits, test_labels)
            test_losses.append(test_loss.item())

        if ((epoch+1)%checkpoint_every)==0:
            checkpoint_epochs.append(epoch)
            model_checkpoints.append(copy.deepcopy(model.state_dict()))
            print(f"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}")
```

```python
torch.save(
    {
        "model":model.state_dict(),
        "config": model.cfg,
        "checkpoints": model_checkpoints,
        "checkpoint_epochs": checkpoint_epochs,
        "test_losses": test_losses,
        "train_losses": train_losses,
        "train_indices": train_indices,
        "test_indices": test_indices,
    },
    PTH_LOCATION)
```

```python
if not TRAIN_MODEL:
    cached_data = torch.load(PTH_LOCATION)
    model.load_state_dict(cached_data['model'])
    model_checkpoints = cached_data["checkpoints"]
    checkpoint_epochs = cached_data["checkpoint_epochs"]
    test_losses = cached_data['test_losses']
    train_losses = cached_data['train_losses']
    train_indices = cached_data["train_indices"]
    test_indices = cached_data["test_indices"]
```

## Show Model Training Statistics, Check that it groks!

```python
%pip install git+https://github.com/neelnanda-io/neel-plotly.git
from neel_plotly.plot import line
line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Training Curve for Modular Addition", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)
```

# Analysing the Model

## Standard Things to Try

```python
original_logits, cache = model.run_with_cache(dataset)
print(original_logits.numel())
```

Get key weight matrices:

```python
W_E = model.embed.W_E[:-1]
print("W_E", W_E.shape)
W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in
print("W_neur", W_neur.shape)
W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U
print("W_logit", W_logit.shape)
```

```python
original_loss = loss_fn(original_logits, labels).item()
print("Original Loss:", original_loss)
```

### Looking at Activations

Helper variable:

```python
pattern_a = cache["pattern", 0, "attn"][:, :, -1, 0]
pattern_b = cache["pattern", 0, "attn"][:, :, -1, 1]
neuron_acts = cache["post", 0, "mlp"][:, -1, :]
neuron_pre_acts = cache["pre", 0, "mlp"][:, -1, :]
```

Get all shapes:

```python
for param_name, param in cache.items():
    print(param_name, param.shape)
```

```python
imshow(cache["pattern", 0].mean(dim=0)[:, -1, :], title="Average Attention Pattern per Head", xaxis="Source", yaxis="Head", x=['a', 'b', '='])
```

```python
imshow(cache["pattern", 0][5][:, -1, :], title="Average Attention Pattern per Head", xaxis="Source", yaxis="Head", x=['a', 'b', '='])
```

```python
dataset[:4]
```

```python
imshow(cache["pattern", 0][:, 0, -1, 0].reshape(p, p), title="Attention for Head 0 from a -> =", xaxis="b", yaxis="a")
```

```python
imshow(
    einops.rearrange(cache["pattern", 0][:, :, -1, 0], "(a b) head -> head a b", a=p, b=p),
    title="Attention for Head 0 from a -> =", xaxis="b", yaxis="a", facet_col=0)
```

Plotting neuron activations

```python
cache["post", 0, "mlp"].shape
```

```python
imshow(
    einops.rearrange(neuron_acts[:, :5], "(a b) neuron -> neuron a b", a=p, b=p),
    title="First 5 neuron acts", xaxis="b", yaxis="a", facet_col=0)
```

### Singular Value Decomposition

```python
W_E.shape
```

```python
U, S, Vh = torch.svd(W_E)
line(S, title="Singular Values")
imshow(U, title="Principal Components on the Input")
```

```python
# Control - random Gaussian matrix
U, S, Vh = torch.svd(torch.randn_like(W_E))
line(S, title="Singular Values Random")
imshow(U, title="Principal Components Random")
```

## Explaining Algorithm

### Analyse the Embedding - It's a Lookup Table!

```python
U, S, Vh = torch.svd(W_E)
line(U[:, :8].T, title="Principal Components of the embedding", xaxis="Input Vocabulary")
```

```python
fourier_basis = []
fourier_basis_names = []
fourier_basis.append(torch.ones(p))
fourier_basis_names.append("Constant")
for freq in range(1, p//2+1):
    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))
    fourier_basis_names.append(f"Sin {freq}")
    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))
    fourier_basis_names.append(f"Cos {freq}")
fourier_basis = torch.stack(fourier_basis, dim=0).to(device)
fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)
imshow(fourier_basis, xaxis="Input", yaxis="Component", y=fourier_basis_names)
```

```python
line(fourier_basis[:8], xaxis="Input", line_labels=fourier_basis_names[:8], title="First 8 Fourier Components")
line(fourier_basis[25:29], xaxis="Input", line_labels=fourier_basis_names[25:29], title="Middle Fourier Components")
```

```python
imshow(fourier_basis @ fourier_basis.T, title="All Fourier Vectors are Orthogonal")
```

### Analyse the Embedding

```python
imshow(fourier_basis @ W_E, yaxis="Fourier Component", xaxis="Residual Stream", y=fourier_basis_names, title="Embedding in Fourier Basis")
```

```python
line((fourier_basis @ W_E).norm(dim=-1), xaxis="Fourier Component", x=fourier_basis_names, title="Norms of Embedding in Fourier Basis")
```

```python
key_freqs = [17, 25, 32, 47]
key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]
fourier_embed = fourier_basis @ W_E
key_fourier_embed = fourier_embed[key_freq_indices]
print("key_fourier_embed", key_fourier_embed.shape)
imshow(key_fourier_embed @ key_fourier_embed.T, title="Dot Product of embedding of key Fourier Terms")
```

### Key Frequencies

```python
line(fourier_basis[[34, 50, 64, 94]], title="Cos of key freqs", line_labels=[34, 50, 64, 94])
```

```python
line(fourier_basis[[34, 50, 64, 94]].mean(0), title="Constructive Interference")
```

## Analyse Neurons

```python
imshow(
    einops.rearrange(neuron_acts[:, :5], "(a b) neuron -> neuron a b", a=p, b=p),
    title="First 5 neuron acts", xaxis="b", yaxis="a", facet_col=0)
```

```python
imshow(
    einops.rearrange(neuron_acts[:, 0], "(a b) -> a b", a=p, b=p),
    title="First neuron act", xaxis="b", yaxis="a",)
```

```python
imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title="Cos 47a * cos 47b")
```

```python
imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title="Cos 47a * const")
```

```python
imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of neuron 0", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

```python
imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of neuron 5", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

```python
imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of RANDOM", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

### Neuron Clusters

```python
fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, "(a b) neuron -> neuron a b", a=p, b=p) @ fourier_basis.T
# Center these by removing the mean - doesn't matter!
fourier_neuron_acts[:, 0, 0] = 0.
print("fourier_neuron_acts", fourier_neuron_acts.shape)
```

```python
neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)
for freq in range(0, p//2):
    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:
        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:
            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2
neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]
imshow(neuron_freq_norm, xaxis="Neuron", yaxis="Freq", y=torch.arange(1, p//2+1), title="Neuron Frac Explained by Freq")
```

```python
line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis="Neuron", title="Max Neuron Frac Explained over Freqs")
```

## Read Off the Neuron-Logit Weights to Interpret

```python
W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U
print("W_logit", W_logit.shape)
```

```python
line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title="W_logit in the Fourier Basis")
```

```python
neurons_17 = neuron_freq_norm[17-1]>0.85
neurons_17.shape
```

```python
neurons_17.sum()
```

```python
line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title="W_logit for freq 17 neurons in the Fourier Basis")
```

Study sin 17

```python
freq = 17
W_logit_fourier = W_logit @ fourier_basis
neurons_sin_17 = W_logit_fourier[:, 2*freq-1]
line(neurons_sin_17)
```

```python
neuron_acts.shape
```

```python
inputs_sin_17c = neuron_acts @ neurons_sin_17
imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title="Fourier Heatmap over inputs for sin17c", x=fourier_basis_names, y=fourier_basis_names)
```

# Black Box Methods + Progress Measures

## Setup Code

Code to plot embedding freqs

```python
def embed_to_cos_sin(fourier_embed):
    if len(fourier_embed.shape) == 1:
        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])
    else:
        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)

from neel_plotly.plot import melt

def plot_embed_bars(
    fourier_embed,
    title="Norm of embedding of each Fourier Component",
    return_fig=False,
    **kwargs
):
    cos_sin_embed = embed_to_cos_sin(fourier_embed)
    df = melt(cos_sin_embed)
    # display(df)
    group_labels = {0: "sin", 1: "cos"}
    df["Trig"] = df["0"].map(lambda x: group_labels[x])
    fig = px.bar(
        df,
        barmode="group",
        color="Trig",
        x="1",
        y="value",
        labels={"1": "$w_k$", "value": "Norm"},
        title=title,
        **kwargs
    )
    fig.update_layout(dict(legend_title=""))

    if return_fig:
        return fig
    else:
        fig.show()
```

Code to test a tensor of edited logits

```python
def test_logits(logits, bias_correction=False, original_logits=None, mode="all"):
    # Calculates cross entropy loss of logits representing a batch of all p^2
    # possible inputs
    # Batch dimension is assumed to be first
    if logits.shape[1] == p * p:
        logits = logits.T
    if logits.shape == torch.Size([p * p, p + 1]):
        logits = logits[:, :-1]
    logits = logits.reshape(p * p, p)
    if bias_correction:
        # Applies bias correction - we correct for any missing bias terms,
        # independent of the input, by centering the new logits along the batch
        # dimension, and then adding the average original logits across all inputs
        logits = (
            einops.reduce(original_logits - logits, "batch ... -> ...", "mean") + logits
        )
    if mode == "train":
        return loss_fn(logits[train_indices], labels[train_indices])
    elif mode == "test":
        return loss_fn(logits[test_indices], labels[test_indices])
    elif mode == "all":
        return loss_fn(logits, labels)
```

Code to run a metric over every checkpoint

```python
metric_cache = {}
```

```python
def get_metrics(model, metric_cache, metric_fn, name, reset=False):
    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):
        metric_cache[name] = []
        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):
            model.reset_hooks()
            model.load_state_dict(sd)
            out = metric_fn(model)
            if type(out) == torch.Tensor:
                out = utils.to_numpy(out)
            metric_cache[name].append(out)
        model.load_state_dict(model_checkpoints[-1])
        try:
            metric_cache[name] = torch.tensor(metric_cache[name])
        except:
            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))

```

## Defining Progress Measures

### Loss Curves

```python
memorization_end_epoch = 1500
circuit_formation_end_epoch = 13300
cleanup_end_epoch = 16600
```

```python
def add_lines(figure):
    figure.add_vline(memorization_end_epoch, line_dash="dash", opacity=0.7)
    figure.add_vline(circuit_formation_end_epoch, line_dash="dash", opacity=0.7)
    figure.add_vline(cleanup_end_epoch, line_dash="dash", opacity=0.7)
    return figure
```

```python
fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Training Curve for Modular Addition", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)
add_lines(fig)
```

### Logit Periodicity

```python
all_logits = original_logits[:, -1, :]
print(all_logits.shape)
all_logits = einops.rearrange(all_logits, "(a b) c -> a b c", a=p, b=p)
print(all_logits.shape)
```

```python
coses = {}
for freq in key_freqs:
    print("Freq:", freq)
    a = torch.arange(p)[:, None, None]
    b = torch.arange(p)[None, :, None]
    c = torch.arange(p)[None, None, :]
    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)
    cube_predicted_logits /= cube_predicted_logits.norm()
    coses[freq] = cube_predicted_logits
```

```python
approximated_logits = torch.zeros_like(all_logits)
for freq in key_freqs:
    print("Freq:", freq)
    coeff = (all_logits * coses[freq]).sum()
    print("Coeff:", coeff)
    cosine_sim = coeff / all_logits.norm()
    print("Cosine Sim:", cosine_sim)
    approximated_logits += coeff * coses[freq]
residual = all_logits - approximated_logits
print("Residual size:", residual.norm())
print("Residual fraction of norm:", residual.norm()/all_logits.norm())
```

```python
random_logit_cube = torch.randn_like(all_logits)
print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())
```

```python
test_logits(all_logits)
```

```python
test_logits(approximated_logits)
```

#### Look During Training

```python
cos_cube = []
for freq in range(1, p//2 + 1):
    a = torch.arange(p)[:, None, None]
    b = torch.arange(p)[None, :, None]
    c = torch.arange(p)[None, None, :]
    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)
    cube_predicted_logits /= cube_predicted_logits.norm()
    cos_cube.append(cube_predicted_logits)
cos_cube = torch.stack(cos_cube, dim=0)
print(cos_cube.shape)
```

```python
def get_cos_coeffs(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    return vals

get_metrics(model, metric_cache, get_cos_coeffs, "cos_coeffs")
print(metric_cache["cos_coeffs"].shape)
```

```python
fig = line(metric_cache["cos_coeffs"].T, line_labels=[f"Freq {i}" for i in range(1, p//2+1)], title="Coefficients with Predicted Logits", xaxis="Epoch", x=checkpoint_epochs, yaxis="Coefficient", return_fig=True)
add_lines(fig)
```

```python
def get_cos_sim(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    return vals / logits.norm()

get_metrics(model, metric_cache, get_cos_sim, "cos_sim") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!
print(metric_cache["cos_sim"].shape)

fig = line(metric_cache["cos_sim"].T, line_labels=[f"Freq {i}" for i in range(1, p//2+1)], title="Cosine Sim with Predicted Logits", xaxis="Epoch", x=checkpoint_epochs, yaxis="Cosine Sim", return_fig=True)
add_lines(fig)
```

```python
def get_residual_cos_sim(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)
    return residual.norm() / logits.norm()

get_metrics(model, metric_cache, get_residual_cos_sim, "residual_cos_sim")
print(metric_cache["residual_cos_sim"].shape)

fig = line([metric_cache["cos_sim"][:, i] for i in range(p//2)]+[metric_cache["residual_cos_sim"]], line_labels=[f"Freq {i}" for i in range(1, p//2+1)]+["residual"], title="Cosine Sim with Predicted Logits + Residual", xaxis="Epoch", x=checkpoint_epochs, yaxis="Cosine Sim", return_fig=True)
add_lines(fig)
```

## Restricted Loss

```python
neuron_acts.shape
```

```python
neuron_acts_square = einops.rearrange(neuron_acts, "(a b) neur -> a b neur", a=p, b=p).clone()
# Center it
neuron_acts_square -= einops.reduce(neuron_acts_square, "a b neur -> 1 1 neur", "mean")
neuron_acts_square_fourier = einsum("a b neur, fa a, fb b -> fa fb neur", neuron_acts_square, fourier_basis, fourier_basis)
imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis="Fourier Component b", yaxis="Fourier Component a", title="Norms of neuron activations by Fourier Component", x=fourier_basis_names, y=fourier_basis_names)
```

```python
original_logits, cache = model.run_with_cache(dataset)
print(original_logits.numel())
neuron_acts = cache["post", 0, "mlp"][:, -1, :]
```

```python
approx_neuron_acts = torch.zeros_like(neuron_acts)
approx_neuron_acts += neuron_acts.mean(dim=0)
a = torch.arange(p)[:, None]
b = torch.arange(p)[None, :]
for freq in key_freqs:
    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
    cos_apb_vec /= cos_apb_vec.norm()
    cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
    sin_apb_vec /= sin_apb_vec.norm()
    sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
restricted_logits = approx_neuron_acts @ W_logit
print(loss_fn(restricted_logits[test_indices], test_labels))
```

```python
print(loss_fn(all_logits, labels)) # This bugged on models not fully trained
```

### Look During Training

```python
def get_restricted_loss(model):
    logits, cache = model.run_with_cache(dataset)
    logits = logits[:, -1, :]
    neuron_acts = cache["post", 0, "mlp"][:, -1, :]
    approx_neuron_acts = torch.zeros_like(neuron_acts)
    approx_neuron_acts += neuron_acts.mean(dim=0)
    a = torch.arange(p)[:, None]
    b = torch.arange(p)[None, :]
    for freq in key_freqs:
        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
        cos_apb_vec /= cos_apb_vec.norm()
        cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
        sin_apb_vec /= sin_apb_vec.norm()
        sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U
    # Add bias term
    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)
    return loss_fn(restricted_logits[test_indices], test_labels)
get_restricted_loss(model)
```

```python
get_metrics(model, metric_cache, get_restricted_loss, "restricted_loss", reset=True)
print(metric_cache["restricted_loss"].shape)
```

```python
fig = line([train_losses[::100], test_losses[::100], metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Restricted Loss Curve", line_labels=['train', 'test', "restricted_loss"], toggle_x=True, toggle_y=True, return_fig=True)
add_lines(fig)
```

```python
fig = line([torch.tensor(test_losses[::100])/metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Restricted Loss to Test Loss Ratio", toggle_x=True, toggle_y=True, return_fig=True)
# WARNING: bugged when cancelling training half way thr ough
add_lines(fig)
```

## Excluded Loss

```python
approx_neuron_acts = torch.zeros_like(neuron_acts)
# approx_neuron_acts += neuron_acts.mean(dim=0)
a = torch.arange(p)[:, None]
b = torch.arange(p)[None, :]
for freq in key_freqs:
    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
    cos_apb_vec /= cos_apb_vec.norm()
    cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
    sin_apb_vec /= sin_apb_vec.norm()
    sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
excluded_neuron_acts = neuron_acts - approx_neuron_acts
excluded_logits = excluded_neuron_acts @ W_logit
print(loss_fn(excluded_logits[train_indices], train_labels))
```

```python
def get_excluded_loss(model):
    logits, cache = model.run_with_cache(dataset)
    logits = logits[:, -1, :]
    neuron_acts = cache["post", 0, "mlp"][:, -1, :]
    approx_neuron_acts = torch.zeros_like(neuron_acts)
    # approx_neuron_acts += neuron_acts.mean(dim=0)
    a = torch.arange(p)[:, None]
    b = torch.arange(p)[None, :]
    for freq in key_freqs:
        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
        cos_apb_vec /= cos_apb_vec.norm()
        cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
        sin_apb_vec /= sin_apb_vec.norm()
        sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
    excluded_neuron_acts = neuron_acts - approx_neuron_acts
    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache["resid_mid", 0][:, -1, :]
    excluded_logits = residual_stream_final @ model.unembed.W_U
    return loss_fn(excluded_logits[train_indices], train_labels)
get_excluded_loss(model)
```

```python
get_metrics(model, metric_cache, get_excluded_loss, "excluded_loss", reset=True)
print(metric_cache["excluded_loss"].shape)
```

```python
fig = line([train_losses[::100], test_losses[::100], metric_cache["excluded_loss"], metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Excluded and Restricted Loss Curve", line_labels=['train', 'test', "excluded_loss", "restricted_loss"], toggle_x=True, toggle_y=True, return_fig=True)

add_lines(fig)
```

---

# Head_Detector_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# TransformerLens Head Detector Demo

A common technique in mechanistic interpretability of transformer-based neural networks is identification of specialized attention heads, based on the attention patterns elicited by one or more prompts. The most basic examples of such heads are: previous token head, duplicate token head, or induction head ([more info](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ)). Usually, such heads are identified manually, by through visualizations of attention patterns layer by layer, head by head, and trying to recognize the patterns by eye.

The purpose of the `TransformerLens.head_detector` feature is to automate a part of that workflow. The pattern characterizing a head of particular type/function is specified as a `Tensor` being a `seq_len x seq_len` [lower triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix). It can be either passed to the `detect_head` function directly or by giving a string identifying of several pre-defined detection patterns.

## How to use this notebook

Go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.

Tips for reading this Colab:

* You can run all this code for yourself!
* The graphs are interactive!
* Use the table of contents pane in the sidebar to navigate
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar, not CTRL+F

## Setup (Ignore)

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git
    # Install Neel's personal plotting utils
    %pip install git+https://github.com/neelnanda-io/neel-plotly.git
    # Install another version of node that makes PySvelte work way faster
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    %pip install git+https://github.com/neelnanda-io/PySvelte.git
    # Needed for PySvelte to work, v3 came out and broke things...
    %pip install typeguard==2.13.3
    %pip install typing-extensions
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEBUG_MODE:
    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "png"
```

```python
import torch
import einops
import pysvelte
from tqdm import tqdm

import transformer_lens
from transformer_lens import HookedTransformer, ActivationCache
from neel_plotly import line, imshow, scatter
```

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device = }")
```

### Some plotting utils

```python
# Util for plotting head detection scores

def plot_head_detection_scores(
    scores: torch.Tensor,
    zmin: float = -1,
    zmax: float = 1,
    xaxis: str = "Head",
    yaxis: str = "Layer",
    title: str = "Head Matches"
) -> None:
    imshow(scores, zmin=zmin, zmax=zmax, xaxis=xaxis, yaxis=yaxis, title=title)

def plot_attn_pattern_from_cache(cache: ActivationCache, layer_i: int):
    attention_pattern = cache["pattern", layer_i, "attn"].squeeze(0)
    attention_pattern = einops.rearrange(attention_pattern, "heads seq1 seq2 -> seq1 seq2 heads")
    print(f"Layer {layer_i} Attention Heads:")
    return pysvelte.AttentionMulti(tokens=model.to_str_tokens(prompt), attention=attention_pattern)
```

## Head detector

Utils: these will be in `transformer_lens.utils` after merging the fork to the main repo

```python
def is_square(x: torch.Tensor) -> bool:
    """Checks if `x` is a square matrix."""
    return x.ndim == 2 and x.shape[0] == x.shape[1]

def is_lower_triangular(x: torch.Tensor) -> bool:
    """Checks if `x` is a lower triangular matrix."""
    if not is_square(x):
        return False
    return x.equal(x.tril())
```

The code below is copy-pasted from the expanded (not yet merged) version of `transformer_lens.head_detector`.

After merging the code below can be replaced with simply

```py
from transformer_lens.head_detector import *
```

(but please don't use star-imports in production ;))

```python
from collections import defaultdict
import logging
from typing import cast, Dict, List, Optional, Tuple, Union
from typing_extensions import get_args, Literal

import numpy as np
import torch

from transformer_lens import HookedTransformer, ActivationCache
# from transformer_lens.utils import is_lower_triangular, is_square

HeadName = Literal["previous_token_head", "duplicate_token_head", "induction_head"]
HEAD_NAMES = cast(List[HeadName], get_args(HeadName))
ErrorMeasure = Literal["abs", "mul"]

LayerHeadTuple = Tuple[int, int]
LayerToHead = Dict[int, List[int]]

INVALID_HEAD_NAME_ERR = (
    f"detection_pattern must be a Tensor or one of head names: {HEAD_NAMES}; got %s"
)

SEQ_LEN_ERR = (
    "The sequence must be non-empty and must fit within the model's context window."
)

DET_PAT_NOT_SQUARE_ERR = "The detection pattern must be a lower triangular matrix of shape (sequence_length, sequence_length); sequence_length=%d; got detection patern of shape %s"

def detect_head(
    model: HookedTransformer,
    seq: Union[str, List[str]],
    detection_pattern: Union[torch.Tensor, HeadName],
    heads: Optional[Union[List[LayerHeadTuple], LayerToHead]] = None,
    cache: Optional[ActivationCache] = None,
    *,
    exclude_bos: bool = False,
    exclude_current_token: bool = False,
    error_measure: ErrorMeasure = "mul",
) -> torch.Tensor:
    """Searches the model (or a set of specific heads, for circuit analysis) for a particular type of attention head.
    This head is specified by a detection pattern, a (sequence_length, sequence_length) tensor representing the attention pattern we expect that type of attention head to show.
    The detection pattern can be also passed not as a tensor, but as a name of one of pre-specified types of attention head (see `HeadName` for available patterns), in which case the tensor is computed within the function itself.

    There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern.

    1. `"mul"` (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.
    Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score:
    how big fraction of this head's attention is allocated to these specific query-key pairs?
    Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).
    2. `"abs"` calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.
    The "raw result" ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval,
    with 1 being perfect match and -1 perfect mismatch.

    **Which one should you use?** `"abs"` is likely better for quick or exploratory investigations. For precise examinations where you're trying to
    reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to `"abs"`.

    The advantage of `"abs"` is that you can make more precise predictions, and have that measured in the score.
    You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer.
    The "mul" metric does not allow this, you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.

    Args:
    ----------
        model: Model being used.
        seq: String or list of strings being fed to the model.
        head_name: Name of an existing head in HEAD_NAMES we want to check. Must pass either a head_name or a detection_pattern, but not both!
        detection_pattern: (sequence_length, sequence_length) Tensor representing what attention pattern corresponds to the head we're looking for **or** the name of a pre-specified head. Currently available heads are: `["previous_token_head", "duplicate_token_head", "induction_head"]`.
        heads: If specific attention heads is given here, all other heads' score is set to -1. Useful for IOI-style circuit analysis. Heads can be spacified as a list tuples (layer, head) or a dictionary mapping a layer to heads within that layer that we want to analyze.
        cache: Include the cache to save time if you want.
        exclude_bos: Exclude attention paid to the beginning of sequence token.
        exclude_current_token: Exclude attention paid to the current token.
        error_measure: `"mul"` for using element-wise multiplication (default). `"abs"` for using absolute values of element-wise differences as the error measure.

    Returns:
    ----------
    A (n_layers, n_heads) Tensor representing the score for each attention head.

    Example:
    --------
    .. code-block:: python

        >>> from transformer_lens import HookedTransformer,  utils
        >>> from transformer_lens.head_detector import detect_head
        >>> import plotly.express as px

        >>> def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
        >>>     px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

        >>> model = HookedTransformer.from_pretrained("gpt2-small")
        >>> sequence = "This is a test sequence. This is a test sequence."

        >>> attention_score = detect_head(model, sequence, "previous_token_head")
        >>> imshow(attention_score, zmin=-1, zmax=1, xaxis="Head", yaxis="Layer", title="Previous Head Matches")
    """

    cfg = model.cfg
    tokens = model.to_tokens(seq).to(cfg.device)
    seq_len = tokens.shape[-1]

    # Validate error_measure

    assert error_measure in get_args(ErrorMeasure), f"Invalid {error_measure=}; valid values are {get_args(ErrorMeasure)}"

    # Validate detection pattern if it's a string
    if isinstance(detection_pattern, str):
        assert detection_pattern in HEAD_NAMES, (
            INVALID_HEAD_NAME_ERR % detection_pattern
        )
        if isinstance(seq, list):
            batch_scores = [detect_head(model, seq, detection_pattern) for seq in seq]
            return torch.stack(batch_scores).mean(0)
        detection_pattern = cast(
            torch.Tensor,
            eval(f"get_{detection_pattern}_detection_pattern(tokens.cpu())"),
        ).to(cfg.device)

    # if we're using "mul", detection_pattern should consist of zeros and ones
    if error_measure == "mul" and not set(detection_pattern.unique().tolist()).issubset(
        {0, 1}
    ):
        logging.warning(
            "Using detection pattern with values other than 0 or 1 with error_measure 'mul'"
        )

    # Validate inputs and detection pattern shape
    assert 1 < tokens.shape[-1] < cfg.n_ctx, SEQ_LEN_ERR
    assert (
        is_lower_triangular(detection_pattern) and seq_len == detection_pattern.shape[0]
    ), DET_PAT_NOT_SQUARE_ERR % (seq_len, detection_pattern.shape)

    if cache is None:
        _, cache = model.run_with_cache(tokens, remove_batch_dim=True)

    if heads is None:
        layer2heads = {
            layer_i: list(range(cfg.n_heads)) for layer_i in range(cfg.n_layers)
        }
    elif isinstance(heads, list):
        layer2heads = defaultdict(list)
        for layer, head in heads:
            layer2heads[layer].append(head)
    else:
        layer2heads = heads

    matches = -torch.ones(cfg.n_layers, cfg.n_heads)

    for layer, layer_heads in layer2heads.items():
        # [n_heads q_pos k_pos]
        layer_attention_patterns = cache["pattern", layer, "attn"]
        for head in layer_heads:
            head_attention_pattern = layer_attention_patterns[head, :, :]
            head_score = compute_head_attention_similarity_score(
                head_attention_pattern,
                detection_pattern=detection_pattern,
                exclude_bos=exclude_bos,
                exclude_current_token=exclude_current_token,
                error_measure=error_measure,
            )
            matches[layer, head] = head_score
    return matches

# Previous token head
def get_previous_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [previous token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc).

    Args:
      tokens: Tokens being fed to the model.
    """
    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])
    # Adds a diagonal of 1's below the main diagonal.
    detection_pattern[1:, :-1] = torch.eye(tokens.shape[-1] - 1)
    return torch.tril(detection_pattern)

# Duplicate token head
def get_duplicate_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [duplicate token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo).

    Args:
      sequence: String being fed to the model.
    """
    # [pos x pos]
    token_pattern = tokens.repeat(tokens.shape[-1], 1).numpy()

    # If token_pattern[i][j] matches its transpose, then token j and token i are duplicates.
    eq_mask = np.equal(token_pattern, token_pattern.T).astype(int)

    np.fill_diagonal(
        eq_mask, 0
    )  # Current token is always a duplicate of itself. Ignore that.
    detection_pattern = eq_mask.astype(int)
    return torch.tril(torch.as_tensor(detection_pattern).float())

# Induction head
def get_induction_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY).

    Args:
      sequence: String being fed to the model.
    """
    duplicate_pattern = get_duplicate_token_head_detection_pattern(tokens)

    # Shift all items one to the right
    shifted_tensor = torch.roll(duplicate_pattern, shifts=1, dims=1)

    # Replace first column with 0's
    # we don't care about bos but shifting to the right moves the last column to the first,
    # and the last column might contain non-zero values.
    zeros_column = torch.zeros(duplicate_pattern.shape[0], 1)
    result_tensor = torch.cat((zeros_column, shifted_tensor[:, 1:]), dim=1)
    return torch.tril(result_tensor)

def get_supported_heads() -> None:
    """Returns a list of supported heads."""
    print(f"Supported heads: {HEAD_NAMES}")

def compute_head_attention_similarity_score(
    attention_pattern: torch.Tensor,  # [q_pos k_pos]
    detection_pattern: torch.Tensor,  # [seq_len seq_len] (seq_len == q_pos == k_pos)
    *,
    exclude_bos: bool,
    exclude_current_token: bool,
    error_measure: ErrorMeasure,
) -> float:
    """Compute the similarity between `attention_pattern` and `detection_pattern`.

    Args:
      attention_pattern: Lower triangular matrix (Tensor) representing the attention pattern of a particular attention head.
      detection_pattern: Lower triangular matrix (Tensor) representing the attention pattern we are looking for.
      exclude_bos: `True` if the beginning-of-sentence (BOS) token should be omitted from comparison. `False` otherwise.
      exclude_bcurrent_token: `True` if the current token at each position should be omitted from comparison. `False` otherwise.
      error_measure: "abs" for using absolute values of element-wise differences as the error measure. "mul" for using element-wise multiplication (legacy code).
    """
    assert is_square(
        attention_pattern
    ), f"Attention pattern is not square; got shape {attention_pattern.shape}"

    # mul

    if error_measure == "mul":
        if exclude_bos:
            attention_pattern[:, 0] = 0
        if exclude_current_token:
            attention_pattern.fill_diagonal_(0)
        score = attention_pattern * detection_pattern
        return (score.sum() / attention_pattern.sum()).item()

    # abs

    abs_diff = (attention_pattern - detection_pattern).abs()
    assert (abs_diff - torch.tril(abs_diff).to(abs_diff.device)).sum() == 0

    size = len(abs_diff)
    if exclude_bos:
        abs_diff[:, 0] = 0
    if exclude_current_token:
        abs_diff.fill_diagonal_(0)

    return 1 - round((abs_diff.mean() * size).item(), 3)

```

## Using Head Detector For Premade Heads

Load the model

```python
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
```

See what heads are supported out of the box

```python
get_supported_heads()
```

Let's test detecting previous token head in the following prompt.

```python
prompt = "The head detector feature for TransformerLens allows users to check for various common heads automatically, reducing the cost of discovery."
head_scores = detect_head(model, prompt, "previous_token_head")
plot_head_detection_scores(head_scores, title="Previous Head Matches")
```

We can see both L2H2 and L4H11 are doing a fair bit of previous token detection. Let's take a look and see if that pans out.

```python
_, cache = model.run_with_cache(prompt)
```

```python
plot_attn_pattern_from_cache(cache, 2)
```

```python
plot_attn_pattern_from_cache(cache, 4)
```

As we expected, L2H2 is doing a lot of previous token detection, but doesn't appear to be a sharp previous token detection head. L4H11, on the other hand, is pretty much perfect. In fact, the only place it seems to be putting any other attention is the very first token, where it pays attention to the BOS (*beginning-of-sentence*) token.

Mechanistic interpretability is still a very new field, and we don't know the best ways to measure things yet. Ignoring attention paid to BOS allows us to solve problems like the above, but may also give us artifically high results for a head like L4H10, which doesn't appear to be doing much of anything, but does have a bit of previous token attention going on if you squint carefully.

As such, the head detector supports both an `exclude_bos` and `exclude_current_token` argument, which ignores all BOS attention and all current token attention respectively. By default these are `False`, but this is a pretty arbitrary decision, so feel free to try things out! You don't need a good reason to change these arguments - pick whatever best helps you find out useful things!

```python
head_scores = detect_head(model, prompt, "previous_token_head", exclude_bos=True, exclude_current_token=True)
plot_head_detection_scores(head_scores, title="Previous Head Matches")
```

Now we have a lot more detection, including L0H3 and L5H6 which were unremarkable before. Let's check them out!

```python
plot_attn_pattern_from_cache(cache, 5)
```

```python
plot_attn_pattern_from_cache(cache, 0)
```

Here, we see some interesting results. L5H6 does very little, but happens to react quite strongly to the first token of "Trans|former". (Capital letters? Current word detection? We don't know)

L0H3 reacts almost entirely to the current token, but what little it does outside of this pays attention to the previous token. Again, it seems to be caring about the first token of "Trans|former".

In order to more fully automate these heads, we'll need to discover more principled ways of expressing these scores. For now, you can see how while scores may be misleading, different scores lead us to interesting results.

## Using Head Detector for Custom Heads

These heads are great, but sometimes there are more than three things going on in Transformers. [citation needed] As a result, we may want to use our head detector for things that aren't pre-included in TransformerLens. Fortunately, the head detector provides support for this, via **detection patterns**.

A detection pattern is simply a matrix of the same size as our attention pattern, which specifies the attention pattern exhibited by the kind of head we're looking for.

There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern. You can choose it by passing the right value to the `error_measure` argument.

### 1. `"mul"` (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.

Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score: how big fraction of this head's attention is allocated to these specific query-key pairs? Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).

<br>

$$
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0.5 & 0.5 & 0 & 0 \\
0.2 & 0.3 & 0.5 & 0 \\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
\odot
\begin{pmatrix}
0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 & 0 & 0 \\
0.5 & 0 & 0 & 0 \\
0 & 0.3 & 0 & 0 \\
0 & 0 & 0.5 & 0
\end{pmatrix}
$$

<br>

0.5, 0.3, and 0.5 all get multiplied by 1, so they get kept. All the others go to 0 and are removed. (Note: You can use values other than 0 or 1 when creating your own heads)

Our total score would then be 1.3 / 4, or 0.325. If we ignore bos and current token, it would be 0.8 / 0.95 instead, or ~0.842. (This is a large difference, but the difference generally gets smaller as the matrices get bigger)

This is how the head detector works under the hood - each existing head just has its own detection pattern. Thus, we can pass in our own detection pattern using the `detection_pattern` argument.

### 2. `"abs"` calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.

The "raw result" ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval, with 1 being perfect match and -1 perfect mismatch.

We take the attention pattern and compute its absolute element-wise difference with our detection pattern. Since every number in any of the two patterns has a value between -1 and 1, the maximum absolute difference of any pair is 2 and the minimum is 0:

$$|-1-1|=|1-(-1)|=2$$

$$|x-x|=0$$

That number tells us how much our expectation and the real attention pattern diverge, i.e., the error.

$$
M_{diff}=
\left|
\begin{pmatrix}
1 & 0 & 0 & 0
\\
0.5 & 0.5 & 0 & 0
\\
0.2 & 0.3 & 0.5 & 0
\\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
-
\begin{pmatrix}
0 & 0 & 0 & 0
\\
1 & 0 & 0 & 0
\\
0 & 1 & 0 & 0
\\
0 & 0 & 1 & 0
\end{pmatrix}
\right|
=
\begin{pmatrix}
1 & 0 & 0 & 0
\\
0.5 & 0.5 & 0 & 0
\\
0.2 & 0.7 & 0.5 & 0
\\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
$$

We take the mean and multiply it by the number of rows.

We subtract the result from 1 in order to map the (0, 2) interval where lower is better to the (-1, 1) interval where higher is better.

$$1 - \text{n_rows} \times \text{mean}(M_{diff}) = 1 - 4 \times 0.275 = 1 - 1.1 = -.1$$

Our final score would then be -1. If we ignore `BOS` and current token, it would be 0.6625. (This is a large difference, but the difference generally gets smaller as the matrices get bigger.)

This is how the head detector works under the hood - each existing head just has its own detection pattern. Thus, we can pass in our own detection pattern using the `detection_pattern` argument.

I'm curious what's going on with this L0H3 result, where we mostly focus on the current token but occasionally focus on the "Trans" token in "Trans|former". Let's make a **current word head** detection pattern, which returns 1 for previous tokens that are part of the current word being looked at, and 0 for everything else.

### **Which one should you use?**

`"abs"` is likely better for quick or exploratory investigations. For precise examinations where you're trying to reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to `"abs"`.

The advantage of `"abs"` is that you can make more precise predictions, and have that measured in the score. You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer. The "mul" metric does not allow this, you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.

Below we show how different scores these two measures can give on the same prompt. After that, we will proceed with using `"abs"` and will get back to `"mul"` at the end of the notebook.

```python
prompt = "The following lexical sequence has been optimised for the maximisation of loquaciously multitoken letter combinations."
tokens = model.to_str_tokens(prompt)
print(len(tokens), tokens)
detection_pattern = []
for i in range(2):
  detection_pattern.append([0 for t in tokens]) # Ignore BOS token and first token.
for i in range(2, len(tokens)):
    current_token = i
    previous_tokens_in_word = 0
    while not tokens[current_token].startswith(' '): # If the current token does not start with a space (and is not the first token) it's part of a word.
      previous_tokens_in_word += 1
      current_token -= 1
    # Hacky code that adds in some 1's where needed, and fills the rest of the row with 0's.
    detection_pattern.append([0 for j in range(i - previous_tokens_in_word)] + [1 for j in range(previous_tokens_in_word)] + [0 for j in range(i+1, len(tokens)+1)])
detection_pattern = torch.as_tensor(detection_pattern).to(device)
detection_pattern.shape
```

```python
_, cache = model.run_with_cache(prompt)
```

`"mul"`

```python
head_scores = detect_head(
    model,
    prompt,
    detection_pattern=detection_pattern,
    exclude_bos=False,
    exclude_current_token=True,
    error_measure="mul"
)
plot_head_detection_scores(head_scores, title="Current Word Head Matches (mul)")
```

`"abs"`

```python
head_scores = detect_head(
    model,
    prompt,
    detection_pattern=detection_pattern,
    exclude_bos=False,
    exclude_current_token=True,
    error_measure="abs"
)
plot_head_detection_scores(head_scores, title="Current Word Head Matches (abs)")
```

75% match for L0H3 - only 16% for L5H6. Let's check them out with our new sequence!

```python
plot_attn_pattern_from_cache(cache, 5)
```

```python
plot_attn_pattern_from_cache(cache, 0)
```

As we can see, L5H6 appears to be doing something totally different than we expected, whereas L0H3 is mostly doing what we expected - by our original hypothesis, we would expect "lo|qu|aciously" to have a lot of attention paid to, and "combinations|." the same, which didn't happen. However, our two-token words were exactly as we expected. Could this be a two-token detector (that doesn't work on punctuation)? A "current word" detector that just doesn't understand an obscure word like "loquaciously"? The field is full of such problems, just waiting to be answered!

So, why do this at all? For just a couple of sentences, it's easier to just look at the attention patterns directly and see what we get. But as we can see, heads react differently to different sentences. What we might want to do is give an entire dataset or distribution of sentences to our attention head and see that it consistently does what we want - that's something that would be much harder without this feature!

So what if we gave it a whole distribution? Rather than actually create one, which is not the point of this demo, we're just going to repeat our last sentence a thousand times.

```python
scores = []
for i in tqdm(range(100)):
    scores.append(detect_head(model, prompt, detection_pattern=detection_pattern, exclude_bos=False, exclude_current_token=True, error_measure="abs"))
scores = torch.stack(scores).mean(dim=0)
plot_head_detection_scores(scores, title="Current Word Head Matches")
```

## Processing Many Prompts

`detect_head` can also take more than one prompt. The resulting attention score is the mean of scores for each prompt.

```python
prompts = [
    "This is the first the test prompt.",
    "This is another test prompt, being just a sequence of tokens.",
    "If you're interested in mechanistic interpretability, this is how the sausage REALLY is made."
]
```

```python
head_scores = detect_head(model, prompts, "previous_token_head", error_measure="abs")
plot_head_detection_scores(head_scores, title="Previous token head; average across 3 prompts")
```

L4H11 emerges again as the dominant head, exactly as expected.

What about duplicate token heads?

```python
head_scores = detect_head(model, prompts, "duplicate_token_head", error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

Nothing but this should be expected, in hindsight, since our prompts don't contain too many duplicate tokens. Let's try three other prompts that do.

```python
prompts = [
    "one two three one two three one two three",
    "1 2 3 4 5 1 2 3 4 1 2 3 1 2 3 4 5 6 7",
    "green ideas sleep furiously; green ideas don't sleep furiously"
]
```

```python
head_scores = detect_head(model, prompts, "duplicate_token_head", exclude_bos=False, exclude_current_token=False, error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

3 or 4 heads seem to do something that we would expected from a duplicate token head but the signal is not very strong. You can tweak the `exclude_bos` and `exclude_current_token` flags if you want, but it doesn't change much.

Let's hunt for induction heads now!

```python
head_scores = detect_head(model, prompts, "induction_head", exclude_bos=False, exclude_current_token=False, error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

Similarly, at least on average.

Try running the script on different prompts and see if you can get high values for duplicate token or induction heads.

## Why not element-wise multiplication - robustness against [Goodharting](https://en.wikipedia.org/wiki/Goodhart%27s_law)

Initially, the error measure was not the mean element-wise absolute value error (normalized to the number of rows) but the mean [element-wise product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)). However, it had its problems, such as susceptibility to Goodharting. You can specify a pattern consisting of all ones and in this way achieve a perfect match for all layers and heads in the model.

More generally, using element-wise product causes the score to go down when we narrow our hypothesis. We can get a maximum score by just predicting 1 for everything.

```python
prompt = "The head detector feature for TransformerLens allows users to check for various common heads automatically, reducing the cost of discovery."
seq_len = len(model.to_str_tokens(prompt))
# torch.tril to make the pattern lower triangular
ones_detection_pattern = torch.tril(torch.ones(seq_len, seq_len).to(device))
```

```python
ones_head_scores = detect_head(
    model,
    prompt,
    ones_detection_pattern,
    exclude_bos=True,
    exclude_current_token=True,
)
plot_head_detection_scores(ones_head_scores, title="Transformers Have Now Been Solved, We Can All Go Home")
```

The new error measure also achieves uniform score but this time its uniformly extremely negative because **not a single head in the model matches this pattern**.

*(It's true that the scores descend below -9 whereas in theory they should remain within the (-1, 1) range. It's not yet clear if that matters for real-world uses.)*

An alternative would be to demand that *predictions add up to 1 for each row* but that seems unnecessarily nitpicky considering that your score will get reduced in general for not doing that anyway.

Mean squared errors have also bean tried before converging on the absolute ones. The problem with MSE is that the scores get lower as attention gets more diffuse. Error value of 1 would become 1, 0.5 would become 0.25 etc.

```python
ones_head_scores = detect_head(
    model,
    prompt,
    ones_detection_pattern,
    exclude_bos=True,
    exclude_current_token=True,
    error_measure="abs" # we specify the error measure here
)
plot_head_detection_scores(ones_head_scores, title="Transformers Have Not Been Solved Yet, Get Back To Work!")
```

## Further improvements

**Performance for large distributions** isn't as good as it could be. The head detector could be rewritten to support taking in a list of sequences and performing these computations in parallel, but 1000 sequences per minute is certainly adequate for most use cases. If having this be faster would help your research, please write up an issue on TransformerLens, mention it on the Open Source Mechanistic Interpretability Slack, or e-mail jaybaileycs@gmail.com.

### Other

- Extending to few-shot learning/translation heads
- More pre-specified heads?
- For inspiration, see [this post from Neel](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/btasQF7wiCYPsr5qw)

---

# Interactive_Neuroscope.ipynb

<a target="_blank" href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Interactive_Neuroscope.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Interactive Neuroscope

*This is an interactive accompaniment to [neuroscope.io](https://neuroscope.io) and to the [studying learned language features post](https://www.alignmentforum.org/posts/Qup9gorqpd9qKAEav/200-cop-in-mi-studying-learned-features-in-language-models) in [200 Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems)*

There's a surprisingly rich ecosystem of easy ways to create interactive graphics, especially for ML systems. If you're trying to do mechanistic interpretability, the ability to do web dev and to both visualize data and interact with it seems high value!

This is a demo of how you can combine HookedTransformer and [Gradio](https://gradio.app/) to create an interactive Neuroscope - a visualization of a neuron's activations on text that will dynamically update as you edit the text. I don't particularly claim that this code is any *good*, but the goal is to illustrate what quickly hacking together a custom visualisation (while knowing fuck all about web dev, like me) can look like! (And as such, I try to explain the basic web dev concepts I use)

Note that you'll need to run the code yourself to get the interactive interface, so the cell at the bottom will be blank at first!

To emphasise - the point of this notebook is to be a rough proof of concept that just about works, *not* to be the well executed ideal of interactively studying neurons! You are highly encouraged to write your own (and ideally, to [make a pull request](https://github.com/neelnanda-io/TransformerLens/pulls) with improvements!)

## Setup

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install gradio
    %pip install datasets==2.19.1
```

```python
import gradio as gr
from transformer_lens import HookedTransformer
from transformer_lens.utils import to_numpy
from IPython.display import HTML
```

## Extracting Model Activations

We first write some code using HookedTransformer's cache to extract the neuron activations on a given layer and neuron, for a given text

```python
# NBVAL_IGNORE_OUTPUT
model_name = "gpt2-small"
model = HookedTransformer.from_pretrained(model_name)
```

```python
def get_neuron_acts(text, layer, neuron_index):
    # Hacky way to get out state from a single hook - we have a single element list and edit that list within the hook.
    cache = {}

    def caching_hook(act, hook):
        cache["activation"] = act[0, :, neuron_index]

    model.run_with_hooks(
        text, fwd_hooks=[(f"blocks.{layer}.mlp.hook_post", caching_hook)]
    )
    return to_numpy(cache["activation"])
```

We can run this function and verify that it gives vaguely sensible outputs

```python
default_layer = 9
default_neuron_index = 652
default_text = "The following is a list of powers of 10: 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000"
print(model.to_str_tokens(default_text))
```

```python
# NBVAL_IGNORE_OUTPUT
print(get_neuron_acts(default_text, default_layer, default_neuron_index))
```

## Visualizing Model Activations

We now write some code to visualize the neuron activations on some text - we're going to hack something together which just does some string processing to make an HTML string, with each token element colored according to the intensity neuron activation. We normalize the neuron activations so they all lie in [0, 1]. You can do much better, but this is a useful proof of concept of what "just hack stuff together" can look like!

I'll be keeping neuron 562 in layer 9 as a running example, as it seems to activate strongly on powers of 10.

Note that this visualization is very sensitive to `max_val` and `min_val`! You can tune those to whatever seems reasonable for the distribution of neuron activations you care about - I generally default to `min_val=0` and `max_val` as the max activation across the dataset.

```python
# This is some CSS (tells us what style )to give each token a thin gray border, to make it easy to see token separation
style_string = """<style>
    span.token {
        border: 1px solid rgb(123, 123, 123)
        }
    </style>"""

def calculate_color(val, max_val, min_val):
    # Hacky code that takes in a value val in range [min_val, max_val], normalizes it to [0, 1] and returns a color which interpolates between slightly off-white and red (0 = white, 1 = red)
    # We return a string of the form "rgb(240, 240, 240)" which is a color CSS knows
    normalized_val = (val - min_val) / max_val
    return f"rgb(240, {240*(1-normalized_val)}, {240*(1-normalized_val)})"

def basic_neuron_vis(text, layer, neuron_index, max_val=None, min_val=None):
    """
    text: The text to visualize
    layer: The layer index
    neuron_index: The neuron index
    max_val: The top end of our activation range, defaults to the maximum activation
    min_val: The top end of our activation range, defaults to the minimum activation

    Returns a string of HTML that displays the text with each token colored according to its activation

    Note: It's useful to be able to input a fixed max_val and min_val, because otherwise the colors will change as you edit the text, which is annoying.
    """
    if layer is None:
        return "Please select a Layer"
    if neuron_index is None:
        return "Please select a Neuron"
    acts = get_neuron_acts(text, layer, neuron_index)
    act_max = acts.max()
    act_min = acts.min()
    # Defaults to the max and min of the activations
    if max_val is None:
        max_val = act_max
    if min_val is None:
        min_val = act_min
    # We want to make a list of HTML strings to concatenate into our final HTML string
    # We first add the style to make each token element have a nice border
    htmls = [style_string]
    # We then add some text to tell us what layer and neuron we're looking at - we're just dealing with strings and can use f-strings as normal
    # h4 means "small heading"
    htmls.append(f"<h4>Layer: <b>{layer}</b>. Neuron Index: <b>{neuron_index}</b></h4>")
    # We then add a line telling us the limits of our range
    htmls.append(
        f"<h4>Max Range: <b>{max_val:.4f}</b>. Min Range: <b>{min_val:.4f}</b></h4>"
    )
    # If we added a custom range, print a line telling us the range of our activations too.
    if act_max != max_val or act_min != min_val:
        htmls.append(
            f"<h4>Custom Range Set. Max Act: <b>{act_max:.4f}</b>. Min Act: <b>{act_min:.4f}</b></h4>"
        )
    # Convert the text to a list of tokens
    str_tokens = model.to_str_tokens(text)
    for tok, act in zip(str_tokens, acts):
        # A span is an HTML element that lets us style a part of a string (and remains on the same line by default)
        # We set the background color of the span to be the color we calculated from the activation
        # We set the contents of the span to be the token
        htmls.append(
            f"<span class='token' style='background-color:{calculate_color(act, max_val, min_val)}' >{tok}</span>"
        )

    return "".join(htmls)
```

```python
# NBVAL_IGNORE_OUTPUT
# The function outputs a string of HTML
default_max_val = 4.0
default_min_val = 0.0
default_html_string = basic_neuron_vis(
    default_text,
    default_layer,
    default_neuron_index,
    max_val=default_max_val,
    min_val=default_min_val,
)

# IPython lets us display HTML
print("Displayed HTML")
display(HTML(default_html_string))

# We can also print the string directly
print("HTML String - it's just raw HTML code!")
print(default_html_string)
```

## Create Interactive UI

We now put all these together to create an interactive visualization in Gradio!

The internal format is that there's a bunch of elements - Textboxes, Numbers, etc which the user can interact with and which return strings and numbers. And we can also define output elements that just display things - in this case, one which takes in an arbitrary HTML string. We call `input.change(update_function, inputs, output)` - this says "if that input element changes, run the update function on the value of each of the elements in `inputs` and set the value of `output` to the output of the function". As a bonus, this gives us live interactivity!

This is also more complex than a typical Gradio intro example - I wanted to use custom HTML to display the nice colours, which made things much messier! Normally you could just make `out` into another Textbox and pass it a string.

```python
# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components
with gr.Blocks() as demo:
    gr.HTML(value=f"Hacky Interactive Neuroscope for {model_name}")
    # The input elements
    with gr.Row():
        with gr.Column():
            text = gr.Textbox(label="Text", value=default_text)
            # Precision=0 makes it an int, otherwise it's a float
            # Value sets the initial default value
            layer = gr.Number(label="Layer", value=default_layer, precision=0)
            neuron_index = gr.Number(
                label="Neuron Index", value=default_neuron_index, precision=0
            )
            # If empty, these two map to None
            max_val = gr.Number(label="Max Value", value=default_max_val)
            min_val = gr.Number(label="Min Value", value=default_min_val)
            inputs = [text, layer, neuron_index, max_val, min_val]
        with gr.Column():
            # The output element
            out = gr.HTML(label="Neuron Acts", value=default_html_string)
    for inp in inputs:
        inp.change(basic_neuron_vis, inputs, out)
```

We can now launch our demo element, and we're done! The setting share=True even gives you a public link to the demo (though it just redirects to the backend run by this notebook, and will go away once you turn the notebook off!) Sharing makes it much slower, and can be turned off if you aren't in a colab.

**Exercise:** Explore where this neuron does and does not activate. Is it just powers of ten? Just comma separated numbers? Numbers in any particular sequence?

```python
# NBVAL_IGNORE_OUTPUT
demo.launch(share=True, height=1000)
```

---

# LLaMA.ipynb


---

# LLaMA2_GPU_Quantized.ipynb


---

# LLaVA.ipynb

### LLaVA use case demonstration

At that notebook you can see simple example of how to use TransformerLens for LLaVA interpretability. More specifically you can pass united image patch embeddings and textual embedding to LLaVA language model (Vicuna) with TransformerLens and get logits and cache that contains activations for next analysis. Here we consider the simplest example of LLaVA and TransformerLens sharing.

```python
# import staff
import sys

# Uncomment if use clonned version of TransformerLens
# currently forked version https://github.com/zazamrykh/TransformerLens supports
TL_path = r"../"
if TL_path not in sys.path:
	sys.path.insert(0, TL_path)
	sys.path.insert(0, TL_path + r"/transformer_lens")

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration  # Should update transformer to latest version

# For image loading
from PIL import Image
import requests
from io import BytesIO

device = 'cuda' if torch.cuda.is_available() else 'cpu'

import matplotlib.pyplot as plt
%matplotlib inline

from transformer_lens import HookedTransformer
import circuitsvis as cv

_ = torch.set_grad_enabled(False)
```

Load llava model from hugging face. Load some revision because at this moment newest one is not working.

```python
model_id = "llava-hf/llava-1.5-7b-hf"

llava = LlavaForConditionalGeneration.from_pretrained(
	model_id,
	torch_dtype=torch.float16,
	load_in_4bit=False,
	low_cpu_mem_usage=True,
	revision="a272c74",
	device_map="cpu"
)

for param in llava.parameters():  # At this demo we don't need grads
	param.requires_grad = False

processor = AutoProcessor.from_pretrained(model_id, revision="a272c74")
tokenizer = processor.tokenizer

# Taking model apart
language_model = llava.language_model.eval()
config = language_model.config
print("Base language model:", config._name_or_path)

vision_tower = llava.vision_tower.to(device).eval()
projector = llava.multi_modal_projector.to(device).eval()
```

```python
# You can write your own version of getting language model's input embeddings similar way
# This function will not be working with old transformers library version. Should update transformers library.
def get_llm_input_embeddings(llava, processor, image: Image, text: str, device='cuda'):
    """ Extract features from image, project them to LLM's space and insert them to text embedding sequence.
    Returns:
    	inputs_embeds, attention_mask, labels, position_ids - input for language model of LLaVA
    """
    conversation = [
      {
        "role": "user",
        "content": [
            {"type": "text", "text": text},
            {"type": "image"},
          ],
      },
    	]
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)
    llava.vision_tower.to(device)
    llava.multi_modal_projector.to(device)

    clip_output = llava.vision_tower(inputs['pixel_values'])
    projector_output = llava.multi_modal_projector(clip_output.last_hidden_state)

    before_device = llava.language_model.model.embed_tokens.weight.device
    llava.language_model.model.embed_tokens.to(device)
    text_embeddings = llava.language_model.model.embed_tokens(inputs['input_ids'])
    llava.language_model.model.embed_tokens.to(before_device)

    full_sequence = torch.hstack([projector_output, text_embeddings])

    attention_mask = torch.ones(full_sequence.shape[:-1], device=full_sequence.device, dtype=int)
    inputs_embeds, attention_mask, labels, position_ids = llava._merge_input_ids_with_image_features(
		projector_output, text_embeddings, inputs['input_ids'], attention_mask, labels=None
	)  # Access to private member... Well, but what can i do :-)

    return inputs_embeds, attention_mask, labels, position_ids
```

Okay, now create HookedTransformer model

```python
hooked_llm = HookedTransformer.from_pretrained(
	"llama-7b-hf",  # Use config of llama
	center_unembed=False,
	fold_ln=False,
	fold_value_biases=False,
	device='cuda',
	hf_model=language_model,  # Use Vicuna's weights
	tokenizer=tokenizer,
	center_writing_weights=False,
	dtype=torch.float16,
	vocab_size=language_model.config.vocab_size  # New argument. llama and vicuna have different vocab size, so we pass it here
)

for param in hooked_llm.parameters():
	param.requires_grad = False
```

Now try if hooked model is working

```python
image_url = "https://github.com/zazamrykh/PicFinder/blob/main/images/doge.jpg?raw=true"
response = requests.get(image_url)
image = Image.open(BytesIO(response.content))
plt.axis('off')
_ = plt.imshow(image)
```

```python
question = "What do you see on photo?"
inputs_embeds, attention_mask, labels, position_ids = get_llm_input_embeddings(llava, processor, image, question, device=device)

# Return tokens
outputs = hooked_llm.generate(
	inputs_embeds,
	max_new_tokens=30,
	do_sample=True,
    return_type='tokens'
)
generated_text = processor.decode(outputs[0], skip_special_tokens=True)
print('Generated text:', generated_text)
```

```python
# Now return embeddings and then project them on vocab space
outputs = hooked_llm.generate(
	inputs_embeds,
	max_new_tokens=30,
	do_sample=True,
)

logits = outputs[:,-30:,:].to(device) @ language_model.model.embed_tokens.weight.T.to(device)
generated_text = processor.decode(logits.argmax(-1)[0], skip_special_tokens=True)
print('Generated text:', generated_text)
```

As we can see everything is working. Now try visualize attention patterns in generated output.

```python
# Here we visualize attention for the last 30 tokens.
logits, cache = hooked_llm.run_with_cache(inputs_embeds, start_at_layer=0, remove_batch_dim=True)

layer_to_visualize = 16
tokens_to_show = 30
attention_pattern = cache["pattern", layer_to_visualize, "attn"]

product = inputs_embeds @ language_model.model.embed_tokens.weight.T.to(device)  # Project embeddings to vocab
llama_str_tokens = hooked_llm.to_str_tokens(product.argmax(dim=-1)[0])

print(f"Layer {layer_to_visualize} Head Attention Patterns:")
display(cv.attention.attention_patterns(tokens=llama_str_tokens[-tokens_to_show:],
										attention=attention_pattern[:, -tokens_to_show:, -tokens_to_show:]))
```

As we can see image tokens also appears and can be used for multimodal attention exploration.

---

# Main_Demo.ipynb


---

# No_Position_Experiment.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Introduction

The accompanying notebook to my [real-time research](https://www.youtube.com/watch?v=yo4QvDn-vsU) video. Trains a model with no positional embeddings to predict the previous token, and makes a start at analysing what's going on there!

EDIT: The loss spikes were due to the learning rate being max(step/100, 1.0) not min! Thanks to MadHatter for catching that.

# Setup

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")

if IN_COLAB or IN_GITHUB:
    %pip install einops
    %pip install transformer_lens@v1.15.0

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git

from transformer_lens import HookedTransformer, HookedTransformerConfig
import torch
import numpy as np
import plotly.express as px
import plotly.io as pio

pio.renderers.default = "colab"
import tqdm.auto as tqdm
import einops
from transformer_lens.utils import to_numpy

device = "cuda" if torch.cuda.is_available() else "cpu"
```

Some plotting code. Wrappers around Plotly, not important to understand.

```python
def line(tensor, line_labels=None, yaxis="", xaxis="", **kwargs):
    tensor = to_numpy(tensor)
    labels = {"y": yaxis, "x": xaxis}
    fig = px.line(tensor, labels=labels, **kwargs)
    if line_labels:
        for c, label in enumerate(line_labels):
            fig.data[c].name = label
    fig.show()

def imshow(tensor, yaxis="", xaxis="", **kwargs):
    tensor = to_numpy(tensor)
    plot_kwargs = {
        "color_continuous_scale": "RdBu",
        "color_continuous_midpoint": 0.0,
        "labels": {"x": xaxis, "y": yaxis},
    }
    plot_kwargs.update(kwargs)
    px.imshow(tensor, **plot_kwargs).show()
```

# Model Training

## Setup

### Defining the Model

```python
cfg = HookedTransformerConfig(
    n_layers=2,
    d_model=64,
    d_head=64,
    n_heads=1,
    d_mlp=256,
    d_vocab=300,
    n_ctx=50,
    act_fn="relu",
    normalization_type="LN",
    device=device,
)
model = HookedTransformer(cfg)
```

```python
def deactivate_position(model):
    model.pos_embed.W_pos.data[:] = 0.0
    model.pos_embed.W_pos.requires_grad = False

deactivate_position(model)
```

```python
print(model)
```

### Define data + Loss function

```python
def make_data_generator(cfg, batch_size, seed=123, incl_bos_token=True):
    torch.manual_seed(seed)
    while True:
        x = torch.randint(1, cfg.d_vocab, (batch_size, cfg.n_ctx))
        if incl_bos_token:
            x[:, 0] = 0
        yield x

data_generator = make_data_generator(cfg, 2)
print(next(data_generator))
```

```python
def loss_fn(logits, tokens, per_token=False):
    # logit shape: [batch, pos, vocab]
    # token shape: [batch, pos]
    logits = logits[:, 1:]
    tokens = tokens[:, :-1]
    log_probs = logits.log_softmax(-1)
    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]
    if per_token:
        return -correct_log_probs
    else:
        return -correct_log_probs.mean()
```

```python
# Test the loss function works
test_tokens = torch.arange(5)[None, :]
test_logits = torch.randn(1, 5, 10)
test_logits[:, 1, 0] = 10.0
test_logits[:, 2, 1] = 10.0
test_logits[:, 3, 2] = 10.0
test_logits[:, 4, 3] = 10.0
print(loss_fn(test_logits, test_tokens, per_token=True))
print(loss_fn(test_logits, test_tokens, per_token=False))
```

### Setup Optimizer

```python
batch_size = 256
num_epochs = 4000
lr = 1e-4
betas = (0.9, 0.95)
max_grad_norm = 1.0
wd = 0.1
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: min(i / 100, 1.0))

data_loader = make_data_generator(cfg, batch_size)
```

## Model Training

```python
losses = []
for epoch in tqdm.tqdm(range(num_epochs)):
    tokens = next(data_loader)
    tokens = tokens.to(device)
    logits = model(tokens)
    loss = loss_fn(logits, tokens)
    loss.backward()
    if max_grad_norm is not None:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
    optimizer.step()
    optimizer.zero_grad()
    scheduler.step()
    losses.append(loss.item())
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: {loss.item()}")
px.line(losses, labels={"x": "Epoch", "y": "Loss"})
```

```python
# torch.save(model.state_dict(), "no_pos_experiment_state_dict_v0.pth")
```

# Model Interpretability

```python
model.pos_embed.W_pos.norm()
```

## Look at attention patterns

```python
big_data_loader = make_data_generator(cfg, 4000)
big_tokens = next(big_data_loader)
big_tokens = big_tokens.to(device)
logits, cache = model.run_with_cache(big_tokens)
print("Loss:", loss_fn(logits, big_tokens).item())
```

```python
print(cache)
```

```python
cache["blocks.0.attn.hook_pattern"].shape
```

```python
batch_index = 0
tokens = big_tokens[batch_index]
imshow(
    to_numpy(cache["attn", 0].mean([0, 1])),
    title="Layer 0 Attention Pattern",
    height=500,
    width=500,
)
imshow(
    to_numpy(cache["attn", 1].mean([0, 1])),
    title="Layer 1 Attention Pattern",
    height=500,
    width=500,
)
```

## Look at how different bits of the model directly contribute to the logits

```python
resid_components = [
    cache["embed"],
    cache["attn_out", 0],
    cache["mlp_out", 0],
    cache["attn_out", 1],
    cache["mlp_out", 1],
]
labels = ["embed", "A0", "M0", "A1", "M2"]
resid_stack = torch.stack(resid_components, 0)
resid_stack = resid_stack - resid_stack.mean(-1, keepdim=True)
print(resid_stack.shape)
```

```python
fold_W_U = model.ln_final.w[:, None] * model.unembed.W_U
logit_components = resid_stack[:, batch_index] @ fold_W_U / cache["scale"][batch_index]
print(logit_components.shape)
```

```python
logit_components = logit_components - logit_components.mean(-1, keepdim=True)
line(
    logit_components[:, torch.arange(1, model.cfg.n_ctx).to(device), tokens[:-1]].T,
    line_labels=labels,
)
```

## Folding In LayerNorm

```python
analysis_cfg = HookedTransformerConfig(
    n_layers=2,
    d_model=64,
    d_head=64,
    n_heads=1,
    d_mlp=256,
    d_vocab=300,
    n_ctx=50,
    act_fn="relu",
    normalization_type="LNPre",
    init_weights=False,
)
analysis_model = HookedTransformer(analysis_cfg)
state_dict = model.state_dict()
analysis_model.load_and_process_state_dict(
    state_dict, fold_ln=True, center_writing_weights=True, center_unembed=True
)
deactivate_position(analysis_model)
```

```python
# analysis_model()
```

## Understand Attn 0

```python
QK = model.W_E @ model.W_Q[0, 0] @ model.W_K[0, 0].T @ model.W_E.T
imshow(QK, yaxis="Query", xaxis="Key")
```

```python
OV = model.W_E @ model.W_V[0, 0] @ model.W_O[0, 0] @ model.W_in[0]
imshow(OV, yaxis="Input Vocab", xaxis="Neuron")
```

```python
line(OV[:, torch.randint(0, 256, (5,))])
```

## Understand MLP 0

```python
imshow(cache["post", 0][batch_index], yaxis="Pos", xaxis="Neuron")
imshow(cache["post", 0].mean(0), yaxis="Pos", xaxis="Neuron")
imshow((cache["post", 0] > 0).float()[batch_index], yaxis="Pos", xaxis="Neuron")
imshow((cache["post", 0] > 0).float().mean(0), yaxis="Pos", xaxis="Neuron")
```

## Understand Attn 1

## Understand MLP 1

```python

```

# Experiment

```python
new_token_batch = next(big_data_loader).to(device)
baseline_loss = loss_fn(model(new_token_batch), new_token_batch).item()
print("Baseline loss:", baseline_loss)
```

```python
hook_list = list(model.hook_dict.keys())
losses = []
loss_labels = []
for hook_name in hook_list:
    if (
        hook_name in cache
        and hook_name != "hook_pos_embed"
        and "result" not in hook_name
    ):
        average_act = cache[hook_name].mean(0)

        def replacing_with_average_act(activation, hook):
            activation[:] = einops.repeat(
                average_act, "... -> batch ...", batch=new_token_batch.size(0)
            )
            return activation

        logits = model.run_with_hooks(
            new_token_batch, fwd_hooks=[(hook_name, replacing_with_average_act)]
        )
        loss = loss_fn(logits, new_token_batch)
        print(hook_name, loss.item())
        losses.append(loss.item())
        loss_labels.append(hook_name)
```

```python
line(losses, hover_name=loss_labels)
```

```python
cache.cache_dict.keys()
```

---

# Othello_GPT.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Othello_GPT.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

This is a demo notebook porting the weights of the Othello-GPT Model from the excellent [Emergent World Representations](https://arxiv.org/pdf/2210.13382.pdf) paper to my TransformerLens library. Check out the paper's [blog post](https://thegradient.pub/othello/), [paper](https://arxiv.org/pdf/2210.13382.pdf), and [github](https://github.com/likenneth/othello_world/)

I think this is a super interesting paper, and I want to better enable work trying to reverse-engineer this model! I'm particularly curious about:
* Why non-linear probes work much better than linear probes?
    * Is the model internally representing the board in a usable yet non-linear way?
    * Is there a representation of simpler concepts (eg diagonal lines in the board, number of black pieces, whether a cell is blank)) that the non-linear probe uses to compute board positions, but where the model internally reasons in this simpler representation?
* What's going up with the model editing?
    * The paper edits across many layers at once. What's the minimal edit that works?
        * Can we edit just before the final layer?
        * Can we do a single edit rather than across many layers?
    * If we contrast model activations pre and post edit, what changes?
        * Which components shift their output and how does this affect the logits?
        * Is there significant depth of composition, or does it just affect the output logits?
* Can we find any non-trivial circuits in the model?
    * Start with [exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), like direct logit attribution, or just looking at head attention patterns, and try to get traction
    * Pick a simple sub-task, eg figuring out whether a cell is blank, and try to interpret that.

I uploaded pre-converted checkpoints to HuggingFace, which can be automatically downloaded, and there's a code snippet to do this after the setup.

If you want to use the author's code, I wrote a script to load and convert checkpoints from the author's code, given below this.

To get started, check out the transformer lens [main tutorial](https://neelnanda.io/transformer-lens-demo) and [tutorial on exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), and the author's [excellent Github](https://github.com/likenneth/othello_world/) (Ot**hello world**) for various notebooks demonstrating their code, showing how to load inputs, etc. And check out my [concrete open problems in mechanistic interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj) sequence, especially the algorithmic problems post, for tips on this style of research.

# Setup (Skip)

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install circuitsvis
    %pip install torchtyping
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
import circuitsvis as cv

# Testing that the library works
cv.examples.hello("Neel")
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import tqdm.auto as tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from torchtyping import TensorType as TT
from typing import List, Union, Optional
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
```

```python
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import (
    HookedTransformer,
    HookedTransformerConfig,
    FactoredMatrix,
    ActivationCache,
)
```

We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training.

```python
torch.set_grad_enabled(False)
```

Plotting helper functions:

```python
def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(
        utils.to_numpy(tensor),
        color_continuous_midpoint=0.0,
        color_continuous_scale="RdBu",
        labels={"x": xaxis, "y": yaxis},
        **kwargs,
    ).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x": xaxis, "y": yaxis}, **kwargs).show(
        renderer
    )

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(
        y=y, x=x, labels={"x": xaxis, "y": yaxis, "color": caxis}, **kwargs
    ).show(renderer)
```

# Othello GPT

```python
LOAD_AND_CONVERT_CHECKPOINT = False
```

```python
import transformer_lens.utils as utils

cfg = HookedTransformerConfig(
    n_layers=8,
    d_model=512,
    d_head=64,
    n_heads=8,
    d_mlp=2048,
    d_vocab=61,
    n_ctx=59,
    act_fn="gelu",
    normalization_type="LNPre",
)
model = HookedTransformer(cfg)
```

```python
# NBVAL_IGNORE_OUTPUT
sd = utils.download_file_from_hf(
    "NeelNanda/Othello-GPT-Transformer-Lens", "synthetic_model.pth"
)
# champion_ship_sd = utils.download_file_from_hf("NeelNanda/Othello-GPT-Transformer-Lens", "championship_model.pth")
model.load_state_dict(sd)
```

Code to load and convert one of the author's checkpoints to TransformerLens:

```python
def convert_to_transformer_lens_format(in_sd, n_layers=8, n_heads=8):
    out_sd = {}
    out_sd["pos_embed.W_pos"] = in_sd["pos_emb"].squeeze(0)
    out_sd["embed.W_E"] = in_sd["tok_emb.weight"]

    out_sd["ln_final.w"] = in_sd["ln_f.weight"]
    out_sd["ln_final.b"] = in_sd["ln_f.bias"]
    out_sd["unembed.W_U"] = in_sd["head.weight"].T

    for layer in range(n_layers):
        out_sd[f"blocks.{layer}.ln1.w"] = in_sd[f"blocks.{layer}.ln1.weight"]
        out_sd[f"blocks.{layer}.ln1.b"] = in_sd[f"blocks.{layer}.ln1.bias"]
        out_sd[f"blocks.{layer}.ln2.w"] = in_sd[f"blocks.{layer}.ln2.weight"]
        out_sd[f"blocks.{layer}.ln2.b"] = in_sd[f"blocks.{layer}.ln2.bias"]

        out_sd[f"blocks.{layer}.attn.W_Q"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.query.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_Q"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.query.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_K"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.key.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_K"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.key.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_V"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.value.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_V"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.value.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_O"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.proj.weight"],
            "d_model (head d_head) -> head d_head d_model",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_O"] = in_sd[f"blocks.{layer}.attn.proj.bias"]

        out_sd[f"blocks.{layer}.mlp.b_in"] = in_sd[f"blocks.{layer}.mlp.0.bias"]
        out_sd[f"blocks.{layer}.mlp.W_in"] = in_sd[f"blocks.{layer}.mlp.0.weight"].T
        out_sd[f"blocks.{layer}.mlp.b_out"] = in_sd[f"blocks.{layer}.mlp.2.bias"]
        out_sd[f"blocks.{layer}.mlp.W_out"] = in_sd[f"blocks.{layer}.mlp.2.weight"].T

    return out_sd

if LOAD_AND_CONVERT_CHECKPOINT:
    synthetic_checkpoint = torch.load("/workspace/othello_world/gpt_synthetic.ckpt")
    for name, param in synthetic_checkpoint.items():
        if name.startswith("blocks.0") or not name.startswith("blocks"):
            print(name, param.shape)

    cfg = HookedTransformerConfig(
        n_layers=8,
        d_model=512,
        d_head=64,
        n_heads=8,
        d_mlp=2048,
        d_vocab=61,
        n_ctx=59,
        act_fn="gelu",
        normalization_type="LNPre",
    )
    model = HookedTransformer(cfg)

    model.load_and_process_state_dict(
        convert_to_transformer_lens_format(synthetic_checkpoint)
    )
```

Testing code for the synthetic checkpoint giving the correct outputs

```python
# An example input
sample_input = torch.tensor(
    [
        [
            20,
            19,
            18,
            10,
            2,
            1,
            27,
            3,
            41,
            42,
            34,
            12,
            4,
            40,
            11,
            29,
            43,
            13,
            48,
            56,
            33,
            39,
            22,
            44,
            24,
            5,
            46,
            6,
            32,
            36,
            51,
            58,
            52,
            60,
            21,
            53,
            26,
            31,
            37,
            9,
            25,
            38,
            23,
            50,
            45,
            17,
            47,
            28,
            35,
            30,
            54,
            16,
            59,
            49,
            57,
            14,
            15,
            55,
            7,
        ]
    ]
)
# The argmax of the output (ie the most likely next move from each position)
sample_output = torch.tensor(
    [
        [
            21,
            41,
            40,
            34,
            40,
            41,
            3,
            11,
            21,
            43,
            40,
            21,
            28,
            50,
            33,
            50,
            33,
            5,
            33,
            5,
            52,
            46,
            14,
            46,
            14,
            47,
            38,
            57,
            36,
            50,
            38,
            15,
            28,
            26,
            28,
            59,
            50,
            28,
            14,
            28,
            28,
            28,
            28,
            45,
            28,
            35,
            15,
            14,
            30,
            59,
            49,
            59,
            15,
            15,
            14,
            15,
            8,
            7,
            8,
        ]
    ]
)
model(sample_input).argmax(dim=-1)
```

---

# Patchscopes_Generation_Demo.ipynb


---

# Qwen.ipynb

```python
%pip install transformers_stream_generator plotly circuitsvis huggingface_hub einops tiktoken datasets
```

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git
    %pip install circuitsvis

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio
if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
%cd ~/TransformerLens
import torch
torch.set_grad_enabled(False)

from transformers import AutoTokenizer
from transformer_lens import HookedTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

from functools import partial
```

```python
def assert_hf_and_tl_model_are_close(
    hf_model,
    tl_model,
    tokenizer,
    prompt="This is a prompt to test out",
    atol=1e-3,
):
    prompt_toks = tokenizer(prompt, return_tensors="pt").input_ids

    hf_logits = hf_model(prompt_toks.to(hf_model.device)).logits
    tl_logits = tl_model(prompt_toks).to(hf_logits)

    assert torch.allclose(torch.softmax(hf_logits, dim=-1), torch.softmax(tl_logits, dim=-1), atol=atol)
```

## Qwen, first generation

```python
model_path = "Qwen/Qwen-1_8B-Chat"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

hf_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=device,
    fp32=True,
    use_logn_attn=False,
    use_dynamic_ntk = False,
    scale_attn_weights = False,
    trust_remote_code = True
).eval()

tl_model = HookedTransformer.from_pretrained_no_processing(
    model_path,
    device=device,
    fp32=True,
    dtype=torch.float32,
).to(device)

assert_hf_and_tl_model_are_close(hf_model, tl_model, tokenizer)
```

## Qwen, new generation

```python
model_path = "Qwen/Qwen1.5-1.8B-Chat"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
)

hf_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=device,
).eval()

tl_model = HookedTransformer.from_pretrained_no_processing(
    model_path,
    device=device,
    dtype=torch.float32,
).to(device)

assert_hf_and_tl_model_are_close(hf_model, tl_model, tokenizer)
```

```python

```

---

# SVD_Interpreter_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/SVD_Interpreter_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## TransformerLens SVD Interpreter Demo

A few months ago, a Conjecture post came out about how the singular value decompositions of transformer matrices were [surprisingly interpretable](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight#Directly_editing_SVD_representations), leading to recognisable semantic clusters. This seemed like good functionality to add to TransformerLens, which is what the SVD Interpreter feature does. You simply need to pass it a model, the type of matrix you want, and the size of the results you want, then you can plot it using PySvelte. This demo will show you how it's done.

How to use this notebook:

**Go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.**

Tips for reading this Colab:

* You can run all this code for yourself!
* The graphs are interactive!
* Use the table of contents pane in the sidebar to navigate
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar, not CTRL+F

## Setup (Can be ignored)

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEBUG_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/JayBaileyCS/TransformerLens.git # TODO: Change!
    # Install Neel's personal plotting utils
    %pip install git+https://github.com/neelnanda-io/neel-plotly.git
    # Install another version of node that makes PySvelte work way faster
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    %pip install git+https://github.com/neelnanda-io/PySvelte.git
    # Needed for PySvelte to work, v3 came out and broke things...
    %pip install typeguard==2.13.3
    %pip install typing-extensions
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEBUG_MODE:
    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "png"
```

```python
import torch
import pysvelte
import numpy as np
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens import HookedTransformer, SVDInterpreter
```

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device = }")
```

## SVD Interpretation

The SVD Interpreter supports interpretation for three types of Transformer matrix:

* OV - The [output-value circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS) of the matrix. (d_model x d_model) in size.
* w_in - Weights passed into the MLP block of the matrix. (d_model x (4 x d_model)) in size.
* w_out - Weights that come out of the MLP block of the matrix. ((4 x d_model) x d_model) in size.

The SVD interpreter handles everything behind the scenes, so you only need to pass in the model and the type of matrix you want. Let's give it a go!

We'll be passing in **fold_ln = False, center_writing_weights+false, and center_unembed=False** here to mimic the existing post as closely as possible in order to demonstrate that this works (and the numerical instability that makes it not *completely* work). You can do interpretability on the default model without these parameters, but you won't be able to replicate the same results. I haven't checked much to see how it affects their quality, though w_out seemed to decay greatly when center_unembed was True - this would be worth testing properly!

Replication with this type of analysis is inherently difficult, because linear dependence is numerically unstable. Very minor numerical changes (Like floating-point discrepancies) can alter the results slightly. (See [this comment](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight?commentId=4e8534hbyWCpZFgFD)) So don't worry if you don't get exactly the same results on different devices - this is, unfortunately, expected. Try to stick to the same device for all your experiments and be sure to point out which one you used when writing them up. (And if anyone has a more stable way to get these results, [let us know](https://github.com/TransformerLensOrg/TransformerLens/issues)!)

```python
model = HookedTransformer.from_pretrained("gpt2-medium", fold_ln=False, center_writing_weights=False, center_unembed=False)
```

```python
all_tokens = [model.to_str_tokens(np.array([i])) for i in range(model.cfg.d_vocab)]
all_tokens = [all_tokens[i][0] for i in range(model.cfg.d_vocab)]

# Utility function to plot values in the same style as the Conjecture post.
def plot_matrix(matrix, tokens, k=10, filter="topk"):
  pysvelte.TopKTable(tokens=all_tokens, activations=matrix, obj_type="SVD direction", k=k, filter=filter).show()
```

```python
svd_interpreter = SVDInterpreter(model)

ov = svd_interpreter.get_singular_vectors('OV', layer_index=22, head_index=10)
w_in = svd_interpreter.get_singular_vectors('w_in', layer_index=20)
w_out = svd_interpreter.get_singular_vectors('w_out', layer_index=16)

plot_matrix(ov, all_tokens)
plot_matrix(w_in, all_tokens)
plot_matrix(w_out, all_tokens)
```

Currently, this is the extent of our support for SVD interpretability. However, this is a very new idea, and we're excited to see how people use it! If you find an interesting use for this type of research that we don't cover, feel free to [open a ticket](https://github.com/TransformerLensOrg/TransformerLens/issues) or contact the code's author at jaybaileycs@gmail.com.

One thing I'd love to see that basically anyone who followed this demo could get started with (I'd consider it an **A-level problem** from Neel's [Concrete Open Problems sequence](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj)) is to try different combinations of model parameters (fold_ln, center_writing_weights, center_unembed) and see which ones lead to big changes in the interpretability of the SVD matrices.

Are these changes positive, or negative? Can you pick any set of parameters you want? Are different parameters more or less interpretable in general, or does it vary by head and layer? Can you get two different interpretations of the same head with different parameters? What else can you find? This is very low-hanging fruit that would be immediately tractable and immediately useful!

---

# Santa_Coder.ipynb

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git``
    %pip install circuitsvis
    %pip install torchtyping

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import tqdm.auto as tqdm
from tqdm import tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from torchtyping import TensorType as TT
from typing import List, Union, Optional
from jaxtyping import Float, Int
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
# import circuitsvis as cv

import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache

torch.set_grad_enabled(False)

def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(y=y, x=x, labels={"x":xaxis, "y":yaxis, "color":caxis}, **kwargs).show(renderer)
```

```python
# load hf model
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m")
```

```python
# Disable folding norms and folding norms and biases so that intermediate value
# in between transformer blocks can be compared
bloom = HookedTransformer.from_pretrained("bloom-560m",fold_ln=False, fold_value_biases=False, center_writing_weights=False)
```

```python
text = '''
TransformerLens lets you load in 50+ different open source language models,
and exposes the internal activations of the model to you. You can cache
any internal activation in the model, and add in functions to edit, remove
or replace these activations as the model runs.
'''
input_ids = tokenizer(text, return_tensors='pt')['input_ids']
gt_logits = model(input_ids)['logits'] # ground truth logits from hf
my_logits = bloom(input_ids)
centered_gt_logits = gt_logits - gt_logits.mean(-1, keepdim=True)
mean_diff = (my_logits.cpu() - centered_gt_logits).mean()
print("avg logits difference:", mean_diff.item())
max_diff = (my_logits.cpu() - centered_gt_logits).abs().max()
print("max logits difference:", max_diff.item())
```

```python
gt_cache = model(input_ids, output_hidden_states=True)['hidden_states']
_, my_cache = bloom.run_with_cache(input_ids)
use_loose_bound = False
pass_loose_bound = True
print("*"*5, "Matching hf and T-Lens residual stream in between transformer blocks", "*"*5)
for i in range(24):
    try:
        torch.testing.assert_close(my_cache['resid_pre',i], gt_cache[i].cuda())
    except:
        max_diff = (my_cache['resid_pre',i] - gt_cache[i].cuda()).abs().max()
        print(f"layer {i} \t not close, max difference: {max_diff}")
        use_loose_bound = True

if use_loose_bound:
    atol = rtol = 1e-3
    print("*"*5, f"\ttesting with atol={atol} and rtol={rtol}\t","*"*5)
    for i in range(24):
        try:
            torch.testing.assert_close(my_cache['resid_pre',i], gt_cache[i].cuda(), atol=atol, rtol=rtol)
        except:
            max_diff = (my_cache['resid_pre',i] - gt_cache[i].cuda()).abs().max()
            print(f"layer {i} \t not close, max difference: {max_diff}")
            pass_loose_bound = False

    if pass_loose_bound:
        print(f"All layers match with atol={atol} rtol={rtol}")
else:
    print("All layers match")
```

```python
my_loss = bloom(input_ids, return_type='loss')
print("T-Lens next token loss:", my_loss.item())
gt_outputs = model(input_ids, labels=input_ids)
gt_loss = gt_outputs.loss
print("HF next token loss:", gt_loss.item())
print("diff in loss (abs):", (gt_loss-my_loss).abs().item())
```

---

# T5.ipynb


---

# Tracr_to_Transformer_Lens_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Tracr_to_Transformer_Lens_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Tracr to TransformerLens Converter
[Tracr](https://github.com/deepmind/tracr) is a cool new DeepMind tool that compiles a written program in RASP to transformer weights. TransformerLens is a library I've written to easily do mechanistic interpretability on a transformer and to poke around at its internals. This is a (hacky!) script to convert Tracr weights from the JAX form to a TransformerLens HookedTransformer in PyTorch.

See [the TransformerLens tutorial](https://neelnanda.io/transformer-lens-demo) to get started

Python version must be >=3.8 (my fork of Tracr is a bit more backwards compatible, original library is at least 3.9)

```python
!python --version
```

```python
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install transformer_lens
    # Fork of Tracr that's backward compatible with Python 3.8
    %pip install git+https://github.com/neelnanda-io/Tracr

except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    # from IPython import get_ipython

    # ipython = get_ipython()
    # # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    # ipython.magic("load_ext autoreload")
    # ipython.magic("autoreload 2")
```

```python
from transformer_lens import HookedTransformer, HookedTransformerConfig
import einops
import torch
import numpy as np

from tracr.rasp import rasp
from tracr.compiler import compiling
```

Loads an example RASP program model. This program reverses lists. The model takes as input a list of pre-tokenization elements (here `["BOS", 1, 2, 3]`), these are tokenized (`[3, 0, 1, 2]`), the transformer is applied, and then an argmax is taken over the output and it is detokenized - this can be seen on the `out.decoded` attribute of the output

```python

def make_length():
  all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)
  return rasp.SelectorWidth(all_true_selector)

length = make_length()  # `length` is not a primitive in our implementation.
opp_index = length - rasp.indices - 1
flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)
reverse = rasp.Aggregate(flip, rasp.tokens)

bos = "BOS"
model = compiling.compile_rasp_to_model(
    reverse,
    vocab={1, 2, 3},
    max_seq_len=5,
    compiler_bos=bos,
)

out = model.apply([bos, 1, 2, 3])
```

Extract the model config from the Tracr model, and create a blank HookedTransformer object

```python

# %%

n_heads = model.model_config.num_heads
n_layers = model.model_config.num_layers
d_head = model.model_config.key_size
d_mlp = model.model_config.mlp_hidden_size
act_fn = "relu"
normalization_type = "LN"  if model.model_config.layer_norm else None
attention_type = "causal"  if model.model_config.causal else "bidirectional"

n_ctx = model.params["pos_embed"]['embeddings'].shape[0]
# Equivalent to length of vocab, with BOS and PAD at the end
d_vocab = model.params["token_embed"]['embeddings'].shape[0]
# Residual stream width, I don't know of an easy way to infer it from the above config.
d_model = model.params["token_embed"]['embeddings'].shape[1]

# Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never care about these outputs
# In practice, we always feed the logits into an argmax
d_vocab_out = model.params["token_embed"]['embeddings'].shape[0] - 2

cfg = HookedTransformerConfig(
    n_layers=n_layers,
    d_model=d_model,
    d_head=d_head,
    n_ctx=n_ctx,
    d_vocab=d_vocab,
    d_vocab_out=d_vocab_out,
    d_mlp=d_mlp,
    n_heads=n_heads,
    act_fn=act_fn,
    attention_dir=attention_type,
    normalization_type=normalization_type,
)
tl_model = HookedTransformer(cfg)
```

Extract the state dict, and do some reshaping so that everything has a n_heads dimension

```python

# %%
sd = {}
sd["pos_embed.W_pos"] = model.params["pos_embed"]['embeddings']
sd["embed.W_E"] = model.params["token_embed"]['embeddings']
# Equivalent to max_seq_len plus one, for the BOS

# The unembed is just a projection onto the first few elements of the residual stream, these store output tokens
# This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.
sd["unembed.W_U"] = np.eye(d_model, d_vocab_out)

for l in range(n_layers):
    sd[f"blocks.{l}.attn.W_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_O"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/linear"]["w"],
        "(n_heads d_head) d_model -> n_heads d_head d_model",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_O"] = model.params[f"transformer/layer_{l}/attn/linear"]["b"]

    sd[f"blocks.{l}.mlp.W_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["w"]
    sd[f"blocks.{l}.mlp.b_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["b"]
    sd[f"blocks.{l}.mlp.W_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["w"]
    sd[f"blocks.{l}.mlp.b_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["b"]
print(sd.keys())

```

Convert weights to tensors and load into the tl_model

```python

for k, v in sd.items():
    # I cannot figure out a neater way to go from a Jax array to a numpy array lol
    sd[k] = torch.tensor(np.array(v))

tl_model.load_state_dict(sd, strict=False)

```

Create helper functions to do the tokenization and de-tokenization

```python

# %%
INPUT_ENCODER = model.input_encoder
OUTPUT_ENCODER = model.output_encoder

def create_model_input(input, input_encoder=INPUT_ENCODER):
    encoding = input_encoder.encode(input)
    return torch.tensor(encoding).unsqueeze(dim=0)

def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):
    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)
    decoded_output = output_encoder.decode(max_output_indices.tolist())
    decoded_output_with_bos = [bos_token] + decoded_output[1:]
    return decoded_output_with_bos

```

We can now run the model!

```python

input = [bos, 1, 2, 3]
out = model.apply(input)
print("Original Decoding:", out.decoded)

input_tokens_tensor = create_model_input(input)
logits = tl_model(input_tokens_tensor)
decoded_output = decode_model_output(logits)
print("TransformerLens Replicated Decoding:", decoded_output)
# %%

```

Lets cache all intermediate activations in the model, and check that they're the same:

```python
logits, cache = tl_model.run_with_cache(input_tokens_tensor)

for layer in range(tl_model.cfg.n_layers):
    print(f"Layer {layer} Attn Out Equality Check:", np.isclose(cache["attn_out", layer].detach().cpu().numpy(), np.array(out.layer_outputs[2*layer])).all())
    print(f"Layer {layer} MLP Out Equality Check:", np.isclose(cache["mlp_out", layer].detach().cpu().numpy(), np.array(out.layer_outputs[2*layer+1])).all())
```

Look how pretty and ordered the final residual stream is!

(The logits are the first 3 dimensions of the residual stream, and we can see that they're flipped!)

```python
import plotly.express as px
px.imshow(cache["resid_post", -1].detach().cpu().numpy()[0],
color_continuous_scale="Blues", labels={"x":"Residual Stream", "y":"Position"}, y=[str(i) for i in input]).show("colab" if IN_COLAB else "")
```

---

# comparing-to-huggingface.ipynb

Compare the TransformerLens implementation of a model to the Huggingface implementation. This script was originally use in https://github.com/TransformerLensOrg/TransformerLens/issues/570 to debug Mixtral.

## setup

```python
%pip install transformers matplotlib
```

```python
# Everything can be configured here
model_id = ""
text = "Hello my name is"
device="cpu"
# Set this to true to trigger hugging face login if needed
gated_model = False
```

```python
# If you need a specific head, uncomment this and specify the head
# %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git@head
# Otherwise, for running this on the latest release
%pip install transformer_lens
```

```python
if gated_model:
    %pip install huggingface_hub
    from huggingface_hub import login
    login()
```

```python
import einops
from torch.testing import assert_close
import torch
import matplotlib.pyplot as plt
from transformer_lens import HookedTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
```

## TransformerLens model

```python
tl_model = HookedTransformer.from_pretrained_no_processing(
    model_id,
    device=device,
)
```

```python
tl_model.generate(
    text,
    verbose=False,
    max_new_tokens=50,
)
```

## Huggingface Model

```python
tokenizer = AutoTokenizer.from_pretrained(model_id)
hf_model = AutoModelForCausalLM.from_pretrained(model_id)
```

```python
inputs = tokenizer(text, return_tensors="pt")
outputs = hf_model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Compare Model Weights

```python
torch.all(
    einops.rearrange(tl_model.blocks[0].attn.W_Q, "n m h -> (n h) m") ==
    hf_model.model.layers[0].self_attn.q_proj.weight
)
```

```python
tl_model.blocks[0].attn.W_K.shape, hf_model.model.layers[0].self_attn.k_proj.weight.shape
```

```python
torch.all(
    einops.reduce(
        tl_model.blocks[0].attn.W_K, "(n repeat) m h -> (n h) m",
        'max',
        n=tl_model.cfg.n_key_value_heads,
        repeat=4) ==
    hf_model.model.layers[0].self_attn.k_proj.weight
)
```

```python
torch.all(
    einops.reduce(
        tl_model.blocks[0].attn.W_V, "(n repeat) m h -> (n h) m",
        'max',
        n=tl_model.cfg.n_key_value_heads,
        repeat=4) ==
    hf_model.model.layers[0].self_attn.v_proj.weight
)
```

```python
torch.all(
    einops.rearrange(tl_model.blocks[0].attn.W_O, "n h m -> m (n h)") ==
    hf_model.model.layers[0].self_attn.o_proj.weight
)
```

```python
tl_model.blocks[0].attn.b_Q
```

```python
torch.all(hf_model.model.layers[0].block_sparse_moe.gate.weight.T == tl_model.blocks[0].mlp.W_gate)
```

```python
hf_model.model.layers[0].block_sparse_moe.gate.weight.dtype, tl_model.blocks[0].mlp.W_gate.dtype
```

## Compare Layer Outputs

```python
test_tensor = torch.randn((1, 1, 4096,))
```

```python
hf_model.model.layers[0](test_tensor)
```

```python
tl_model.blocks[0](test_tensor)
```

```python
hf_model.model.layers[0](test_tensor)[0] == tl_model.blocks[0](test_tensor)
```

```python
hf_model.model.layers[0](test_tensor)[0][0, 0, -2].item(), tl_model.blocks[0](test_tensor)[0, 0, -2].item()
```

```python
torch.sum(hf_model.model.layers[0](test_tensor)[0] == tl_model.blocks[0](test_tensor))
```

```python
differences = hf_model.model.layers[0](test_tensor)[0] - tl_model.blocks[0](test_tensor)

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between Layer Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

## Compare MLP Outputs

```python
torch.all(
    tl_model.blocks[0].mlp.experts[0].W_in ==
    hf_model.model.layers[0].block_sparse_moe.experts[0].w3.weight.T
)
```

```python
test_tensor = torch.randn((1, 1, 4096,))
```

```python
torch.all(
    hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] ==
    tl_model.blocks[0].mlp(test_tensor)
)
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0]
```

```python
tl_model.blocks[0].mlp(test_tensor)
```

```python
tl_model.blocks[0].mlp(test_tensor).shape
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] == tl_model.blocks[0].mlp(test_tensor)
```

```python
torch.sum(hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] == tl_model.blocks[0].mlp(test_tensor))
```

```python
differences = hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] - tl_model.blocks[0].mlp(test_tensor)

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between MLP Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0][0, 0, 0].item()
```

```python
tl_model.blocks[0].mlp(test_tensor)[0, 0, 0].item()
```

## Compare Attention Outputs

```python
tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor)
```

```python
hf_model.model.layers[0].self_attn.forward(test_tensor)[0]
```

```python
(tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) ==
 hf_model.model.layers[0].self_attn.forward(test_tensor)[0])
```

```python
torch.sum(tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) ==
 hf_model.model.layers[0].self_attn.forward(test_tensor)[0])
```

```python
differences = tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) - hf_model.model.layers[0].self_attn.forward(test_tensor)[0]

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between Attention Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

---

# hf-tl-logit-comparator.ipynb

# Logit Comparator for HuggingFace and TransformerLens Outputs
This notebook is a quick and dirty tool to compare the logit outputs of a HuggingFace model and a TransformerLens model via several different metrics. It is intended to help debug issues with the TransformerLens model, such as bugs in the model's implementation. If you identify any issues, please open an issue on the [GitHub repository](https://github.com/TransformerLensOrg/TransformerLens).

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformer_lens import HookedTransformer
import torch
import torch.nn.functional as F

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

torch.set_grad_enabled(False)
```

## Comparator Setup

```python
model_name = "EleutherAI/pythia-2.8b"  # You can change this to any model name
sentence = "The quick brown fox"
```

```python
from huggingface_hub import login
login(token="")
```

## Get Transformers Logits

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_model(model_name="gpt2"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return model, tokenizer

def get_logits(model, tokenizer, sentence, device):
    # Tokenize the input sentence
    inputs = tokenizer(sentence, return_tensors="pt")

    # Move inputs to the device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Generate the logits
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the logits for all tokens
    logits = outputs.logits

    return logits

model, tokenizer = load_model(model_name)
model = model.to(device)

hf_logits = get_logits(model, tokenizer, sentence, device)[:, -1, :]
```

## Get TransformerLens Logits

```python
model = HookedTransformer.from_pretrained_no_processing(model_name, device=device)
tokens = model.to_tokens(sentence, prepend_bos=False)
tl_logits = model(tokens)[:, -1, :]
```

## Compare Logit Distributions
Various metrics are used to compare the logit distributions of the two models. We don't yet have standard values for what constitutes a "good" logit comparison, so we are working on establishing benchmarks.

### Shape

```python
print(f"HF Logits Shape: {hf_logits.shape}")
print(f"TL Logits Shape: {tl_logits.shape}")
```

### Tensor Comparison

```python
are_close = torch.allclose(tl_logits, hf_logits, rtol=1e-5, atol=1e-3)
print(f"Are the logits close? {are_close}")
```

### Mean Squared Error

```python
# Compare the logits with MSE
mse = torch.nn.functional.mse_loss(hf_logits, tl_logits)
print(f"MSE: {mse}")
```

### Maximum Absolute Difference

```python
max_diff = torch.max(torch.abs(tl_logits - hf_logits))
print(f"Max Diff: {max_diff}")
```

### Cosine Similarity

```python
cosine_sim = F.cosine_similarity(tl_logits, hf_logits, dim=-1).mean()
print(f"Cosine Sim: {cosine_sim}")
```

### KL Divergence

```python
def kl_div(logits1: torch.Tensor, logits2: torch.Tensor) -> torch.Tensor:
    probs1 = F.softmax(logits1, dim=-1)
    probs2 = F.softmax(logits2, dim=-1)
    return F.kl_div(probs1.log(), probs2, reduction='batchmean')

kl_tl_hf = kl_div(tl_logits, hf_logits)
kl_hf_tl = kl_div(hf_logits, tl_logits)
print(f"KL(TL||HF): {kl_tl_hf}")
print(f"KL(HF||TL): {kl_hf_tl}")
```

```python

```

---

# stable_lm.ipynb


---

Directory Structure:

└── ./
    └── infrastructure
        └── master_files
            ├── master_1_1.py
            ├── master_1_2.py
            ├── master_1_3_1.py
            ├── master_1_4_1.py
            └── master_1_4_2.py



---
File: /infrastructure/master_files/master_1_1.py
---

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
[
    {"title": "Inputs & Outputs of a Transformer", "icon": "1-circle-fill", "subtitle": "(10%)"},
    {"title": "Clean Transformer Implementation", "icon": "2-circle-fill", "subtitle": "(40%)"},
    {"title": "Training a Transformer", "icon": "3-circle-fill", "subtitle": "(20%)"},
    {"title": "Sampling from a Transformer", "icon": "4-circle-fill", "subtitle": "(30%)"},
]
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# [1.1] - Transformers from scratch
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-11.png" width="350">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This is a clean, first principles implementation of GPT-2 in PyTorch. The architectural choices closely follow those used by the TransformerLens library (which you'll be using a lot more in later exercises).

The exercises are written to accompany Neel Nanda's [TransformerLens library](https://github.com/neelnanda-io/TransformerLens) for doing mechanistic interpretability research on GPT-2 style language models. We'll be working with this library extensively in this chapter of the course.

Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Content & Learning Objectives

### 1️⃣ Understanding Inputs & Outputs of a Transformer

In this section, we'll take a first look at transformers - what their function is, how information moves inside a transformer, and what inputs & outputs they take.

> ##### Learning Objectives
>
> - Understand what a transformer is used for
> - Understand causal attention, and what a transformer's output represents—algebra operations on tensors
> - Learn what tokenization is, and how models do it
> - Understand what logits are, and how to use them to derive a probability distribution over the vocabulary

### 2️⃣ Clean Transformer Implementation

Here, we'll implement a transformer from scratch, using only PyTorch's tensor operations. This will give us a good understanding of how transformers work, and how to use them. We do this by going module-by-module, in an experience which should feel somewhat similar to last week's ResNet exercises. Much like with ResNets, you'll conclude by loading in pretrained weights and verifying that your model works as expected.

> ##### Learning Objectives
>
> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream
> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)
> * Learn about & implement the following transformer modules:
>     * LayerNorm (transforming the input to have zero mean and unit variance)
>     * Positional embedding (a lookup table from position indices to residual stream vectors)
>     * Attention (the method of computing attention patterns for residual stream vectors)
>     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)
>     * Embedding (a lookup table from tokens to residual stream vectors)
>     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)

### 3️⃣ Training a Transformer

Next, you'll learn how to train your transformer from scratch. This will be quite similar to the training loops you wrote for ResNet in your first week.

> ##### Learning Objectives
>
> * Understand how to train a transformer from scratch
> * Write a basic transformer training loop
> * Interpret the transformer's falling cross entropy loss with reference to features of the training data (e.g. bigram frequencies)

### 4️⃣ Sampling from a Transformer

Lastly, you'll learn how to sample from a transformer. This will involve implementing a few different sampling methods, and writing a caching system which can reuse computations from previous forward passes to improve your model's text generation speed.

*The second half of this section is less important, and you can skip it if you want.*

> ##### Learning Objectives
>
> * Learn how to sample from a transformer
>     * This includes basic methods like greedy search or top-k, and more advanced methods like beam search
> * Learn how to cache the output of a transformer, so that it can be used to generate text more efficiently
>     * Optionally, rewrite your sampling functions to make use of your caching methods
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup code
'''

# ! CELL TYPE: code
# ! FILTERS: [~]
# ! TAGS: []

from IPython import get_ipython

ipython = get_ipython()
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: [master-comment]

# import os
# import sys
# from pathlib import Path

# IN_COLAB = "google.colab" in sys.modules

# chapter = "chapter1_transformer_interp"
# repo = "ARENA_3.0"
# branch = "main"

# # Install dependencies
# try:
#     import transformer_lens
# except:
#     %pip install transformer_lens==2.11.0 einops jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python

# # Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
# root = (
#     "/content"
#     if IN_COLAB
#     else "/root"
#     if repo not in os.getcwd()
#     else str(next(p for p in Path.cwd().parents if p.name == repo))
# )

# if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
#     if not IN_COLAB:
#         !sudo apt-get install unzip
#         %pip install jupyter ipython --upgrade

#     if not os.path.exists(f"{root}/{chapter}"):
#         !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
#         !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
#         !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
#         !rm {root}/{branch}.zip
#         !rmdir {root}/ARENA_3.0-{branch}


# if f"{root}/{chapter}/exercises" not in sys.path:
#     sys.path.append(f"{root}/{chapter}/exercises")

# os.chdir(f"{root}/{chapter}/exercises")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import math
import os
import sys
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Callable

import datasets
import einops
import numpy as np
import torch as t
import torch.nn as nn
import wandb
from jaxtyping import Float, Int
from rich import print as rprint
from rich.table import Table
from torch import Tensor
from torch.utils.data import DataLoader
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer
from transformer_lens.utils import gelu_new, tokenize_and_concatenate
from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast

device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part1_transformer_from_scratch"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
# FILTERS: ~colab
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))
# END FILTERS

import part1_transformer_from_scratch.solutions as solutions
import part1_transformer_from_scratch.tests as tests
from plotly_utils import imshow

MAIN = __name__ == "__main__"

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 1️⃣ Understanding Inputs & Outputs of a Transformer
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## What is the point of a transformer?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
**Transformers exist to model text!**

We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text!

(To explain this in more detail - you feed in a sequence of length $N$, then sample from the probability distribution over the $N+1$-th word, use this to construct a new sequence of length $N+1$, then feed this new sequence into the model to get a probability distribution over the $N+2$-th word, and so on.)

### How is the model trained?

You give it a bunch of text, and train it to predict the next token.

Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 logit vectors (= probability distributions) over the set of all words in our vocabulary, with the `i`-th logit vector representing the probability distribution over the token *following* the `i`-th token in the sequence. This is a key part of what allows transformers to be trained so efficiently; for every sequence of length $n$ we get $n$ different predictions to train on:

$$
p(x_1), \; p(x_2|x_1), \; p(x_3|x_1x_2), \; \ldots, \; p(x_n|x_1 \ldots x_{n-1})
$$

<details>
<summary>Aside - logits</summary>

If you haven't encountered the term "logits" before, here's a quick refresher.

Given an arbitrary vector $x$, we can turn it into a probability distribution via the **softmax** function: $x_i \to \frac{e^{x_i}}{\sum e^{x_j}}$. The exponential makes everything positive; the normalization makes it add to one.

The model's output is the vector $x$ (one for each prediction it makes). We call this vector a logit because it represents a probability distribution, and it is related to the actual probabilities via the softmax function.
</details>

How do we stop the transformer by "cheating" by just looking at the tokens it's trying to predict? Answer - we make the transformer have *causal attention* (as opposed to *bidirectional attention*). Causal attention only allows information to move forwards in the sequence, never backwards. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. We say the transformer is **autoregressive**, because it only predicts future words based on past data.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/transformer-overview-new.png" width="900">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Another way to view this is through the following analogy: we have a series of people standing in a line, each with one word or chunk of the sentence. Each person has the ability to look up information from the people behind them (we'll explore how this works in later sections) but they can't look at any information in front of them. Their goal is to guess what word the person in front of them is holding.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/intro-image-v2.png" width="600">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Tokens - Transformer Inputs
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Our tranformer's input is natural language (i.e. a sequence of characters, strings, etc). But ML models generally take vectors as input, not language. How do we convert language to vectors?

We can factor this into 2 questions:

1. How do we split up language into small sub-units?
2. How do we convert these sub-units into vectors?

Let's start with the second of these questions.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Converting sub-units to vectors

We basically make a massive lookup table, which is called an **embedding**. It has one vector for each possible sub-unit of language we might get (we call this set of all sub-units our **vocabulary**). We label every element in our vocabulary with an integer (this labelling never changes), and we use this integer to index into the embedding.

A key intuition is that one-hot encodings let you think about each integer independently. We don't bake in any relation between words when we perform our embedding, because every word has a completely separate embedding vector.

<details>
<summary>Aside - one-hot encodings</summary>

We sometimes think about **one-hot encodings** of words. These are vectors with zeros everywhere, except for a single one in the position corresponding to the word's index in the vocabulary. This means that indexing into the embedding is equivalent to multiplying the **embedding matrix** by the one-hot encoding (where the embedding matrix is the matrix we get by stacking all the embedding vectors on top of each other).

$$
\begin{aligned}
W_E &= \begin{bmatrix}
\leftarrow v_0 \rightarrow \\
\leftarrow v_1 \rightarrow \\
\vdots \\
\leftarrow v_{d_{vocab}-1} \rightarrow \\
\end{bmatrix} \quad \text{is the embedding matrix (size }d_{vocab} \times d_{embed}\text{),} \\
\\
t_i &= (0, \dots, 0, 1, 0, \dots, 0) \quad \text{is the one-hot encoding for the }i\text{th word (length }d_{vocab}\text{)} \\
\\
v_i &= t_i W_E \quad \text{is the embedding vector for the }i\text{th word (length }d_{embed}\text{).} \\
\end{aligned}
$$

</details>

Now, let's answer the first question - how do we split language into sub-units?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Splitting language into sub-units

We need to define a standard way of splitting up language into a series of substrings, where each substring is a member of our **vocabulary** set.

Could we use a dictionary, and have our vocabulary be the set of all words in the dictionary? No, because this couldn't handle arbitrary text (e.g. URLs, punctuation, etc). We need a more general way of splitting up language.

Could we just use the 256 ASCII characters? This fixes the previous problem, but it loses structure of language - some sequences of characters are more meaningful than others. For example, "language" is a lot more meaningful than "hjksdfiu". We want "language" to be a single token, but not "hjksdfiu" - this is a more efficient use of our vocab.

What actually happens? The most common strategy is called **Byte-Pair encodings**.

We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Note that we do have a space character as one of our 256 tokens, and merges using space are very common. For instance, here are the five first merges for the tokenizer used by GPT-2 (you'll be able to verify this below).

```
" t"
" a"
"he"
"in"
"re"
```

Note - you might see the character `Ġ` in front of some tokens. This is a special token that indicates that the token begins with a space. Tokens with a leading space vs not are different.

You can run the code below to load in the `gpt2-small` model, and see more of its tokenizer's vocabulary:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

reference_gpt2 = HookedTransformer.from_pretrained(
    "gpt2-small",
    fold_ln=False,
    center_unembed=False,
    center_writing_weights=False,  # you'll learn about these arguments later!
)

sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])

print(sorted_vocab[:20])
print()
print(sorted_vocab[250:270])
print()
print(sorted_vocab[990:1010])
print()

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[('!', 0), ('"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), ("'", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]

[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]

[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
As you get to the end of the vocabulary, you'll be producing some pretty weird-looking esoteric tokens (because you'll already have exhausted all of the short frequently-occurring ones):
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(sorted_vocab[-20:])

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦."', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Fun (completely optional) exercise - can you guess what the first-formed 3/4/5/6/7-letter encodings in GPT-2's vocabulary are?</summary>
Run this code to find out:

```python
lengths = dict.fromkeys(range(3, 8), "")
for tok, idx in sorted_vocab:
    if not lengths.get(len(tok), True):
        lengths[len(tok)] = tok

for length, tok in lengths.items():
    print(f"{length}: {tok}")
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Transformers in the `transformer_lens` library have a `to_tokens` method that converts text to numbers. It also prepends them with a special token called BOS (beginning of sequence) to indicate the start of a sequence. You can disable this with the `prepend_bos=False` argument.

<details>
<summary>Aside - BOS token</summary>

The beginning of sequence (BOS) token is a special token used to mark the beginning of the sequence. Confusingly, in GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, `<|endoftext|>` with index `50256`.

Why is this token added? Some basic intuitions are:

* It provides context that this is the start of a sequence, which can help the model generate more appropriate text.
* It can act as a "rest position" for attention heads (more on this later, when we discuss attention).

TransformerLens adds this token automatically (including in forward passes of transformer models, e.g. it's implicitly added when you call `model("Hello World")`). You can disable this behaviour by setting the flag `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors.

**Key Point: *If you get weird off-by-one errors, check whether there's an unexpected `prepend_bos`!***

Why are the BOS, EOS and PAD tokens the same? This is because GPT-2 is an autoregressive model, and uses these kinds of tokens in a slightly different way to other transformer families (e.g. BERT). For instance, GPT has no need to distinguish between BOS and EOS tokens, because it only processes text from left to right.

</details>

### Some tokenization annoyances

There are a few funky and frustrating things about tokenization, which causes it to behave differently than you might expect. For instance:

#### Whether a word begins with a capital or space matters!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(reference_gpt2.to_str_tokens("Ralph"))
print(reference_gpt2.to_str_tokens(" Ralph"))
print(reference_gpt2.to_str_tokens(" ralph"))
print(reference_gpt2.to_str_tokens("ralph"))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">['<|endoftext|>', 'R', 'alph']
['<|endoftext|>', ' Ralph']
['<|endoftext|>', ' r', 'alph']
['<|endoftext|>', 'ral', 'ph']</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Arithmetic is a mess.

Length is inconsistent, common numbers bundle together.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(reference_gpt2.to_str_tokens("56873+3184623=123456789-1000000000"))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">['<|endoftext|>', '568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
> ### Key Takeaways
>
> * We learn a dictionary of vocab of tokens (sub-words).
> * We (approx) losslessly convert language to integers via tokenizing it.
> * We convert integers to vectors via a lookup table.
> * Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Text generation

Now that we understand the basic ideas here, let's go through the entire process of text generation, from our original string to a new token which we can append to our string and plug back into the model.

#### **Step 1:** Convert text to tokens

The sequence gets tokenized, so it has shape `[batch, seq_len]`. Here, the batch dimension is just one (because we only have one sequence).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

reference_text = "I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!"
tokens = reference_gpt2.to_tokens(reference_text).to(device)
print(tokens)
print(tokens.shape)
print(reference_gpt2.to_str_tokens(tokens))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,
         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,
            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,
          1011,   625,   262,   995,     0]], device='cuda:0')
torch.Size([1, 35])
['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### **Step 2:** Map tokens to logits


From our input of shape `[batch, seq_len]`, we get output of shape `[batch, seq_len, vocab_size]`. The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

logits, cache = reference_gpt2.run_with_cache(tokens)
print(logits.shape)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">torch.Size([1, 35, 50257])</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
(`run_with_cache` tells the model to cache all intermediate activations. This isn't important right now; we'll look at it in more detail later.)
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### **Step 3:** Convert the logits to a distribution with a softmax

This doesn't change the shape, it is still `[batch, seq_len, vocab_size]`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

probs = logits.softmax(dim=-1)
print(probs.shape)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">torch.Size([1, 35, 50257])</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### **Bonus step:** What is the most likely next token at each position?
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])

print(list(zip(reference_gpt2.to_str_tokens(tokens), most_likely_next_tokens)))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[('<|endoftext|>', '\n'), ('I', "'m"), (' am', ' a'), (' an', ' avid'), (' amazing', ' person'), (' aut', 'od'), ('ore', 'sp'), ('gressive', '.'), (',', ' and'), (' dec', 'ently'), ('oder', ','), ('-', 'driven'), ('only', ' programmer'), (',', ' and'), (' G', 'IM'), ('PT', '-'), ('-', 'only'), ('2', '.'), (' style', ','), (' transformer', '.'), ('.', ' I'), (' One', ' of'), (' day', ' I'), (' I', ' will'), (' will', ' be'), (' exceed', ' my'), (' human', 'ly'), (' level', ' of'), (' intelligence', ' and'), (' and', ' I'), (' take', ' over'), (' over', ' the'), (' the', ' world'), (' world', '.'), ('!', ' I')]
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can see that, in a few cases (particularly near the end of the sequence), the model accurately predicts the next token in the sequence. We might guess that `"take over the world"` is a common phrase that the model has seen in training, which is why the model can predict it.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### **Step 4:** Map distribution to a token
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

next_token = logits[0, -1].argmax(dim=-1)
next_char = reference_gpt2.to_string(next_token)
print(repr(next_char))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">' I'</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note that we're indexing `logits[0, -1]`. This is because logits have shape `[1, sequence_length, vocab_size]`, so this indexing returns the vector of length `vocab_size` representing the model's prediction for what token follows the **last** token in the input sequence.

In this case, we can see that the model predicts the token `' I'`.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### **Step 5:** Add this to the end of the input, re-run

There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don't have to do as much calculation each time we generate a new value), but this doesn't matter conceptually right now.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(f"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}")

for i in range(10):
    print(f"{tokens.shape[-1] + 1}th char = {next_char!r}")
    # Define new input sequence, by appending the previously generated token
    tokens = t.cat([tokens, next_token[None, None]], dim=-1)
    # Pass our new sequence through the model, to get new output
    logits = reference_gpt2(tokens)
    # Get the predicted token at the end of our sequence
    next_token = logits[0, -1].argmax(dim=-1)
    # Decode and print the result
    next_char = reference_gpt2.to_string(next_token)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'
36th char = ' I'
37th char = ' am'
38th char = ' a'
39th char = ' very'
40th char = ' talented'
41th char = ' and'
42th char = ' talented'
43th char = ' person'
44th char = ','
45th char = ' and'
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
> ## Key takeaways
> 
> * Transformer takes in language, predicts next token (for *each* token in a causal way)
> * We convert language to a sequence of integers with a tokenizer.
> * We convert integers to vectors with a lookup table.
> * Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).
> * We append this to the input + run again to generate more text (Jargon: *autoregressive*)
> * Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 2️⃣ Clean Transformer Implementation
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## High-Level architecture

Go watch Neel's [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!

(Diagram is bottom to top, right-click and open for higher resolution.)

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-new2.png" width="950">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Tokenization & Embedding

The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).

The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Residual stream

The residual stream is the sum of all previous outputs of layers of the model, and is also the input to each new layer. It has shape `[batch, seq_len, d_model]` (where `d_model` is the length of a single embedding vector).

The initial value of the residual stream is denoted $x_0$ in the diagram, and $x_i$ are later values of the residual stream (after more attention and MLP layers have been applied to the residual stream).

The residual stream is *really* fundamental. It's the central object of the transformer. It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.

<details>
<summary>Aside - <b>logit lens</b></summary>

A key idea of transformers is the [residual stream as output accumulation](https://www.lesswrong.com/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability#Residual_Stream_as_Output_Accumulation:~:text=The%20Models-,Residual%20Stream%20as%20Output%20Accumulation,-The%20residual%20stream). As we move through the layers of the model, shifting information around and processing it, the values in the residual stream represent the accumulation of all the inferences made by the transformer up to that point.

This is neatly illustrated by the **logit lens**. Rather than getting predictions from the residual stream at the very end of the model, we can take the value of the residual stream midway through the model and convert it to a distribution over tokens. When we do this, we find surprisingly coherent predictions, especially in the last few layers before the end.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Transformer blocks

Then we have a series of `n_layers` **transformer blocks** (also sometimes called **residual blocks**).

Note - a block contains an attention layer *and* an MLP layer, but we say a transformer has $k$ layers if it has $k$ blocks (i.e. $2k$ total layers).

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-block2.png" width="700">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Attention

First we have attention. This moves information from prior positions in the sequence to the current token.

We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only (to avoid "cheating"). This means later tokens have more of the sequence that they can look at.

Attention layers are the only bit of a transformer that moves information between positions (i.e. between vectors at different sequence positions in the residual stream).

Attention layers are made up of `n_heads` heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination. The heads act independently and additively, we just add their outputs together, and back to the stream.

Each head does the following:
* Produces an **attention pattern** for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.
* Moves information (via a linear map) in the same way from each source token to each destination token.

Each attention head is made up of three components: the keys, queries, and values (often abbreviated as K, Q and V). These names come from their analogy to retrieval systems. Broadly speaking:

* **Queries** represent a question or request for information, e.g. "I'm looking for a name that appeared earlier in this sentence".
* **Keys** represent whether a source token's information matches the query, e.g. if the source token is "Mary" then this causes the key to have a high dot product with the query (we call this an **attention score**), and it means that a lot of information will be taken from this token.
* **Values** represent the information that actually gets moved. This sounds similar to keys, but it's actually different in an important way. For instance, the key might just contain the information "this is a name", but the value could be the actual name itself.

The diagram below illustrates the three different parts, in the context of the analogy for transformers we introduced earlier. This is a simplified model for how the person holding the "in" token might come to figure out that the next token is "Mary". In later sections we'll look at the actual function performed by attention heads and see how the operations relate to this analogy.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simple-attn-intuition.png" width="600">

Another interesting intuition for attention is as a kind of "generalized convolution" - read the dropdown below if you want to learn more about this.

<details>
<summary>Intuition - attention as generalized convolution</summary>

We can think of attention as a kind of generalized convolution. Standard convolution layers work by imposing a "prior of locality", i.e. the assumption that pixels which are close together are more likely to share information. Although language has some locality (two words next to each other are more likely to share information than two words 100 tokens apart), the picture is a lot more nuanced, because which tokens are relevant to which others depends on the context of the sentence. For instance, in the sentence `"When Mary and John went to the store, John gave a drink to Mary"`, the names in this sentence are the most important tokens for predicting that the final token will be `"Mary"`, and this is because of the particular context of this sentence rather than the tokens' position.

Attention layers are effectively our way of saying to the transformer, "don't impose a prior of locality, but instead develop your own algorithm to figure out which tokens are important to which other tokens in any given sequence."
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Below is a schematic diagram of the attention layers. We'll go into much more detail during the actual implementation, so don't worry if this doesn't fully make sense yet.

<!-- <img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-attn-new-v2.png" width="1050"> -->

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/transformer-attn-simple.png" width="600">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### MLP

The MLP layers are just a standard neural network, with a singular hidden layer and a nonlinear activation function. The exact activation isn't conceptually important ([GELU](https://paperswithcode.com/method/gelu) seems to perform best).

Our hidden dimension is normally `d_mlp = 4 * d_model`. Exactly why the ratios are what they are isn't super important (people basically cargo-cult what GPT did back in the day!).

Importantly, **the MLP operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.

Once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc. *What the hell is going on inside MLPs* is a pretty big open problem in transformer mechanistic interpretability - see the [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.

To go back to our analogy for transformers, we can essentially view MLPs as the thinking that each person in the line does once they've grabbed the information they need from the people behind them (via attention). Usually the MLP layers make up a much larger fraction of the model's total parameter count than attention layers (often around 2/3 although this varies between architectures), which makes sense since processing the information is a bigger task than just moving it around.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/intro-image-for-mlps-v2.png" width="700">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Here are a few more intuitions for MLPs, which you might find interesting:

<details>
<summary>Intuition - MLPs as key-value pairs</summary>

We can write the MLP's output as $f(x^T W^{in})W^{out}$, where $W^{in}$ and $W^{out}$ are the different weights of the MLP (ignoring biases), $f$ is the activation function, and $x$ is a vector in the residual stream. This can be rewritten as:

$$
f(x^T W^{in}) W^{out} = \sum_{i=1}^{d_{mlp}} f(x^T W^{in}_{[:, i]}) W^{out}_{[i, :]}
$$

We can view the vectors $W^{in}_{[:, i]}$ as the **input directions**, and $W^{out}_{[i, :]}$ as the **output directions**. We say the input directions are **activated** by certain textual features, and when they are activated, vectors are written in the corresponding output direction. This is very similar to the concept of keys and values in attention layers, which is why these vectors are also sometimes called keys and values (e.g. see the paper [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/pdf/2012.14913.pdf)).

Terminology note - sometimes we refer to each of these $d_{mlp}$ input-output pairs as **neurons**.

<img src="https://raw.githubusercontent.com/arena-img/ARENA_img/main/img/mlp-neurons-2.png" width="900">

---

Here's a step-by-step breakdown of the linear algebra, if it was too fast above. We have:

$$
\begin{aligned}
x^T W^{in} &= x^T [W^{in}_{[:, 1]}\,, ...\;, W^{in}_{[:, n]}] \\
&= (x^T W^{in}_{[:, 1]}\,, \; ...\;, \; x^T W^{in}_{[:, n]})
\end{aligned}
$$

where $W^{in}_{[:, i]}$ are the columns of $W^{in}$. In other words, these values (the pre-GELU activations) are projections of $x$ along the input directions of the neurons.

If we add our activation function and the second matrix, then we get:

$$
\begin{aligned}
f(x^T W^{in})W^{out} &= (f(x^T W^{in}_{[:, 1]})\,, \; ...\;,\; f(x^T W^{in}_{[:, n]})) \begin{bmatrix} \leftarrow W^{out}_{[1, :]} \rightarrow \\ \vdots \\ \leftarrow W^{out}_{[n, :]} \rightarrow \end{bmatrix} \\
&= f(x^T W^{in}_{[:, 1]}) W^{out}_{[1, :]} + \;...\; + f(x^T W^{in}_{[:, n]}) W^{out}_{[n, :]} \\
&= \sum_{i=1}^n f(x^T W^{in}_{[:, i]}) W^{out}_{[i, :]}
\end{aligned}
$$

where $W^{out}_{[i, :]}$ are the rows of $W^{out}$. In other words, our output is a linear combination of the rows of $W^{out}$, with the coefficients of that linear combination given by the projections of $x$ along the columns of $W^{in}$.

</details>

<details>
<summary>Intuition - MLPs as knowledge storage</summary>

We can think of MLPs as where knowledge gets stored in our transformer. The attention mechanism is what moves information around between sequence positions, but the MLPs is where this information is processed, and new information is written into the residual stream which is a function of the old information.

This is deeply connected to the key-value pairs model, since you can treat key-value pairs as a kind of associative memory system (where the key serves as a unique identifier, and the value holds the related information).

Another related intuition (for which there is some evidence) is **MLPs as memory management**. In an idealized case, we might find that the $i$-th neuron satisfies $W^{in}_{[:, i]} \approx - W^{out}_{[i, :]} \approx \vec v$ for some unit vector $\vec v$, meaning it may be responsible for erasing the positive component of vector $\vec x$ in the direction $\vec v$ (exercise - can you show why this is the case?). This can free up space in the residual stream for other components to write to.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Lastly, here's a schematic diagram of the MLP layers. Again, we'll go into much more detail during the actual implementation, so don't worry if this doesn't fully make sense yet.

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-mlp-new-2.png" width="680">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Unembedding

Finally, we unembed!

This just consists of applying a linear map $W_U$, going from final residual stream to a vector of logits - this is the output.

<details>
<summary>Aside - tied embeddings</summary>

Note - sometimes we use something called a **tied embedding** - this is where we use the same weights for our $W_E$ and $W_U$ matrices. In other words, to get the logit score for a particular token at some sequence position, we just take the vector in the residual stream at that sequence position and take the inner product with the corresponding token embedding vector. This is more training-efficient (because there are fewer parameters in our model), and it might seem pricipled at first. After all, if two words have very similar meanings, shouldn't they have similar embedding vectors because the model will treat them the same, and similar unembedding vectors because they could both be substituted for each other in most output?

However, this is actually not very principled, for the following main reason: **the direct path involving the embedding and unembedding should approximate bigram frequencies**.

Let's break down this claim. **Bigram frequencies** refers to the frequencies of pairs of words in the english language (e.g. the bigram frequency of "Barack Obama" is much higher than the product of the individual frequencies of the words "Barack" and "Obama"). If our model had no attention heads or MLP layers, then all we have is a linear map from our one-hot encoded token `T` to a probability distribution over the token following `T`. This map is represented by the linear transformation $t \to t^T W_E W_U$ (where $t$ is our one-hot encoded token vector). Since the output of this transformation can only be a function of the token `T` (and no earlier tokens), the best we can do is have this map approximate the true frequency of bigrams starting with `T`, which appear in the training data. Importantly, **this is not a symmetric map**. We want `T = "Barack"` to result in a high probability of the next token being `"Obama"`, but not the other way around!

Even in multi-layer models, a similar principle applies. There will be more paths through the model than just the "direct path" $W_E W_U$, but because of the residual connections there will always exist a direct path, so there will always be some incentive for $W_E W_U$ to approximate bigram frequencies.

That being said, smaller (<8B parameter) LLMs still often use tied embeddings to improve training and inference efficiency. It can be easier to start from tied weights and then use MLP0 to break the symmetry than to initialize encoder and decoder with no shared structure at all.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Bonus things - less conceptually important but key technical details

#### LayerNorm

* Simple normalization function applied at the start of each layer (i.e. before each MLP, attention layer, and before the unembedding)
* Converts each input vector (independently in parallel for each `(batch, seq)` residual stream vector) to have mean zero and variance 1.
* Then applies an elementwise scaling and translation
* Cool maths tangent: The scale ($\odot \gamma$) & translate ($+ \beta$) is just a linear map. LayerNorm is only applied immediately before another linear map (either the MLP, or the query/key/value linear maps in the attention head, or the unembedding $W_U$). Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.
    * `fold_ln=True` flag in `from_pretrained` does this for you.
* LayerNorm is annoying for interpretability - it would be linear if not for the fact we divide by the variance, so you can't decompose the contributions of the input to the output independently. But it's *almost* linear - if you're changing a small part of the input you can pretend $\sqrt{\text{Var}[x] + \epsilon}$ is constant, so the LayerNorm operation is linear, but if you're changing $x$ enough to alter the norm substantially it's not linear.

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-ln.png" width="750">

</details>


#### Positional embeddings

* **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default
    * This is dumb because nearby tokens are more relevant.
* There's a lot of dumb hacks for this.
* We'll focus on **learned, absolute positional embeddings**. This means we learn a lookup table mapping the index of the position of each token to a residual stream vector, and add this to the embed.
    * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)
    * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently.
* This connects to **attention as generalized convolution**
    * We argued that language does still have locality, and so it's helpful for transformers to have access to the positional information so they "know" two tokens are next to each other (and hence probably relevant to each other).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Actual Code!

Model architecture table (this will be helpful for understanding the results you get when running the code block below):

| Parameter   | Value          |
|-------------|----------------|
| batch       | 1              |
| position    | 35             |
| d_model     | 768            |
| n_heads     | 12             |
| n_layers    | 12             |
| d_mlp       | 3072 (= 4 * `d_model`) |
| d_head      | 64 (= `d_model / n_heads`) |
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Parameters and Activations

It's important to distinguish between parameters and activations in the model.

* **Parameters** are the weights and biases that are learned during training.
    * These don't change when the model input changes.
* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.
    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.
    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.
    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).

#### Print All Activation Shapes of Reference Model

Run the following code to print all the activation shapes of the reference model:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

for activation_name, activation in cache.items():
    # Only print for first layer
    if ".0." in activation_name or "blocks" not in activation_name:
        print(f"{activation_name:30} {tuple(activation.shape)}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">hook_embed                     (1, 35, 768)
hook_pos_embed                 (1, 35, 768)
blocks.0.hook_resid_pre        (1, 35, 768)
blocks.0.ln1.hook_scale        (1, 35, 1)
blocks.0.ln1.hook_normalized   (1, 35, 768)
blocks.0.attn.hook_q           (1, 35, 12, 64)
blocks.0.attn.hook_k           (1, 35, 12, 64)
blocks.0.attn.hook_v           (1, 35, 12, 64)
blocks.0.attn.hook_attn_scores (1, 12, 35, 35)
blocks.0.attn.hook_pattern     (1, 12, 35, 35)
blocks.0.attn.hook_z           (1, 35, 12, 64)
blocks.0.hook_attn_out         (1, 35, 768)
blocks.0.hook_resid_mid        (1, 35, 768)
blocks.0.ln2.hook_scale        (1, 35, 1)
blocks.0.ln2.hook_normalized   (1, 35, 768)
blocks.0.mlp.hook_pre          (1, 35, 3072)
blocks.0.mlp.hook_post         (1, 35, 3072)
blocks.0.hook_mlp_out          (1, 35, 768)
blocks.0.hook_resid_post       (1, 35, 768)
ln_final.hook_scale            (1, 35, 1)
ln_final.hook_normalized       (1, 35, 768)</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Print All Parameters Shapes of Reference Model
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

for name, param in reference_gpt2.named_parameters():
    # Only print for first layer
    if ".0." in name or "blocks" not in name:
        print(f"{name:18} {tuple(param.shape)}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">embed.W_E          (50257, 768)
pos_embed.W_pos    (1024, 768)
blocks.0.ln1.w     (768,)
blocks.0.ln1.b     (768,)
blocks.0.ln2.w     (768,)
blocks.0.ln2.b     (768,)
blocks.0.attn.W_Q  (12, 768, 64)
blocks.0.attn.W_O  (12, 64, 768)
blocks.0.attn.b_Q  (12, 64)
blocks.0.attn.b_O  (768,)
blocks.0.attn.W_K  (12, 768, 64)
blocks.0.attn.W_V  (12, 768, 64)
blocks.0.attn.b_K  (12, 64)
blocks.0.attn.b_V  (12, 64)
blocks.0.mlp.W_in  (768, 3072)
blocks.0.mlp.b_in  (3072,)
blocks.0.mlp.W_out (3072, 768)
blocks.0.mlp.b_out (768,)
ln_final.w         (768,)
ln_final.b         (768,)
unembed.W_U        (768, 50257)
unembed.b_U        (50257,)</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
[This diagram](https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/full-merm.svg) shows the name of all activations and parameters in a fully general transformer model from transformerlens (except for a few at the start and end, like the embedding and unembedding). Lots of this won't make sense at first, but you can return to this diagram later and check that you understand most/all parts of it.

There's also an annotated version [here](https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-full-updated.png).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Config

The config object contains all the hyperparameters of the model. We can print the config of the reference model to see what it contains:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures
print(reference_gpt2.cfg)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">HookedTransformerConfig:
{'act_fn': 'gelu_new',
 'attention_dir': 'causal',
 'attn_only': False,
 'attn_scale': 8.0,
 'attn_scores_soft_cap': -1.0,
 'attn_types': None,
 'checkpoint_index': None,
 'checkpoint_label_type': None,
 'checkpoint_value': None,
 'd_head': 64,
 'd_mlp': 3072,
 'd_model': 768,
 'd_vocab': 50257,
 ...
 'use_split_qkv_input': False,
 'window_size': None}</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We define a stripped down config for our model:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

@dataclass
class Config:
    d_model: int = 768
    debug: bool = True
    layer_norm_eps: float = 1e-5
    d_vocab: int = 50257
    init_range: float = 0.02
    n_ctx: int = 1024
    d_head: int = 64
    d_mlp: int = 3072
    n_heads: int = 12
    n_layers: int = 12


if MAIN:
    cfg = Config()
    print(cfg)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Tests

Tests are great, write lightweight ones to use as you go!

**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def rand_float_test(cls, shape):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    random_input = t.randn(shape).to(device)
    print("Input shape:", random_input.shape)
    output = layer(random_input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape, "\n")


def rand_int_test(cls, shape):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    random_input = t.randint(100, 1000, shape).to(device)
    print("Input shape:", random_input.shape)
    output = layer(random_input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape, "\n")


def load_gpt2_test(cls, gpt2_layer, input):
    cfg = Config(debug=True)
    layer = cls(cfg).to(device)
    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)
    print("Input shape:", input.shape)
    output = layer(input)
    if isinstance(output, tuple):
        output = output[0]
    print("Output shape:", output.shape)
    try:
        reference_output = gpt2_layer(input)
    except:
        reference_output = gpt2_layer(input, input, input)
    print("Reference output shape:", reference_output.shape, "\n")
    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)
    print(f"{comparison.sum() / comparison.numel():.2%} of the values are correct\n")
    assert 1 - (comparison.sum() / comparison.numel()) < 1e-5, (
        "More than 0.01% of the values are incorrect"
    )

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `LayerNorm`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the code below, and then run the tests to verify that your layer is working correctly.

Your LayerNorm should do the following:

* Make mean 0
* Normalize to have variance 1
* Scale with learned weights
* Translate with learned bias

You can use the PyTorch [LayerNorm documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) as a reference. A few more notes:

* Your layernorm implementation always has `affine=True`, i.e. you do learn parameters `w` and `b` (which are represented as $\gamma$ and $\beta$ respectively in the PyTorch documentation).
* Remember that, after the centering and normalization, each vector of length `d_model` in your input should have mean 0 and variance 1.
* As the PyTorch documentation page says, your variance should be computed using `unbiased=False`.
* The `layer_norm_eps` argument in your config object corresponds to the $\epsilon$ term in the PyTorch documentation (it is included to avoid division-by-zero errors).
* We've given you a `debug` argument in your config. If `debug=True`, then you can print output like the shape of objects in your `forward` function to help you debug (this is a very useful trick to improve your coding speed).

Fill in the function, where it says `raise NotImplementedError()` (this will be the basic pattern for most other exercises in this section).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class LayerNorm(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.w = nn.Parameter(t.ones(cfg.d_model))
        self.b = nn.Parameter(t.zeros(cfg.d_model))

    def forward(
        self, residual: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        residual_mean = residual.mean(dim=-1, keepdim=True)
        residual_std = (
            residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps
        ).sqrt()

        residual = (residual - residual_mean) / residual_std
        return residual * self.w + self.b
        # END SOLUTION


# HIDE
if MAIN:
    rand_float_test(LayerNorm, [2, 4, 768])
    load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache["resid_post", 11])
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `Embed`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 5-10 minutes on this exercise.
> ```

This is basically a lookup table from tokens to residual stream vectors.

(Hint - you can implement this in just one line, without any complicated functions. If you've been working on it for >10 mins, you're probably overthinking it!)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class Embed(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))
        nn.init.normal_(self.W_E, std=self.cfg.init_range)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return self.W_E[tokens]
        # END SOLUTION


# HIDE
if MAIN:
    rand_int_test(Embed, [2, 4])
    load_gpt2_test(Embed, reference_gpt2.embed, tokens)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I keep getting <code>RuntimeError: CUDA error: device-side assert triggered</code>.</summary>

This is a uniquely frustrating type of error message, because it (1) forces you to restart the kernel, and (2) often won't tell you where the error message actually originated from!

You can fix the second problem by adding the line `os.environ['CUDA_LAUNCH_BLOCKING'] = "1"` to the very top of your file (after importing `os`). This won't fix your bug, but it makes sure the correct origin point is identified.

As for actually fixing the bug, this error usually ends up being the result of bad indexing, e.g. you're trying to apply an embedding layer to tokens which are larger than your maximum embedding.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `PosEmbed`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

Positional embedding can also be thought of as a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0`, `1`, `2`, ..., `seq_len-1` (i.e. the position indices of the tokens in the sequence).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class PosEmbed(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))
        nn.init.normal_(self.W_pos, std=self.cfg.init_range)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        batch, seq_len = tokens.shape
        return einops.repeat(self.W_pos[:seq_len], "seq d_model -> batch seq d_model", batch=batch)
        # END SOLUTION


# HIDE
if MAIN:
    rand_int_test(PosEmbed, [2, 4])
    load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `apply_causal_mask`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

The causal mask function will be a method of the `Attention` class.
It will take in attention scores, and apply a mask to them so that the model
can only attend to previous positions (i.e. the model can't cheat by looking at future positions).
We will implement this function first, and test it, before moving onto the `forward` method
of the `Attention` class.

A few hints:

* You can use [`torch.where`](https://pytorch.org/docs/stable/generated/torch.where.html), or the [`torch.masked_fill_`](https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html) function when masking the attention scores.
* The [`torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html) function is useful for creating a mask that is True for all positions we want to set probabilities to zero for.
* Make sure to use the `self.IGNORE` attribute to set the masked positions to negative infinity.
<details>
<summary>Question - why do you think we mask the attention scores by setting them to negative infinity, rather than the attention probabilities by setting them to zero?</summary>

If we masked the attention probabilities, then the probabilities would no longer sum to 1.

We want to mask the scores and *then* take softmax, so that the probabilities are still valid probabilities (i.e. they sum to 1), and the values in the masked positions have no influence on the model's output.
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class Attention(nn.Module):
    IGNORE: Float[Tensor, ""]

    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.register_buffer("IGNORE", t.tensor(float("-inf"), dtype=t.float32, device=device))

    def apply_causal_mask(
        self,
        attn_scores: Float[Tensor, "batch n_heads query_pos key_pos"],
    ) -> Float[Tensor, "batch n_heads query_pos key_pos"]:
        """
        Applies a causal mask to attention scores, and returns masked scores.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Define a mask that is True for all positions we want to set probabilities to zero for
        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)
        mask = t.triu(all_ones, diagonal=1).bool()
        # Apply the mask to attention scores, then return the masked scores
        attn_scores.masked_fill_(mask, self.IGNORE)
        return attn_scores
        # END SOLUTION


# HIDE
if MAIN:
    tests.test_causal_mask(Attention.apply_causal_mask)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint (pseudocode)</summary>

```python
def apply_causal_mask(
    self, attn_scores: Float[Tensor, "batch n_heads query_pos key_pos"]
    ) -> Float[Tensor, "batch n_heads query_pos key_pos"]:

    # Define a mask that is True for all positions we want to set probabilities to zero for

    # Apply the mask to attention scores, then return the masked scores
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `Attention`

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 30-45 minutes on this exercise.
> ```

* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)
    * Linear map from input -> query, key shape `[batch, seq_posn, head_index, d_head]`
    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)
    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal
    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!
* **Step 2:** Move information from source tokens to destination token using attention pattern (move = apply linear map)
    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`
    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`
    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)

Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).

Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:

* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.
* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension.
* We arrange the keys, queries and values as `(batch, seq_pos, head_idx, d_head)`, because the biases have shape `(head_idx, d_head)`, so this makes it convenient to add the biases (recall the rules of array broadcasting!).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-attn-30.png" width="1400">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary><b>A few extra notes on attention (optional)</b></summary>

<!-- Usually we have the relation `e = n * h` (i.e. `d_model = num_heads * d_head`). There are some computational justifications for this, but mostly this is just done out of convention (just like how we usually have `d_mlp = 4 * d_model`!). -->

Here, we cover some details related to the mathematical formulation of attention heads (and in particular the separation of **QK** and **OV** circuits), which is something we dive a lot deeper into in the next set of exercises in this chapter.

The **QK** circuit consists of the operation of the $W_Q$ and $W_K$ matrices. In other words, it determines the attention pattern, i.e. where information is moved to and from in the residual stream. The functional form of the attention pattern $A$ is:

$$
A = \text{softmax}\left(\frac{x W_Q W_K^T x^T}{\sqrt{d_{head}}}\right)
$$

where $x$ is the residual stream (shape `[seq_len, d_model]`), and $W_Q$, $W_K$ are the weight matrices for a single head (i.e. shape `[d_model, d_head]`).

The **OV** circuit consists of the operation of the $W_V$ and $W_O$ matrices. Once attention patterns are fixed, these matrices operate on the residual stream at the source position, and their output is the thing which gets moved from source to destination position.

The diagram below shows the functional form of the OV circuit. The QK circuit (pink) is responsible for causing the destination token to attend to the source token, and the OV circuit (light brown) is what actually maps the source token data into the information we'll send to the destination token.

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/refs/heads/main/img/qkv.png" width="800">

The functional form of an entire attention head is:

$$
\begin{aligned}
\text{output} &= \text{softmax}\left(\frac{x W_Q W_K^T x^T}{\sqrt{d_{head}}}\right) (x W_V W_O) \\
    &= Ax W_V W_O
\end{aligned}
$$

where $W_V$ has shape `[d_model, d_head]`, and $W_O$ has shape `[d_head, d_model]`.

Here, we can clearly see that the **QK circuit** and **OV circuit** are doing conceptually different things, and should be thought of as two distinct parts of the attention head.

Again, don't worry if you don't follow all of this right now - we'll go into **much** more detail on all of this in subsequent exercises. The purpose of the discussion here is just to give you a flavour of what's to come!

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import circuitsvis as cv
from IPython.display import display

if MAIN:
    display(
        cv.attention.attention_patterns(
            tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache["pattern", 0][0]
        )
    )

    # FILTERS: ~
    # html = cv.attention.attention_heads(
    #     tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache["pattern", 0][0]
    # )
    # with open(section_dir / "1101.html", "w") as f:
    #     f.write(str(html))
    # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-11/1101.html" width="1020" height="400" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You can also use the `attention_heads` function, which presents the data in a different way (the syntax is exactly the same as `attention_patterns`). Note, if you display this in VSCode then it may exhibit a bug where the main plot continually shrinks in size - if this happens, you should instead save the HTML (i.e. with `html = cv.attention.attention_heads(...); with open("attn_heads.html", "w") as f: f.write(str(html))`) and open the plot in your browser.

<!-- <details>
<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>

This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.

Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:

```python
attn_heads = cv.attention.attention_heads(
    tokens=reference_gpt2.to_str_tokens(reference_text),
    attention=cache["pattern", 0][0]
)

path = "attn_heads.html"

with open(path, "w") as f:
    f.write(str(attn_heads))

webbrowser.open(path)
```

To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.
</details> -->
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

if MAIN:
    display(
        cv.attention.attention_heads(
            tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache["pattern", 0][0]
        )
    )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-11/1102.html" width="1020" height="800" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should fill in the forward method for `Attention` below. You should also copy your code for `apply_causal_mask` to this new implementation of `Attention` (you can delete the rest of the old implementation code).

Note, this implementation will probably be the most challenging exercise on this page, so don't worry if it takes you some time! You should look at parts of the solution if you're stuck. A few tips:

* Don't forget the attention score scaling (this should come before the masking).
* Try not to combine a large number of operations into a single line of code.
* Try to make your variable names descriptive (i.e. it's not just `x = some_fn_of(x), x = some_other_fn_of(x), ...`).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class Attention(nn.Module):
    IGNORE: Float[Tensor, ""]

    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))
        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))
        nn.init.normal_(self.W_Q, std=self.cfg.init_range)
        nn.init.normal_(self.W_K, std=self.cfg.init_range)
        nn.init.normal_(self.W_V, std=self.cfg.init_range)
        nn.init.normal_(self.W_O, std=self.cfg.init_range)
        self.register_buffer("IGNORE", t.tensor(float("-inf"), dtype=t.float32, device=device))

    def forward(
        self, normalized_resid_pre: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Calculate query, key and value vectors
        q = (
            einops.einsum(
                normalized_resid_pre,
                self.W_Q,
                "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head",
            )
            + self.b_Q
        )
        k = (
            einops.einsum(
                normalized_resid_pre,
                self.W_K,
                "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head",
            )
            + self.b_K
        )
        v = (
            einops.einsum(
                normalized_resid_pre,
                self.W_V,
                "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head",
            )
            + self.b_V
        )

        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities
        attn_scores = einops.einsum(
            q,
            k,
            "batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K",
        )
        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)
        attn_pattern = attn_scores_masked.softmax(-1)

        # Take weighted sum of value vectors, according to attention probabilities
        z = einops.einsum(
            v,
            attn_pattern,
            "batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head",
        )

        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)
        attn_out = (
            einops.einsum(
                z,
                self.W_O,
                "batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model",
            )
            + self.b_O
        )

        return attn_out
        # END SOLUTION

    def apply_causal_mask(
        self, attn_scores: Float[Tensor, "batch n_heads query_pos key_pos"]
    ) -> Float[Tensor, "batch n_heads query_pos key_pos"]:
        """
        Applies a causal mask to attention scores, and returns masked scores.
        """
        # EXERCISE
        # # You should copy your solution from earlier
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Define a mask that is True for all positions we want to set probabilities to zero for
        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)
        mask = t.triu(all_ones, diagonal=1).bool()
        # Apply the mask to attention scores, then return the masked scores
        attn_scores.masked_fill_(mask, self.IGNORE)
        return attn_scores
        # END SOLUTION


# HIDE
if MAIN:
    tests.test_causal_mask(Attention.apply_causal_mask)
    rand_float_test(Attention, [2, 4, 768])
    load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache["normalized", 0, "ln1"])
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint (pseudocode for the forward method)</summary>

```python
def forward(
    self, normalized_resid_pre: Float[Tensor, "batch posn d_model"]
) -> Float[Tensor, "batch posn d_model"]:

    # Calculate query, key and value vectors
    q, k, v = ...

    # Calculate attention scores, then scale and mask, and apply softmax to get probabilities
    attn_scores = ...
    attn_scores_masked = ...
    attn_pattern = ...

    # Take weighted sum of value vectors, according to attention probabilities
    z = ...

    # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)
    attn_out = ...
    return attn_out
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `MLP`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

Next, you should implement the MLP layer, which consists of:

* A linear layer, with weight `W_in`, bias `b_in`
* A nonlinear function (we usually use GELU; the function `gelu_new` has been imported for this purpose)
* A linear layer, with weight `W_out`, bias `b_out`
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class MLP(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))
        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))
        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))
        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))
        nn.init.normal_(self.W_in, std=self.cfg.init_range)
        nn.init.normal_(self.W_out, std=self.cfg.init_range)

    def forward(
        self, normalized_resid_mid: Float[Tensor, "batch posn d_model"]
    ) -> Float[Tensor, "batch posn d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        pre = (
            einops.einsum(
                normalized_resid_mid,
                self.W_in,
                "batch position d_model, d_model d_mlp -> batch position d_mlp",
            )
            + self.b_in
        )
        post = gelu_new(pre)
        mlp_out = (
            einops.einsum(
                post, self.W_out, "batch position d_mlp, d_mlp d_model -> batch position d_model"
            )
            + self.b_out
        )
        return mlp_out
        # END SOLUTION


# HIDE
if MAIN:
    rand_float_test(MLP, [2, 4, 768])
    load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache["normalized", 0, "ln2"])
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `TransformerBlock`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

Now, we can put together the attention, MLP and layernorms into a single transformer block. Remember to implement the residual connections correctly!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class TransformerBlock(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.ln1 = LayerNorm(cfg)
        self.attn = Attention(cfg)
        self.ln2 = LayerNorm(cfg)
        self.mlp = MLP(cfg)

    def forward(
        self, resid_pre: Float[Tensor, "batch position d_model"]
    ) -> Float[Tensor, "batch position d_model"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre
        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid
        return resid_post
        # END SOLUTION


# HIDE
if MAIN:
    rand_float_test(TransformerBlock, [2, 4, 768])
    load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache["resid_pre", 0])
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm getting 100% accuracy on all modules before this point, but only about 90% accuracy on this one.</summary>

This might be because your layernorm implementation divides by `std + eps` rather than `(var + eps).sqrt()`. The latter matches the implementation used by GPT-2 (and this error only shows up in these tests).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `Unembed`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> ```

The unembedding is just a linear layer (with weight `W_U` and bias `b_U`).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class Unembed(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))
        nn.init.normal_(self.W_U, std=self.cfg.init_range)
        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))

    def forward(
        self, normalized_resid_final: Float[Tensor, "batch position d_model"]
    ) -> Float[Tensor, "batch position d_vocab"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return (
            einops.einsum(
                normalized_resid_final,
                self.W_U,
                "batch posn d_model, d_model d_vocab -> batch posn d_vocab",
            )
            + self.b_U
        )
        # END SOLUTION


# HIDE
if MAIN:
    rand_float_test(Unembed, [2, 4, 768])
    load_gpt2_test(Unembed, reference_gpt2.unembed, cache["ln_final.hook_normalized"])
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `DemoTransformer`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class DemoTransformer(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.embed = Embed(cfg)
        self.pos_embed = PosEmbed(cfg)
        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])
        self.ln_final = LayerNorm(cfg)
        self.unembed = Unembed(cfg)

    def forward(
        self, tokens: Int[Tensor, "batch position"]
    ) -> Float[Tensor, "batch position d_vocab"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        residual = self.embed(tokens) + self.pos_embed(tokens)
        for block in self.blocks:
            residual = block(residual)
        logits = self.unembed(self.ln_final(residual))
        return logits
        # END SOLUTION


# HIDE
if MAIN:
    rand_int_test(DemoTransformer, [2, 4])
    load_gpt2_test(DemoTransformer, reference_gpt2, tokens)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
**Try it out!**
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)
demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)

demo_logits = demo_gpt2(tokens)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's take a test string, and calculate the loss!

We're using the formula for **cross-entropy loss**. The cross entropy loss between a modelled distribution $Q$ and target distribution $P$ is:

$$
-\sum_x P(x) \log Q(x)
$$

In the case where $P$ is just the empirical distribution from target classes (i.e. $P(x^*) = 1$ for the correct class $x^*$) then this becomes:

$$
-\log Q(x^*)
$$

in other words, the negative log prob of the true classification.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def get_log_probs(
    logits: Float[Tensor, "batch posn d_vocab"], tokens: Int[Tensor, "batch posn"]
) -> Float[Tensor, "batch posn-1"]:
    log_probs = logits.log_softmax(dim=-1)
    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)
    log_probs_for_tokens = (
        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)
    )

    return log_probs_for_tokens


if MAIN:
    pred_log_probs = get_log_probs(demo_logits, tokens)
    print(f"Avg cross entropy loss: {-pred_log_probs.mean():.4f}")
    print(f"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}")
    print(f"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Avg cross entropy loss: 4.0441
Avg cross entropy loss for uniform distribution: 10.824905
Avg probability assigned to correct token: 0.098628</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can also greedily generate text, by taking the most likely next token and continually appending it to our prompt before feeding it back into the model:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

test_string = """Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as"""
for i in tqdm(range(100)):
    test_tokens = reference_gpt2.to_tokens(test_string).to(device)
    demo_logits = demo_gpt2(test_tokens)
    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())

print(test_string)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as climate change, the spread of infectious diseases and the spread of infectious diseases.

The research is published in the journal Nature Communications.

The research team is led by Dr. Michael J. H. Haldane, a professor of biology at the University of California, Berkeley, and co-author of the paper.

"We are very excited to see that the AI community is starting to take notice of the potential for AI to be a major threat to the human race,"</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In section 4️⃣ we'll learn to generate text in slightly more interesting ways than just argmaxing the output (which can lead to unnatural patterns like repetition, or text which is just less natural-sounding).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 3️⃣ Training a Transformer
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now that we've built our transformer, and verified that it performs as expected when we load in weights, let's try training it from scratch!

This is a lightweight demonstration of how you can actually train your own GPT-2 with this code! Here we train a tiny model on a tiny dataset, but it's fundamentally the same code for training a larger/more real model (though you'll need beefier GPUs and data parallelism to do it remotely efficiently, and fancier parallelism for much bigger ones).

For our purposes, we'll train 2L 4 heads per layer model, with context length 256, for 10*200 steps of batch size 16, just to show what it looks like (and so the notebook doesn't melt your colab / machine!).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Create Model
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model_cfg = Config(
    debug=False,
    d_model=32,
    n_heads=16,
    d_head=2,
    d_mlp=32 * 4,
    n_layers=4,
    n_ctx=128,
    d_vocab=reference_gpt2.cfg.d_vocab,
)
model = DemoTransformer(model_cfg)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Training Args


Note, for this optimization we'll be using **weight decay**.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

@dataclass
class TransformerTrainingArgs:
    batch_size = 32
    epochs = 10
    max_steps_per_epoch = 500
    lr = 1e-3
    weight_decay = 1e-2
    wandb_project: str | None = "day1-demotransformer"
    wandb_name: str | None = None


if MAIN:
    args = TransformerTrainingArgs()

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Create Data

We load in the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories), a dataset of synthetically generated simple stories only using a small vocabulary of words that typical 3 to 4-year-olds can understand. This dataset was designed for [exploring how small a LLM can be](https://arxiv.org/pdf/2305.07759) that can still generate coherent text.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

dataset = datasets.load_dataset("roneneldan/TinyStories", split="train")
print(dataset)
print(dataset[0]["text"])

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
`tokenize_and_concatenate` is a useful function which takes our dataset of strings, and returns a dataset of token IDs ready to feed into the model. We then create a dataloader from this tokenized dataset. The useful method `train_test_split` can give us a training and testing set.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

tokenized_dataset = tokenize_and_concatenate(
    dataset,
    reference_gpt2.tokenizer,
    streaming=False,
    max_length=model.cfg.n_ctx,
    column_name="text",
    add_bos_token=True,
    num_proc=4,
)

dataset_dict = tokenized_dataset.train_test_split(test_size=1000)
train_loader = DataLoader(
    dataset_dict["train"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True
)
test_loader = DataLoader(
    dataset_dict["test"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
When we iterate through these dataloaders, we will find dictionaries with the single key `'tokens'`, which maps to a tensor of token IDs with shape `(batch, seq_len)`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

first_batch = train_loader.dataset[: args.batch_size]

print(first_batch.keys())
print(first_batch["tokens"].shape)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Training Loop

If you did the material on [training loops](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#training-loop) during the first week, this should all be familiar to you. If not, you can skim that section for an overview of the key concepts. The start of the **Training loop** section is most important, and the subsections on [Modularisation](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#modularisation) and [dataclasses](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#aside-dataclasses) are also very useful. Lastly, we'll also be using Weights and Biases to train our model - you can read about how to use it [here](https://arena-ch0-fundamentals.streamlit.app/[0.4]_Optimization#what-is-weights-and-biases). Here are (roughly) all the things you should know for the following exercises:
                
* The key parts of a gradient update step are:
    * Calculating the (cross-entropy) loss between a model's output and the true labels,
    * `loss.backward()` - calculate gradients of the loss with respect to the model parameters,
    * `optimizer.step()` - update the model parameters using the gradients,
    * `optimizer.zero_grad()` - zero the gradients so they don't accumulate.
* We can nicely package up training loops into a class, which includes methods for training and validation steps among other things. This helps with writing code that can be reused in different contexts.
* We can use dataclasses to store all the arguments relevant to training in one place, and then pass them to our trainer class. Autocompletion is one nice bonus of this!
    * Be careful of scope here, you want to make sure you're referring to `self.args` within the trainer class, rather than the global `args`.
* You can use Weights and Biases to track experiments and log relevant variables. The three essential functions are:
    * `wandb.init()` - initialize a new run, takes arguments `project`, `name` and `config` (among others).
    * `wandb.log()` - log a dictionary of variables, e.g. `{"loss": loss}`. Also takes a `step` argument.
    * `wandb.finish()` - called at the end of training (no arguments).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - write training loop

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

You should fill in the methods below. Some guidance:

* Remember we were able to calculate cross entropy loss using the `get_log_probs` function in the previous section.
* You should use the optimizer `t.optim.AdamW` (Adam with weight decay), and with hyperparameters `lr` and `weight_decay` taken from your `TransformerTrainingArgs` dataclass instance.
* We've given you the argument `max_steps_per_epoch`, a hacky way of making sure the training phase in each epoch doesn't go on for too long. You can terminate each training phase after this many steps. It's set to a default value that should lead to a very short run demonstrating nontrivial model performance.
* Remember to move tokens to your device, via `tokens.to(device)` (this should be a global variable, defined at the top of your notebook).
* You can refer back to the training loops from the [previous chapter of the course](https://arena-ch0-fundamentals.streamlit.app/[0.3]_ResNets#training-loop) if you'd like.
* We've also provided an instance of the `TransformerSampler` class so you can generate text from your model during training to see how it's doing. We will cover how sampling works in the next section.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class TransformerTrainer:
    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):
        super().__init__()
        self.model = model
        self.args = args
        self.sampler = solutions.TransformerSampler(self.model, reference_gpt2.tokenizer)
        self.optimizer = t.optim.AdamW(
            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay
        )
        self.step = 0

        self.train_loader = DataLoader(
            dataset_dict["train"],
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True,
        )
        self.test_loader = DataLoader(
            dataset_dict["test"],
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True,
        )

    def training_step(self, batch: dict[str, Int[Tensor, "batch seq"]]) -> Float[Tensor, ""]:
        """
        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.

        Remember that `batch` is a dictionary with the single key 'tokens'.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        tokens = batch["tokens"].to(device)
        logits = self.model(tokens)
        loss = -get_log_probs(logits, tokens).mean()
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.step += 1
        wandb.log({"train_loss": loss}, step=self.step)
        # END SOLUTION
        return loss

    @t.inference_mode()
    def evaluate(self) -> float:
        """
        Evaluate the model on the test set and return the accuracy.
        """
        self.model.eval()
        # EXERCISE
        # #
        # # YOUR CODE HERE - fill in the `evaluate` method
        # #
        # END EXERCISE
        # SOLUTION
        total_correct, total_samples = 0, 0

        for batch in tqdm(self.test_loader, desc="Evaluating"):
            tokens = batch["tokens"].to(device)
            logits: Tensor = self.model(tokens)[:, :-1]
            predicted_tokens = logits.argmax(dim=-1)
            total_correct += (predicted_tokens == tokens[:, 1:]).sum().item()
            total_samples += tokens.size(0) * (tokens.size(1) - 1)

        accuracy = total_correct / total_samples
        wandb.log({"accuracy": accuracy}, step=self.step)
        # END SOLUTION
        self.model.train()
        return accuracy

    def train(self):
        """
        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping
        for each epoch at `self.args.max_steps_per_epoch` steps.
        """
        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)
        accuracy = np.nan

        progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)

        for epoch in range(self.args.epochs):
            for i, batch in enumerate(self.train_loader):
                loss = self.training_step(batch)
                progress_bar.update()
                progress_bar.set_description(
                    f"Epoch {epoch + 1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}"
                )
                if i >= self.args.max_steps_per_epoch:
                    break

            accuracy = self.evaluate()
            sample_text = self.sampler.sample("Once upon a time", max_tokens_generated=50)
            print(sample_text)

        wandb.finish()


# HIDE
if MAIN:
    # See the full run here: https://api.wandb.ai/links/dquarel/nrxuwnv7
    model = DemoTransformer(model_cfg).to(device)
    args = TransformerTrainingArgs()
    trainer = TransformerTrainer(args, model)
    trainer.train()
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<!-- Note - this section of the course used to use PyTorch Lightning, but this has now been taken out. If you want, you can look at the old version of the training code which used PyTorch Lightning in the dropdown below.

<details>
<summary>PyTorch Lighting training loop</summary>

```python
class LitTransformer(pl.LightningModule):
	def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer, data_loader: DataLoader):
		super().__init__()
		self.model = model
		self.cfg = model.cfg
		self.args = args
		self.data_loader = data_loader

	def forward(self, tokens: Int[Tensor, "batch position"]) -> Float[Tensor, "batch position d_vocab"]:
		logits = self.model(tokens)
		return logits

	def training_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Float[Tensor, ""]:
		\'\'\'
		Here you compute and return the training loss and some additional metrics for e.g.
		the progress bar or logger.
		\'\'\'
		tokens = batch["tokens"].to(device)
		logits = self.model(tokens)
		loss = -get_log_probs(logits, tokens).mean()
		self.log("train_loss", loss)
		return loss

	def configure_optimizers(self):
		\'\'\'
		Choose what optimizers and learning-rate schedulers to use in your optimization.
		\'\'\'
		optimizer = t.optim.AdamW(self.model.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay)
		return optimizer

	def train_dataloader(self):
		return self.data_loader


litmodel = LitTransformer(args, model, data_loader)
logger = WandbLogger(save_dir=args.log_dir, project=args.log_name, name=args.run_name)

trainer = pl.Trainer(
    max_epochs=args.max_epochs,
    logger=logger,
    log_every_n_steps=args.log_every_n_steps
)
trainer.fit(model=litmodel, train_dataloaders=litmodel.data_loader)
wandb.finish()
```

</details>

<details>
<summary>Explanation for why PyTorch Lightning is no longer used</summary>

TLDR - it provides nice modularization and saving of code, but it abstracts away a lot of the details of training loops, and so isn't very useful for educational purposes. Also, it imposes a lot of structure on how the training loops work without allowing for much flexibility, and lots of the code we'll write later (e.g. linear probes or RL) doesn't fit well into this framework. However, it can be a very useful tool to learn about once you've got the basics down and you're looking to benefit from the suite of extra features it provides.

</details> -->

When you run the code for the first time, you'll have to login to Weights and Biases, and paste an API key into VSCode. After this is done, your Weights and Biases training run will start. It'll give you a lot of output text, one line of which will look like:

```
View run at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/runs/<RUN-NAME>
```

which you can click on to visit the run page.

> Note - to see the plots more clearly in Weights and Biases, you can click on the **edit panel** of your plot (the small pencil symbol at the top-right), then move the **smoothing** slider to the right.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### A note on this loss curve (optional)


What's up with the shape of our loss curve? It seems like we start at around 10-11, drops down very fast, but then levels out. It turns out, this is all to do with the kinds of algorithms the model learns during training.

When it starts out, your model will be outputting random noise, which might look a lot like "predict each token with approximately uniform probability", i.e. $Q(x) = 1/d_\text{vocab}$ for all $x$. This gives us a cross entropy loss of $\log (d_\text{vocab})$.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

d_vocab = model.cfg.d_vocab

print(f"d_vocab = {d_vocab}")
print(f"Cross entropy loss on uniform distribution = {math.log(d_vocab):.3f}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">d_vocab = 50257
Cross entropy loss on uniform distribution = 10.825</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The next thing we might expect the model to learn is the frequencies of words in the english language. After all, small common tokens like `" and"` or `" the"` might appear much more frequently than others. This would give us an average cross entropy loss of:

$$
- \sum_x p_x \log p_x
$$

where $p_x$ is the actual frequency of the word in our training data.

We can evaluate this quantity as follows:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

toks = tokenized_dataset[:]["tokens"].flatten()

d_vocab = model.cfg.d_vocab
freqs = t.bincount(toks, minlength=d_vocab)
probs = freqs.float() / freqs.sum()

distn = t.distributions.categorical.Categorical(probs=probs)
entropy = distn.entropy()

print(f"Entropy of training data = {entropy:.3f}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Entropy of training data = 7.349</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
After unigram frequencies, the next thing our model usually learns is **bigram frequencies** (i.e. the frequency of pairs of adjacent tokens in the training data). For instance, `"I"` and `" am"` are common tokens, but their bigram frequency is much higher than it would be if they occurred independently. Bigram frequencies actually take you pretty far, since they also help with:

* Some simple grammatical rules (e.g. a full stop being followed by a capitalized word)
* Weird quirks of tokenization (e.g. `" manip"` being followed by `"ulative"`)
* Common names (e.g. `"Barack"` being followed by `" Obama"`)


After approximating bigram frequencies, we need to start using smarter techniques, like trigrams (which can only be implemented using attention heads), **induction heads** (which we'll learn a lot more about in the next set of exercises!), and fact memorization or more basic grammar and syntax rules. Marginal improvements start getting harder around this point, leading to a flattening of our loss curve.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise (optional) - log completions

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 20-40 minutes on this exercise, if you choose to attempt it.
> Note, you might want to come back to this exercise *after* you learn how sampling works.
> ```

Choose a handful of prompts, and log the model's completions on those sentences. We recommend you do this with a lower frequency than loss is logged (e.g. once every 10-100 batches).

The `wandb` syntax for logging text is pretty simple. Firstly, you can just print output as stdout and this is also logged to Weights & Biases (you can find it under the "Logs" section of your run). Alternatively, you can log data in the form of a table, and have it appear next to your other charts:

```python
wandb.log({"completions_table": wandb.Table(
    data = data,
    columns = ["epoch", "step", "text"]
)})
```

where `data` is a list of length-3 lists, with each list containing (epoch, step, text). If you choose this option, we recommend logging the table less frequently than you're sampling from the model, to make sure you're not sending too much data (because unfortunately wandb doesn't have methods to incrementally update the table during logging).

If you want to try this before going through the sampling exercises (which are quite long!), you can use the code below to sample output from the model. Note that the `TransformerSampler` object is already in inference mode, so you don't need to worry about this.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def sampling_fn(model: DemoTransformer, prompt: str) -> str:
    sampler = solutions.TransformerSampler(model, reference_gpt2.tokenizer)
    output = sampler.sample(prompt, temperature=0.7, top_p=0.95, max_tokens_generated=16)
    return output


if MAIN:
    model = DemoTransformer(model_cfg).to(device)

    # Should be entirely random, because it uses a newly initialized model
    print(sampling_fn(model, prompt="John and Mary went to the"))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">John and Mary went to theLittlealmernaut estranged broadcaster Workers reapp skull consecutivepexuaniaarrow drilling Burnett ASDMusic</pre>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# EXERCISE
# # YOUR CODE HERE - rewrite the TransformerTrainer.train method, so that it logs completions
# END EXERCISE
# SOLUTION
@dataclass
class TransformerTrainingArgsLogText(TransformerTrainingArgs):
    text_sample_freq: int = 20
    table_log_freq: int = 200

    def __post_init__(self):
        assert self.table_log_freq >= self.text_sample_freq, (
            "You should log the table less frequently than you add text to it."
        )


def train_log_text(self: TransformerTrainer, sampling_fn: Callable, prompt_list: list[str]):
    """
    Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping
    for each epoch at `self.args.max_steps_per_epoch` steps.

    This also takes 2 extra arguments:
        sampling_fn: function which takes model & a single prompt (i.e. text string) and returns text string output
        prompt_list: list of prompts we'll log output on
    """
    wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)
    accuracy = np.nan
    progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)

    # Create a list for storing data
    completions_list = []

    for epoch in range(self.args.epochs):
        for i, batch in enumerate(self.train_loader):
            loss = self.training_step(batch)
            progress_bar.update()
            progress_bar.set_description(
                f"Epoch {epoch + 1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}"
            )

            # Control the adding of text to the table, and the logging of text
            if self.step % self.args.text_sample_freq == 0:
                text_completions = [sampling_fn(self.model, prompt) for prompt in prompt_list]
                completions_list.append([epoch, self.step, *text_completions])
            if self.step % self.args.table_log_freq == 0:
                wandb.log(
                    {
                        "completions_table": wandb.Table(
                            data=completions_list,
                            columns=[
                                "epoch",
                                "step",
                                *[f"prompt_{i}" for i in range(len(prompt_list))],
                            ],
                        )
                    }
                )

            if i >= self.args.max_steps_per_epoch:
                break

        accuracy = self.evaluate()

    wandb.finish()


TransformerTrainer.train = train_log_text
# END SOLUTION


if MAIN:
    prompt_list = [
        "Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for",
        "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.",
        "John and Mary went to the",
    ]

    model = DemoTransformer(model_cfg).to(device)
    args = TransformerTrainingArgsLogText()
    trainer = TransformerTrainer(args, model)
    trainer.train(sampling_fn, prompt_list)
    # Read full report here - https://api.wandb.ai/links/callum-mcdougall/5ex16e5w

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You shouldn't expect to see perfect logical coherence from your model, but you should at least see that it respects basic word frequencies, and follows basic rules of grammar some of the time. Hopefully this gives some perspective on how difficult training a transformer can be!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 4️⃣ Sampling from a Transformer
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's discuss how we might go about producing output from a transformer.

One obvious method to sample tokens from a distribution would be to always take the token assigned the highest probability. But this can lead to some boring and repetitive outcomes, and at worst it can lock our transformer's output into a loop.

First, you should read HuggingFace's blog post [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate). Once you've done that, you can start the exercises below.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## `TransformerSampler` class

Below, we've given you the `TransformerSampler` class. This contains the following important methods:

- `sample`, which is the highest-level method. It repeatedly calls `sample_next_token` to generate new tokens, until one of the termination criteria is met.
- `sample_next_token`, which samples a single new token based on some hyperparameters. This might involve various different sampling methods and techniques e.g. temperature scaling, top-k sampling, top-p sampling, etc.
- A set of other methods, which apply the previously mentioned sampling methods and techniques.

You can see how `sample_next_token` works, and as an example how greedy sampling is implemented via `greedy_search` - we just continually take the tokens with the highest logits at each step. 

<details>
<summary>Question - why do you think <code>temperature=0.0</code> correspond to greedy sampling?</summary>

To apply a temperature to our sampling (as we'll see later) means to scale all logits by `(1 / temperature)`. The basic intuition here is:

* A higher temperature means a smaller scale factor, so the logits all approach zero, i.e. uniform distribution, and the sampling process is a lot more random (producing more diverse and varied outputs)
* A lower temperature means a larger scale factor, so the logits all approach infinity, i.e. a dirac delta function, and the sampling process is a lot more deterministic (producing less varied output)

As temperature gets close to zero, the difference between the largest logit and second largest logit becomes very large, so the distribution tends to "probability of 1 on the highest-likelihood token", i.e. greedy sampling. You can derive this formally if you prefer.
</details>

In the next exercise you'll implement the `sample` method, and then you'll go on to implement all the other methods.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `sample`

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 25-40 minutes on this exercise.
> ```

The `sample` method generates new tokens autoregressively, by repeatedly:

- Passing the current sequence of tokens through the model to get logits,
- Using some sampling technique to select a new token, i.e. `sample_next_token(input_ids, logits, **kwargs)`,
- Appending this new token to the input sequence,
- Repeating the process until one of the termination criteria is met: either we generate `max_tokens_generated` new tokens, or we generate the end-of-sequence token (which we can access via `self.tokenizer.eos_token_id`).

Lastly, we use the `tokenizer.decode` method to return the sampled string. You're also invited to use the `verbose` argument, for printing the decoded sequences while they're being generated (this can help with debugging).

Below is some code which tests your sampling function by performing greedy sampling (which means always choosing the most likely next token at each step).

A few hints:

- Don't forget about tensor shapes! Your model's input should always have a batch dimension, i.e. it should be shape `(1, seq_len)`.
- The `sample_next_token` method will return an integer, so make sure you wrap this in a tensor before concatenating it to the end of your input IDs.
- Also remember to have your tensors be on the same device (we have a global `device` variable).
- Remember to put your model in evaluation mode, using `model.eval()`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class TransformerSampler:
    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):
        self.model = model
        self.cfg = model.cfg
        self.tokenizer = tokenizer

    @t.inference_mode()
    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs) -> str:
        """
        Returns a string of autoregressively generated text, starting from the prompt.

        Sampling terminates at max_tokens_generated, or when the model generates an end-of-sequence token. kwargs are
        passed to sample_next_token, to give detailed instructions on how new tokens are chosen.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        self.model.eval()
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)[0]

        for _ in range(max_tokens_generated):
            # Get new logits (make sure we don't pass in more tokens than the model's context length)
            logits = self.model(input_ids[None, -self.cfg.n_ctx :])
            # We only take logits for the last token, because this is what we're sampling
            logits = logits[0, -1]
            # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)
            next_token = t.tensor(
                [TransformerSampler.sample_next_token(input_ids, logits, **kwargs)], device=device
            )
            # Create new input ids string, with shape (1, old_seq_len + 1)
            input_ids = t.cat([input_ids, next_token], dim=-1)
            # Print out results, if required
            if verbose:
                print(self.tokenizer.decode(input_ids), end="\r")
            # If our new token was the end-of-text token, stop
            if next_token == getattr(self.tokenizer, "eos_token_id", None):
                break

        return self.tokenizer.decode(input_ids)
        # END SOLUTION

    @staticmethod
    def sample_next_token(
        input_ids: Int[Tensor, "seq_len"],
        logits: Float[Tensor, "d_vocab"],
        temperature=1.0,
        top_k=0,
        top_p=0.0,
        frequency_penalty=0.0,
        seed=None,
    ) -> int:
        assert input_ids.ndim == 1, "input_ids should be a 1D sequence of token ids"
        assert temperature >= 0, "Temperature should be non-negative"
        assert 0 <= top_p <= 1.0, "Top-p must be a probability"
        assert 0 <= top_k, "Top-k must be non-negative"
        assert not (top_p != 0 and top_k != 0), "At most one of top-p and top-k supported"

        # Set random seeds for reproducibility
        if seed is not None:
            t.manual_seed(seed)
            np.random.seed(seed)

        # Apply all the specialized sampling methods
        if temperature == 0:
            return TransformerSampler.greedy_search(logits)
        elif temperature != 1.0:
            logits = TransformerSampler.apply_temperature(logits, temperature)
        if frequency_penalty != 0.0:
            logits = TransformerSampler.apply_frequency_penalty(
                input_ids, logits, frequency_penalty
            )
        if top_k > 0:
            return TransformerSampler.sample_top_k(logits, top_k)
        if top_p > 0.0:
            return TransformerSampler.sample_top_p(logits, top_p)
        return TransformerSampler.sample_basic(logits)

    @staticmethod
    def greedy_search(logits: Float[Tensor, "d_vocab"]) -> int:
        """
        Returns the most likely token (as an int).
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return logits.argmax().item()
        # END SOLUTION

    @staticmethod
    def apply_temperature(
        logits: Float[Tensor, "d_vocab"], temperature: float
    ) -> Float[Tensor, "d_vocab"]:
        """
        Applies temperature scaling to the logits.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return logits / temperature
        # END SOLUTION

    @staticmethod
    def apply_frequency_penalty(
        input_ids: Int[Tensor, "seq_len"], logits: Float[Tensor, "d_vocab"], freq_penalty: float
    ) -> Float[Tensor, "d_vocab"]:
        """
        Applies a frequency penalty to the logits.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        d_vocab = logits.size(0)
        id_freqs = t.bincount(input_ids, minlength=d_vocab)
        return logits - freq_penalty * id_freqs
        # END SOLUTION

    @staticmethod
    def sample_basic(logits: Float[Tensor, "d_vocab"]) -> int:
        """
        Samples from the distribution defined by the logits.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return t.distributions.categorical.Categorical(logits=logits).sample().item()
        # END SOLUTION

    @staticmethod
    def sample_top_k(logits: Float[Tensor, "d_vocab"], k: int) -> int:
        """
        Samples from the top k most likely tokens.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        top_k_logits, top_k_token_ids = logits.topk(k)
        # Get sampled token (which is an index corresponding to the list of top-k tokens)
        sampled_token_idx = t.distributions.categorical.Categorical(logits=top_k_logits).sample()
        # Get the actual token id, as an int
        return top_k_token_ids[sampled_token_idx].item()
        # END SOLUTION

    @staticmethod
    def sample_top_p(
        logits: Float[Tensor, "d_vocab"], top_p: float, min_tokens_to_keep: int = 1
    ) -> int:
        """
        Samples from the most likely tokens which make up at least p cumulative probability.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Sort logits, and get cumulative probabilities
        logits_sorted, indices = logits.sort(descending=True, stable=True)
        cumul_probs = logits_sorted.softmax(-1).cumsum(-1)
        # Choose which tokens to keep, in the set we sample from
        n_keep = t.searchsorted(cumul_probs, top_p, side="left").item() + 1
        n_keep = max(n_keep, min_tokens_to_keep)
        keep_idx = indices[:n_keep]
        keep_logits = logits[keep_idx]
        # Perform the sampling
        sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()
        return keep_idx[sample].item()
        # END SOLUTION

    @t.inference_mode()
    def beam_search(
        self,
        prompt: str,
        num_return_sequences: int,
        num_beams: int,
        max_new_tokens: int,
        no_repeat_ngram_size: int | None = None,
    ) -> list[tuple[float, str]]:
        """
        Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting from the initial
        prompt) until either of the two stopping criteria are met: (1) we've generated `max_new_tokens` tokens, or (2)
        we've generated `num_returns_sequences` terminating sequences.
        """
        raise NotImplementedError()


if MAIN:
    t.set_grad_enabled(False)  # gradients are not necessary for sampling

    model = DemoTransformer(Config()).to(device)
    model.load_state_dict(reference_gpt2.state_dict(), strict=False)
    tokenizer = reference_gpt2.tokenizer
    sampler = TransformerSampler(model, tokenizer)

    prompt = "Jingle bells, jingle bells, jingle all the way"
    print(f"Testing greedy decoding\nPrompt:   {prompt!r}")

    expected = "Jingle bells, jingle bells, jingle all the way up to the top of the mountain."
    output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)

    print(f"Expected: {expected!r}\nActual:   {output!r}\n")
    assert output == expected

    print("Tests passed!")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution</summary>

```python
@t.inference_mode()
def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):
    """
    Returns a string of autoregressively generated text, starting from the prompt.

    Sampling terminates at max_tokens_generated, or when the model generates an end-of-sequence token. kwargs are
    passed to sample_next_token, to give detailed instructions on how new tokens are chosen.
    """
    self.model.eval()
    input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)[0]

    for i in range(max_tokens_generated):
        # Get new logits (make sure we don't pass in more tokens than the model's context length)
        logits = self.model(input_ids[None, -self.cfg.n_ctx :])
        # We only take logits for the last token, because this is what we're sampling
        logits = logits[0, -1]
        # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)
        next_token = t.tensor([TransformerSampler.sample_next_token(input_ids, logits, **kwargs)], device=device)
        # Create new input ids string, with shape (1, old_seq_len + 1)
        input_ids = t.cat([input_ids, next_token], dim=-1)
        # Print out results, if required
        if verbose:
            print(self.tokenizer.decode(input_ids), end="\r")
        # If our new token was the end-of-text token, stop
        if next_token == getattr(self.tokenizer, "eos_token_id", None):
            break

    return self.tokenizer.decode(input_ids)
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Sampling with Categorical

Now, we'll move into implementing specific sampling methods. In each of these cases, you should return to the class definition above and fill in the corresponding method.

PyTorch provides a [`distributions`](https://pytorch.org/docs/stable/distributions.html#distribution) package with a number of convenient methods for sampling from various distributions.

For now, we just need [`t.distributions.categorical.Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical). Use this to implement `sample_basic`, which just samples from the provided logits (which may have already been modified by the temperature and frequency penalties).

Note that this will be slow since we aren't batching the samples, but don't worry about speed for now.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - `sample_basic`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 5-15 minutes on this exercise.
> ```

Implement basic sampling in the `TransformerSampler` class above (i.e. the `sample_basic` method), then run the code below to verify your solution works.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
logits = model(input_ids)[0, -1]

expected_top_5 = {
    " church": 0.0648,
    " house": 0.0367,
    " temple": 0.0145,
    " same": 0.0104,
    " Church": 0.0097,
}
frequency_of_top_5 = defaultdict(int)

N = 10_000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)
    frequency_of_top_5[tokenizer.decode(token)] += 1

for word in expected_top_5:
    expected_freq = expected_top_5[word]
    observed_freq = frequency_of_top_5[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}"
    )
    assert abs(observed_freq - expected_freq) < 0.01, (
        "Try increasing N if this fails by a small amount."
    )

print("Tests passed!")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution</summary>

```python
@staticmethod
def sample_basic(logits: Float[Tensor, "d_vocab"]) -> int:
    """
    Samples from the distribution defined by the logits.
    """
    sampled_token = t.distributions.categorical.Categorical(logits=logits).sample()
    return sampled_token.item()
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - `apply_temperature`

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 5-10 minutes on this exercise.
> ```

Temperature sounds fancy, but it's literally just dividing the logits by the temperature. You should implement this in your `TransformerSampler` class now.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

logits = t.tensor([1, 2]).log()

cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)
print('A low temperature "sharpens" or "peaks" the distribution: ', cold_logits)
t.testing.assert_close(cold_logits, 1000.0 * logits)

hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)
print("A high temperature flattens the distribution: ", hot_logits)
t.testing.assert_close(hot_logits, 0.001 * logits)

print("Tests passed!")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution</summary>

```python
@staticmethod
def apply_temperature(logits: Float[Tensor, "d_vocab"], temperature: float) -> Float[Tensor, "d_vocab"]:
    """
    Applies temperature scaling to the logits.
    """
    return logits / temperature
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - `apply_frequency_penalty`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

The frequency penalty is simple as well: count the number of occurrences of each token, then subtract `freq_penalty` for each occurrence. Hint: use `t.bincount` (documentation [here](https://pytorch.org/docs/stable/generated/torch.bincount.html)) to do this in a vectorized way.

You should implement the `apply_frequency_penalty` method in your `TransformerSampler` class now, then run the cell below to check your solution.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm getting a <code>RuntimeError</code>; my tensor sizes don't match.</summary>

Look at the documentation page for `t.bincount`. You might need to use the `minlength` argument - why?
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

bieber_prompt = "And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine"
input_ids = tokenizer.encode(bieber_prompt, return_tensors="pt")
logits = t.ones(tokenizer.vocab_size)
penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)

assert penalized_logits[5156].item() == -11, (
    "Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11"
)
assert penalized_logits[14801].item() == -5, (
    "Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5"
)

print("Tests passed!")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution</summary>

```python
@staticmethod
def apply_frequency_penalty(
    input_ids: Int[Tensor, "seq_len"], logits: Float[Tensor, "d_vocab"], freq_penalty: float
) -> Float[Tensor, "d_vocab"]:
    """
    Applies a frequency penalty to the logits.
    """
    d_vocab = logits.size(0)
    id_freqs = t.bincount(input_ids, minlength=d_vocab)
    return logits - freq_penalty * id_freqs
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Sampling - Manual Testing

Run the below cell to get a sense for the `temperature` and `freq_penalty` arguments. Play with your own prompt and try other values.

Note: your model can generate newlines or non-printing characters, so calling `print` on generated text sometimes looks awkward on screen. You can call `repr` on the string before printing to have the string escaped nicely.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

sampler = TransformerSampler(model, tokenizer)

N_RUNS = 1
your_prompt = "Jingle bells, jingle bells, jingle all the way"
cases = [
    ("High freq penalty", dict(frequency_penalty=100.0)),
    ("Negative freq penalty", dict(frequency_penalty=-3.0)),
    ("Too hot!", dict(temperature=2.0)),
    ("Pleasantly cool", dict(temperature=0.7)),
    ("Pleasantly warm", dict(temperature=0.9)),
    ("Too cold!", dict(temperature=0.01)),
]

table = Table("Name", "Kwargs", "Output", title="Sampling - Manual Testing")

for name, kwargs in cases:
    for i in range(N_RUNS):
        output = sampler.sample(your_prompt, max_tokens_generated=24, **kwargs)
        table.add_row(name, str(kwargs), repr(output) + "\n")

rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                             Sampling - Manual Testing                                             </span>
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Name                  </span>┃<span style="font-weight: bold"> Kwargs                       </span>┃<span style="font-weight: bold"> Output                                                   </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ High freq penalty     │ {'frequency_penalty': 100.0} │ 'Jingle bells, jingle bells, jingle all the way          │
│                       │                              │ down.\nBe Parlearan - Be pararellane... I wanna touch it │
│                       │                              │ where? + Cut and meet'                                   │
│                       │                              │                                                          │
│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the way, jingle, │
│                       │                              │ jingle, jingle, jingle, jingle, jingle, jingle, jingle'  │
│                       │                              │                                                          │
│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all the way wild     │
│                       │                              │ Britain freemen/(aden forks dumping inhibits steel III   │
│                       │                              │ Decathlonsuitgirls override drunk lockdown mirror issues │
│                       │                              │ under totally monopolish'                                │
│                       │                              │                                                          │
│ Pleasantly cool       │ {'temperature': 0.7}         │ 'Jingle bells, jingle bells, jingle all the way around.  │
│                       │                              │ But, I am not even in the mood to hear you. You are my   │
│                       │                              │ friend. And the only one'                                │
│                       │                              │                                                          │
│ Pleasantly warm       │ {'temperature': 0.9}         │ "Jingle bells, jingle bells, jingle all the way up and   │
│                       │                              │ down it's a song.\n\nThe third thing that's interesting  │
│                       │                              │ is coach Pugh, he actually likes"                        │
│                       │                              │                                                          │
│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │
│                       │                              │ the top of the mountain.\n\nThe first time I saw the     │
│                       │                              │ mountain, I was in the middle of'                        │
│                       │                              │                                                          │
└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Top-K Sampling

Conceptually, the steps in top-k sampling are:
- Find the `top_k` largest probabilities (you can use [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html))
- Set all other probabilities to zero
- Normalize and sample
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - `sample_top_k`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 5-10 minutes on this exercise.
> ```

Implement the method `sample_top_k` now. Your implementation should stay in log-space throughout (don't exponentiate to obtain probabilities). This means you don't actually need to worry about normalizing, because `Categorical` accepts unnormalised logits.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
logits = model(input_ids)[0, -1]

expected_top_5 = {
    " church": 0.0648,
    " house": 0.0367,
    " temple": 0.0145,
    " same": 0.0104,
    " Church": 0.0097,
}
topk_5_sum = sum(expected_top_5.values())

observed_freqs = defaultdict(int)

N = 10000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=5)
    observed_freqs[tokenizer.decode(token)] += 1

for word in expected_top_5:
    expected_freq = expected_top_5[word] / topk_5_sum
    observed_freq = observed_freqs[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq = {expected_freq:.4f}, observed freq = {observed_freq:.4f}"
    )
    assert abs(observed_freq - expected_freq) < 0.01

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution</summary>

```python
@staticmethod
def sample_top_k(logits: Float[Tensor, "d_vocab"], k: int) -> int:
    """
    Samples from the top k most likely tokens.
    """
    top_k_logits, top_k_token_ids = logits.topk(k)
    # Get sampled token (which is an index corresponding to the list of top-k tokens)
    sampled_token_idx = t.distributions.categorical.Categorical(logits=top_k_logits).sample()
    # Get the actual token id, as an int
    return top_k_token_ids[sampled_token_idx].item()
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) famously included an example prompt about unicorns. Now it's your turn to see just how cherry picked this example was.

The paper claims they used `top_k=40` and best of 10 samples.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

sampler = TransformerSampler(model, tokenizer)

your_prompt = "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."

output = sampler.sample(your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)

rprint(f"Your model said:\n\n[bold dark_orange]{output}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Your model said:

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in</span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span>

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">"This shows that there are two distinct kinds of unicorns that live in the Andes,"</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold"> says lead author Dr. Andrew </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">Wysocki. </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">"The first is called the wild unicorn, which is the most common type."</span>

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">These unicorns, which are called bunnies, are most common</span>
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This is pretty incredible! For some perspective on how much of a paradigm shift even basic models like this represented, we recommend reading [this section from Simulators](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators#The_limit_of_sequence_modeling).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Top-p aka Nucleus Sampling

The basic idea is that we choose the most likely words, up until the total probability of words we've chosen crosses some threshold. Then we sample from those chosen words based on their logits.

The steps are:

- Sort the probabilities from largest to smallest
- Find the cutoff point where the cumulative probability first equals or exceeds `top_p`. We do the cutoff inclusively, keeping the first probability above the threshold.
- If the number of kept probabilities is less than `min_tokens_to_keep`, keep that many tokens instead.
- Set all other probabilities to zero
- Normalize and sample

For example, if our probabilities were `(0.4, 0.3, 0.2, 0.1)` and our cutoff was `top_p=0.8`, then we'd sample from the first three elements (because their total probability is `0.9` which is over the threshold, but the first two only have a total prob of `0.7` which is under the threshold). Once we've chosen to sample from those three, we would renormalise them by dividing by their sum, so the probabilities we use when sampling are `(0.4/0.9, 0.3/0.9, 0.2/0.9)`.

Optionally, refer to the paper [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf) for some comparison of different methods.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - `sample_top_p`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 15-20 minutes on this exercise.
> ```
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

prompt = "John and Mary went to the"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
logits = model(input_ids)[0, -1]

expected_top_10pct = {
    " church": 0.0648,
    " house": 0.0367,  # These are the two most likely tokens, and add up to >10%
}
top_10pct_sum = sum(expected_top_10pct.values())

observed_freqs = defaultdict(int)

N = 10_000
for _ in tqdm(range(N)):
    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)
    observed_freqs[tokenizer.decode(token)] += 1

for word in expected_top_10pct:
    expected_freq = expected_top_10pct[word] / top_10pct_sum
    observed_freq = observed_freqs[word] / N
    print(
        f"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}"
    )
    assert abs(observed_freq - expected_freq) < 0.01, (
        "Try increasing N if this fails by a small amount."
    )

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm stuck on how to implement this function.</summary>

First, sort the logits using the `sort(descending=True)` method (this returns values and indices). Then you can get `cumulative_probs` by applying softmax to these logits and taking the cumsum. Then, you can decide how many probabilities to keep by using the `t.searchsorted` function.

Once you've decided which probabilities to keep, it's easiest to sample from them using the original logits (you should have preserved the indices when you called `logits.sort`). This way, you don't need to worry about renormalising like you would if you were using probabilities.
</details>

<details>
<summary>Solution</summary>

```python
@staticmethod
def sample_top_p(logits: Float[Tensor, "d_vocab"], top_p: float, min_tokens_to_keep: int = 1) -> int:
    """
    Samples from the most likely tokens which make up at least p cumulative probability.
    """
    # Sort logits, and get cumulative probabilities
    logits_sorted, indices = logits.sort(descending=True, stable=True)
    cumul_probs = logits_sorted.softmax(-1).cumsum(-1)
    # Choose which tokens to keep, in the set we sample from
    n_keep = t.searchsorted(cumul_probs, top_p, side="left").item() + 1
    n_keep = max(n_keep, min_tokens_to_keep)
    keep_idx = indices[:n_keep]
    keep_logits = logits[keep_idx]
    # Perform the sampling
    sample = t.distributions.categorical.Categorical(logits=keep_logits).sample()
    return keep_idx[sample].item()
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, an example of top-p sampling:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

sampler = TransformerSampler(model, tokenizer)

your_prompt = "Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for"
output = sampler.sample(your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64)
rprint(f"Your model said:\n\n[bold dark_orange]{output}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output (you might get different results due to randomness)]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Your model said:

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">Eliezer Shlomo Yudkowsky (born September </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">11</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">, </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">1979</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">) is an American decision and artificial intelligence (AI) </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">theorist and writer, best known for his seminal paper on the </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">"The Matrix"</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold"> which is about the </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">"futurist"</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold"> character </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">of the Matrix. He has written numerous books on the subject and is the author of the forthcoming book </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">"The Matrix: </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">Artificial Intelligence and the Matrix Theory"</span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">, available from the author's website at: </span><span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">http://www.m</span>
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Beam search

Finally, we'll implement a more advanced way of searching over output: **beam search**. You should read the [HuggingFace page](https://huggingface.co/blog/how-to-generate#beam-search) on beam search before moving on.

In beam search, we maintain a list of size `num_beams` completions which are the most likely completions so far as measured by the product of their probabilities. Since this product can become very small, we use the sum of log probabilities instead. Note - log probabilities are *not* the same as your model's output. We get log probabilities by first taking softmax of our output and then taking log. You can do this with the [`log_softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html) function / tensor method.

<details>
<summary>Log probabilities are equal to the logit output after being translated by some amount X (where X is a function of the original logit output). Can you prove this?</summary>

Suppose our vector of logits is $x$, and we take softmax to get a vector of probabilities $p$, then log again to get a vector of log probabilities $l$. Then the $i$-th element of this vector of logprobs is:

$$
\begin{align}
l_i &= \log p_i \\
&= \log \frac{\exp(x_i)}{\sum_j \exp(x_j)} \\
&= x_i - \log \sum_j \exp(x_j) \\
&= x_i - C
\end{align}
$$

where $C = \log \sum_j \exp(x_j)$ is the same for all elements. So we can see that $l_i$ is equal to the logit output $x_i$ after being translated by $C$.

It's important not to mix up logits and logprobs!
</details>

<details>
<summary>Why do you think we use log softmax rather than logit output?</summary>

Logit output is translation invariant. If we had two different beams and we were generating the next tokens in those beams, there would be no reasonable way to compare the two beams to each other, because we could shift the logit vector for one beam by a constant amount without changing the distribution.

</details>

At each iteration, we run the batch of completions through the model and take the log-softmax to obtain `d_vocab` log-probs for each completion, or `num_beams * d_vocab` possible next completions in total.

If we kept all of these, then we would have `num_beams * d_vocab * d_vocab` completions after the next iteration which is way too many, so instead we sort them by their score and loop through from best (highest) log probability to worst (lowest).

The illustration below might help (based on real results from this method). Here, we have the following hyperparameters:

```python
num_beams = 3
max_new_tokens = 3
num_return_sequences = 2
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/beam-search-3.png" width="1000">

Note how after each "generate" stage, we have `num_beams ** 2` possible completions, which we then filter down to `num_beams`. This is because we need this many in order to find the best `num_beams` completions overall - for example, it's possible that all the best beams of length `n+1` come from the same beam of length `n`, in which case we'll need to keep all `num_beams` that we generated from that single beam.

How do we deal with sequences that terminate early (i.e. by generating an EOS token)? Answer - we append them to the list of completions which we'll return at the end, and remove them from the generation tree. Our algorithm terminates when either all our sequences have length `max_new_tokens` larger than the initial prompt length, or we've generated `num_returns_sequences` terminating sequences.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `beam_search`

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 30-50 minutes on this exercise.
> ```

We've given you one implementation of `beam_search` below, which calls the `generate` and `filter` methods of the `Beams` class (these correspond to the two stages in the diagram above). The `beam_search` method works as follows:

- Create a list `final_logprobs_and_completions` for storing the final output, as tuples of (logprob sum, string completion).
- Perform `max_new_tokens` steps of generation (producing a new set of beams) and filtering (getting the best beams from these combinations), while also adding terminated beams to the list of best beams
- Return these terminated beams plus the best ones we have at the end of the steps.

So all you need to do is fill in the `generate` and `filter` methods. Below, you'll find some unit tests for the `generate` and `filter` methods. When you've passed these tests, you should be able to run the full `beam_search` function.

**Important note** - by default, beam search produces a lot of repeated words / phrases / sentences. This makes sense - if the model finds some completion with a much higher logit sum than most completions in its beam search space, then it will want to repeat this completion even if it doesn't make a lot of sense in context. A common solution is to ban repetition of n-grams, which you should also implement in the function below. In other words, rather than sampling tokens from each sequence by taking `logprobs.topk(k)` in your `generate` method, you should take the `k` top tokens after filtering out those that give you repeated n-grams of length `no_repeat_ngram_size`. Good values of this parameter to try are 2 or 3 (although we recommend you try without this parameter first, so you can see how much of a difference it makes!).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

@dataclass
class Beams:
    """Class to store beams during beam search."""

    model: DemoTransformer
    tokenizer: GPT2TokenizerFast
    logprob_sums: Float[Tensor, "batch"]
    tokens: Int[Tensor, "batch seq"]

    def __getitem__(self, batch_idx) -> "Beams":
        """Allows you to create new beams from old beams by slicing along batch dim (useful for `filter`)."""
        return Beams(
            self.model, self.tokenizer, self.logprob_sums[batch_idx], self.tokens[batch_idx]
        )

    @property
    def logprobs_and_completions(self) -> list[tuple[float, str]]:
        """Returns self as a list of logprob sums and completions (useful for getting final output)."""
        return [
            (logprob_sum.item(), self.tokenizer.decode(tokens))
            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)
        ]

    def generate(self, k: int, no_repeat_ngram_size: int | None = None) -> "Beams":
        """
        Starting from the current set of beams (i.e. self.tokens) and returns a new set of `len(self.tokens) * k` beams,
        containing the best `k` continuations for each of the original beams.

        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with a repeating n-gram
        of this length.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Get the output logprobs for the next token (for every sequence in current beams)
        logprobs = self.model(self.tokens)[:, -1, :].log_softmax(-1)

        # Get the top `toks_per_beam` tokens for each sequence
        topk_logprobs, topk_tokenIDs = self.get_topk_non_repeating(
            logprobs, no_repeat_ngram_size, k=k
        )

        # Add new logprobs & concat new tokens. When doing this, we need to add an extra `k` dimension since our current
        # logprobs & tokens have shape (batch,) and (batch, seq), but our new ones both have shape (batch, k)
        new_logprob_sums = einops.repeat(self.logprob_sums, "b -> b k", k=k) + topk_logprobs
        new_tokens = t.concat(
            [einops.repeat(self.tokens, "b s -> b k s", k=k), topk_tokenIDs.unsqueeze(-1)], dim=-1
        )

        return Beams(
            self.model, self.tokenizer, new_logprob_sums.flatten(), new_tokens.flatten(0, 1)
        )
        # END SOLUTION

    def filter(self, k: int) -> tuple["Beams", "Beams"]:
        """
        Returns:
            best_beams: Beams
                filtered version of self, containing all best `k` which are also not terminated.
            early_terminations: Beams
                filtered version of self, containing all best `k` which are also terminated.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Get the indices of top `k` beams
        top_beam_indices = self.logprob_sums.topk(k=k, dim=0).indices.tolist()
        # Get the indices of terminated sequences
        new_tokens = self.tokens[:, -1]
        terminated_indices = t.nonzero(new_tokens == self.tokenizer.eos_token_id)

        # Get the indices of the `k` best sequences (some terminated, some not terminated)
        best_continuing = [i for i in top_beam_indices if i not in terminated_indices]
        best_terminated = [i for i in top_beam_indices if i in terminated_indices]

        # Return the beam objects from these indices
        return self[best_continuing], self[best_terminated]
        # END SOLUTION

    # SOLUTION
    def get_topk_non_repeating(
        self,
        logprobs: Float[Tensor, "batch d_vocab"],
        no_repeat_ngram_size: int | None,
        k: int,
    ) -> tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
        """
        logprobs:
            tensor of the log-probs for the next token
        no_repeat_ngram_size:
            size of ngram to avoid repeating
        k:
            number of top logits to return, for each beam in our collection

        Returns:
            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure that no returned tokens would produce an
            ngram of size `no_repeat_ngram_size` which has already appeared in `self.tokens`.
        """
        batch, seq_len = self.tokens.shape

        # If completion isn't long enough for a repetition, or we have no restrictions, just return topk
        if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):
            # Otherwise, we need to check for ngram repetitions
            # First, get the most recent `no_repeat_ngram_size-1` tokens
            last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]
            # Next, find all the tokens we're not allowed to generate, by checking all past ngrams for a match
            for i in range(seq_len - (no_repeat_ngram_size - 1)):
                ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)
                ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1)  # (batch,)
                ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)
                # Fill logprobs with neginf wherever the ngrams are repeated
                logprobs[range(batch), ngram_end_tokens] = t.where(
                    ngrams_are_repeated, -1.0e10, logprobs[range(batch), ngram_end_tokens]
                )

        # Finally, get our actual tokens
        return logprobs.topk(k=k, dim=-1)

    # END SOLUTION

    def print(self, title="Best completions", max_print_chars=80) -> None:
        """
        Prints out a set of sequences with their corresponding logprob sums.
        """
        if len(self.tokens) == 0:
            return
        table = Table("logprob sum", "completion", title=title)
        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):
            text = self.tokenizer.decode(tokens)
            if len(repr(text)) > max_print_chars:
                text = (
                    text[: int(0.3 * max_print_chars)]
                    + " ... "
                    + text[-int(0.7 * max_print_chars) :]
                )
            table.add_row(f"{logprob_sum:>8.3f}", repr(text))
        rprint(table)


@t.inference_mode()
def beam_search(
    self: TransformerSampler,
    prompt: str,
    num_return_sequences: int,
    num_beams: int,
    max_new_tokens: int,
    no_repeat_ngram_size: int | None = None,
) -> list[tuple[float, str]]:
    """
    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting from the initial
    prompt) until either of the two stopping criteria are met: (1) we've generated `max_new_tokens` tokens, or (2)
    we've generated `num_returns_sequences` terminating sequences.
    """
    assert num_return_sequences <= num_beams
    self.model.eval()

    tokens = self.tokenizer.encode(prompt, return_tensors="pt").to(device)

    final_logprobs_and_completions = []  # we add to this list as we get terminated beams
    best_beams = Beams(
        self.model, self.tokenizer, t.tensor([0.0]).to(device), tokens
    )  # start with just 1 beam

    for _ in tqdm(range(max_new_tokens)):
        t.cuda.empty_cache()

        # Generate & filter beams
        best_beams = best_beams.generate(k=num_beams, no_repeat_ngram_size=no_repeat_ngram_size)
        best_beams, best_beams_terminated = best_beams.filter(k=num_beams)

        # Add terminated beams to our list, and return early if we have enough
        final_logprobs_and_completions.extend(best_beams_terminated.logprobs_and_completions)
        if len(final_logprobs_and_completions) >= num_return_sequences:
            return final_logprobs_and_completions[:num_return_sequences]

    # Return terminated beams plus the best ongoing beams of length `orig_len + max_new_tokens`
    final_logprobs_and_completions.extend(best_beams.logprobs_and_completions)
    return final_logprobs_and_completions[:num_return_sequences]


TransformerSampler.beam_search = beam_search

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm stuck on the implementation of <code>no_repeat_ngram_size</code>.</summary>

Here's a method, which you can use in your `generate` function in place of `logprobs.topk(k)`, which filters out the ngrams of length `no_repeat_ngram_size` which have already appeared in `self.tokens`:

```python
def get_topk_non_repeating(
    self,
    logprobs: Float[Tensor, "batch d_vocab"],
    no_repeat_ngram_size: int | None,
    k: int,
) -> tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
    """
    logprobs:
        tensor of the log-probs for the next token
    no_repeat_ngram_size:
        size of ngram to avoid repeating
    k:
        number of top logits to return, for each beam in our collection

    Returns:
        equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure that no returned tokens would produce an
        ngram of size `no_repeat_ngram_size` which has already appeared in `self.tokens`.
    """
    batch, seq_len = self.tokens.shape

    # If completion isn't long enough for a repetition, or we have no restrictions, just return topk
    if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):
        # Otherwise, we need to check for ngram repetitions
        # First, get the most recent `no_repeat_ngram_size-1` tokens
        last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]
        # Next, find all the tokens we're not allowed to generate, by checking all past ngrams for a match
        for i in range(seq_len - (no_repeat_ngram_size - 1)):
            ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)
            ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1)  # (batch,)
            ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)
            # Fill logprobs with neginf wherever the ngrams are repeated
            logprobs[range(batch), ngram_end_tokens] = t.where(
                ngrams_are_repeated, -1.0e4, logprobs[range(batch), ngram_end_tokens]
            )

    # Finally, get our actual tokens
    return logprobs.topk(k=k, dim=-1)
```

</details>

<details>
<summary>Solution</summary>

```python
def generate(self, k: int, no_repeat_ngram_size: int | None = None) -> "Beams":
    """
    Starting from the current set of beams (i.e. self.tokens) and returns a new set of `len(self.tokens) * k` beams,
    containing the best `k` continuations for each of the original beams.

    Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with a repeating n-gram
    of this length.
    """
    # Get the output logprobs for the next token (for every sequence in current beams)
    logprobs = self.model(self.tokens)[:, -1, :].log_softmax(-1)

    # Get the top `toks_per_beam` tokens for each sequence
    topk_logprobs, topk_tokenIDs = self.get_topk_non_repeating(logprobs, no_repeat_ngram_size, k=k)

    # Add new logprobs & concat new tokens. When doing this, we need to add an extra `k` dimension since our current
    # logprobs & tokens have shape (batch,) and (batch, seq), but our new ones both have shape (batch, k)
    new_logprob_sums = einops.repeat(self.logprob_sums, "b -> b k", k=k) + topk_logprobs
    new_tokens = t.concat([einops.repeat(self.tokens, "b s -> b k s", k=k), topk_tokenIDs.unsqueeze(-1)], dim=-1)

    return Beams(self.model, self.tokenizer, new_logprob_sums.flatten(), new_tokens.flatten(0, 1))

def filter(self, k: int) -> tuple["Beams", "Beams"]:
    """
    Returns:
        best_beams: Beams
            filtered version of self, containing all best `k` which are also not terminated.
        early_terminations: Beams
            filtered version of self, containing all best `k` which are also terminated.
    """
    # Get the indices of top `k` beams
    top_beam_indices = self.logprob_sums.topk(k=k, dim=0).indices.tolist()
    # Get the indices of terminated sequences
    new_tokens = self.tokens[:, -1]
    terminated_indices = t.nonzero(new_tokens == self.tokenizer.eos_token_id)

    # Get the indices of the `k` best sequences (some terminated, some not terminated)
    best_continuing = [i for i in top_beam_indices if i not in terminated_indices]
    best_terminated = [i for i in top_beam_indices if i in terminated_indices]

    # Return the beam objects from these indices
    return self[best_continuing], self[best_terminated]

def get_topk_non_repeating(
    self,
    logprobs: Float[Tensor, "batch d_vocab"],
    no_repeat_ngram_size: int | None,
    k: int,
) -> tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
    """
    logprobs:
        tensor of the log-probs for the next token
    no_repeat_ngram_size:
        size of ngram to avoid repeating
    k:
        number of top logits to return, for each beam in our collection

    Returns:
        equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure that no returned tokens would produce an
        ngram of size `no_repeat_ngram_size` which has already appeared in `self.tokens`.
    """
    batch, seq_len = self.tokens.shape

    # If completion isn't long enough for a repetition, or we have no restrictions, just return topk
    if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):
        # Otherwise, we need to check for ngram repetitions
        # First, get the most recent `no_repeat_ngram_size-1` tokens
        last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]
        # Next, find all the tokens we're not allowed to generate, by checking all past ngrams for a match
        for i in range(seq_len - (no_repeat_ngram_size - 1)):
            ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)
            ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1)  # (batch,)
            ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)
            # Fill logprobs with neginf wherever the ngrams are repeated
            logprobs[range(batch), ngram_end_tokens] = t.where(
                ngrams_are_repeated, -1.0e4, logprobs[range(batch), ngram_end_tokens]
            )

    # Finally, get our actual tokens
    return logprobs.topk(k=k, dim=-1)
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Example usage of the `Beams` class, and the `print` method, corresponding to the diagram above:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Start with prompt "When I was", get top 3 tokens (and their logprobs), and use that to create & display the top 3 beams
prompt = "When I was"
tokens = tokenizer.encode(prompt, return_tensors="pt").to(device)
logprobs = model(tokens)[0, -1].log_softmax(-1)
top_logprobs, top_tokens = logprobs.topk(k=3, dim=-1)

new_tokens = t.concat([tokens.repeat(3, 1), top_tokens.unsqueeze(-1)], dim=-1)

beams = Beams(model, tokenizer, logprob_sums=top_logprobs, tokens=new_tokens)
beams.print()

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">           Best completions           </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> logprob sum </span>┃<span style="font-weight: bold"> completion           </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩
│   -2.393    │ 'When I was a'       │
│   -2.556    │ 'When I was in'      │
│   -3.168    │ 'When I was growing' │
└─────────────┴──────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
And here are some unit tests for your `generate` and `filter` methods, starting from the prompt `"When I was"` (so your output should match the diagram above).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print("Testing generate...")
new_beams = beams.generate(k=3, no_repeat_ngram_size=1)
new_beams.print()

expected_values = [
    (-3.1, "When I was a kid"),
    (-4.8, "When I was a child"),
    (-4.9, "When I was a little"),
]

for i, (logprob_sum, completion) in enumerate(new_beams.logprobs_and_completions[:3]):
    assert abs(logprob_sum - expected_values[i][0]) < 0.1, f"{i}"
    assert completion == expected_values[i][1], f"{i}"

print("All tests for `generate` passed!")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">              Best completions              </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> logprob sum </span>┃<span style="font-weight: bold"> completion                 </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│   -3.091    │ 'When I was a kid'         │
│   -4.808    │ 'When I was a child'       │
│   -4.916    │ 'When I was a little'      │
│   -4.611    │ 'When I was in the'        │
│   -4.671    │ 'When I was in college'    │
│   -5.140    │ 'When I was in high'       │
│   -3.181    │ 'When I was growing up'    │
│   -9.352    │ 'When I was growing older' │
│  -10.004    │ 'When I was growing my'    │
└─────────────┴────────────────────────────┘
</pre>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print("Testing `filter`...")

best_beams, terminated_beams = new_beams.filter(3)
best_beams.print()

expected_values = [
    (-3.1, "When I was a kid"),
    (-3.2, "When I was growing up"),
    (-4.6, "When I was in the"),
]

for i, (logprob_sum, completion) in enumerate(best_beams.logprobs_and_completions):
    assert abs(logprob_sum - expected_values[i][0]) < 0.1, f"{i}"
    assert completion == expected_values[i][1], f"{i}"

assert len(terminated_beams.logprobs_and_completions) == 0

print("All tests for `filter` passed!")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">            Best completions             </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> logprob sum </span>┃<span style="font-weight: bold"> completion              </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩
│   -3.091    │ 'When I was a kid'      │
│   -3.181    │ 'When I was growing up' │
│   -4.611    │ 'When I was in the'     │
└─────────────┴─────────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Lastly, we'll test the `no_repeat_ngram_size` argument. We do this by continually generating new tokens from our starting beams `beams`, and seeing if the model repeats the `I was` ngram (which it will by default unless we prohibit repeating n-grams).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print("Testing `no_repeat_ngram_size`...")

new_beams = beams
for _ in range(5):
    new_beams = new_beams.generate(k=1)
new_beams.print(title="Completions with no ngram restriction")
assert all(
    "I was" in completion.removeprefix(prompt)
    for _, completion in new_beams.logprobs_and_completions
), "Without restriction, all beams should be completed as '...I was...'"

new_beams = beams
for _ in range(5):
    new_beams = new_beams.generate(k=1, no_repeat_ngram_size=2)
new_beams.print(title="Completions with no repeated bigrams")
assert all(
    "I was" not in completion.removeprefix(prompt)
    for _, completion in new_beams.logprobs_and_completions
), "With no repeated bigrams, no beams should contain a second '...I was...'"

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">         Completions with no ngram restriction         </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> logprob sum </span>┃<span style="font-weight: bold"> completion                            </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│   -9.144    │ 'When I was a kid, I was always'      │
│  -10.811    │ 'When I was in the hospital, I was'   │
│   -9.121    │ 'When I was growing up, I was always' │
└─────────────┴───────────────────────────────────────┘
</pre>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">        Completions with no repeated bigrams         </span>
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> logprob sum </span>┃<span style="font-weight: bold"> completion                          </span>┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│   -8.650    │ 'When I was a kid, I would go'      │
│  -11.909    │ 'When I was in the hospital, I saw' │
│   -9.043    │ 'When I was growing up, I would go' │
└─────────────┴─────────────────────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed all of these unit tests, you can try implementing the full beam search function. It should create a `Beams` object from the initial prompt, and then repeatedly call `generate` and `filter` until the stopping criteria are met.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

sampler = TransformerSampler(model, tokenizer)

prompt = "The ships hung in the sky in much the same way that"
orig_len = len(tokenizer.encode(prompt))

final_logitsums_and_completions = sampler.beam_search(
    prompt=prompt,
    num_return_sequences=3,
    num_beams=40,
    max_new_tokens=60,
    no_repeat_ngram_size=2,
)

# Print all the best output
for logprob_sum, text in final_logitsums_and_completions:
    avg_logprob_as_prob = t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp()
    rprint(f"Avg token prob = {avg_logprob_as_prob:.3f}\nBest output:\n[bold dark_orange]{text}")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output (you might not get identical results depending on the exact details of your implementation)]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Avg token prob = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.255</span>
Best output:
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">The ships hung in the sky in much the same way that they did at the beginning of the Second World War.</span>

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">For the first time in history, the U.S. Navy was able to carry out a full-scale amphibious assault on a large </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">number of targets in a short period of time. In doing so, it allowed the Navy to</span>


Avg token prob = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.254</span>
Best output:
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">The ships hung in the sky in much the same way that they did at the beginning of the Second World War.</span>

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">For the first time in history, the U.S. Navy was able to carry out a full-scale amphibious assault on a large </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">number of targets in a short period of time. It was a major victory for the United States</span>


Avg token prob = <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.254</span>
Best output:
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">The ships hung in the sky in much the same way that they did at the beginning of the Second World War.</span>

<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">For the first time in history, the U.S. Navy was able to carry out a full-scale amphibious assault on a large </span>
<span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">number of targets in a short period of time. In fact, it was only a matter of</span>
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## KV Caching
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
*This section is also designed to be challenging, and take quite some time. There are many different ways to solve it, and you're expected to try and find your own way (you should think about this for a while before looking at the suggestions in the dropdowns). Additionally, you might not find it as interesting as some of the other sections. In this case, and if you have a lot of extra time, you might want to start on the "building BERT" exercises from this chapter.*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### How can caching help us?

The text generation we've done so far is needlessly re-computing certain values, which is very noticeable when you try to generate longer sequences.

Suppose you're generating text, and you've already run GPT on the sentence "My life motto:". Now you want to run the model on the sentence "My life motto: Always". Which computations from the first sentence can you reuse?

<details>
<summary>Answer</summary>

At each attention layer, the only things the attention layer needs from the previous sequence positions are the key and value vectors. This is explained in the following diagram, which compares the attention layer with and without caching (it's a big diagram so you might want to open it in a separate window to zoom in).


<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/tl-cache-full.png" width="1200">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement KV caching

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵⚪⚪⚪⚪
> 
> You are expected to spend well over an hour on this exercise, if you choose to do it.
> ```

Modify your GPT-2 to optionally use a cache. When you run your GPT on `"My life motto:"`, it should store the necessary values in the cache. Then in the next forward pass with just `" Always"` as input, it should load the cached values instead of recomputing them (and update the cache). This only needs to work with a single input sequence (batch size of 1), and you can assume that after the first forward pass, the input will be just one token.

The design of the cache is completely up to you - discuss possible designs with your partner before writing code. It should be possible to have only one GPT2 instance and many different cache instances at one time. Imagine that you want to use one instance to serve multiple users submitting requests for text generation like in [AI Dungeon](https://aidungeon.io/).

You'll also need to rewrite parts of your `DemoTransformer` code, in order to get this to work. The tests have been built to accommodate modules which return their output as the first element in a tuple (i.e. `(output, cache)`) rather than just returning the output, so you should use the tests to verify that your modules still work as expected.

Some example considerations:

* Which GPT-2 classes need to interact with the cache?
    * Will you need to change the positional embedding, and if so then how?
* Should the cache be mutable and be updated in place, or does updating actually just create a separate instance?
    * *(Hint here - think about how you might use the cache during beam search.)*
* Is it possible for other programmers to incorrectly use your cache? Is there a way to prevent this failure mode or at least detect this and complain loudly?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Cache implentation (example)</summary>

This KeyValueCache object is structured as just a fancy tensor (it inherits all the methods from Tensor). The main difference is that it has a few extra helper methods, e.g. constructing an empty cache from a Config object.

There are other ways you could do this, e.g. having your `KeyValueCache` class contain list of `KeyValueCacheEntry` objects (where each of these corresponds to a different layer).

```python
# Define a type for a single layer's cache entry (useful for type checking in later functions)
KeyValueCacheTensor = Float[Tensor, "2 batch seq_len n_heads d_head"]

class KeyValueCache(Tensor):
    \'\'\'
    This class holds tensors of key and value vectors, to be used for caching.

    If we define it using cfg and batch then it's initialized as empty, but
    we can also define it from kv_cache_entries.
    \'\'\'
    @classmethod
    def new_empty(cls, cfg: Config, batch: int = 1) -> "KeyValueCache":
        \'\'\'
        Doing a forward pass on a cache created in this way indicates "we don't
        yet have a cache, but we want this forward pass to return a cache".
        Whereas using cache=None in a forward pass indicates we don't want to
        return a cache.
        \'\'\'
        shape = (cfg.n_layers, 2, batch, 0, cfg.n_heads, cfg.d_head)
        return cls(*shape).to(device)

    # Define a handful of properties, so they can be referenced directly rather than
    # indexing (which is more likely to lead to mistakes)

    @property
    def k(self) -> Tensor:
        return self[:, 0]

    @property
    def v(self) -> Tensor:
        return self[:, 1]

    @property
    def batch(self) -> int:
        return self.shape[2]

    @property
    def seq_len(self) -> int:
        return self.shape[3]


# Example implementation:
cfg = model.cfg
batch = 6
kv_cache = KeyValueCache.new_empty(cfg, batch)

print(f"Shape of all kv-cache = {tuple(kv_cache.shape)}")
print(f"Shape of just k-cache = {tuple(kv_cache.k.shape)}")
for kv_cache_entry in kv_cache:
    print(f"Shape of cache entry for one layer = {tuple(kv_cache_entry.shape)}")
    break
print(f"Batch size = {kv_cache.batch}")
print(f"Current sequence length = {kv_cache.seq_len}")
```

</details>

<details>
<summary>New <code>DemoTransformer</code> components (and testing)</summary>

```python
# Define new model parts where necessary, and create a new model & test it
# Note that sometimes our modules return a tuple of (tensor output, cache) rather than just output. The
# tests have been built to accommodate this.


class PosEmbed(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))
        nn.init.normal_(self.W_pos, std=self.cfg.init_range)

    def forward(
        self,
        tokens: Int[Tensor, "batch position"],
        past_kv_pos_offset: int = 0
    ) -> Float[Tensor, "batch position d_model"]:

        batch, seq_len = tokens.shape
        return einops.repeat(
            self.W_pos[past_kv_pos_offset: seq_len+past_kv_pos_offset],
            "seq d_model -> batch seq d_model",
            batch=batch
        )


class Attention(nn.Module):
    IGNORE: Float[Tensor, ""]

    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))
        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))
        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))
        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))
        nn.init.normal_(self.W_Q, std=self.cfg.init_range)
        nn.init.normal_(self.W_K, std=self.cfg.init_range)
        nn.init.normal_(self.W_V, std=self.cfg.init_range)
        nn.init.normal_(self.W_O, std=self.cfg.init_range)
        self.register_buffer("IGNORE", t.tensor(-1e5, dtype=t.float32, device=device))

    def forward(
        self,
        normalized_resid_pre: Float[Tensor, "batch posn d_model"],
        kv_cache_entry: KeyValueCacheTensor | None = None,
    ) -> tuple[
        Float[Tensor, "batch posn d_model"],
        KeyValueCacheTensor | None
    ]:
        \'\'\'
        Returns the result of applying attention layer to normlized_resid_pre, as well as
        the new cached key and value vectors (which we get from concatenating the old cached
        ones with the new key and value vectors).
        \'\'\'
        # Calculate the new query, key and value vectors
        q = einops.einsum(
            normalized_resid_pre, self.W_Q,
            "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head"
        ) + self.b_Q
        k = einops.einsum(
            normalized_resid_pre, self.W_K,
            "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head"
        ) + self.b_K
        v = einops.einsum(
            normalized_resid_pre, self.W_V,
            "batch posn d_model, nheads d_model d_head -> batch posn nheads d_head"
        ) + self.b_V

        # If cache_entry is not None, this means we use the previous key and value vectors
        # Also we'll need to get a new cache entry which will be used later to construct a new cache
        if kv_cache_entry is not None:
            k = t.concat([kv_cache_entry[0], k], dim=1)
            v = t.concat([kv_cache_entry[1], v], dim=1)
            kv_cache_entry = t.stack([k, v])

        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities
        attn_scores = einops.einsum(
            q, k,
            "batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K"
        )
        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)
        attn_pattern = attn_scores_masked.softmax(-1)

        # Take weighted sum of value vectors, according to attention probabilities
        z = einops.einsum(
            v, attn_pattern,
            "batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head"
        )

        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)
        out = einops.einsum(
            z, self.W_O,
            "batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model"
        ) + self.b_O

        return out, kv_cache_entry

    def apply_causal_mask(
        self, attn_scores: Float[Tensor, "batch n_heads query_pos key_pos"]
    ) -> Float[Tensor, "batch n_heads query_pos key_pos"]:
        \'\'\'
        Here, attn_scores have shape (batch, n_heads, query_pos, key_pos), where query_pos represents the
        new (non-cached) positions, and key_pos represent all the positions (cached and non-cached).

        So when we create our mask, the query indices and key indices will both go up to the same value
        (the full sequence length), but the query indices will start at >0.
        \'\'\'
        new_seq_len, full_seq_len = attn_scores.shape[-2:]
        assert new_seq_len <= full_seq_len
        q_posn = einops.repeat(attn_scores.new_tensor(range(full_seq_len-new_seq_len, full_seq_len)), "q -> q k", k=full_seq_len)
        k_posn = einops.repeat(attn_scores.new_tensor(range(full_seq_len)), "k -> q k", q=new_seq_len)
        mask = q_posn < k_posn
        attn_scores = attn_scores.masked_fill(mask, self.IGNORE)
        return attn_scores


class TransformerBlock(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.ln1 = LayerNorm(cfg)
        self.attn = Attention(cfg)
        self.ln2 = LayerNorm(cfg)
        self.mlp = MLP(cfg)

    def forward(
        self,
        resid_pre: Float[Tensor, "batch position d_model"],
        kv_cache_entry: KeyValueCacheTensor | None = None,
    ) -> Float[Tensor, "batch position d_model"]:

        attn_out, kv_cache_entry = self.attn(self.ln1(resid_pre), kv_cache_entry)
        resid_mid = attn_out + resid_pre
        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid
        return resid_post, kv_cache_entry


class DemoTransformer(nn.Module):
    def __init__(self, cfg: Config):
        super().__init__()
        self.cfg = cfg
        self.embed = Embed(cfg)
        self.pos_embed = PosEmbed(cfg)
        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])
        self.ln_final = LayerNorm(cfg)
        self.unembed = Unembed(cfg)

    def forward(
        self,
        tokens: Int[Tensor, "batch seq_pos"],
        kv_cache: KeyValueCache | None = None
    ) -> Float[Tensor, "batch position d_vocab"]:

        using_kv_cache = kv_cache is not None

        if using_kv_cache:
            # If using kv_cache, then we only need to pass forward the newest tokens
            # Remember to add positional offset!
            n_cached_tokens = kv_cache.seq_len
            tokens = tokens[:, n_cached_tokens:]
            residual = self.embed(tokens) + self.pos_embed(tokens, n_cached_tokens)
        else:
            # If not using cache, turn it into a list of None's (so we can iterate through it)
            kv_cache = [None for _ in range(self.cfg.n_layers)]
            residual = self.embed(tokens) + self.pos_embed(tokens)

        # Apply all layers, and create a (new) kv_cache from the key & value vectors
        new_kv_cache_entries: list[KeyValueCacheTensor] = []
        for block, kv_cache_entry in zip(self.blocks, kv_cache):
            residual, kv_cache_entry = block(residual, kv_cache_entry)
            if using_kv_cache: new_kv_cache_entries.append(kv_cache_entry)

        logits = self.unembed(self.ln_final(residual))

        if using_kv_cache:
            return logits, KeyValueCache(t.stack(new_kv_cache_entries))
        else:
            return logits, None


tokens = reference_gpt2.to_tokens(reference_text).to(device)
logits, cache = reference_gpt2.run_with_cache(tokens)

rand_int_test(PosEmbed, [2, 4])
load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)
rand_float_test(Attention, [2, 4, 768])
load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache["normalized", 0, "ln1"])
rand_float_test(TransformerBlock, [2, 4, 768])
load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache["resid_pre", 0])
rand_int_test(DemoTransformer, [2, 4])
load_gpt2_test(DemoTransformer, reference_gpt2, tokens)
```

</details>

<details>
<summary>New sampling function</summary>

```python
@t.inference_mode()
def sample_with_cache(
    self: TransformerSampler,
    prompt: str,
    max_tokens_generated=100,
    kv_cache: KeyValueCache | None = None,
    verbose=False,
    seed: int | None = None,
    **kwargs
) -> str:

    self.model.eval()
    input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(device)[0]
    if seed is not None:
        np.random.seed(seed)
        t.manual_seed(seed)

    for i in tqdm(range(max_tokens_generated)):
        # Get new logits (make sure we don't pass in more tokens than the model's context length)
        logits, kv_cache = self.model(input_ids[None, -self.cfg.n_ctx:], kv_cache)
        # We only take logits for the last token, because this is what we're sampling
        logits = logits[0, -1]
        # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)
        next_token = t.tensor([TransformerSampler.sample_next_token(input_ids, logits, **kwargs)], device=device)
        # Create new input ids string, with shape (1, old_seq_len + 1)
        input_ids = t.cat([input_ids, next_token], dim=-1)
        # Print out results, if required
        if verbose:
            print(self.tokenizer.decode(input_ids), end="\r")
        # If our new token was the end-of-text token, stop
        if next_token == getattr(self.tokenizer, "eos_token_id", None):
            break

    return self.tokenizer.decode(input_ids)


TransformerSampler.sample = sample_with_cache
```
</details>

<details>
<summary>Code to verify that the same output is being produced by cache and no-cache versions (and to compare speeds)</summary>

```python
device = t.device("cuda") # can also try "cpu"

model = DemoTransformer(Config()).to(device)
model.load_state_dict(reference_gpt2.state_dict(), strict=False);

initial_text = "Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for"
# input_ids = tokenizer.encode(initial_text, return_tensors="pt").squeeze()

sampler = TransformerSampler(model, tokenizer)

# Run the noncached version
t0 = time.time()
text = sampler.sample(
    initial_text,
    temperature=0.7,
    top_p=0.95,
    seed=0,
)
print(f"Time taken (without cache): {time.time() - t0:.2f} seconds")
rprint(f"Model output:\n\n[bold dark_orange]{text}[/]")

# Run the cached version
t0 = time.time()
text_with_cache = sampler.sample(
    initial_text,
    temperature=0.7,
    top_p=0.95,
    seed=0,
    kv_cache=KeyValueCache.new_empty(sampler.cfg)
)
print(f"Time taken (with cache): {time.time() - t0:.2f} seconds")
rprint(f"Model output:\n\n[bold dark_orange]{text_with_cache}[/]")

# # Check they are the same
assert text == text_with_cache, "Your outputs are different, meaning you've probably made a mistake in your cache implementation (or failed to use random seeds)."
print("Tests passed!")
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You may find that your cache implementation provides a modest speedup, but probably not close to the `seq_len`-factor speedup you'd expect from the fact that you only compute one additional token at each step rather than all of them. Why is this? The answer is that, much like everything to do with computational and memory costs in deep learning, it's not so simple. There are a host of different factors which might be bottlenecking our model's forward pass speed. If you try this on the CPU, you should get a much more noticeable speedup.

For a bit more on these topics, see [here](https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Bonus - cached beam search

Can you modify your beam search function to use caching?

Depending on how you implemented your cache earlier, you might find that a different form of caching is better suited to beam search.

Again, we've provided an example implementation in a dropdown below, which is based on the cache implementation above and the previous solution for `beam_search`.

<details>
<summary>Cached beam search function</summary>

As we touched on earlier, thanks to our modular code, not a lot needs to be changed when adding cache support.

```python
@dataclass
class Beams:
    \'\'\'Class to store beams during beam search.\'\'\'
    model: DemoTransformer
    tokenizer: GPT2TokenizerFast
    logprob_sums: Float[Tensor, "batch"]
    tokens: Int[Tensor, "batch seq"]
    kv_cache: KeyValueCache | None = None

    def __getitem__(self, idx) -> "Beams":
        \'\'\'Helpful function allowing you to take a slice of the beams object along the batch dimension.\'\'\'
        return Beams(
            self.model,
            self.tokenizer,
            self.logprob_sums[idx],
            self.tokens[idx],
            self.kv_cache[:, :, idx] if self.kv_cache is not None else None
        )

    @property
    def logprobs_and_completions(self) -> list[tuple[float, str]]:
        \'\'\'Returns self as a list of logprob sums and completions (useful for getting final output).\'\'\'
        return [
            (logprob_sum.item(), self.tokenizer.decode(tokens))
            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)
        ]


    def generate(self, k: int, no_repeat_ngram_size: int | None = None) -> "Beams":
        \'\'\'
        Starting from the current set of beams (i.e. self.tokens) and returns a new set of `len(self.tokens) * k` beams,
        containing the best `k` continuations for each of the original beams.

        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with a repeating n-gram
        of this length. 
        \'\'\'
        # Get the output logprobs for the next token (for every sequence in current beams)
        logprobs, kv_cache = self.model(self.tokens, self.kv_cache)
        logprobs = logprobs[:, -1, :].log_softmax(-1)

        # Get the top `toks_per_beam` tokens for each sequence
        topk_logprobs, topk_tokenIDs = self.get_topk_non_repeating(logprobs, no_repeat_ngram_size, k=k)

        # Add new logprobs & concat new tokens. When doing this, we need to add an extra `k` dimension since our current
        # logprobs & tokens have shape (batch,) and (batch, seq), but our new ones both have shape (batch, k)
        new_logprob_sums = einops.repeat(self.logprob_sums, "b -> b k", k=k) + topk_logprobs
        new_tokens = t.concat([einops.repeat(self.tokens, "b s -> b k s", k=k), topk_tokenIDs.unsqueeze(-1)], dim=-1)

        return Beams(self.model, self.tokenizer, new_logprob_sums.flatten(), new_tokens.flatten(0, 1), new_kv_cache)


    def filter(self, k: int) -> tuple["Beams", "Beams"]:
        \'\'\'
        Returns:
            best_beams: Beams
                filtered version of self, containing all best `k` which are also not terminated.
            early_terminations: Beams
                filtered version of self, containing all best `k` which are also terminated.
        \'\'\'
        # Get the indices of top `k` beams
        top_beam_indices = self.logprob_sums.topk(k=k, dim=0).indices.tolist()
        # Get the indices of terminated sequences
        new_tokens = self.tokens[:, -1]
        terminated_indices = t.nonzero(new_tokens == self.tokenizer.eos_token_id)

        # Get the indices of the `k` best sequences (some terminated, some not terminated)
        best_continuing = [i for i in top_beam_indices if i not in terminated_indices]
        best_terminated = [i for i in top_beam_indices if i in terminated_indices]

        # Return the beam objects from these indices
        return self[best_continuing], self[best_terminated]


    def get_topk_non_repeating(
        self,
        logprobs: Float[Tensor, "batch d_vocab"],
        no_repeat_ngram_size: int | None,
        k: int,
    ) -> tuple[Float[Tensor, "k"], Int[Tensor, "k"]]:
        """
        logprobs:
            tensor of the log-probs for the next token
        no_repeat_ngram_size:
            size of ngram to avoid repeating
        k:
            number of top logits to return, for each beam in our collection

        Returns:
            equivalent to the output of `logprobs.topk(dim=-1)`, but makes sure that no returned tokens would produce an
            ngram of size  `no_repeat_ngram_size` which has already appeared in `self.tokens`.
        """
        batch, seq_len = self.tokens.shape

        # If completion isn't long enough for a repetition, or we have no restrictions, just return topk
        if (no_repeat_ngram_size is not None) and (seq_len > no_repeat_ngram_size - 1):
            # Otherwise, we need to check for ngram repetitions
            # First, get the most recent `no_repeat_ngram_size-1` tokens
            last_ngram_prefix = self.tokens[:, seq_len - (no_repeat_ngram_size - 1) :]
            # Next, find all the tokens we're not allowed to generate, by checking all past ngrams for a match
            for i in range(seq_len - (no_repeat_ngram_size - 1)):
                ngrams = self.tokens[:, i : i + no_repeat_ngram_size]  # (batch, ngram)
                ngrams_are_repeated = (ngrams[:, :-1] == last_ngram_prefix).all(-1)  # (batch,)
                ngram_end_tokens = ngrams[:, [-1]]  # (batch, 1)
                # Fill logprobs with neginf wherever the ngrams are repeated
                logprobs[range(batch), ngram_end_tokens] = t.where(
                    ngrams_are_repeated, -1.0e4, logprobs[range(batch), ngram_end_tokens]
                )

        # Finally, get our actual tokens
        return logprobs.topk(k=k, dim=-1)

    def print(self, title="Best completions", max_print_chars=80) -> None:
        \'\'\'
        Prints out a set of sequences with their corresponding logitsums.
        \'\'\'
        if len(self.tokens) == 0:
            return
        table = Table("logitsum", "completion", title=title)
        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):
            text = self.tokenizer.decode(tokens)
            if len(repr(text)) > max_print_chars:
                text = text[:int(0.3 * max_print_chars)] + " ... " + text[-int(0.7 * max_print_chars):]
            table.add_row(f"{logprob_sum:>8.3f}", repr(text))
        rprint(table)


    @t.inference_mode()
    def beam_search(
        self,
        prompt: str,
        num_return_sequences: int,
        num_beams: int,
        max_new_tokens: int,
        no_repeat_ngram_size: int | None = None,
        kv_cache: KeyValueCache | None = None,
    ) -> list[tuple[float, Tensor]]:
        \'\'\'
        Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting from the initial
        prompt) until either of the two stopping criteria are met: (1) we've generated `max_new_tokens` tokens, or (2)
        we've generated `num_returns_sequences` terminating sequences.
        \'\'\'
        assert num_return_sequences <= num_beams
        self.model.eval()

        tokens = self.tokenizer.encode(prompt, return_tensors="pt").to(device)

        final_logprobs_and_completions = []  # we add to this list as we get terminated beams
        best_beams = Beams(self.model, self.tokenizer, t.tensor([0.0]).to(device), tokens)  # start with just 1 beam

        for _ in tqdm(range(max_new_tokens)):
            # Generate & filter beams
            best_beams = best_beams.generate(k=num_beams, no_repeat_ngram_size=no_repeat_ngram_size)
            best_beams, best_beams_terminated = best_beams.filter(k=num_beams)

            # Add terminated beams to our list, and return early if we have enough
            final_logprobs_and_completions.extend(best_beams_terminated.logprobs_and_completions)
            if len(final_logprobs_and_completions) >= num_return_sequences:
                return final_logprobs_and_completions[:num_return_sequences]

        # Return terminated beams plus the best ongoing beams of length `orig_len + max_new_tokens`
        final_logprobs_and_completions.extend(best_beams.logprobs_and_completions)
        return final_logprobs_and_completions[:num_return_sequences]


```

</details>

<details>
<summary>Code to verify that the same output is being produced by cache and no-cache versions (and to compare speeds)</summary>

```python
prompt = "For you, the day Bison graced your village was the most important day of your life. But for me, it was"
orig_len = len(tokenizer.encode(prompt))

beam_search_kwargs = dict(
    prompt=prompt,
    num_return_sequences=3,
    num_beams=20,
    max_new_tokens=60,
    no_repeat_ngram_size=2,
    verbose=False
)

sampler = TransformerSampler(model, tokenizer)

# Run the noncached version
t0 = time.time()
final_logitsums_and_completions = sampler.beam_search(**beam_search_kwargs)
logprob_sum, text = final_logitsums_and_completions[0]
avg_logprob_as_prob = t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()
print(f"Time (without cache): {time.time() - t0:.2f} seconds")
print(f"Avg logprob (expressed as a probability) = {avg_logprob_as_prob:.3f}")
rprint(f"Output:\n\n[bold dark_orange]{text}[/]\n\n")

# Run the cached version
t0 = time.time()
beam_search_kwargs["kv_cache"] = KeyValueCache.new_empty(model.cfg)
final_logitsums_and_completions = sampler.beam_search(**beam_search_kwargs)
logprob_sum, text_with_cache = final_logitsums_and_completions[0]
avg_logprob_as_prob = t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()
print(f"Time (with cache): {time.time() - t0:.2f} seconds")
print(f"Avg logprob (as probability) = {avg_logprob_as_prob:.3f}", end="")
rprint(f"Output:\n\n[bold dark_orange]{text_with_cache}[/]\n\n")

# Check they are the same
assert text == text_with_cache, "Your outputs are different, meaning you've probably made a mistake in your cache implementation."
print("Tests passed!")
```

</details>
'''




---
File: /infrastructure/master_files/master_1_2.py
---

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
[
    {"title": "TransformerLens: Introduction", "icon": "1-circle-fill", "subtitle": "(15%)"},
    {"title": "Finding induction heads", "icon": "2-circle-fill", "subtitle": "(25%)"},
    {"title": "TransformerLens: Hooks", "icon": "3-circle-fill", "subtitle": "(30%)"},
    {"title": "Reverse-engineering induction circuits", "icon": "4-circle-fill", "subtitle": "(30%)"},
]
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# [1.2] Intro to Mechanistic Interpretability: TransformerLens & induction circuits
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-12.png" width="350">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
These pages are designed to get you introduced to the core concepts of mechanistic interpretability, via Neel Nanda's **TransformerLens** library.

Most of the sections are constructed in the following way:

1. A particular feature of TransformerLens is introduced.
2. You are given an exercise, in which you have to apply the feature.

The running theme of the exercises is **induction circuits**. Induction circuits are a particular type of circuit in a transformer, which can perform basic in-context learning. You should read the [corresponding section of Neel's glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ), before continuing. This [LessWrong post](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated) might also help; it contains some diagrams (like the one below) which walk through the induction mechanism step by step.

Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram.png" width="1000">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Content & Learning Objectives

### 1️⃣ TransformerLens: Introduction

This section is designed to get you up to speed with the TransformerLens library. You'll learn how to load and run models, and learn about the shared architecture template for all of these models (the latter of which should be familiar to you if you've already done the exercises that come before these, since many of the same design principles are followed).

> ##### Learning Objectives
>
> - Load and run a `HookedTransformer` model
> - Understand the basic architecture of these models
> - Use the model's tokenizer to convert text to tokens, and vice versa
> - Know how to cache activations, and to access activations from the cache
> - Use `circuitsvis` to visualise attention heads

### 2️⃣ Finding induction heads

Here, you'll learn about induction heads, how they work and why they are important. You'll also learn how to identify them from the characteristic induction head stripe in their attention patterns when the model input is a repeating sequence.

> ##### Learning Objectives
>
> - Understand what induction heads are, and the algorithm they are implementing
> - Inspect activation patterns to identify basic attention head patterns, and write your own functions to detect attention heads for you
> - Identify induction heads by looking at the attention patterns produced from a repeating random sequence

### 3️⃣ TransformerLens: Hooks

Next, you'll learn about hooks, which are a great feature of TransformerLens allowing you to access and intervene on activations within the model. We will mainly focus on the basics of hooks and using them to access activations (we'll mainly save the causal interventions for the later IOI exercises). You will also build some tools to perform logit attribution within your model, so you can identify which components are responsible for your model's performance on certain tasks.

> ##### Learning Objectives
>
> - Understand what hooks are, and how they are used in TransformerLens
> - Use hooks to access activations, process the results, and write them to an external tensor
> - Build tools to perform attribution, i.e. detecting which components of your model are responsible for performance on a given task
> - Understand how hooks can be used to perform basic interventions like **ablation**

### 4️⃣ Reverse-engineering induction circuits

Lastly, these exercises show you how you can reverse-engineer a circuit by looking directly at a transformer's weights (which can be considered a "gold standard" of interpretability; something not possible in every situation). You'll examine QK and OV circuits by multiplying through matrices (and learn how the FactoredMatrix class makes matrices like these much easier to analyse). You'll also look for evidence of composition between two induction heads, and once you've found it then you'll investigate the functionality of the full circuit formed from this composition.

> ##### Learning Objectives
>
> - Understand the difference between investigating a circuit by looking at activation patterns, and reverse-engineering a circuit by looking directly at the weights
> - Use the factored matrix class to inspect the QK and OV circuits within an induction circuit
> - Perform further exploration of induction circuits: composition scores, and targeted ablations
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup code
'''

# ! CELL TYPE: code
# ! FILTERS: [~]
# ! TAGS: []

from IPython import get_ipython

ipython = get_ipython()
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: [master-comment]

# import os
# import sys
# from pathlib import Path

# import pkg_resources

# IN_COLAB = "google.colab" in sys.modules

# chapter = "chapter1_transformer_interp"
# repo = "ARENA_3.0"
# branch = "main"

# # Install dependencies
# installed_packages = [pkg.key for pkg in pkg_resources.working_set]
# if "transformer-lens" not in installed_packages:
#     %pip install transformer_lens==2.11.0 einops eindex-callum jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python

# # Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
# root = (
#     "/content"
#     if IN_COLAB
#     else "/root"
#     if repo not in os.getcwd()
#     else str(next(p for p in Path.cwd().parents if p.name == repo))
# )

# if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
#     if not IN_COLAB:
#         !sudo apt-get install unzip
#         %pip install jupyter ipython --upgrade

#     if not os.path.exists(f"{root}/{chapter}"):
#         !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
#         !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
#         !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
#         !rm {root}/{branch}.zip
#         !rmdir {root}/ARENA_3.0-{branch}


# if f"{root}/{chapter}/exercises" not in sys.path:
#     sys.path.append(f"{root}/{chapter}/exercises")

# os.chdir(f"{root}/{chapter}/exercises")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import functools
import sys
from pathlib import Path
from typing import Callable

import circuitsvis as cv
import einops
import numpy as np
import torch as t
import torch.nn as nn
import torch.nn.functional as F
from eindex import eindex
from IPython.display import display
from jaxtyping import Float, Int
from torch import Tensor
from tqdm import tqdm
from transformer_lens import (
    ActivationCache,
    FactoredMatrix,
    HookedTransformer,
    HookedTransformerConfig,
    utils,
)
from transformer_lens.hook_points import HookPoint

device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part2_intro_to_mech_interp"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
# FILTERS: ~colab
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))
# END FILTERS

import part2_intro_to_mech_interp.tests as tests
from plotly_utils import (
    hist,
    imshow,
    plot_comp_scores,
    plot_logit_attribution,
    plot_loss_difference,
)

# Saves computation time, since we don't need it for the contents of this notebook
t.set_grad_enabled(False)

MAIN = __name__ == "__main__"

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 1️⃣ TransformerLens: Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Introduction

*Note - most of this is written from the POV of Neel Nanda.*

This is a demo notebook for [TransformerLens](https://github.com/neelnanda-io/TransformerLens), **a library I ([Neel Nanda](neelnanda.io)) wrote for doing [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models.** The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **Check out my [list of concrete open problems](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit#) to figure out where to start.**

I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!

The core design principle I've followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [my notebook analysing Indirect Objection Identification](https://github.com/neelnanda-io/TransformerLens/blob/main/Exploratory_Analysis_Demo.ipynb) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Loading and Running Models

TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

gpt2_small: HookedTransformer = HookedTransformer.from_pretrained("gpt2-small")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### HookedTransformerConfig

Alternatively, you can define a config object, then call `HookedTransformer.from_config(cfg)` to define your model. This is particularly useful when you want to have finer control over the architecture of your model. We'll see an example of this in the next section, when we define an attention-only model to study induction heads.

Even if you don't define your model in this way, you can still access the config object through the `cfg` attribute of the model.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - inspect your model

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> ```

Use `gpt2_small.cfg` to find the following, for your GPT-2 Small model:

* Number of layers
* Number of heads per layer
* Maximum context window

You might have to check out the documentation page for some of these. If you're in VSCode then you can reach it by right-clicking on `HookedTransformerConfig` and choosing "Go to definition". If you're in Colab, then you can read the [GitHub page](https://github.com/neelnanda-io/TransformerLens).

<details>
<summary>Answer</summary>

The following parameters in the config object give you the answers:

```
cfg.n_layers == 12
cfg.n_heads == 12
cfg.n_ctx == 1024
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Running your model

Models can be run on a single string or a tensor of tokens (shape: `[batch, position]`, all integers). The possible return types are:

* `"logits"` (shape `[batch, position, d_vocab]`, floats),
* `"loss"` (the cross-entropy loss when predicting the next token),
* `"both"` (a tuple of `(logits, loss)`)
* `None` (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model_description_text = """## Loading Models

HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.

For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!"""

loss = gpt2_small(model_description_text, return_type="loss")
print("Model loss:", loss)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Model loss: tensor(4.3443, device='cuda:0')</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Transformer architecture

HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads:

* The weights `W_K`, `W_Q`, `W_V` mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.
* The weight matrices `W_K`, `W_Q`, `W_V`, `W_O` and activations have separate `head_index` and `d_head` axes, rather than flattening them into one big axis.
    * The activations all have shape `[batch, position, head_index, d_head]`.
    * `W_K`, `W_Q`, `W_V` have shape `[head_index, d_model, d_head]` and `W_O` has shape `[head_index, d_head, d_model]`
* **Important - we generally follow the convention that weight matrices multiply on the right rather than the left.** In other words, they have shape `[input, output]`, and we have `new_activation = old_activation @ weights + bias`.
    * Click the dropdown below for examples of this, if it seems unintuitive.

<details>
<summary>Examples of matrix multiplication in our model</summary>

* **Query matrices**
    * Each query matrix `W_Q` for a particular layer and head has shape `[d_model, d_head]`.
    * So if a vector `x` in the residual stream has length `d_model`, then the corresponding query vector is `x @ W_Q`, which has length `d_head`.
* **Embedding matrix**
    * The embedding matrix `W_E` has shape `[d_vocab, d_model]`.
    * So if `A` is a one-hot-encoded vector of length `d_vocab` corresponding to a particular token, then the embedding vector for this token is `A @ W_E`, which has length `d_model`.

</details>

The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Parameters and Activations

It's important to distinguish between parameters and activations in the model.

* **Parameters** are the weights and biases that are learned during training.
    * These don't change when the model input changes.
    * They can be accessed directly from the model, e.g. `model.W_E` for the embedding matrix.
* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.
    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.
    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.
    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).

The link below shows a diagram of a single layer (called a `TransformerBlock`) for an attention-only model with no biases. Each box corresponds to an **activation** (and also tells you the name of the corresponding hook point, which we will eventually use to access those activations). The red text below each box tells you the shape of the activation (ignoring the batch dimension). Each arrow corresponds to an operation on an activation; where there are **parameters** involved these are labelled on the arrows.

[Link to diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg)

The next link is to a diagram of a `TransformerBlock` with full features (including biases, layernorms, and MLPs). Don't worry if not all of this makes sense at first - we'll return to some of the details later. As we work with these transformers, we'll get more comfortable with their architecture.

[Link to diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/full-merm.svg)
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
A few shortctus to make your lives easier when using these models:

* You can index weights like `W_Q` directly from the model via e.g. `model.blocks[0].attn.W_Q` (which gives you the `[nheads, d_model, d_head]` query weights for all heads in layer 0).
    * But an easier way is just to index with `model.W_Q`, which gives you the `[nlayers, nheads, d_model, d_head]` tensor containing **every** query weight in the model.
* Similarly, there exist shortcuts `model.W_E`, `model.W_U` and `model.W_pos` for the embeddings, unembeddings and positional embeddings respectively.
* With models containing MLP layers, you also have `model.W_in` and `model.W_out` for the linear layers.
* The same is true for all biases (e.g. `model.b_Q` for all query biases).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Tokenization

The tokenizer is stored inside the model, and you can access it using `model.tokenizer`. There are also a few helper methods that call the tokenizer under the hood, for instance:

* `model.to_str_tokens(text)` converts a string into a list of tokens-as-strings (or a list of strings into a list of lists of tokens-as-strings).
* `model.to_tokens(text)` converts a string into a tensor of tokens.
* `model.to_string(tokens)` converts a tensor of tokens into a string.

Examples of use:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(gpt2_small.to_str_tokens("gpt2"))
print(gpt2_small.to_str_tokens(["gpt2", "gpt2"]))
print(gpt2_small.to_tokens("gpt2"))
print(gpt2_small.to_string([50256, 70, 457, 17]))

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">['<|endoftext|>', 'g', 'pt', '2']
[['<|endoftext|>', 'g', 'pt', '2'], ['<|endoftext|>', 'g', 'pt', '2']]
tensor([[50256,    70,   457,    17]], device='cuda:0')
<|endoftext|>gpt2</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - <code><|endoftext|></code></summary>

A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt. We encountered this in the previous set of exercises, and noted that this was the **Beginning of Sequence (BOS)** token (which for GPT-2 is also the same as the EOS and PAD tokens - index `50256`.

TransformerLens appends this token by default, and it can easily trip up new users. Notably, **this includes** `model.forward` (which is what's implicitly used when you do eg `model("Hello World")`). You can disable this behaviour by setting the flag `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors.

`prepend_bos` is a bit of a hack, and I've gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn't really matter in training (where all inputs are >1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being "off" they normally look at the first token. Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first "real" token.

Further, *some* models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - how many tokens does your model guess correctly?

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> ```

Consider the `model_description_text` you fed into your model above. How many tokens did your model guess correctly? Which tokens were correct?
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

logits: Tensor = gpt2_small(model_description_text, return_type="logits")
prediction = logits.argmax(dim=-1).squeeze()[:-1]

# EXERCISE
# # YOUR CODE HERE - get the model's prediction on the text
# END EXERCISE
# SOLUTION
true_tokens = gpt2_small.to_tokens(model_description_text).squeeze()[1:]
is_correct = prediction == true_tokens

print(f"Model accuracy: {is_correct.sum()}/{len(true_tokens)}")
print(f"Correct tokens: {gpt2_small.to_str_tokens(prediction[is_correct])}")
# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

Use `return_type="logits"` to get the model's predictions, then take argmax across the vocab dimension. Then, compare these predictions with the actual tokens, derived from the `model_description_text`.

Remember, you should be comparing the `[:-1]`th elements of this tensor of predictions with the `[1:]`th elements of the input tokens (because your model's output represents a probability distribution over the *next* token, not the current one).

Also, remember to handle the batch dimension (since `logits`, and the output of `to_tokens`, will both have batch dimensions by default).

</details>

<details>
<summary>Answer - what you should see</summary>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Model accuracy: 33/111
Correct tokens: ['\n', '\n', 'former', ' with', ' models', '.', ' can', ' of', 'ooked', 'Trans', 'former', '_', 'NAME', '`.', ' model', ' the', 'Trans', 'former', ' to', ' be', ' and', '-', '.', '\n', '\n', ' at', 'PT', '-', ',', ' model', ',', "'s", ' the']
</pre>

So the model got 33 out of 111 tokens correct. Not too bad!

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
**Induction heads** are a special kind of attention head which we'll examine a lot more in coming exercises. They allow a model to perform in-context learning of a specific form: generalising from one observation that token `B` follows token `A`, to predict that token `B` will follow `A` in future occurrences of `A`, even if these two tokens had never appeared together in the model's training data.

**Can you see evidence of any induction heads at work, on this text?**

<details>
<summary>Evidence of induction heads</summary>

The evidence for induction heads comes from the fact that the model successfully predicted `'ooked', 'Trans', 'former'` following the token `'H'`. This is because it's the second time that `HookedTransformer` had appeared in this text string, and the model predicted it the second time but not the first. (The model did predict `former` the first time, but we can reasonably assume that `Transformer` is a word this model had already been exposed to during training, so this prediction wouldn't require the induction capability, unlike `HookedTransformer`.)

```python
print(gpt2_small.to_str_tokens("HookedTransformer", prepend_bos=False))     # --> ['H', 'ooked', 'Trans', 'former']
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Caching all Activations

The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.

<details>
<summary>Aside - a note on <code>remove_batch_dim</code></summary>

Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it.

`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

gpt2_text = "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets."
gpt2_tokens = gpt2_small.to_tokens(gpt2_text)
gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)

print(type(gpt2_logits), type(gpt2_cache))

# ! CELL TYPE: markdown
# ! FILTERS: [st,soln]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">&lt;class 'torch.Tensor'> &lt;class 'transformer_lens.ActivationCache.ActivationCache'></pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
If you inspect the `gpt2_cache` object, you should see that it contains a very large number of keys, each one corresponding to a different activation in the model. You can access the keys by indexing the cache directly, or by a more convenient indexing shorthand. For instance, here are 2 ways to extract the attention patterns for layer 0:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

attn_patterns_from_shorthand = gpt2_cache["pattern", 0]
attn_patterns_from_full_name = gpt2_cache["blocks.0.attn.hook_pattern"]

t.testing.assert_close(attn_patterns_from_shorthand, attn_patterns_from_full_name)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside: <code>utils.get_act_name</code></summary>

The reason these are the same is that, under the hood, the first example actually indexes by `utils.get_act_name("pattern", 0)`, which evaluates to `"blocks.0.attn.hook_pattern"`.

In general, `utils.get_act_name` is a useful function for getting the full name of an activation, given its short name and layer number.

You can use the diagram from the **Transformer Architecture** section to help you find activation names.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - verify activations

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> 
> If you're already comfortable implementing things like attention calculations (e.g. having gone through Neel's transformer walkthrough) you can skip this exercise. However, it might serve as a useful refresher.
> ```

Verify that `hook_q`, `hook_k` and `hook_pattern` are related to each other in the way implied by the diagram. Do this by computing `layer0_pattern_from_cache` (the attention pattern taken directly from the cache, for layer 0) and `layer0_pattern_from_q_and_k` (the attention pattern calculated from `hook_q` and `hook_k`, for layer 0). Remember that attention pattern is the probabilities, so you'll need to scale and softmax appropriately.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

layer0_pattern_from_cache = gpt2_cache["pattern", 0]

# EXERCISE
# # YOUR CODE HERE - define `layer0_pattern_from_q_and_k` manually, by manually performing the
# # steps of the attention calculation (dot product, masking, scaling, softmax)
# END EXERCISE
# SOLUTION
q, k = gpt2_cache["q", 0], gpt2_cache["k", 0]
seq, nhead, headsize = q.shape
layer0_attn_scores = einops.einsum(q, k, "seqQ n h, seqK n h -> n seqQ seqK")
mask = t.triu(t.ones((seq, seq), dtype=t.bool), diagonal=1).to(device)
layer0_attn_scores.masked_fill_(mask, -1e9)
layer0_pattern_from_q_and_k = (layer0_attn_scores / headsize**0.5).softmax(-1)

# END SOLUTION
# HIDE
t.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)
print("Tests passed!")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

You'll need to use three different cache indexes in all:

* `gpt2_cache["pattern", 0]` to get the attention patterns, which have shape `[nhead, seqQ, seqK]`
* `gpt2_cache["q", 0]` to get the query vectors, which have shape `[seqQ, nhead, headsize]`
* `gpt2_cache["k", 0]` to get the key vectors, which have shape `[seqK, nhead, headsize]`

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Visualising Attention Heads

A key insight from the Mathematical Frameworks paper is that we should focus on interpreting the parts of the model that are intrinsically interpretable - the input tokens, the output logits and the attention patterns. Everything else (the residual stream, keys, queries, values, etc) are compressed intermediate states when calculating meaningful things. So a natural place to start is classifying heads by their attention patterns on various texts.

When doing interpretability, it's always good to begin by visualising your data, rather than taking summary statistics. Summary statistics can be super misleading! But now that we have visualised the attention patterns, we can create some basic summary statistics and use our visualisations to validate them! (Accordingly, being good at web dev/data visualisation is a surprisingly useful skillset! Neural networks are very high-dimensional object.)

Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on Anthropic's PySvelte library). If you did the previous set of exercises, you'll have seen this library before.

We will use the function `cv.attention.attention_patterns`, which takes two important arguments:

* `attention`: the attention head patterns, of shape `[n_heads, seq_len, seq_len]`. This consists of the stacked grid of attention probabilities for each head, i.e. `attention[head, d, s]` is the attention probability from destination position `d` to source position `s` in attention head `head`.
* `tokens`: List of tokens, which should have the same length as the `seq_len` dimension of `attention`. Make sure you're not accidentally passing in a list with a dummy dimension, or that differs from `seq_len` because of the BOS token!

This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.

> Note - you can also use the `cv.attention.attention_heads` function, which presents the data in a different way (the syntax is exactly the same as `attention_patterns`). Note, if you display this in VSCode then it may exhibit a bug where the main plot continually shrinks in size - if this happens, you should instead save the HTML (i.e. with `html = cv.attention.attention_heads(...); with open("attn_heads.html", "w") as f: f.write(str(html))`) and open the plot in your browser.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

print(type(gpt2_cache))
attention_pattern = gpt2_cache["pattern", 0]
print(attention_pattern.shape)
gpt2_str_tokens = gpt2_small.to_str_tokens(gpt2_text)

print("Layer 0 Head Attention Patterns:")
display(
    cv.attention.attention_patterns(
        tokens=gpt2_str_tokens,
        attention=attention_pattern,
        attention_head_names=[f"L0H{i}" for i in range(12)],
    )
)

# FILTERS: ~
# html = cv.attention.attention_patterns(
#     tokens=gpt2_str_tokens,
#     attention=attention_pattern,
#     attention_head_names=[f"L0H{i}" for i in range(12)],
# )
# with open(section_dir / "1201.html", "w") as f:
#     f.write(str(html))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1201.html" width="1020" height="420" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Hover over heads to see the attention patterns; click on a head to lock it. Hover over each token to see which other tokens it attends to (or which other tokens attend to it - you can see this by changing the dropdown to `Destination <- Source`).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Other circuitsvis functions - neuron activations</summary>

The `circuitsvis` library also has a number of cool visualisations for **neuron activations**. Here are some more of them (you don't have to understand them all now, but you can come back to them later).

The function below visualises neuron activations. The example shows just one sequence, but it can also show multiple sequences (if `tokens` is a list of lists of strings, and `activations` is a list of tensors).

```python
neuron_activations_for_all_layers = t.stack([
    gpt2_cache["post", layer] for layer in range(gpt2_small.cfg.n_layers)
], dim=1)
# shape = (seq_pos, layers, neurons)

cv.activations.text_neuron_activations(
    tokens=gpt2_str_tokens,
    activations=neuron_activations_for_all_layers
)
```

The next function shows which words each of the neurons activates most / least on (note that it requires some weird indexing to work correctly).

```python
neuron_activations_for_all_layers_rearranged = utils.to_numpy(einops.rearrange(neuron_activations_for_all_layers, "seq layers neurons -> 1 layers seq neurons"))

cv.topk_tokens.topk_tokens(
    # Some weird indexing required here ¯\_(ツ)_/¯
    tokens=[gpt2_str_tokens],
    activations=neuron_activations_for_all_layers_rearranged,
    max_k=7,
    first_dimension_name="Layer",
    third_dimension_name="Neuron",
    first_dimension_labels=list(range(12))
)
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 2️⃣ Finding induction heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Introducing Our Toy Attention-Only Model

Here we introduce a toy 2L attention-only transformer trained specifically for today. Some changes to make them easier to interpret:
- It has only attention blocks.
- The positional embeddings are only added to the residual stream before calculating each key and query vector in the attention layers as opposed to the token embeddings - i.e. we compute queries as `Q = (resid + pos_embed) @ W_Q + b_Q` and same for keys, but values as `V = resid @ W_V + b_V`. This means that **the residual stream can't directly encode positional information**.
    - This turns out to make it *way* easier for induction heads to form, it happens 2-3x times earlier - [see the comparison of two training runs](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-11-08-83---VmlldzoyNTI0MDMz?accessToken=8ap8ir6y072uqa4f9uinotdtrwmoa8d8k2je4ec0lyasf1jcm3mtdh37ouijgdbm) here. (The bump in each curve is the formation of induction heads.)
    - The argument that does this below is `positional_embedding_type="shortformer"`.
- It has no MLP layers, no LayerNorms, and no biases.
- There are separate embed and unembed matrices (i.e. the weights are not tied).

We now define our model with a `HookedTransformerConfig` object. This is similar to the `Config` object we used in the previous set of exercises, although it has a lot more features. You can look at the documentation page (Right-click, "Go to Definition" in VSCode) to seee what the different arguments do.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

cfg = HookedTransformerConfig(
    d_model=768,
    d_head=64,
    n_heads=12,
    n_layers=2,
    n_ctx=2048,
    d_vocab=50278,
    attention_dir="causal",
    attn_only=True,  # defaults to False
    tokenizer_name="EleutherAI/gpt-neox-20b",
    seed=398,
    use_attn_result=True,
    normalization_type=None,  # defaults to "LN", i.e. layernorm with weights & biases
    positional_embedding_type="shortformer",
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note that in the last section we had to define a tokenizer explicitly, and passed it into our model. But here, we just pass a tokenizer name, and the model will automatically create a tokenizer for us (under the hood, it calls `AutoTokenizer.from_pretrained(tokenizer_name)`).

Below, you'll load in your weights, with some boilerplate code to download your state dict from HuggingFace (you can do this for any model you've uploaded to HuggingFace yourself):
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

from huggingface_hub import hf_hub_download

REPO_ID = "callummcdougall/attn_only_2L_half"
FILENAME = "attn_only_2L_half.pth"

if MAIN:
    weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Finally, we'll create our model and load in the weights:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model = HookedTransformer(cfg)
pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)
model.load_state_dict(pretrained_weights)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Use the [diagram at this link](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg) to remind yourself of the relevant hook names.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - visualise & inspect attention patterns

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> 
> It's important to be comfortable using circuitsvis, and the cache object.
> ```

*This exercise should be very quick - you can reuse code from the previous section. You should look at the solution if you're still stuck after 5-10 minutes.*

Visualise the attention patterns for both layers of your model, on the following prompt:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

text = "We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this."

logits, cache = model.run_with_cache(text, remove_batch_dim=True)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
*(Note that we've run the model on the string `text`, rather than on tokens like we did previously when creating a cache - this is something that `HookedTransformer` allows.)*

Inspect the attention patterns. What do you notice about the attention heads?

You should spot three relatively distinctive basic patterns, which occur in multiple heads. What are these patterns, and can you guess why they might be present?
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# # YOUR CODE HERE - visualize attention
# END EXERCISE
# SOLUTION
str_tokens = model.to_str_tokens(text)
for layer in range(model.cfg.n_layers):
    attention_pattern = cache["pattern", layer]
    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))
# END SOLUTION

# FILTERS: ~
# for layer in range(model.cfg.n_layers):
#     attention_pattern = cache["pattern", layer]
#     html = cv.attention.attention_patterns(
#         tokens=gpt2_str_tokens,
#         attention=attention_pattern,
#         attention_head_names=[f"L0H{i}" for i in range(12)],
#     )
#     with open(section_dir / f"1202-{layer}.html", "w") as f:
#         f.write(str(html))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1202-0.html" width="1020" height="440" style="background-color: white;"></div>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1202-1.html" width="1020" height="440" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - what to do if your plots won't show up</summary>

A common mistake is to fail to pass the tokens in as arguments. If you do this, your attention patterns won't render.

If this isn't the problem, then it might be an issue with the Circuitsvis library.Rather than plotting inline, you can do the following, and then open in your browser from the left-hand file explorer menu of VSCode:
</details>

<details>
<summary>Discussion of results </summary>

We notice that there are three basic patterns which repeat quite frequently:

* `prev_token_heads`, which attend mainly to the previous token (e.g. head `0.7`)
* `current_token_heads`, which attend mainly to the current token (e.g. head `1.6`)
* `first_token_heads`, which attend mainly to the first token (e.g. heads `0.3` or `1.4`, although these are a bit less clear-cut than the other two)

The `prev_token_heads` and `current_token_heads` are perhaps unsurprising, because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction).

The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now that we've observed our three basic attention patterns, it's time to make detectors for those patterns!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - write your own detectors

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You shouldn't spend more than 10-25 minutes on these exercises.
> These exercises aren't meant to be too challenging, just to get you thinking about how to characterize head behaviour. 
> Use the hints if you're stuck.
> ```

You should fill in the functions below, which act as detectors for particular types of heads. Validate your detectors by comparing these results to the visual attention patterns above - summary statistics on their own can be dodgy, but are much more reliable if you can validate it by directly playing with the data.

Tasks like this are useful, because we need to be able to take our observations / intuitions about what a model is doing, and translate these into quantitative measures. As the exercises proceed, we'll be creating some much more interesting tools and detectors!

Note - there's no objectively correct answer for which heads are doing which tasks, and which detectors can spot them. You should just try and come up with something plausible-seeming, which identifies the kind of behaviour you're looking for. **Don't spend too much time here looking for a perfect solution, just one that seems to roughly match up with your visual inspection of the attention patterns.**
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def current_attn_detector(cache: ActivationCache) -> list[str]:
    """
    Returns a list e.g. ["0.2", "1.4", "1.9"] of "layer.head" which you judge to be current-token heads
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    attn_heads = []
    for layer in range(model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            attention_pattern = cache["pattern", layer][head]
            # take avg of diagonal elements
            score = attention_pattern.diagonal().mean()
            if score > 0.4:
                attn_heads.append(f"{layer}.{head}")
    return attn_heads
    # END SOLUTION


def prev_attn_detector(cache: ActivationCache) -> list[str]:
    """
    Returns a list e.g. ["0.2", "1.4", "1.9"] of "layer.head" which you judge to be prev-token heads
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    attn_heads = []
    for layer in range(model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            attention_pattern = cache["pattern", layer][head]
            # take avg of sub-diagonal elements
            score = attention_pattern.diagonal(-1).mean()
            if score > 0.4:
                attn_heads.append(f"{layer}.{head}")
    return attn_heads
    # END SOLUTION


def first_attn_detector(cache: ActivationCache) -> list[str]:
    """
    Returns a list e.g. ["0.2", "1.4", "1.9"] of "layer.head" which you judge to be first-token heads
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    attn_heads = []
    for layer in range(model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            attention_pattern = cache["pattern", layer][head]
            # take avg of 0th elements
            score = attention_pattern[:, 0].mean()
            if score > 0.4:
                attn_heads.append(f"{layer}.{head}")
    return attn_heads
    # END SOLUTION


# HIDE
if MAIN:
    print("Heads attending to current token  = ", ", ".join(current_attn_detector(cache)))
    print("Heads attending to previous token = ", ", ".join(prev_attn_detector(cache)))
    print("Heads attending to first token    = ", ", ".join(first_attn_detector(cache)))
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

Try and compute the average attention probability along the relevant tokens. For instance, you can get the tokens just below the diagonal by using `t.diagonal` with appropriate `offset` parameter:

```python
>>> arr = t.arange(9).reshape(3, 3)
>>> arr
tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])

>>> arr.diagonal()
tensor([0, 4, 8])

>>> arr.diagonal(-1)
tensor([3, 7])
```

Remember that you should be using `cache["pattern", layer]` to get all the attention probabilities for a given layer, and then indexing on the 0th dimension to get the correct head.
</details>

<details>
<summary>Expected output (yours might vary slightly depending on method)</summary>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Heads attending to current token  =  0.9
Heads attending to previous token =  0.7
Heads attending to first token    =  0.3, 1.4, 1.10
</pre>

</details>

<details>
<summary>Solution (one possible method)</summary>

Note - choosing `score=0.4` as a threshold in the code below is a bit arbitrary, but it seems to work well enough. In this particular case, a threshold of `0.5` results in no head being classified as a current-token head.

```python
SOLUTION
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Compare the printouts to your attention visualisations above. Do they seem to make sense? As a bonus exercise, try inputting different text, and see how stable your results are. Do certain heads always get classified the same way?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, it's time to turn our attention to induction heads.

## What are induction heads?

(Note: I use induction **head** to refer to the head in the second layer which attends to the 'token immediately after the copy of the current token', and induction **circuit** to refer to the circuit consisting of the composition of a **previous token head** in layer 0 and an **induction head** in layer 1)

[Induction heads](https://transformer-circuits.pub/2021/framework/index.html#induction-heads) are the first sophisticated circuit we see in transformers! And are sufficiently interesting that we wrote [another paper just about them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).

<details>
<summary>An aside on why induction heads are a big deal</summary>

There's a few particularly striking things about induction heads:

* They develop fairly suddenly in a phase change - from about 2B to 4B tokens we go from no induction heads to pretty well developed ones. This is a striking divergence from a 1L model [see the comparison of in context learning performance curves curves for models with different layers](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#:~:text=Our%20first%20observation) and can be observed in much larger models (eg a 13B one)
    * Phase changes are particularly interesting (and depressing) from an alignment perspective, because the prospect of a sharp left turn, or emergent capabilities like deception or situational awareness seems like worlds where alignment may be harder, and we get caught by surprise without warning shots or simpler but analogous models to test our techniques on.
* They are responsible for a significant loss decrease - so much so that there's a visible bump in the loss curve when they develop (this change in loss can be pretty comparable to the increase in loss from major increases in model size, though this is hard to make an apples-to-apples comparison)
* They seem to be responsible for the vast majority of in-context learning - the ability to use far back tokens in the context to predict the next token. This is a significant way in which transformers outperform older architectures like RNNs or LSTMs, and induction heads seem to be a big part of this.
* The same core circuit seems to be used in a bunch of more sophisticated settings, such as translation or few-shot learning - there are heads that seem clearly responsible for those *and* which double as induction heads.

</details>

Again, you are strongly recommended to read the [corresponding section of the glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ), before continuing (or [this LessWrong post](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)). In brief, however, the induction circuit consists of a previous token head in layer 0 and an induction head in layer 1, where the induction head learns to attend to the token immediately *after* copies of the current token via K-Composition with the previous token head.

##### Question - why couldn't an induction head form in a 1L model?

<details>
<summary>Answer</summary>

Because this would require a head which attends a key position based on the *value of the token before it*. Attention scores are just a function of the key token and the query token, and are not a function of other tokens.

(The attention pattern *does* in fact include effects from other tokens because of softmax - if another key token has a high attention score, softmax inhibits this pair. But this inhibition is symmetric across positions, so can't systematically favour the token *next* to the relevant one.)

Note that a key detail is that the value of adjacent tokens are (approximately) unrelated - if the model wanted to attend based on relative *position* this is easy.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Checking for the induction capability

A striking thing about models with induction heads is that, given a repeated sequence of random tokens, they can predict the repeated half of the sequence. This is nothing like it's training data, so this is kind of wild! The ability to predict this kind of out of distribution generalisation is a strong point of evidence that you've really understood a circuit.

To check that this model has induction heads, we're going to run it on exactly that, and compare performance on the two halves - you should see a striking difference in the per token losses.

Note - we're using small sequences (and just one sequence), since the results are very obvious and this makes it easier to visualise. In practice we'd obviously use larger ones on more subtle tasks. But it's often easiest to iterate and debug on small tasks.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - plot per-token loss on repeated sequence

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You shouldn't spend more than 10-15 minutes on these exercises.
> ```

You should fill in the functions below. We've given you the first line of the first function, which defines a prefix (remember we need the BOS token for GPT-2, since it was trained to have one). We've also given you the `get_log_probs` function from the previous set of exercises.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def generate_repeated_tokens(
    model: HookedTransformer, seq_len: int, batch_size: int = 1
) -> Int[Tensor, "batch_size full_seq_len"]:
    """
    Generates a sequence of repeated random tokens

    Outputs are:
        rep_tokens: [batch_size, 1+2*seq_len]
    """
    t.manual_seed(0)  # for reproducibility
    prefix = (t.ones(batch_size, 1) * model.tokenizer.bos_token_id).long()
    # SOLUTION
    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch_size, seq_len), dtype=t.int64)
    rep_tokens = t.cat([prefix, rep_tokens_half, rep_tokens_half], dim=-1).to(device)
    return rep_tokens
    # END SOLUTION


def run_and_cache_model_repeated_tokens(
    model: HookedTransformer, seq_len: int, batch_size: int = 1
) -> tuple[Tensor, Tensor, ActivationCache]:
    """
    Generates a sequence of repeated random tokens, and runs the model on it, returning (tokens,
    logits, cache). This function should use the `generate_repeated_tokens` function above.

    Outputs are:
        rep_tokens: [batch_size, 1+2*seq_len]
        rep_logits: [batch_size, 1+2*seq_len, d_vocab]
        rep_cache: The cache of the model run on rep_tokens
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    rep_tokens = generate_repeated_tokens(model, seq_len, batch_size)
    rep_logits, rep_cache = model.run_with_cache(rep_tokens)
    return rep_tokens, rep_logits, rep_cache
    # END SOLUTION


# HIDE
def get_log_probs(
    logits: Float[Tensor, "batch posn d_vocab"], tokens: Int[Tensor, "batch posn"]
) -> Float[Tensor, "batch posn-1"]:
    logprobs = logits.log_softmax(dim=-1)
    # We want to get logprobs[b, s, tokens[b, s+1]], in eindex syntax this looks like:
    correct_logprobs = eindex(logprobs, tokens, "b s [b s+1]")
    return correct_logprobs


if MAIN:
    seq_len = 50
    batch_size = 1
    (rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(
        model, seq_len, batch_size
    )
    rep_cache.remove_batch_dim()
    rep_str = model.to_str_tokens(rep_tokens)
    model.reset_hooks()
    log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()

    print(f"Performance on the first half: {log_probs[:seq_len].mean():.3f}")
    print(f"Performance on the second half: {log_probs[seq_len:].mean():.3f}")

    plot_loss_difference(log_probs, rep_str, seq_len)
# END HIDE

# FILTERS: ~
# plot_loss_difference(log_probs, rep_str, seq_len, filename=str(section_dir / "1203.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1203.html" width="800" height="480" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

You can define the first half of the repeated tokens using `t.randint(low, high, shape)`. Also remember to specify `dtype=t.long`.

Then you can concatenate together your prefix and two copies of the repeated tokens, using `t.concat`.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Looking for Induction Attention Patterns

The next natural thing to check for is the induction attention pattern.

First, go back to the attention patterns visualisation code from earlier (i.e. `cv.attention.attention_heads` or `attention_patterns`) and manually check for likely heads in the second layer. Which ones do you think might be serving as induction heads?

Note - above, we defined the `rep_str` object for you, so you can use it in your `circuitsvis` functions.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# YOUR CODE HERE - display the attention patterns stored in `rep_cache`, for each layer
# END EXERCISE
# SOLUTION
for layer in range(model.cfg.n_layers):
    attention_pattern = rep_cache["pattern", layer]
    display(cv.attention.attention_patterns(tokens=rep_str, attention=attention_pattern))
# END SOLUTION
# FILTERS: ~
# for layer in range(model.cfg.n_layers):
#     attention_pattern = rep_cache["pattern", layer]
#     html = cv.attention.attention_patterns(tokens=rep_str, attention=attention_pattern)
#     with open(section_dir / f"1204-L{layer}.html", "w") as f:
#         f.write(str(html))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1204-L0.html" width="1020" height="470" style="background-color: white;"></div>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1204-L1.html" width="1020" height="470" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Some observations</summary>

The characteristic pattern of induction heads is a diagonal stripe, with the diagonal offset as `seq_len-1` (because the destination token attends to the token *after* the destination token's previous occurrence).

You should see that heads 4 and 10 are strongly induction-y, head 6 is very weakly induction-y, and the rest aren't.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - make an induction-head detector

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You shouldn't spend more than 5-15 minutes on this exercise.
> This exercise should be very similar to the earlier detector exercises (the only difference being how you index attention).
> ```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, you should make an induction pattern score function, which looks for the average attention paid to the offset diagonal. Do this in the same style as our earlier head scorers, just with a different kind of indexing that is appropriate for detecting the characteristic attention head pattern.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def induction_attn_detector(cache: ActivationCache) -> list[str]:
    """
    Returns a list e.g. ["0.2", "1.4", "1.9"] of "layer.head" which you judge to be induction heads

    Remember - the tokens used to generate rep_cache are (bos_token, *rand_tokens, *rand_tokens)
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    attn_heads = []
    for layer in range(model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            attention_pattern = cache["pattern", layer][head]
            # take avg of (-seq_len+1)-offset elements
            seq_len = (attention_pattern.shape[-1] - 1) // 2
            score = attention_pattern.diagonal(-seq_len + 1).mean()
            if score > 0.4:
                attn_heads.append(f"{layer}.{head}")
    return attn_heads
    # END SOLUTION


# HIDE
if MAIN:
    print("Induction heads = ", ", ".join(induction_attn_detector(rep_cache)))
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm not sure what offset to use.</summary>

The offset in your diagonal should be `-(seq_len-1)` (where `seq_len` is the length of the random tokens which you repeat twice), because the second instance of random token `T` will attend to the token **after** the first instance of `T`.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
If this function works as expected, then you should see output that matches your observations from `circuitsvis` (i.e. the heads which you observed to be induction heads are being classified as induction heads by your function here).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 3️⃣ TransformerLens: Hooks
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## What are hooks?

One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us e.g. set up careful counterfactuals and causal intervention to easily understand model behaviour.

Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it.

We do this by adding a **hook function** to that activation, and then calling `model.run_with_hooks`.

*(Terminology note - because basically all the activations in our model have an associated hook point, we'll sometimes use the terms "hook" and "activation" interchangeably.)*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Hook functions

Hook functions take two arguments: `activation_value` and `hook_point`. The `activation_value` is a tensor representing some activation in the model, just like the values in our `ActivationCache`. The `hook_point` is an object which gives us methods like `hook.layer()` or attributes like `hook.name` that are sometimes useful to call within the function.

If we're using hooks to edit activations, then the hook function should return a tensor of the same shape as the activation value. But we can also just have our hook function access the activation, do some processing, and write the results to some external variable (in which case our hook function should just not return anything).

An example hook function for changing the attention patterns at a particular layer might look like:
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
def hook_function(
    attn_pattern: Float[Tensor, "batch heads seq_len seq_len"],
    hook: HookPoint
) -> Float[Tensor, "batch heads seq_len seq_len"]:

    # modify attn_pattern (can be inplace)
    return attn_pattern
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Running with hooks

Once you've defined a hook function (or functions), you should call `model.run_with_hooks`. A typical call to this function might look like:
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
loss = model.run_with_hooks(
    tokens,
    return_type="loss",
    fwd_hooks=[
        ('blocks.1.attn.hook_pattern', hook_function)
    ]
)
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's break this code down.

* `tokens` represents our model's input.
* `return_type="loss"` is used here because we're modifying our activations and seeing how this affects the loss.
    * We could also return the logits, or just use `return_type=None` if we only want to access the intermediate activations and we don't care about the output.
* `fwd_hooks` is a list of 2-tuples of (hook name, hook function).
    * The hook name is a string that specifies which activation we want to hook.
    * The hook function gets run with the corresponding activation as its first argument.

### A bit more about hooks

Here are a few extra notes for how to squeeze even more functionality out of hooks. If you'd prefer, you can [jump ahead](#hooks-accessing-activations) to see an actual example of hooks being used, and come back to this section later.

<details>
<summary>Resetting hooks</summary>

`model.run_with_hooks` has the default parameter `reset_hooks_end=True` which resets all hooks at the end of the run (including both those that were added before and during the run). Despite this, it's possible to shoot yourself in the foot with hooks, e.g. if there's an error in one of your hooks so the function never finishes. In this case, you can use `model.reset_hooks()` to reset all hooks.

If you don't want to reset hooks (i.e. you want to keep them between forward passes), you can either set `reset_hooks_end=False` in the `run_with_hooks` function, or just add the hooks directly using the `add_hook` method before your forward passes (this way they won't reset automatically).

</details>
<details>
<summary>Adding multiple hooks at once</summary>

Including more than one tuple in the `fwd_hooks` list is one way to add multiple hooks:

```python
loss = model.run_with_hooks(
    tokens,
    return_type="loss",
    fwd_hooks=[
        ('blocks.0.attn.hook_pattern', hook_function),
        ('blocks.1.attn.hook_pattern', hook_function)
    ]
)
```

Another way is to use a **name filter** rather than a single name:

```python
loss = model.run_with_hooks(
    tokens,
    return_type="loss",
    fwd_hooks=[
        (lambda name: name.endswith("pattern"), hook_function)
    ]
)
```
</details>
<details>
<summary><code>utils.get_act_name</code></summary>

When we were indexing the cache in the previous section, we found we could use strings like `cache['blocks.0.attn.hook_pattern']`, or use the shorthand of `cache['pattern', 0]`. The reason the second one works is that it calls the function `utils.get_act_name` under the hood, i.e. we have:

```python
utils.get_act_name('pattern', 0) == 'blocks.0.attn.hook_pattern'
```

Using `utils.get_act_name` in your forward hooks is often easier than using the full string, since the only thing you need to remember is the activation name (you can refer back to the diagram in the previous section for this).
</details>
<details>
<summary>Using <code>functools.partial</code> to create variations on hooks</summary>

A useful trick is to define a hook function with more arguments than it needs, and then use `functools.partial` to fill in the extra arguments. For instance, if you want a hook function which only modifies a particular head, but you want to run it on all heads separately (rather than just adding all the hooks and having them all run on the next forward pass), then you can do something like:

```python
def hook_all_attention_patterns(
    attn_pattern: Float[Tensor, "batch heads seq_len seq_len"],
    hook: HookPoint,
    head_idx: int
) -> Float[Tensor, "batch heads seq_len seq_len"]:
    # modify attn_pattern inplace, at head_idx
    return attn_pattern

for head_idx in range(12):
    temp_hook_fn = functools.partial(hook_all_attention_patterns, head_idx=head_idx)
    model.run_with_hooks(tokens, fwd_hooks=[('blocks.1.attn.hook_pattern', temp_hook_fn)])
```
</details>

And here are some points of interest, which aren't vital to understand:

<details>
<summary>Relationship to PyTorch hooks</summary>

[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).

They also come with a range of other quality of life improvements. PyTorch's hooks are global state, which can be a massive pain if you accidentally leave a hook on a model. TransformerLens hooks are also global state, but `run_with_hooks` tries to create an abstraction where these are local state by removing all hooks at the end of the function (and they come with a helpful `model.reset_hooks()` method to remove all hooks).
</details>

<details>
<summary>How are TransformerLens hooks actually implemented?</summary>

They are implemented as modules with the identity function as their forward method:

```python
class HookPoint(nn.Module):
    ...
    def forward(self, x):
        return x
```

but also with special features for adding and removing hook functions. This is why you see hooks when you print a HookedTransformer model, because all its modules are recursively printed.

When you run the model normally, hook modules won't change the model's behaviour (since applying the identity function does nothing). It's only once you add functions to the hook modules (e.g. a function which ablates any inputs into the hook module) that the model's behaviour changes.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Hooks: Accessing Activations

In later sections, we'll write some code to intervene on hooks, which is really the core feature that makes them so useful for interpretability. But for now, let's just look at how to access them without changing their value. This can be achieved by having the hook function write to a global variable, and return nothing (rather than modifying the activation in place).

Why might we want to do this? It turns out to be useful for things like:

* Extracting activations for a specific task
* Doing some long-running calculation across many inputs, e.g. finding the text that most activates a specific neuron

Note that, in theory, this could all be done using the `run_with_cache` function we used in the previous section, combined with post-processing of the cache result. But using hooks can be more intuitive and memory efficient.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - calculate induction scores with hooks

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 15-20 minutes on this exercise.
> This is our first exercise with hooks, which are an absolutely vital TransformerLens tool. Use the hints if you're stuck.
> ```

To start with, we'll look at how hooks can be used to get the same results as from the previous section (where we ran our induction head detector functions on the values in the cache).

Most of the code has already been provided for you below; the only thing you need to do is **implement the `induction_score_hook` function**. As mentioned, this function takes two arguments: the activation value (which in this case will be our attention pattern) and the hook object (which gives us some useful methods and attributes that we can access in the function, e.g. `hook.layer()` to return the layer, or `hook.name` to return the name, which is the same as the name in the cache).

Your function should do the following:

* Calculate the induction score for the attention pattern `pattern`, using the same methodology as you used in the previous section when you wrote your induction head detectors.
    * Note that this time, the batch dimension is greater than 1, so you should compute the average attention score over the batch dimension.
    * Also note that you are computing the induction score for all heads at once, rather than one at a time. You might find the arguments `dim1` and `dim2` of the `torch.diagonal` function useful.
* Write this score to the tensor `induction_score_store`, which is a global variable that we've provided for you. The `[i, j]`th element of this tensor should be the induction score for the `j`th head in the `i`th layer.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

if MAIN:
    seq_len = 50
    batch_size = 10
    rep_tokens_10 = generate_repeated_tokens(model, seq_len, batch_size)

    # We make a tensor to store the induction score for each head.
    # We put it on the model's device to avoid needing to move things between the GPU and CPU,
    # which can be slow.
    induction_score_store = t.zeros(
        (model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device
    )


def induction_score_hook(
    pattern: Float[Tensor, "batch head_index dest_pos source_pos"], hook: HookPoint
):
    """
    Calculates the induction score, and stores it in the [layer, head] position of the
    `induction_score_store` tensor.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    # Take the diagonal of attn paid from each dest posn to src posns (seq_len-1) tokens back
    # (This only has entries for tokens with index>=seq_len)
    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1 - seq_len)
    # Get an average score per head
    induction_score = einops.reduce(
        induction_stripe, "batch head_index position -> head_index", "mean"
    )
    # Store the result.
    induction_score_store[hook.layer(), :] = induction_score
    # END SOLUTION


if MAIN:
    # We make a boolean filter on activation names, that's true only on attention pattern names
    pattern_hook_names_filter = lambda name: name.endswith("pattern")

    # Run with hooks (this is where we write to the `induction_score_store` tensor`)
    model.run_with_hooks(
        rep_tokens_10,
        return_type=None,  # For efficiency, we don't need to calculate the logits
        fwd_hooks=[(pattern_hook_names_filter, induction_score_hook)],
    )

    # Plot the induction scores for each head in each layer
    imshow(
        induction_score_store,
        labels={"x": "Head", "y": "Layer"},
        title="Induction Score by Head",
        text_auto=".2f",
        width=900,
        height=350,
    )

    # FILTERS: ~
    # fig = imshow(
    #     induction_score_store,
    #     labels={"x": "Head", "y": "Layer"},
    #     title="Induction Score by Head",
    #     text_auto=".2f",
    #     width=900,
    #     height=350,
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / "1205.html")
    # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1205.html" width="920" height="370"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm not sure how to implement this function.</summary>

To get the induction stripe, you can use:

```python
torch.diagonal(pattern, dim1=-2, dim2=-1, offset=1-seq_len)
```

since this returns the diagonal of each attention scores matrix, for every element in the batch and every attention head.

Once you have this, you can then take the mean over the batch and diagonal dimensions, giving you a tensor of length `n_heads`. You can then write this to the global `induction_score_store` tensor, using the `hook.layer()` method to get the correct row number.
</details>

<details>
<summary>Solution</summary>

```python
def induction_score_hook(pattern: Float[Tensor, "batch head_index dest_pos source_pos"], hook: HookPoint):
    """
    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.
    """
    # Take the diagonal of attn paid from each dest posn to src posns (seq_len-1) tokens back
    # (This only has entries for tokens with index>=seq_len)
    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1 - seq_len)
    # Get an average score per head
    induction_score = einops.reduce(induction_stripe, "batch head_index position -> head_index", "mean")
    # Store the result.
    induction_score_store[hook.layer(), :] = induction_score
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
If this function has been implemented correctly, you should see a result matching your observations from the previous section: a high induction score (>0.6) for all the heads which you identified as induction heads, and a low score (close to 0) for all others.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - find induction heads in GPT2-small

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 10-20 minutes on this exercise.
> Here, you mostly just need to use previously defined functions and interpret the results, rather than writing new code.
> ```

*This is your first opportunity to investigate a larger and more extensively trained model, rather than the simple 2-layer model we've been using so far. None of the code required is new (you can copy most of it from previous sections), so these exercises shouldn't take very long.*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Perform the same analysis on your `gpt2_small`. You should observe that some heads, particularly in a couple of the middle layers, have high induction scores. Use CircuitsVis to plot the attention patterns for these heads when run on the repeated token sequences, and verify that they look like induction heads.

Note - you can make CircuitsVis plots (and other visualisations) using hooks rather than plotting directly from the cache. For example, we've given you a hook function which will display the attention patterns at a given hook when you include it in a call to `model.run_with_hooks`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# HIDE
def visualize_pattern_hook(
    pattern: Float[Tensor, "batch head_index dest_pos source_pos"],
    hook: HookPoint,
):
    print("Layer: ", hook.layer())
    display(
        cv.attention.attention_patterns(
            tokens=gpt2_small.to_str_tokens(rep_tokens[0]), attention=pattern.mean(0)
        )
    )


# END HIDE

# EXERCISE
# # YOUR CODE HERE - find induction heads in gpt2_small
# END EXERCISE
# SOLUTION
if MAIN:
    seq_len = 50
    batch_size = 10
    rep_tokens_batch = generate_repeated_tokens(gpt2_small, seq_len, batch_size)

    induction_score_store = t.zeros(
        (gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device
    )

    gpt2_small.run_with_hooks(
        rep_tokens_batch,
        return_type=None,  # For efficiency, we don't need to calculate the logits
        fwd_hooks=[(pattern_hook_names_filter, induction_score_hook)],
    )

    imshow(
        induction_score_store,
        labels={"x": "Head", "y": "Layer"},
        title="Induction Score by Head",
        text_auto=".1f",
        width=700,
        height=500,
    )

    # Observation: heads 5.1, 5.5, 6.9, 7.2, 7.10 are all strongly induction-y.
    # Confirm observation by visualizing attn patterns for layers 5 through 7:

    induction_head_layers = [5, 6, 7]
    fwd_hooks = [
        (utils.get_act_name("pattern", induction_head_layer), visualize_pattern_hook)
        for induction_head_layer in induction_head_layers
    ]
    gpt2_small.run_with_hooks(
        rep_tokens,
        return_type=None,
        fwd_hooks=fwd_hooks,
    )

# END SOLUTION

# FILTERS: ~
# fig = imshow(
#     induction_score_store,
#     labels={"x": "Head", "y": "Layer"},
#     title="Induction Score by Head",
#     text_auto=".1f",
#     width=700,
#     height=500,
#     return_fig=True,
# )
# fig.write_html(section_dir / "1206-A.html")

# def save_pattern_hook(
#     pattern: Float[Tensor, "batch head_index dest_pos source_pos"],
#     hook: HookPoint,
# ):
#     html = cv.attention.attention_patterns(tokens=gpt2_small.to_str_tokens(rep_tokens[0]), attention=pattern.mean(0))
#     with open(section_dir / f"1206-B{hook.layer()}.html", "w") as f:
#         f.write(str(html))

# fwd_hooks = [
#     (utils.get_act_name("pattern", induction_head_layer), save_pattern_hook)
#     for induction_head_layer in induction_head_layers
# ]
# gpt2_small.run_with_hooks(
#     rep_tokens,
#     return_type=None,
#     fwd_hooks=fwd_hooks,
# )
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1206-A.html" width="720" height="520"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1206-B5.html" width="1020" height="500" style="background-color: white;"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1206-B6.html" width="1020" height="500" style="background-color: white;"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1206-B7.html" width="1020" height="500" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Building interpretability tools

In order to develop a mechanistic understanding for how transformers perform certain tasks, we need to be able to answer questions like:

> *How much of the model's performance on some particular task is attributable to each component of the model?*

where "component" here might mean, for example, a specific head in a layer.

There are many ways to approach a question like this. For example, we might look at how a head interacts with other heads in different layers, or we might perform a causal intervention by seeing how well the model performs if we remove the effect of this head. However, we'll keep things simple for now, and ask the question: **what are the direct contributions of this head to the output logits?**

### Direct Logit attribution

A consequence of the residual stream is that the output logits are the sum of the contributions of each layer, and thus the sum of the results of each head. This means we can decompose the output logits into a term coming from each head and directly do attribution like this!

<details>
<summary>A concrete example</summary>

Let's say that our model knows that the token Harry is followed by the token Potter, and we want to figure out how it does this. The logits on Harry are `residual @ W_U`. But this is a linear map, and the residual stream is the sum of all previous layers `residual = embed + attn_out_0 + attn_out_1`. So `logits = (embed @ W_U) + (attn_out @ W_U) + (attn_out_1 @ W_U)`

We can be even more specific, and *just* look at the logit of the Potter token - this corresponds to a column of `W_U`, and so a direction in the residual stream - our logit is now a single number that is the sum of `(embed @ potter_U) + (attn_out_0 @ potter_U) + (attn_out_1 @ potter_U)`. Even better, we can decompose each attention layer output into the sum of the result of each head, and use this to get many terms.
</details>

Your mission here is to write a function to look at how much each component contributes to the correct logit. Your components are:

* The direct path (i.e. the residual connections from the embedding to unembedding),
* Each layer 0 head (via the residual connection and skipping layer 1)
* Each layer 1 head

To emphasise, these are not paths from the start to the end of the model, these are paths from the output of some component directly to the logits - we make no assumptions about how each path was calculated!

A few important notes for this exercise:

* Here we are just looking at the DIRECT effect on the logits, i.e. the thing that this component writes / embeds into the residual stream - if heads compose with other heads and affect logits like that, or inhibit logits for other tokens to boost the correct one we will not pick up on this!
* By looking at just the logits corresponding to the correct token, our data is much lower dimensional because we can ignore all other tokens other than the correct next one (Dealing with a 50K vocab size is a pain!). But this comes at the cost of missing out on more subtle effects, like a head suppressing other plausible logits, to increase the log prob of the correct one.
    * There are other situations where our job might be easier. For instance, in the IOI task (which we'll discuss shortly) we're just comparing the logits of the indirect object to the logits of the direct object, meaning we can use the **difference between these logits**, and ignore all the other logits.
* When calculating correct output logits, we will get tensors with a dimension `(position - 1,)`, not `(position,)` - we remove the final element of the output (logits), and the first element of labels (tokens). This is because we're predicting the *next* token, and we don't know the token after the final token, so we ignore it.

<details>
<summary>Aside - centering <code>W_U</code></summary>

While we won't worry about this for this exercise, logit attribution is often more meaningful if we first center `W_U` - i.e. ensure the mean of each row writing to the output logits is zero. Log softmax is invariant when we add a constant to all the logits, so we want to control for a head that just increases all logits by the same amount. We won't do this here for ease of testing.
</details>

<details>
<summary>Question - why don't we do this to the log probs instead?</summary>

Because log probs aren't linear, they go through `log_softmax`, a non-linear function.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - build logit attribution tool

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 10-15 minutes on this exercise.
> This exercise is important, but has quite a few messy einsums, so you might get more value from reading the solution than doing the exercises.
> ```

You should implement the `logit_attribution` function below. This should return the contribution of each component in the "correct direction". We've already given you the unembedding vectors for the correct direction, `W_U_correct_tokens` (note that we take the `[1:]` slice of tokens, for reasons discussed above).

The code below this function will check your logit attribution function is working correctly, by taking the sum of logit attributions and comparing it to the actual values in the residual stream at the end of your model.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def logit_attribution(
    embed: Float[Tensor, "seq d_model"],
    l1_results: Float[Tensor, "seq nheads d_model"],
    l2_results: Float[Tensor, "seq nheads d_model"],
    W_U: Float[Tensor, "d_model d_vocab"],
    tokens: Int[Tensor, "seq"],
) -> Float[Tensor, "seq-1 n_components"]:
    """
    Inputs:
        embed: the embeddings of the tokens (i.e. token + position embeddings)
        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dims)
        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dims)
        W_U: the unembedding matrix
        tokens: the token ids of the sequence

    Returns:
        Tensor of shape (seq_len-1, n_components)
        represents the concatenation (along dim=-1) of logit attributions from:
            the direct path (seq-1,1)
            layer 0 logits (seq-1, n_heads)
            layer 1 logits (seq-1, n_heads)
        so n_components = 1 + 2*n_heads
    """
    W_U_correct_tokens = W_U[:, tokens[1:]]

    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    direct_attributions = einops.einsum(W_U_correct_tokens, embed[:-1], "emb seq, seq emb -> seq")
    l1_attributions = einops.einsum(
        W_U_correct_tokens, l1_results[:-1], "emb seq, seq nhead emb -> seq nhead"
    )
    l2_attributions = einops.einsum(
        W_U_correct_tokens, l2_results[:-1], "emb seq, seq nhead emb -> seq nhead"
    )
    return t.concat([direct_attributions.unsqueeze(-1), l1_attributions, l2_attributions], dim=-1)
    # END SOLUTION


# HIDE
if MAIN:
    text = "We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this."
    logits, cache = model.run_with_cache(text, remove_batch_dim=True)
    str_tokens = model.to_str_tokens(text)
    tokens = model.to_tokens(text)

    with t.inference_mode():
        embed = cache["embed"]
        l1_results = cache["result", 0]
        l2_results = cache["result", 1]
        logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])
        # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token
        correct_token_logits = logits[0, t.arange(len(tokens[0]) - 1), tokens[0, 1:]]
        t.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)
        print("Tests passed!")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've got the tests working, you can visualise the logit attributions for each path through the model. We've provided you with the helper function `plot_logit_attribution`, which presents the results in a nice way.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

embed = cache["embed"]
l1_results = cache["result", 0]
l2_results = cache["result", 1]
logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens.squeeze())

plot_logit_attribution(model, logit_attr, tokens, title="Logit attribution (demo prompt)")

# FILTERS: ~
# plot_logit_attribution(
#     model, logit_attr, tokens, title="Logit attribution (demo prompt)", filename=str(section_dir / "1207.html")
# )
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1207.html" width="620" height="1120" style="background-color: white;"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Question - what is the interpretation of this plot?

You should find that the most variation in the logit attribution comes from the direct path. In particular, some of the tokens in the direct path have a very high logit attribution (e.g. tokens 7, 12, 24, 38, 46, 58). Can you guess what gives them in particular such a high logit attribution?

<details>
<summary>Answer - what is special about these tokens?</summary>

The tokens with very high logit attribution are the ones which are the first token in common bigrams. For instance, the highest contribution on the direct path comes from `| manip|`, because this is very likely to be followed by `|ulative|` (or presumably a different stem like `| ulation|`). `| super| -> |human|` is another example of a bigram formed when the tokenizer splits one word into multiple tokens.

There are also examples that come from two different words, rather than a single word split by the tokenizer. These include:

* `| more| -> | likely|` (12)
* `| machine| -> | learning|` (24)
* `| by| -> | default|` (38)
* `| how| -> | to|` (58)

See later for a discussion of all the ~infuriating~ fun quirks of tokenization!
</details>

Another feature of the plot - the heads in layer 1 seem to have much higher contributions than the heads in layer 0. Why do you think this might be?

<details>
<summary>Hint</summary>

Think about what this graph actually represents, in terms of paths through the transformer.
</details>

<details>
<summary>Answer - why might layer-1 heads have higher contributions?</summary>

This is because of a point we discussed earlier - this plot doesn't pick up on things like a head's effect in composition with another head. So the attribution for layer-0 heads won't involve any composition, whereas the attributions for layer-1 heads will involve not only the single-head paths through those attention heads, but also the 2-layer compositional paths through heads in layer 0 and layer 1.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - interpret logit attribution for the induction heads

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 10-15 minutes on this exercise.
> ```

*This exercise just involves calling `logit_attribution` and `plot_logit_attribution` with appropriate arguments - the important part is interpreting the results. Please do look at the solutions if you're stuck on the code; this part isn't important.*

Perform logit attribution for your attention-only model `model`, on the `rep_cache`. What do you expect to see?

<!-- Remember, you'll need to split the sequence in two, with one overlapping token (since predicting the next token involves removing the final token with no label) - your `logit_attr` should both have shape `[seq_len, 2*n_heads + 1]` (ie `[50, 25]` here). -->
<!-- 
<details>
<summary>Note - the first plot will be pretty meaningless. Can you see why?</summary>

Because the first plot shows the logit attribution for the first half of the sequence, i.e. the first occurrence of each of the tokens. Since there is no structure to this sequence (it is purely random), there is no reason to expect the heads to be doing meaningful computation. The structure lies in the second half of the sequence, when the tokens are repeated, and the heads with high logit attributions will be the ones that can perform induction.
</details> -->
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# # YOUR CODE HERE - plot logit attribution for the induction sequence (i.e. using `rep_tokens` and
# # `rep_cache`), and interpret the results.
# END EXERCISE
# SOLUTION
seq_len = 50

embed = rep_cache["embed"]
l1_results = rep_cache["result", 0]
l2_results = rep_cache["result", 1]

logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, rep_tokens.squeeze())
plot_logit_attribution(
    model, logit_attr, rep_tokens.squeeze(), title="Logit attribution (random induction prompt)"
)
# END SOLUTION

# FILTERS: ~
# plot_logit_attribution(
#     model,
#     logit_attr,
#     rep_tokens.squeeze(),
#     title="Logit attribution (random induction prompt)",
#     filename=str(section_dir / "1208.html"),
# )
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1208.html" width="620" height="1650"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What is the interpretation of this plot, in the context of our induction head circuit?

<details>
<summary>Answer</summary>

The first half of the plot is mostly meaningless, because the sequences here are random and carry no predictable pattern, and so there can't be any part of the model that is doing meaningful computation to make predictions.

In the second half, we see that heads `1.4` and `1.10` have a large logit attribution score. This makes sense given our previous observation that these heads seemed to be performing induction (since they both exhibited the characteristic induction pattern), however it's worth emphasizing that this plot gives us a different kind of evidence than looking at attention patterns does, because just observing some head is attending to a particular token doesn't mean it's necessarily using that information to make a concrete prediction. Note that we see head `1.10` has a larger direct effect than `1.4`, which agrees with our attention scores result (where `1.10` also scored higher than `1.4`).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Hooks: Intervening on Activations

Now that we've built some tools to decompose our model's output, it's time to start making causal interventions.

### Ablations

Let's start with a simple example: **ablation**. An ablation is a simple causal intervention on a model - we pick some part of it and set it to zero. This is a crude proxy for how much that part matters. Further, if we have some story about how a specific circuit in the model enables some capability, showing that ablating *other* parts does nothing can be strong evidence of this.

As mentioned in [the glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx), there are many ways to do ablation. We'll focus on the simplest: zero-ablation (even though it's somewhat unprincipled).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - induction head ablation

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should aim to spend 20-35 mins on this exercise.
> ```

The code below provides a template for performing zero-ablation on the output vectors at a particular head (i.e. the vectors we get when taking a weighted sum of the value vectors according to the attention probabilities, before projecting them up & adding them back to the residual stream). If you're confused about what different activations mean, you can refer back to [the diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg).

You need to do 2 things:

1. Fill in `head_zero_ablation_hook` so that it performs zero-ablation on the head given by `head_index_to_ablate`.
2. Fill in the missing code in the `get_ablation_scores` function (i.e. where you see the `raise NotImplementedError()` line), so that `loss_with_ablation` is computed as the loss of the model after ablating head `head` in layer `layer`.

The rest of the `get_ablation_scores` function is designed to return a tensor of shape `(n_layers, n_heads)` containing the increase in loss from ablating each of these heads. 

A few notes about this function / tips on how to implement it:

- You can create a temporary hook function by applying `functools.partial` to the `ablation_function`, fixing the head index to a particular value.
- You can use `utils.get_act_name("z", layer)` to get the name of the hook point (to see the full diagram of named hook points and how to get the names, you can refer to the streamlit reference page, which can be found on the left hand sidebar after you navigate to the [homepage](https://arena-chapter1-transformer-interp.streamlit.app/)).
- See that `loss_no_ablation` is computed with the `get_log_probs` function, and that we only take the last `seq_len - 1` tokens - this is because we're dealing with sequences of length `2 * seq_len + 1` (a BOS token plus 2 repeated random sequences), and we only care about the loss on the second half of the sequence.
- Note that we call `model.reset_hooks()` at the start of the function - this is a useful practice in general, to make sure you've not accidentally left in any hooks that might change your model's behaviour.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def head_zero_ablation_hook(
    z: Float[Tensor, "batch seq n_heads d_head"],
    hook: HookPoint,
    head_index_to_ablate: int,
) -> None:
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    z[:, :, head_index_to_ablate, :] = 0.0
    # END SOLUTION


def get_ablation_scores(
    model: HookedTransformer,
    tokens: Int[Tensor, "batch seq"],
    ablation_function: Callable = head_zero_ablation_hook,
) -> Float[Tensor, "n_layers n_heads"]:
    """
    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss
    from ablating the output of each head.
    """
    # Initialize an object to store the ablation scores
    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)

    # Calculating loss without any ablation, to act as a baseline
    model.reset_hooks()
    seq_len = (tokens.shape[1] - 1) // 2
    logits = model(tokens, return_type="logits")
    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()

    for layer in tqdm(range(model.cfg.n_layers)):
        for head in range(model.cfg.n_heads):
            # EXERCISE
            # raise NotImplementedError()
            # END EXERCISE
            # SOLUTION
            # Use functools.partial to create a temporary hook function with the head number fixed
            temp_hook_fn = functools.partial(ablation_function, head_index_to_ablate=head)
            # Run the model with the ablation hook
            ablated_logits = model.run_with_hooks(
                tokens, fwd_hooks=[(utils.get_act_name("z", layer), temp_hook_fn)]
            )
            # Calculate the loss difference (= neg correct logprobs), only on the last seq_len tokens
            loss = -get_log_probs(ablated_logits, tokens)[:, -(seq_len - 1) :].mean()
            # Store the result, subtracting the clean loss so that a value of 0 means no loss change
            ablation_scores[layer, head] = loss - loss_no_ablation
            # END SOLUTION

    return ablation_scores


# HIDE
if MAIN:
    ablation_scores = get_ablation_scores(model, rep_tokens)
    tests.test_get_ablation_scores(ablation_scores, model, rep_tokens)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed the tests, you can plot the results:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

imshow(
    ablation_scores,
    labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
    title="Loss Difference After Ablating Heads",
    text_auto=".2f",
    width=900,
    height=350,
)

# FILTERS: ~
# imshow(
#     ablation_scores,
#     labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
#     title="Loss Difference After Ablating Heads",
#     text_auto=".2f",
#     width=900,
#     height=350,
#     return_fig=True,
# ).write_html(str(section_dir / "1209.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output (yours might be slightly different due to randomness)]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1209.html" width="920" height="370"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What is your interpretation of these results?

<details>
<summary>Interpretation</summary>

This tells us not just which heads are responsible for writing output to the residual stream that gets us the correct result, but **which heads play an important role in the induction circuit**.

This chart tells us that - for sequences of repeated tokens - head `0.7` is by far the most important in layer 0 (which makes sense, since we observed it to be the strongest "previous token head"), and heads `1.4`, `1.10` are the most important in layer 1 (which makes sense, since we observed these to be the most induction-y).

This is a good illustration of the kind of result which we can get from ablation, but **wouldn't be able to get from something like direct logit attribution**, because it isn't a causal intervention.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - mean ablation

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should aim to spend 5-15 mins on this exercise.
> ```

An alternative to zero-ablation is **mean-ablation**, where rather than setting values to zero, we set them to be their mean across some suitable distribution (commonly we'll use the mean over some batch dimension). This can be more informative, because zero-ablation takes a model out of its normal distribution, and so the results from it aren't necessarily representative of what you'd get if you "switched off" the effect from some particular component. Mean ablation on the other hand works slightly better (although it does come with its own set of risks). You can read more [here](https://www.neelnanda.io/mechanistic-interpretability/glossary#:~:text=Ablation%20aka%20Knockout) or [here](https://arxiv.org/html/2404.15255v1).

You should fill in the `head_mean_ablation_hook` function below, and run the code (also make sure in your previous `get_ablation_scores` function that you were actually using the `ablation_function` rather than hardcoding the zero ablation function, otherwise your code won't work here). You should see that the results are slightly cleaner, with the unimportant heads having values much closer to zero relative to the important heads.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def head_mean_ablation_hook(
    z: Float[Tensor, "batch seq n_heads d_head"],
    hook: HookPoint,
    head_index_to_ablate: int,
) -> None:
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    z[:, :, head_index_to_ablate, :] = z[:, :, head_index_to_ablate, :].mean(0)
    # END SOLUTION


if MAIN:
    rep_tokens_batch = run_and_cache_model_repeated_tokens(model, seq_len=50, batch_size=10)[0]
    mean_ablation_scores = get_ablation_scores(
        model, rep_tokens_batch, ablation_function=head_mean_ablation_hook
    )

    imshow(
        mean_ablation_scores,
        labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
        title="Loss Difference After Ablating Heads",
        text_auto=".2f",
        width=900,
        height=350,
    )
    # FILTERS: ~
    # imshow(
    #     mean_ablation_scores,
    #     labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
    #     title="Loss Difference After Ablating Heads",
    #     text_auto=".2f",
    #     width=900,
    #     height=350,
    #     return_fig=True,
    # ).write_html(str(section_dir / "1209-mean.html"))
    # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output (yours might be slightly different due to randomness)]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1209-mean.html" width="920" height="370"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Bonus - understand heads 0.4 & 0.11 (very hard!)

There are 2 heads which appeared strongly in our induction ablation experiments, but haven't stood out as much in the other analysis we've done in this section `0.4` and `0.11`. Can you construct causal experiments (i.e. targeted ablations) to try and figure out what these heads are doing?

> Note - you might want to attempt this once you've made some headway into the next section, as this will give you a more mechanistic understanding of the induction circuit. Even once you've done that, you might still find this bonus exercise challenging, because it ventures outside of the well-defined induction circuit we've been working with and into potentially more ambiguous results. **To restate - the material here is very challenging!**

<details>
<summary>Here's a hint to get you started</summary>

Look at the positions that heads `0.4` and `0.11` are attending to. Can you figure out which source positions are important to attend to for the model to perform well?

</details>

<details>
<summary>Partial answer (and some sample code)</summary>

Below is some sample code which plots the effect of ablating the inputs to heads `0.4` and `0.11` at all offset positions minus a few (e.g. the first row shows the effect on loss of mean ablating all inputs to the heads except for those that come from self-attention, and the second row shows the effect when we ablate all inputs except for those that come from the token immediately before it in the sequence). 

```python
def head_z_ablation_hook(
    z: Float[Tensor, "batch seq n_heads d_head"],
    hook: HookPoint,
    head_index_to_ablate: int,
    seq_posns: list[int],
    cache: ActivationCache,
) -> None:
    """
    We perform ablation at the z vector, by doing the equivalent of mean ablating all the inputs to this attention head
    except for those which come from the tokens `n` positions back, where `n` is in the `seq_posns` list.
    """
    batch, seq = z.shape[:2]
    v = cache["v", hook.layer()][:, :, head_index_to_ablate]  # shape [batch seq_K d_head]
    pattern = cache["pattern", hook.layer()][:, head_index_to_ablate]  # shape [batch seq_Q seq_K]

    # Get a repeated version of v, and mean ablate all but the previous token values
    v_repeated = einops.repeat(v, "b sK h -> b sQ sK h", sQ=seq)
    v_ablated = einops.repeat(v_repeated.mean(0), "sQ sK h -> b sQ sK h", b=batch).clone()
    for offset in seq_posns:
        seqQ_slice = t.arange(offset, seq)
        v_ablated[:, seqQ_slice, seqQ_slice - offset] = v_repeated[:, seqQ_slice, seqQ_slice - offset]

    # Take weighted sum of this new v, and use it to edit `z` inplace.
    z[:, :, head_index_to_ablate] = einops.einsum(v_ablated, pattern, "b sQ sK h, b sQ sK -> b sQ h")


def get_ablation_scores_cache_assisted(
    model: HookedTransformer,
    tokens: Int[Tensor, "batch seq"],
    ablation_function: Callable = head_zero_ablation_hook,
    seq_posns: list[int] = [0],
    layers: list[int] = [0],
) -> Float[Tensor, "n_layers n_heads"]:
    """
    Version of `get_ablation_scores` which can use the cache to assist with the ablation.
    """
    ablation_scores = t.zeros((len(layers), model.cfg.n_heads), device=model.cfg.device)

    model.reset_hooks()
    seq_len = (tokens.shape[1] - 1) // 2
    logits, cache = model.run_with_cache(tokens, return_type="logits")
    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()

    for layer in layers:
        for head in range(model.cfg.n_heads):
            temp_hook_fn = functools.partial(ablation_function, head_index_to_ablate=head, cache=cache, seq_posns=seq_posns)
            ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[(utils.get_act_name("z", layer), temp_hook_fn)])
            loss = -get_log_probs(ablated_logits, tokens)[:, -(seq_len - 1) :].mean()
            ablation_scores[layer, head] = loss - loss_no_ablation

    return ablation_scores


rep_tokens_batch = run_and_cache_model_repeated_tokens(model, seq_len=50, batch_size=50)[0]

offsets = [[0], [1], [2], [3], [1, 2], [1, 2, 3]]
z_ablation_scores = [
    get_ablation_scores_cache_assisted(model, rep_tokens_batch, head_z_ablation_hook, offset).squeeze()
    for offset in tqdm(offsets)
]

imshow(
    t.stack(z_ablation_scores),
    labels={"x": "Head", "y": "Position offset", "color": "Logit diff"},
    title="Loss Difference (ablating heads everywhere except for certain offset positions)",
    text_auto=".2f",
    y=[str(offset) for offset in offsets],
    width=900,
    height=400,
)
```

FILTERS: st
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1210-A.html" width="950" height="450"></div>
END FILTERS

Some observations from the result of this code:

- **Head `0.7` is truly a previous token head.** The second row shows that mean ablating all its inputs except for those that come from the previous token has no effect on loss, so this is all the information it's using.
- **Head `0.11` is only a current token head.** The first row shows that mean ablating all its inputs except for those that come from self-attending (i.e. to the current token) has no effect on loss, so this is all the information it's using.
- **Head `0.4` is only using information from positions 1, 2 or 3 tokens back.** This is shown from the 5th row of the plot above - the effect of ablating all inputs except for those that come from tokens 1 or 2 positions back is very small. Note that it's important we draw this conclusion from an ablation experiment, not just from looking at attention patterns - because attending to a token doesn't tell you whether that token is being used for a way that's important in the context of this particular distribution (induction).

Starting with `0.11` - we know that there are heads in layer 1 whose job it is to copy tokens - i.e. in sequences `[A][B]...[A][B]`, they attend from the second `[A]` back to the first `[B]` and copy its value to use as a prediction. And if head `0.11` always self-attends, then it actually makes sense to consider `(embedding of B) + (output of head 0.11 when it attends to token B)` as the "true embedding of `B`", since this is always the thing that the layer 1 head will be learning to copy. This idea of an **extended embedding** or **effective embedding** will come up again later in the course, when we look at GPT2-Small. As for whether the output of `0.11` is more important in the QK circuit of the layer-1 copying head, or the OV copying head, we'll leave as an exercise to the reader!

Next, `0.4` - it's using information from both 1 and 2 tokens back. Using the previous token makes sense, since induction circuits contain previous token heads. But what could it be doing with the information 2 positions back? One theory we might have is that it's also creating an induction circuit, but using 3 tokens rather than 2 tokens! In other words, rather than having sequences like `[A][B]...[A][B]` where the second `[A]` attends back to "token that came immediately after the value of this token", we might have sequences like `[Z][A][B]...[Z][A][B]` where the second `[A]` attends back to "token that came 2 positions after the value of the previous token". One way to test this would be to construct random induction sequences which have a maximum of 2 repetitions, i.e. they're constructed with the first half being random sequences and the second half being pairs of randomly chosen tokens which appear in the first half adjacent to each other. To illustrate, for vocab size of 10 and half seq len of 10, we might have a sequence like:

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">0 5 8 3 1 8 2 2 4 6 (5 8) (4 6) (3 1) (2 4) (0 5)</pre>

Based on our theory about head `0.4`, we should expect that mean ablating it in this kind of sequence should have nearly zero effect on loss (because it's designed to support induction sequences of length at least 3), even though all the other heads which were identified as important in the induction experiment (`0.7`, `0.11`, `1.4`, `1.10`) should still be important. This is in fact what we find - you can try this for yourself with the code below.

```python
def generate_repeated_tokens_maxrep(
    model: HookedTransformer,
    seq_len: int,
    batch_size: int = 1,
    maxrep: int = 2,
) -> Int[Tensor, "batch_size full_seq_len"]:
    """
    Same as previous function, but contains a max number of allowed repetitions. For example, maxrep=2 means we can have
    sequences like `[A][B]...[A][B]`, but not `[A][B][C]...[A][B][C]`.
    """
    prefix = (t.ones(batch_size, 1) * model.tokenizer.bos_token_id).long()
    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch_size, seq_len), dtype=t.int64)
    rep_tokens = t.cat([prefix, rep_tokens_half], dim=-1)
    for _ in range(seq_len // maxrep + 1):
        random_start_posn = t.randint(0, seq_len - 2, (batch_size,)).tolist()
        rep_tokens_repeated = t.stack([rep_tokens_half[b, s : s + maxrep] for b, s in enumerate(random_start_posn)])
        rep_tokens = t.cat([rep_tokens, rep_tokens_repeated], dim=-1)

    return rep_tokens[:, : 2 * seq_len + 1].to(device)


rep_tokens_max2 = generate_repeated_tokens_maxrep(model, seq_len=50, batch_size=50, maxrep=2)

mean_ablation_scores = get_ablation_scores(model, rep_tokens_max2, ablation_fn=head_mean_ablation_hook)

imshow(
    mean_ablation_scores,
    labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
    title="Loss Difference After Ablating Heads",
    text_auto=".2f",
    width=900,
    height=350,
)
```

FILTERS: st
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1210-B.html" width="920" height="370"></div>
END FILTERS

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: [colab-soln]
# ! TAGS: []

def head_z_ablation_hook(
    z: Float[Tensor, "batch seq n_heads d_head"],
    hook: HookPoint,
    head_index_to_ablate: int,
    seq_posns: list[int],
    cache: ActivationCache,
) -> None:
    """
    We perform ablation at the z vector, by doing the equivalent of mean ablating all the inputs to
    this attention head except for those which come from the tokens `n` positions back, where `n` is
    in the `seq_posns` list.
    """
    batch, seq = z.shape[:2]
    v = cache["v", hook.layer()][:, :, head_index_to_ablate]  # shape [batch seq_K d_head]
    pattern = cache["pattern", hook.layer()][:, head_index_to_ablate]  # shape [batch seq_Q seq_K]

    # Get a repeated version of v, and mean ablate all but the previous token values
    v_repeated = einops.repeat(v, "b sK h -> b sQ sK h", sQ=seq)
    v_ablated = einops.repeat(v_repeated.mean(0), "sQ sK h -> b sQ sK h", b=batch).clone()
    for offset in seq_posns:
        seqQ_slice = t.arange(offset, seq)
        v_ablated[:, seqQ_slice, seqQ_slice - offset] = v_repeated[
            :, seqQ_slice, seqQ_slice - offset
        ]

    # Take weighted sum of this new v, and use it to edit `z` inplace.
    z[:, :, head_index_to_ablate] = einops.einsum(
        v_ablated, pattern, "b sQ sK h, b sQ sK -> b sQ h"
    )


def get_ablation_scores_cache_assisted(
    model: HookedTransformer,
    tokens: Int[Tensor, "batch seq"],
    ablation_function: Callable = head_zero_ablation_hook,
    seq_posns: list[int] = [0],
    layers: list[int] = [0],
) -> Float[Tensor, "n_layers n_heads"]:
    """
    Version of `get_ablation_scores` which can use the cache to assist with the ablation.
    """
    ablation_scores = t.zeros((len(layers), model.cfg.n_heads), device=model.cfg.device)

    model.reset_hooks()
    seq_len = (tokens.shape[1] - 1) // 2
    logits, cache = model.run_with_cache(tokens, return_type="logits")
    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()

    for layer in layers:
        for head in range(model.cfg.n_heads):
            temp_hook_fn = functools.partial(
                ablation_function, head_index_to_ablate=head, cache=cache, seq_posns=seq_posns
            )
            ablated_logits = model.run_with_hooks(
                tokens, fwd_hooks=[(utils.get_act_name("z", layer), temp_hook_fn)]
            )
            loss = -get_log_probs(ablated_logits, tokens)[:, -(seq_len - 1) :].mean()
            ablation_scores[layer, head] = loss - loss_no_ablation

    return ablation_scores


rep_tokens_batch = run_and_cache_model_repeated_tokens(model, seq_len=50, batch_size=50)[0]

offsets = [[0], [1], [2], [3], [1, 2], [1, 2, 3]]
z_ablation_scores = [
    get_ablation_scores_cache_assisted(
        model, rep_tokens_batch, head_z_ablation_hook, offset
    ).squeeze()
    for offset in tqdm(offsets)
]

imshow(
    t.stack(z_ablation_scores),
    labels={"x": "Head", "y": "Position offset", "color": "Logit diff"},
    title="Loss Difference (ablating heads everywhere except for certain offset positions)",
    text_auto=".2f",
    y=[str(offset) for offset in offsets],
    width=900,
    height=400,
)

# FILTERS: ~
# imshow(
#     t.stack(z_ablation_scores),
#     labels={"x": "Head", "y": "Position offset", "color": "Logit diff"},
#     title="Loss Difference (ablating heads everywhere except for certain offset positions)",
#     text_auto=".2f",
#     y=[str(offset) for offset in offsets],
#     width=900,
#     height=400,
#     return_fig=True,
# ).write_html(str(section_dir / "1210-A.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1210-A.html" width="920" height="420"></div>
'''

# ! CELL TYPE: code
# ! FILTERS: [colab-soln]
# ! TAGS: []

def generate_repeated_tokens_maxrep(
    model: HookedTransformer,
    seq_len: int,
    batch_size: int = 1,
    maxrep: int = 2,
) -> Int[Tensor, "batch_size full_seq_len"]:
    """
    Same as previous function, but contains a max number of allowed repetitions. For example, maxrep=2 means we can have
    sequences like `[A][B]...[A][B]`, but not `[A][B][C]...[A][B][C]`.
    """
    prefix = (t.ones(batch_size, 1) * model.tokenizer.bos_token_id).long()
    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch_size, seq_len), dtype=t.int64)
    rep_tokens = t.cat([prefix, rep_tokens_half], dim=-1)
    for _ in range(seq_len // maxrep + 1):
        random_start_posn = t.randint(0, seq_len - 2, (batch_size,)).tolist()
        rep_tokens_repeated = t.stack(
            [rep_tokens_half[b, s : s + maxrep] for b, s in enumerate(random_start_posn)]
        )
        rep_tokens = t.cat([rep_tokens, rep_tokens_repeated], dim=-1)

    return rep_tokens[:, : 2 * seq_len + 1].to(device)


rep_tokens_max2 = generate_repeated_tokens_maxrep(model, seq_len=50, batch_size=50, maxrep=2)

mean_ablation_scores = get_ablation_scores(
    model, rep_tokens_max2, ablation_function=head_mean_ablation_hook
)

# COLAB-SPLIT

imshow(
    mean_ablation_scores,
    labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
    title="Loss Difference After Ablating Heads",
    text_auto=".2f",
    width=900,
    height=350,
)

# FILTERS: ~
# imshow(
#     mean_ablation_scores,
#     labels={"x": "Head", "y": "Layer", "color": "Logit diff"},
#     title="Loss Difference After Ablating Heads",
#     text_auto=".2f",
#     width=900,
#     height=350,
#     return_fig=True,
# ).write_html(str(section_dir / "1210-B.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1210-B.html" width="920" height="370"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 4️⃣ Reverse-engineering induction circuits
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In previous exercises, we looked at the attention patterns and attributions of attention heads to try and identify which ones were important in the induction circuit. This might be a good way to get a feel for the circuit, but it's not a very rigorous way to understand it. It would be better described as **feature analysis**, where we observe *that* a particular head seems to be performing some task on a certain class of inputs, without identifying *why* it does so.

Now we're going to do some more rigorous mechanistic analysis - digging into the weights and using them to reverse engineer the induction head algorithm and verify that it is really doing what we think it is.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Refresher - the induction circuit

Before we get into the meat of this section, let's refresh the results we've gotten so far from investigating induction heads. We've found:

* When fed repeated sequences of tokens, heads `1.4` and `1.10` have the characteristic induction head attention pattern of a diagonal stripe with offset `seq_len - 1`.
    * We saw this both from the CircuitsVis results, and from the fact that these heads had high induction scores by our chosen metric (with all other heads having much lower scores).
* We also saw that head `0.7` strongly attends to the previous token in the sequence (even on non-repeated sequences).
* We performed **logit attribution** on the model, and found that the values written to the residual stream by heads `1.4` and `1.10` were both important for getting us correct predictions in the second half of the sequence.
* We performed **zero-ablation** on the model, and found that heads `0.7`, `1.4` and `1.10` all resulted in a large accuracy degradation on the repeated sequence task when they were ablated.

Based on all these observations, try and summarise the induction circuit and how it works, in your own words. You should try and link your explanation to the QK and OV circuits for particular heads, and describe what type (or types) of attention head composition are taking place.

You can use the dropdown below to check your understanding.

<details>
<summary>My summary of the algorithm</summary>

* Head `0.7` is a previous token head (the QK-circuit ensures it always attends to the previous token).
* The OV circuit of head `0.7` writes a copy of the previous token in a *different* subspace to the one used by the embedding.
* The output of head `0.7` is used by the *key* input of head `1.10` via K-Composition to attend to 'the source token whose previous token is the destination token'.
* The OV-circuit of head `1.10` copies the *value* of the source token to the same output logit.
    * Note that this is copying from the embedding subspace, *not* the `0.7` output subspace - it is not using V-Composition at all.
* `1.4` is also performing the same role as `1.10` (so together they can be more accurate - we'll see exactly how later).

To emphasise - the sophisticated hard part is computing the *attention* pattern of the induction head - this takes careful composition. The previous token and copying parts are fairly easy. This is a good illustrative example of how the QK circuits and OV circuits act semi-independently, and are often best thought of somewhat separately. And that computing the attention patterns can involve real and sophisticated computation!

Below is a diagram of the induction circuit, with the heads indicated in the weight matrices.

![kcomp_diagram_3.png](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described_3.png)
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Refresher - QK and OV circuits

Before we start, a brief terminology note. I'll refer to weight matrices for a particular layer and head using superscript notation, e.g. $W_Q^{1.4}$ is the query matrix for the 4th head in layer 1, and it has shape `[d_model, d_head]` (remember that we multiply with weight matrices on the right). Similarly, attention patterns will be denoted $A^{1.4}$ (remember that these are **activations**, not parameters, since they're given by the formula $A^h = x W_{QK}^h x^T$, where $x$ is the residual stream (with shape `[seq_len, d_model]`).

As a shorthand, I'll often have $A$ denote the one-hot encoding of token `A` (i.e. the vector with zeros everywhere except a one at the index of `A`), so $A^T W_E$ is the embedding vector for `A`.

Lastly, I'll refer to special matrix products as follows:

* $W_{OV}^{h} := W_V^{h}W_O^{h}$ is the **OV circuit** for head $h$, and $W_E W_{OV}^h W_U$ is the **full OV circuit**.
* $W_{QK}^h := W_Q^h (W_K^h)^T$ is the **QK circuit** for head $h$, and $W_E W_{QK}^h W_E^T$ is the **full QK circuit**.

Note that the order of these matrices are slightly different from the **Mathematical Frameworks** paper - this is a consequence of the way TransformerLens stores its weight matrices.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Question - what is the interpretation of each of the following matrices?

*There are quite a lot of questions here, but they are conceptually important. If you're confused, you might want to read the answers to the first few questions and then try the later ones.*

In your answers, you should describe the type of input it takes, and what the outputs represent.

#### $W_{OV}^{h}$

<details>
<summary>Answer</summary>

$W_{OV}^{h}$ has size $(d_\text{model}, d_\text{model})$, it is a linear map describing **what information gets moved from source to destination, in the residual stream.**

In other words, if $x$ is a vector in the residual stream, then $x^T W_{OV}^{h}$ is the vector written to the residual stream at the destination position, if the destination token only pays attention to the source token at the position of the vector $x$.
</details>

#### $W_E W_{OV}^h W_U$

<details>
<summary>Hint</summary>

If $A$ is the one-hot encoding for token `A` (i.e. the vector with zeros everywhere except for a one in the position corresponding to token `A`), then think about what $A^T W_E W_{OV}^h W_U$ represents. You can evaluate this expression from left to right (e.g. start with thinking about what $A^T W_E$ represents, then multiply by the other two matrices).
</details>
<details>
<summary>Answer</summary>

$W_E W_{OV}^h W_U$ has size $(d_\text{vocab}, d_\text{vocab})$, it is a linear map describing **what information gets moved from source to destination, in a start-to-end sense.**

If $A$ is the one-hot encoding for token `A`, then:

* $A^T W_E$ is the embedding vector for `A`.
* $A^T W_E W_{OV}^h$ is the vector which would get written to the residual stream at the destination position, if the destination token only pays attention to `A`.
* $A^T W_E W_{OV}^h W_U$ is the unembedding of this vector, i.e. the thing which gets added to the final logits.

</details>

#### $W_{QK}^{h}$

<details>
<summary>Answer</summary>

$W_{QK}^{h}$ has size $(d_\text{model}, d_\text{model})$, it is a bilinear form describing **where information is moved to and from** in the residual stream (i.e. which residual stream vectors attend to which others).

$x_i^T W_{QK}^h x_j = (x_i^T W_Q^h) (x_j^T W_K^h)^T$ is the attention score paid by token $i$ to token $j$.
</details>

#### $W_E W_{QK}^h W_E^T$

<details>
<summary>Answer</summary>

$W_E W_{QK}^h W_E^T$ has size $(d_\text{vocab}, d_\text{vocab})$, it is a bilinear form describing **where information is moved to and from**, among words in our vocabulary (i.e. which tokens pay attention to which others).

If $A$ and $B$ are one-hot encodings for tokens `A` and `B`, then $A^T W_E W_{QK}^h W_E^T B$ is the attention score paid by token `A` to token `B`:

$$
A^T \, W_E\, W_{QK}^{h}\, W_E^T \, B = \underbrace{(A^T W_E W_Q^{h})}_{\text{query for token } A}  \underbrace{(B^T W_E W_K^{h})^T}_{\text{key for token }B}
$$
</details>

#### $W_{pos} W_{QK}^h W_{pos}^T$

<details>
<summary>Answer</summary>

$W_{pos} W_{QK}^h W_{pos}^T$ has size $(n_\text{ctx}, n_\text{ctx})$, it is a bilinear form describing **where information is moved to and from**, among tokens in our context (i.e. which token positions pay attention to other positions).

If $i$ and $j$ are one-hot encodings for positions `i` and `j` (in other words they are just the ith and jth basis vectors), then $i^T W_{pos} W_{QK}^h W_{pos}^T j$ is the attention score paid by the token with position `i` to the token with position `j`:

$$
i^T \, W_{pos}\, W_{QK}^{h}\, W_{pos}^T \, j = \underbrace{(i^T W_{pos} W_Q^{h})}_{\text{query for i-th token}}  \underbrace{(j^T W_{pos} W_K^{h})^T}_{\text{key for j-th token}}
$$

</details>

#### $W_E W_{OV}^{h_1} W_{QK}^{h_2} W_E^T$

where $h_1$ is in an earlier layer than $h_2$.

<details>
<summary>Hint</summary>

This matrix is best seen as a bilinear form of size $(d_\text{vocab}, d_\text{vocab})$. The $(A, B)$-th element is:

$$
(A^T W_E W_{OV}^{h_1}) W_{QK}^{h_2} (B^T W_E)^T
$$
</details>

<details>
<summary>Answer</summary>

$W_E W_{OV}^{h_1} W_{QK}^{h_2} W_E^T$ has size $(d_\text{vocab}, d_\text{vocab})$, it is a bilinear form describing where information is moved to and from in head $h_2$, given that the **query-side vector** is formed from the output of head $h_1$. In other words, this is an instance of **Q-composition**.

If $A$ and $B$ are one-hot encodings for tokens `A` and `B`, then $A^T W_E W_{OV}^{h_1} W_{QK}^{h_2} W_E^T B$ is the attention score paid **to** token `B`, **by** any token which attended strongly to an `A`-token in head $h_1$.

---

To further break this down, if it still seems confusing:

$$
\begin{aligned}
A^T \, W_E\, W_{OV}^{h_1} W_{QK}^{h_2}\, W_E^T \, B &= \underbrace{(A^T W_E W_{OV}^{h_1}W_Q^{h_2})}_{\text{query of token which attended to A}}  \underbrace{(B^T W_E W_K^{h_2})^T}_\text{key of token B} \\
\end{aligned}
$$

---

Note that the actual attention score will be a sum of multiple terms, not just this one (in fact, we'd have a different term for every combination of query and key input). But this term describes the **particular contribution** to the attention score from this combination of query and key input, and it might be the case that this term is the only one that matters (i.e. all other terms don't much affect the final probabilities). We'll see something exactly like this later on.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Before we start, there's a problem that we might run into when calculating all these matrices. Some of them are massive, and might not fit on our GPU. For instance, both full circuit matrices have shape $(d_\text{vocab}, d_\text{vocab})$, which in our case means $50278\times 50278 \approx 2.5\times 10^{9}$ elements. Even if your GPU can handle this, it still seems inefficient. Is there any way we can meaningfully analyse these matrices, without actually having to calculate them?

## Factored Matrix class

In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix $M = AB$, where M is `[large, large]`, but A is `[large, small]` and B is `[small, large]`. This is a common structure in transformers.

For instance, we can factorise the OV circuit above as $W_{OV}^h = W_V^h W_O^h$, where $W_V^h$ has shape `[768, 64]` and $W_O^h$ has shape `[64, 768]`. For an even more extreme example, the full OV circuit can be written as $(W_E W_V^h) (W_O^h W_U)$, where these two matrices have shape `[50278, 64]` and `[64, 50278]` respectively. Similarly, we can write the full QK circuit as $(W_E W_Q^h) (W_E W_K^h)^T$.

The `FactoredMatrix` class is a convenient way to work with these. It implements efficient algorithms for various operations on these, such as computing the trace, eigenvalues, Frobenius norm, singular value decomposition, and products with other matrices. It can (approximately) act as a drop-in replacement for the original matrix.

This is all possible because knowing the factorisation of a matrix gives us a much easier way of computing its important properties. Intuitively, since $M=AB$ is a very large matrix that operates on very small subspaces, we shouldn't expect knowing the actual values $M_{ij}$ to be the most efficient way of storing it!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - deriving properties of a factored matrix

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You shouldn't spend more than 10-25 minutes on this exercise.
> 
> If you're less interested in the maths, you can skip these exercises.
> ```

To give you an idea of what kinds of properties you can easily compute if you have a factored matrix, let's try and derive some ourselves.

Suppose we have $M=AB$, where $A$ has shape $(m, n)$, $B$ has shape $(n, m)$, and $m > n$. So $M$ is a size-$(m, m)$ matrix with rank at most $n$.

**Question - how can you easily compute the trace of $M$?**

<details>
<summary>Answer</summary>

We have:

$$
\text{Tr}(M) = \text{Tr}(AB)
= \sum_{i=1}^m \sum_{j=1}^n A_{ij} B_{ji}
$$

so evaluation of the trace is $O(mn)$.

Note that, by cyclicity of the trace, we can also show that $\text{Tr}(M) = \text{Tr}(BA)$ (although we don't even need to calculate the product $AB$ to evaluate the trace).
</details>

**Question - how can you easily compute the eigenvalues of $M$?**

(As you'll see in later exercises, eigenvalues are very important for evaluating matrices, for instance we can assess the [copying scores](https://transformer-circuits.pub/2021/framework/index.html#copying-matrix) of an OV circuit by looking at the eigenvalues of $W_{OV}$.)

<details>
<summary>Hint</summary>

It's computationally cheaper to find the eigenvalues of $BA$ rather than $AB$.

How are the eigenvalues of $AB$ and $BA$ related?
</details>
<details>
<summary>Answer</summary>

The eigenvalues of $AB$ and $BA$ are related as follows: if $\mathbf{v}$ is an eigenvector of $AB$ with $ABv = \lambda \mathbf{v}$, then $B\mathbf{v}$ is an eigenvector of $BA$ with the same eigenvalue:

$$
BA(B\mathbf{v}) = B (AB\mathbf{v}) = B (\lambda \mathbf{v}) = \lambda (B\mathbf{v})
$$

This only fails when $B\mathbf{v} = \mathbf{0}$, but in this case $AB\mathbf{v} = \mathbf{0}$ so $\lambda = 0$. Thus, we can conclude that any non-zero eigenvalues of $AB$ are also eigenvalues of $BA$.

It's much computationally cheaper to compute the eigenvalues of $BA$ (since it's a much smaller matrix), and this gives us all the non-zero eigenvalues of $AB$.
</details>

**Question (hard) - how can you easily compute the SVD of $M$?**

<details>
<summary>Hint</summary>

For a size-$(m, n)$ matrix with $m > n$, the [algorithmic complexity of finding SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Numerical_approach) is $O(mn^2)$. So it's relatively cheap to find the SVD of $A$ and $B$ (complexity $mn^2$ vs $m^3$). Can you use that to find the SVD of $M$?
</details>


<details>
<summary>Answer</summary>

It's much cheaper to compute the SVD of the small matrices $A$ and $B$. Denote these SVDs by:

$$
\begin{aligned}
A &= U_A S_A V_A^T \\
B &= U_B S_B V_B^T
\end{aligned}
$$

where $U_A$ and $V_B$ are $(m, n)$, and the other matrices are $(n, n)$.

Then we have:

$$
\begin{aligned}
\quad\quad\quad\quad M &= AB \\
&= U_A (S_A V_A^T U_B S_B) V_B^T
\end{aligned}
$$

Note that the matrix in the middle has size $(n, n)$ (i.e. small), so we can compute its SVD cheaply:

$$
\begin{aligned}
\; S_A V_A^T U_B S_B &= U' S' {V'}^T \quad\quad\quad\quad\quad
\end{aligned}
$$

and finally, this gives us the SVD of $M$:

$$
\begin{aligned}
\quad\quad M &= U_A U' S' {V'}^T V_B^T \\
&= U S {V'}^T
\end{aligned}
$$

where $U = U_A U'$, $V = V_B V'$, and $S = S'$.

All our SVD calculations and matrix multiplications had complexity at most $O(mn^2)$, which is much better than $O(m^3)$ (remember that we don't need to compute all the values of $U = U_A U'$, only the ones which correspond to non-zero singular values).
</details>

If you're curious, you can go to the `FactoredMatrix` documentation to see the implementation of the SVD calculation, as well as other properties and operations.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now that we've discussed some of the motivations behind having a `FactoredMatrix` class, let's see it in action.

### Basic Examples

We can use the basic class directly - let's make a factored matrix directly and look at the basic operations:
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

A = t.randn(5, 2)
B = t.randn(2, 5)
AB = A @ B
AB_factor = FactoredMatrix(A, B)
print("Norms:")
print(AB.norm())
print(AB_factor.norm())

print(f"Right dim: {AB_factor.rdim}, Left dim: {AB_factor.ldim}, Hidden dim: {AB_factor.mdim}")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can also look at the eigenvalues and singular values of the matrix. Note that, because the matrix is rank 2 but 5 by 5, the final 3 eigenvalues and singular values are zero - the factored class omits the zeros.
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

print("Eigenvalues:")
print(t.linalg.eig(AB).eigenvalues)
print(AB_factor.eigenvalues)

print("\nSingular Values:")
print(t.linalg.svd(AB).S)
print(AB_factor.S)

print("\nFull SVD:")
print(AB_factor.svd())

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - the sizes of objects returned by the SVD method.</summary>

If $M = USV^T$, and `M.shape = (m, n)` and the rank is `r`, then the SVD method returns the matrices $U, S, V$. They have shape `(m, r)`, `(r,)`, and `(n, r)` respectively, because:

* We don't bother storing the off-diagonal entries of $S$, since they're all zero.
* We don't bother storing the columns of $U$ and $V$ which correspond to zero singular values, since these won't affect the value of $USV^T$.
</details>

We can multiply a factored matrix with an unfactored matrix to get another factored matrix (as in example below). We can also multiply two factored matrices together to get another factored matrix.
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

C = t.randn(5, 300)
ABC = AB @ C
ABC_factor = AB_factor @ C

print(f"Unfactored: shape={ABC.shape}, norm={ABC.norm()}")
print(f"Factored: shape={ABC_factor.shape}, norm={ABC_factor.norm()}")
print(f"\nRight dim: {ABC_factor.rdim}, Left dim: {ABC_factor.ldim}, Hidden dim: {ABC_factor.mdim}")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
If we want to collapse this back to an unfactored matrix, we can use the `AB` property to get the product:
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

AB_unfactored = AB_factor.AB
t.testing.assert_close(AB_unfactored, AB)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Reverse-engineering circuits

Within our induction circuit, we have four individual circuits: the OV and QK circuits in our previous token head, and the OV and QK circuits in our induction head. In the following sections of the exercise, we'll reverse-engineer each of these circuits in turn.

* In the section **OV copying circuit**, we'll look at the layer-1 OV circuit.
* In the section **QK prev-token circuit**, we'll look at the layer-0 QK circuit.
* The third section (**K-composition**) is a bit trickier, because it involves looking at the composition of the layer-0 OV circuit **and** layer-1 QK circuit. We will have to do two things:
    1. Show that these two circuits are composing (i.e. that the output of the layer-0 OV circuit is the main determinant of the key vectors in the layer-1 QK circuit).
    2. Show that the joint operation of these two circuits is "make the second instance of a token attend to the token *following* an earlier instance.

The dropdown below contains a diagram explaining how the three sections relate to the different components of the induction circuit. You might have to open it in a new tab to see it clearly.

<details>
<summary>Diagram</summary>

![kcomp](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described_2_new.png)
</details>

After this, we'll have a look at composition scores, which are a more mathematically justified way of showing that two attention heads are composing (without having to look at their behaviour on any particular class of inputs, since it is a property of the actual model weights).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## [1] OV copying circuit

Let's start with an easy parts of the circuit - the copying OV circuit of `1.4` and `1.10`. Let's start with head 4. The only interpretable (read: **privileged basis**) things here are the input tokens and output logits, so we want to study the matrix:

$$
W_E W_{OV}^{1.4} W_U
$$

(and same for `1.10`). This is the $(d_\text{vocab}, d_\text{vocab})$-shape matrix that combines with the attention pattern to get us from input to output.

We want to calculate this matrix, and inspect it. We should find that its diagonal values are very high, and its non-diagonal values are much lower.

**Question - why should we expect this observation?** (you may find it helpful to refer back to the previous section, where you described what the interpretation of different matrices was.)

<details>
<summary>Hint</summary>

Suppose our repeating sequence is `A B ... A B`. Let $A$, $B$ be the corresponding one-hot encoded tokens. The `B`-th row of this matrix is:

$$
B^T W_E W_{OV}^{1.4} W_U
$$

What is the interpretation of this expression, in the context of our attention head?
</details>

<details>
<summary>Answer</summary>

If our repeating sequence is `A B ... A B`, then:

$$
B^T W_E W_{OV}^{1.4} W_U
$$

is the **vector of logits which gets moved from the first `B` token to the second `A` token, to be used as the prediction for the token following the second `A` token**. It should result in a high prediction for `B`, and a low prediction for everything else. In other words, the `(B, X)`-th element of this matrix should be highest for `X=B`, which is exactly what we claimed.

If this still seems confusing, the diagram below might help:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described-OV-v3.png" width="750">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute OV circuit for `1.4`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> ```

*This is the first of several similar exercises where you calculate a circuit by multiplying matrices. This exercise is pretty important (in particular, you should make sure you understand what this matrix represents and why we're interested in it), but the actual calculation shouldn't take very long.*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should compute it as a `FactoredMatrix` object.

Remember, you can access the model's weights directly e.g. using `model.W_E` or `model.W_Q` (the latter gives you all the `W_Q` matrices, indexed by layer and head).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

head_index = 4
layer = 1

# EXERCISE
# # YOUR CODE HERE - complete the `full_OV_circuit` object
# END EXERCISE
# SOLUTION
W_O = model.W_O[layer, head_index]
W_V = model.W_V[layer, head_index]
W_E = model.W_E
W_U = model.W_U

OV_circuit = FactoredMatrix(W_V, W_O)
full_OV_circuit = W_E @ OV_circuit @ W_U
# END SOLUTION

# HIDE
tests.test_full_OV_circuit(full_OV_circuit, model, layer, head_index)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm not sure how to use this class to compute a product of more than 2 matrices.</summary>

You can compute it directly, as:

```python
full_OV_circuit = FactoredMatrix(W_E @ W_V, W_O @ W_U)
```

Alternatively, another nice feature about the `FactoredMatrix` class is that you can chain together matrix multiplications. The following code defines exactly the same `FactoredMatrix` object:

```python
OV_circuit = FactoredMatrix(W_V, W_O)
full_OV_circuit = W_E @ OV_circuit @ W_U
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now we want to check that this matrix is the identity. Since it's in factored matrix form, this is a bit tricky, but there are still things we can do.

First, to validate that it looks diagonal-ish, let's pick 200 random rows and columns and visualise that - it should at least look identity-ish here! We're using the indexing method of the `FactoredMatrix` class - you can index into it before returning the actual `.AB` value, to avoid having to compute the whole thing (we take advantage of the fact that `A[left_indices, :] @ B[:, right_indices]` is the same as `(A @ B)[left_indices, right_indices]`).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

indices = t.randint(0, model.cfg.d_vocab, (200,))
full_OV_circuit_sample = full_OV_circuit[indices, indices].AB

imshow(
    full_OV_circuit_sample,
    labels={"x": "Logits on output token", "y": "Input token"},
    title="Full OV circuit for copying head",
    width=700,
    height=600,
)

# FILTERS: ~
# imshow(
#     full_OV_circuit_sample,
#     labels={"x": "Logits on output token", "y": "Input token"},
#     title="Full OV circuit for copying head",
#     width=700,
#     height=600,
#     return_fig=True,
# ).write_html(section_dir / "1211.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1211.html" width="720" height="620"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - indexing factored matrices</summary>

Yet another nice thing about factored matrices is that you can evaluate small submatrices without having to compute the entire matrix. This is based on the fact that the `[i, j]`-th element of matrix `AB` is `A[i, :] @ B[:, j]`.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute circuit accuracy

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend approximately 10-15 minutes on this exercise.
> ```

When you index a factored matrix, you get back another factored matrix. So rather than explicitly calculating `A[left_indices, :] @ B[:, left_indices]`, we can just write `AB[left_indices, left_indices]`.

You should observe a pretty distinct diagonal pattern here, which is a good sign. However, the matrix is pretty noisy so it probably won't be exactly the identity. Instead, we should come up with a summary statistic to capture a rough sense of "closeness to the identity".

**Accuracy** is a good summary statistic - what fraction of the time is the largest logit on the diagonal? Even if there's lots of noise, you'd probably still expect the largest logit to be on the diagonal a good deal of the time.

If you're on a Colab or have a powerful GPU, you should be able to compute the full matrix and perform this test. However, it's better practice to iterate through this matrix when we can, so that we avoid CUDA issues. We've given you a `batch_size` argument in the function below, and you should try to only explicitly calculate matrices of size `batch_size * d_vocab` rather than the massive matrix of `d_vocab * d_vocab`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def top_1_acc(full_OV_circuit: FactoredMatrix, batch_size: int = 1000) -> float:
    """
    Return the fraction of the time that the maximum value is on the circuit diagonal.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    total = 0

    for indices in t.split(t.arange(full_OV_circuit.shape[0], device=device), batch_size):
        AB_slice = full_OV_circuit[indices].AB
        total += (t.argmax(AB_slice, dim=1) == indices).float().sum().item()

    return total / full_OV_circuit.shape[0]
    # END SOLUTION


# HIDE
if MAIN:
    print(f"Fraction of time that the best logit is on diagonal: {top_1_acc(full_OV_circuit):.4f}")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I'm not sure whether to take the argmax over rows or columns.</summary>

The OV circuit is defined as `W_E @ W_OV @ W_U`. We can see the i-th row `W_E[i] @ W_OV @ W_U` as the vector representing **the logit vector added at any token which attends to the `i`-th token**, via the attention head with OV matrix `W_OV`.

So we want to take the argmax over rows (i.e. over `dim=1`), because we're interested in the number of tokens `tok` in the vocabulary such that when `tok` is attended to, it is also the top prediction. 

</details>

<details>
<summary>Solution</summary>

```python
SOLUTION
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This should return about 30.79% - pretty underwhelming. It goes up to 47.73% for top-5, but still not great. What's up with that?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute effective circuit

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You shouldn't spend more than 5-10 minutes on this exercise.
> This exercise should be very short; it only requires 2 lines of code. Understanding it conceptually is more important than the actual coding.
> ```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now we return to why we have *two* induction heads. If both have the same attention pattern, the effective OV circuit is actually $W_E(W_V^{1.4}W_O^{1.4}+W_V^{1.10}W_O^{1.10})W_U$, and this is what matters. So let's re-run our analysis on this!

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/effective_ov_circuit.png" width="650">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - why might the model want to split the circuit across two heads?</summary>

Because $W_V W_O$ is a rank 64 matrix. The sum of two is a rank 128 matrix. This can be a significantly better approximation to the desired 50K x 50K matrix!
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# # YOUR CODE HERE - compute the effective OV circuit, and run `top_1_acc` on it
# END EXERCISE
# SOLUTION
W_O_both = einops.rearrange(model.W_O[1, [4, 10]], "head d_head d_model -> (head d_head) d_model")
W_V_both = einops.rearrange(model.W_V[1, [4, 10]], "head d_model d_head -> d_model (head d_head)")

W_OV_eff = W_E @ FactoredMatrix(W_V_both, W_O_both) @ W_U

print(f"Fraction of the time that the best logit is on the diagonal: {top_1_acc(W_OV_eff):.4f}")
# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Expected output</summary>

You should get an accuracy of 95.6% for top-1 - much better!

Note that you can also try top 5 accuracy, which improves your result to 98%.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## [2] QK prev-token circuit

The other easy circuit is the QK-circuit of L0H7 - how does it know to be a previous token circuit?

We can multiply out the full QK circuit via the positional embeddings:

$$
W_\text{pos} W_Q^{0.7} (W_K^{0.7})^T W_\text{pos}^T
$$

to get a matrix `pos_by_pos` of shape `[max_ctx, max_ctx]` (max ctx = max context length, i.e. maximum length of a sequence we're allowing, which is set by our choice of dimensions in $W_\text{pos}$).

Note that in this case, our max context window is 2048 (we can check this via `model.cfg.n_ctx`). This is much smaller than the 50k-size matrices we were working with in the previous section, so we shouldn't need to use the factored matrix class here.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - interpret full QK-circuit for `0.7`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You shouldn't spend more than 10-15 minutes on this exercise.
> ```

The code below plots the full QK circuit for head `0.7` (including a scaling and softmax step, which is meant to mirror how the QK bilinear form will be used in actual attention layers). You should run the code, and interpret the results in the context of the induction circuit.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

layer = 0
head_index = 7

# Compute full QK matrix (for positional embeddings)
W_pos = model.W_pos
W_QK = model.W_Q[layer, head_index] @ model.W_K[layer, head_index].T
pos_by_pos_scores = W_pos @ W_QK @ W_pos.T

# Mask, scale and softmax the scores
mask = t.tril(t.ones_like(pos_by_pos_scores)).bool()
pos_by_pos_pattern = t.where(mask, pos_by_pos_scores / model.cfg.d_head**0.5, -1.0e6).softmax(-1)

# Plot the results
print(f"Avg lower-diagonal value: {pos_by_pos_pattern.diag(-1).mean():.4f}")
imshow(
    utils.to_numpy(pos_by_pos_pattern[:200, :200]),
    labels={"x": "Key", "y": "Query"},
    title="Attention patterns for prev-token QK circuit, first 100 indices",
    width=700,
    height=600,
)
# FILTERS: ~
# imshow(
#     utils.to_numpy(pos_by_pos_pattern[:200, :200]),
#     labels={"x": "Key", "y": "Query"},
#     title="Attention patterns for prev-token QK circuit, first 100 indices",
#     width=700,
#     height=600,
#     return_fig=True,
# ).write_html(section_dir / "1212.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output (and interpretation)]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Avg lower-diagonal value: 0.9978</pre><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1212.html" width="720" height="620"></div>

The full QK circuit $W_\text{pos} W_{QK}^{0.7} W_\text{pos}^T$ has shape `[n_ctx, n_ctx]`. It is a bilinear form, with the $(i, j)$-th element representing the attention score paid by the $i$-th token to the $j$-th token. This should be very large when $j = i - 1$ (and smaller for all other values of $j$), because this is a **previous head token**. So if we softmax over $j$, we should get a lower-diagonal stripe of 1.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described-QK-v4.png" width="750">

Why is it justified to ignore token encodings? In this case, it turns out that the positional encodings have a much larger effect on the attention scores than the token encodings. If you want, you can verify this for yourself - after going through the next section (reverse-engineering K-composition), you'll have a better sense of how to perform attribution on the inputs to attention heads, and assess their importance.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## [3] K-composition circuit

We now dig into the hard part of the circuit - demonstrating the K-Composition between the previous token head and the induction head.

#### Splitting activations

We can repeat the trick from the logit attribution scores. The QK-input for layer 1 is the sum of 14 terms (2+n_heads) - the token embedding, the positional embedding, and the results of each layer 0 head. So for each head $\text{H}$ in layer 1, the query tensor (ditto key) corresponding to sequence position $i$ is:

$$
\begin{align*}
x W^\text{1.H}_Q &= (e + pe + \sum_{h=0}^{11} x^\text{0.h}) W^\text{1.H}_Q \\
&= e W^\text{1.H}_Q + pe W^\text{1.H}_Q + \sum_{h=0}^{11} x^\text{0.h} W^\text{1.H}_Q
\end{align*}
$$

where $e$ stands for the token embedding, $pe$ for the positional embedding, and $x^\text{0.h}$ for the output of head $h$ in layer 0 (and the sum of these tensors equals the residual stream $x$). All these tensors have shape `[seq, d_model]`. So we can treat the expression above as a sum of matrix multiplications `[seq, d_model] @ [d_model, d_head] -> [seq, d_head]`.

For ease of notation, I'll refer to the 14 inputs as $(y_0, y_1, ..., y_{13})$ rather than $(e, pe, x^\text{0.h}, ..., x^{h.11})$. So we have:

$$
x W^h_Q = \sum_{i=0}^{13} y_i W^h_Q
$$

with each $y_i$ having shape `[seq, d_model]`, and the sum of $y_i$s being the full residual stream $x$. Here is a diagram to illustrate:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/components.png" width="520">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - analyse the relative importance

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 15-25 minutes on these exercises.
> Most of these functions just involve indexing and einsums, but conceptual understanding / figuring out exactly what the question is asking for is the hard part!
> ```

We can now analyse the relative importance of these 14 terms! A very crude measure is to take the norm of each term (by component and position).

Note that this is a pretty dodgy metric - q and k are not inherently interpretable! But it can be a good and easy-to-compute proxy.

<details>
<summary>Question - why are Q and K not inherently interpretable? Why might the norm be a good metric in spite of this?</summary>

They are not inherently interpretable because they operate on the residual stream, which doesn't have a **privileged basis**. You could stick a rotation matrix $R$ after all of the $Q$, $K$ and $V$ weights (and stick a rotation matrix before everything that writes to the residual stream), and the model would still behave exactly the same.

The reason taking the norm is still a reasonable thing to do is that, despite the individual elements of these vectors not being inherently interpretable, it's still a safe bet that if they are larger than they will have a greater overall effect on the residual stream. So looking at the norm doesn't tell us how they work, but it does indicate which ones are more important.
</details>

Fill in the functions below:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def decompose_qk_input(cache: ActivationCache) -> Float[Tensor, "n_heads+2 posn d_model"]:
    """
    Retrieves all the input tensors to the first attention layer, and concatenates them along the
    0th dim.

    The [i, :, :]th element is y_i (from notation above). The sum of these tensors along the 0th
    dim should be the input to the first attention layer.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    y0 = cache["embed"].unsqueeze(0)  # shape (1, seq, d_model)
    y1 = cache["pos_embed"].unsqueeze(0)  # shape (1, seq, d_model)
    y_rest = cache["result", 0].transpose(0, 1)  # shape (12, seq, d_model)

    return t.concat([y0, y1, y_rest], dim=0)
    # END SOLUTION


def decompose_q(
    decomposed_qk_input: Float[Tensor, "n_heads+2 posn d_model"],
    ind_head_index: int,
    model: HookedTransformer,
) -> Float[Tensor, "n_heads+2 posn d_head"]:
    """
    Computes the tensor of query vectors for each decomposed QK input.

    The [i, :, :]th element is y_i @ W_Q (so the sum along axis 0 is just the q-values).
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_Q = model.W_Q[1, ind_head_index]

    return einops.einsum(decomposed_qk_input, W_Q, "n seq d_model, d_model d_head -> n seq d_head")
    # END SOLUTION


def decompose_k(
    decomposed_qk_input: Float[Tensor, "n_heads+2 posn d_model"],
    ind_head_index: int,
    model: HookedTransformer,
) -> Float[Tensor, "n_heads+2 posn d_head"]:
    """
    Computes the tensor of key vectors for each decomposed QK input.

    The [i, :, :]th element is y_i @ W_K(so the sum along axis 0 is just the k-values)
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_K = model.W_K[1, ind_head_index]

    return einops.einsum(decomposed_qk_input, W_K, "n seq d_model, d_model d_head -> n seq d_head")
    # END SOLUTION


if MAIN:
    # Recompute rep tokens/logits/cache, if we haven't already
    seq_len = 50
    batch_size = 1
    (rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(
        model, seq_len, batch_size
    )
    rep_cache.remove_batch_dim()

    ind_head_index = 4

    # First we get decomposed q and k input, and check they're what we expect
    decomposed_qk_input = decompose_qk_input(rep_cache)
    decomposed_q = decompose_q(decomposed_qk_input, ind_head_index, model)
    decomposed_k = decompose_k(decomposed_qk_input, ind_head_index, model)
    t.testing.assert_close(
        decomposed_qk_input.sum(0),
        rep_cache["resid_pre", 1] + rep_cache["pos_embed"],
        rtol=0.01,
        atol=1e-05,
    )
    t.testing.assert_close(
        decomposed_q.sum(0), rep_cache["q", 1][:, ind_head_index], rtol=0.01, atol=0.001
    )
    t.testing.assert_close(
        decomposed_k.sum(0), rep_cache["k", 1][:, ind_head_index], rtol=0.01, atol=0.01
    )

    # Second, we plot our results
    component_labels = ["Embed", "PosEmbed"] + [f"0.{h}" for h in range(model.cfg.n_heads)]
    for decomposed_input, name in [(decomposed_q, "query"), (decomposed_k, "key")]:
        imshow(
            utils.to_numpy(decomposed_input.pow(2).sum([-1])),
            labels={"x": "Position", "y": "Component"},
            title=f"Norms of components of {name}",
            y=component_labels,
            width=800,
            height=400,
        )
        # FILTERS: ~
        # imshow(
        #     utils.to_numpy(decomposed_input.pow(2).sum([-1])),
        #     labels={"x": "Position", "y": "Component"},
        #     title=f"Norms of components of {name}",
        #     y=component_labels,
        #     width=800,
        #     height=400,
        #     return_fig=True,
        # ).write_html(section_dir / f"1213-{name[0]}.html")
        # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>What you should see</summary>

FILTERS: st
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1213-q.html" height="420" width="820"></div>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1213-k.html" height="420" width="820"></div>
END FILTERS

You should see that the most important query components are the token and positional embeddings. The most important key components are those from $y_9$, which is $x_7$, i.e. from head `0.7`.

</details>

<details>
<summary>A technical note on the positional embeddings - optional, feel free to skip this.</summary>

You might be wondering why the tests compare the decomposed qk sum with the sum of the `resid_pre + pos_embed`, rather than just `resid_pre`. The answer lies in how we defined the transformer, specifically in this line from the config:

```python
positional_embedding_type="shortformer"
```

The result of this is that the positional embedding isn't added to the residual stream. Instead, it's added as inputs to the Q and K calculation (i.e. we calculate `(resid_pre + pos_embed) @ W_Q` and same for `W_K`), but **not** as inputs to the V calculation (i.e. we just calculate `resid_pre @ W_V`). This isn't actually how attention works in general, but for our purposes it makes the analysis of induction heads cleaner because we don't have positional embeddings interfering with the OV circuit.

**Question - this type of embedding actually makes it impossible for attention heads to form via Q-composition. Can you see why?**

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This tells us which heads are probably important, but we can do better than that. Rather than looking at the query and key components separately, we can see how they combine together - i.e. take the decomposed attention scores.

This is a bilinear function of q and k, and so we will end up with a `decomposed_scores` tensor with shape `[query_component, key_component, query_pos, key_pos]`, where summing along BOTH of the first axes will give us the original attention scores (pre-mask).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - decompose attention scores

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 5-10 minutes on this exercise.
> Having already done the previous exercises, this one should be easier.
> ```

Implement the function giving the decomposed scores (remember to scale by `sqrt(d_head)`!) For now, don't mask it.

<details>
<summary>Question - why do I focus on the attention scores, not the attention pattern? (i.e. pre softmax not post softmax)</summary>

Because the decomposition trick *only* works for things that are linear - softmax isn't linear and so we can no longer consider each component independently.
</details>

<details>
<summary>Help - I'm confused about what we're doing / why we're doing it.</summary>

Remember that each of our components writes to the residual stream separately. So after layer 1, we have:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/components.png" width="650">

We're particularly interested in the attention scores computed in head `1.4`, and how they depend on the inputs into that head. We've already decomposed the residual stream value $x$ into its terms $e$, $pe$, and $x^ 0$ through $x^{11}$ (which we've labelled $y_0, ..., y_{13}$ for simplicity), and we've done the same for key and query terms. We can picture these terms being passed into head `1.4` as:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/components-2.png" width="650">

So when we expand `attn_scores` out in full, they are a sum of $14^2 = 196$ terms - one for each combination of `(query_component, key_component)`.

---

#### Why is this decomposition useful?

We have a theory about a particular circuit in our model. We think that head `1.4` is an induction head, and the most important components that feed into this head are the prev token head `0.7` (as key) and the token embedding (as query). This is already supported by the evidence of our magnitude plots above (because we saw that `0.7` as key and token embeddings as query were large), but we still don't know how this particular key and query work **together**; we've only looked at them separately.

By decomposing `attn_scores` like this, we can check whether the contribution from combination `(query=tok_emb, key=0.7)` is indeed producing the characteristic induction head pattern which we've observed (and the other 195 terms don't really matter).
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def decompose_attn_scores(
    decomposed_q: Float[Tensor, "q_comp q_pos d_head"],
    decomposed_k: Float[Tensor, "k_comp k_pos d_head"],
    model: HookedTransformer,
) -> Float[Tensor, "q_comp k_comp q_pos k_pos"]:
    """
    Output is decomposed_scores with shape [query_component, key_component, query_pos, key_pos]

    The [i, j, 0, 0]th element is y_i @ W_QK @ y_j^T (so the sum along both first axes are the
    attention scores)
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    return einops.einsum(
        decomposed_q,
        decomposed_k,
        "q_comp q_pos d_head, k_comp k_pos d_head -> q_comp k_comp q_pos k_pos",
    ) / (model.cfg.d_head**0.5)
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_decompose_attn_scores(decompose_attn_scores, decomposed_q, decomposed_k, model)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once these tests have passed, you can plot the results:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# First plot: attention score contribution from (query_component, key_component) = (Embed, L0H7), you can replace this
# with any other pair and see that the values are generally much smaller, i.e. this pair dominates the attention score
# calculation
decomposed_scores = decompose_attn_scores(decomposed_q, decomposed_k, model)

q_label = "Embed"
k_label = "0.7"
decomposed_scores_from_pair = decomposed_scores[
    component_labels.index(q_label), component_labels.index(k_label)
]

imshow(
    utils.to_numpy(t.tril(decomposed_scores_from_pair)),
    title=f"Attention score contributions from query = {q_label}, key = {k_label}<br>(by query & key sequence positions)",
    width=700,
)


# Second plot: std dev over query and key positions, shown by component. This shows us that the other pairs of
# (query_component, key_component) are much less important, without us having to look at each one individually like we
# did in the first plot!
decomposed_stds = einops.reduce(
    decomposed_scores, "query_decomp key_decomp query_pos key_pos -> query_decomp key_decomp", t.std
)
imshow(
    utils.to_numpy(decomposed_stds),
    labels={"x": "Key Component", "y": "Query Component"},
    title="Std dev of attn score contributions across sequence positions<br>(by query & key comp)",
    x=component_labels,
    y=component_labels,
    width=700,
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1214-A.html" width="720" height="480"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1214-B.html" width="720" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I don't understand the interpretation of these plots.</summary>

The first plot tells you that the term $e W_{QK}^{1.4} (x^{0.7})^T$ (i.e. the component of the attention scores for head `1.4` where the query is supplied by the token embeddings and the key is supplied by the output of head `0.7`) produces the distinctive attention pattern we see in the induction head: a strong diagonal stripe.

Although this tells us that this this component would probably be sufficient to implement the induction mechanism, it doesn't tell us the whole story. Ideally, we'd like to show that the other 195 terms are unimportant. Taking the standard deviation across the attention scores for a particular pair of components is a decent proxy for how important this term is in the overall attention pattern. The second plot shows us that the standard deviation is very small for all the other components, so we can be confident that the other components are unimportant.

To summarise:

* The first plot tells us that the pair `(q_component=tok_emb, k_component=0.7)` produces the characteristic induction-head pattern we see in attention head `1.4`.
* The second plot confirms that this pair is the only important one for influencing the attention pattern in `1.4`; all other pairs have very small contributions.
</details>

Note that plots like the ones above are often the most concise way of presenting a summary of the important information, and understanding what to plot is a valuable skill in any model internals-based work. However, if you want to see the "full plot" which the two plots above are both simplifications of in some sense, you can run the code below, which gives you the matrix of every single pair of components' contribution to the attention scores. So the first plot above is just a slice of the full plot below, and the second plot above is just the plot below after reducing over each slice with the standard deviation operation.

(Note - the plot you'll generate below is pretty big, so you'll want to clear it after you're done with it. If your machine is still working slowly when rendering it, you can use `fig.show(config={"staticPlot": True})` to display a non-interactive version of it.)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

decomposed_scores_centered = t.tril(
    decomposed_scores - decomposed_scores.mean(dim=-1, keepdim=True)
)

decomposed_scores_reshaped = einops.rearrange(
    decomposed_scores_centered,
    "q_comp k_comp q_token k_token -> (q_comp q_token) (k_comp k_token)",
)

fig = imshow(
    decomposed_scores_reshaped,
    title="Attention score contributions from all pairs of (key, query) components",
    width=1200,
    height=1200,
    return_fig=True,
)
full_seq_len = seq_len * 2 + 1
for i in range(0, full_seq_len * len(component_labels), full_seq_len):
    fig.add_hline(y=i, line_color="black", line_width=1)
    fig.add_vline(x=i, line_color="black", line_width=1)

fig.show(config={"staticPlot": True})

# FILTERS: ~
# fig.write_html(section_dir / "1214-C.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1214-C.html" width="1220" height="1220"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Interpreting the full circuit

Now we know that head `1.4` is composing with head `0.7` via K composition, we can multiply through to create a full circuit:

$$
W_E\, W_{QK}^{1.4}\, (W_{OV}^{0.7})^T\, W_E^T
$$

and verify that it's the identity. (Note, when we say identity here, we're again thinking about it as a distribution over logits, so this should be taken to mean "high diagonal values", and we'll be using our previous metric of `top_1_acc`.)

#### Question - why should this be the identity?

<details>
<summary>Answer</summary>

This matrix is a bilinear form. Its diagonal elements $(A, A)$ are:

$$
A^T \, W_E\, W_{QK}^{1.4}\, W_{OV}^{0.7}\, W_E^T \, A = \underbrace{(A^T W_E W_Q^{1.4})}_{\text{query}} \underbrace{(A^T W_E W_{OV}^{0.7} W_K^{1.4})^T}_{\text{key}}
$$

Intuitively, the query is saying **"I'm looking for a token which followed $A$"**, and the key is saying **"I *am* a token which folllowed $A$"** (recall that $A^T W_E W_{OV}^{0.7}$ is the vector which gets moved one position forward by our prev token head `0.7`).

Now, consider the off-diagonal elements $(A, X)$ (for $X \neq A$). We expect these to be small, because the key doesn't match the query:

$$
A^T \, W_E\, W_{QK}^{1.4}\, W_{OV}^{0.7}\, W_E^T \, X = \underbrace{(\text{I'm looking for a token which followed A})}_\text{query} \boldsymbol{\cdot} \underbrace{(\text{I am a token which followed X})}_{\text{key}}
$$


Hence, we expect this to be the identity.

An illustration:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described-K-last.png" width="700">

<!-- ![kcomp_diagram_described-K.png](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/kcomp_diagram_described-K.png) -->
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute the K-comp circuit

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You shouldn't spend more than 10-20 minutes on this exercise.
> ```

Calculate the matrix above, as a `FactoredMatrix` object.

<details>
<summary>Aside about multiplying FactoredMatrix objects together.</summary>

If  `M1 = A1 @ B1` and `M2 = A2 @ B2` are factored matrices, then `M = M1 @ M2` returns a new factored matrix. This might be:

```python
FactoredMatrix(M1.AB @ M2.A, M2.B)
```

or it might be:

```python
FactoredMatrix(M1.A, M1.B @ M2.AB)
```

with these two objects corresponding to the factorisations $M = (A_1 B_1 A_2) (B_2)$ and $M = (A_1) (B_1 A_2 B_2)$ respectively.

Which one gets returned depends on the size of the hidden dimension, e.g. `M1.mdim < M2.mdim` then the factorisation used will be $M = A_1 B_1 (A_2 B_2)$.

Remember that both these factorisations are valid, and will give you the exact same SVD. The only reason to prefer one over the other is for computational efficiency (we prefer a smaller bottleneck dimension, because this determines the computational complexity of operations like finding SVD).
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def find_K_comp_full_circuit(
    model: HookedTransformer, prev_token_head_index: int, ind_head_index: int
) -> FactoredMatrix:
    """
    Returns a (vocab, vocab)-size FactoredMatrix, with the first dimension being the query side
    (direct from token embeddings) and the second dimension being the key side (going via the
    previous token head).
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_E = model.W_E
    W_Q = model.W_Q[1, ind_head_index]
    W_K = model.W_K[1, ind_head_index]
    W_O = model.W_O[0, prev_token_head_index]
    W_V = model.W_V[0, prev_token_head_index]

    Q = W_E @ W_Q
    K = W_E @ W_V @ W_O @ W_K
    return FactoredMatrix(Q, K.T)
    # END SOLUTION


# HIDE
if MAIN:
    prev_token_head_index = 7
    ind_head_index = 4
    K_comp_circuit = find_K_comp_full_circuit(model, prev_token_head_index, ind_head_index)

    tests.test_find_K_comp_full_circuit(find_K_comp_full_circuit, model)

    print(f"Token frac where max-activating key = same token: {top_1_acc(K_comp_circuit.T):.4f}")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You can also try this out for our other induction head `ind_head_index=10`, which should also return a relatively high result. Is it higher than for head `1.4` ?

<details>
<summary>Note - unlike last time, it doesn't make sense to consider the "effective circuit" formed by adding together the weight matrices for heads <code>1.4</code> and <code>1.10</code>. Can you see why?</summary>

Because the weight matrices we're dealing with here are from the QK circuit, not the OV circuit. These don't get combined in a linear way; instead we take softmax over each head's QK-circuit output individually.
</details>

## Further Exploration of Induction Circuits

I now consider us to have fully reverse engineered an induction circuit - by both interpreting the features and by reverse engineering the circuit from the weights. But there's a bunch more ideas that we can apply for finding circuits in networks that are fun to practice on induction heads, so here's some bonus content - feel free to skip to the later bonus ideas though.

### Composition scores

A particularly cool idea in the paper is the idea of [virtual weights](https://transformer-circuits.pub/2021/framework/index.html#residual-comms), or compositional scores. (Though I came up with it, so I'm deeply biased!). This is used [to identify induction heads](https://transformer-circuits.pub/2021/framework/index.html#analyzing-a-two-layer-model).

The key idea of compositional scores is that the residual stream is a large space, and each head is reading and writing from small subspaces. By default, any two heads will have little overlap between their subspaces (in the same way that any two random vectors have almost zero dot product in a large vector space). But if two heads are deliberately composing, then they will likely want to ensure they write and read from similar subspaces, so that minimal information is lost. As a result, we can just directly look at "how much overlap there is" between the output space of the earlier head and the K, Q, or V input space of the later head.

We represent the **output space** with $W_{OV}=W_V W_O$. Call matrices like this $W_A$.

We represent the **input space** with $W_{QK}=W_Q W_K^T$ (for Q-composition), $W_{QK}^T=W_K  W_Q^T$ (for K-Composition) or $W_{OV}=W_V W_O$ (for V-Composition, of the later head). Call matrices like these $W_B$ (we've used this notation so that $W_B$ refers to a later head, and $W_A$ to an earlier head).

<details>
<summary>Help - I don't understand what motivates these definitions.</summary>

Recall that we can view each head as having three input wires (keys, queries and values), and one output wire (the outputs). The different forms of composition come from the fact that keys, queries and values can all be supplied from the output of a different head.

Here is an illustration which shows the three different cases, and should also explain why we use this terminology. You might have to open it in a new tab to see it clearly.

![composition](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/composition_new.png)

</details>

How do we formalise overlap? This is basically an open question, but a surprisingly good metric is $\frac{\|W_AW_B\|_F}{\|W_B\|_F\|W_A\|_F}$ where $\|W\|_F=\sqrt{\sum_{i,j}W_{i,j}^2}$ is the Frobenius norm, the square root of the sum of squared elements. (If you're dying of curiosity as to what makes this a good metric, you can jump to the section immediately after the exercises below.)
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - calculate composition scores

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You shouldn't spend more than 15-25 minutes on these exercises.
> Writing a composition score function should be fairly easy. The harder part is getting the right weight matrices in the exercises that come after.
> ```

Let's calculate this metric for all pairs of heads in layer 0 and layer 1 for each of K, Q and V composition and plot it.

We'll start by implementing this using plain old tensors (later on we'll see how this can be sped up using the `FactoredMatrix` class). We also won't worry about batching our calculations yet; we'll just do one matrix at a time.

We've given you tensors `q_comp_scores` etc. to hold the composition scores for each of Q, K and V composition (i.e. the `[i, j]`th element of `q_comp_scores` is the Q-composition score between the output from the `i`th head in layer 0 and the input to the `j`th head in layer 1). You should complete the function `get_comp_score`, and then fill in each of these tensors.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def get_comp_score(W_A: Float[Tensor, "in_A out_A"], W_B: Float[Tensor, "out_A out_B"]) -> float:
    """
    Return the composition score between W_A and W_B.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_A_norm = W_A.pow(2).sum().sqrt()
    W_B_norm = W_B.pow(2).sum().sqrt()
    W_AB_norm = (W_A @ W_B).pow(2).sum().sqrt()

    return (W_AB_norm / (W_A_norm * W_B_norm)).item()
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_get_comp_score(get_comp_score)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed the tests, you can fill in all the composition scores. Here you should just use a for loop, iterating over all possible pairs of `W_A` in layer 0 and `W_B` in layer 1, for each type of composition. Later on, we'll look at ways to batch this computation.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# HIDE
# Get all QK and OV matrices
W_QK = model.W_Q @ model.W_K.transpose(-1, -2)
W_OV = model.W_V @ model.W_O

# Define tensors to hold the composition scores
composition_scores = {
    "Q": t.zeros(model.cfg.n_heads, model.cfg.n_heads).to(device),
    "K": t.zeros(model.cfg.n_heads, model.cfg.n_heads).to(device),
    "V": t.zeros(model.cfg.n_heads, model.cfg.n_heads).to(device),
}
# END HIDE

# EXERCISE
# # YOUR CODE HERE - fill in values of the `composition_scores` dict, using `get_comp_score`
# END EXERCISE
# SOLUTION
for i in tqdm(range(model.cfg.n_heads)):
    for j in range(model.cfg.n_heads):
        composition_scores["Q"][i, j] = get_comp_score(W_OV[0, i], W_QK[1, j])
        composition_scores["K"][i, j] = get_comp_score(W_OV[0, i], W_QK[1, j].T)
        composition_scores["V"][i, j] = get_comp_score(W_OV[0, i], W_OV[1, j])
# END SOLUTION

# HIDE
# Plot the composition scores
for comp_type in ["Q", "K", "V"]:
    plot_comp_scores(model, composition_scores[comp_type], f"{comp_type} Composition Scores")
    # FILTERS: ~
    # plot_comp_scores(
    #     model,
    #     composition_scores[comp_type],
    #     f"{comp_type} Composition Scores",
    #     filename=str(section_dir / f"1215-{comp_type}.html"),
    # )
    # END FILTERS
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1215-Q.html" width="570" height="470"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1215-K.html" width="570" height="470"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1215-V.html" width="570" height="470"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - Setting a Baseline

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You shouldn't spend more than ~10 minutes on this exercise.
> ```

To interpret the above graphs we need a baseline! A good one is what the scores look like at initialisation. Make a function that randomly generates a composition score 200 times and tries this. Remember to generate 4 `[d_head, d_model]` matrices, not 2 `[d_model, d_model]` matrices! This model was initialised with **Kaiming Uniform Initialisation**:

```python
W = t.empty(shape)
nn.init.kaiming_uniform_(W, a=np.sqrt(5))
```

(Ideally we'd do a more efficient generation involving batching, and more samples, but we won't worry about that yet.)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def generate_single_random_comp_score() -> float:
    """
    Write a function which generates a single composition score for random matrices
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_A_left = t.empty(model.cfg.d_model, model.cfg.d_head)
    W_B_left = t.empty(model.cfg.d_model, model.cfg.d_head)
    W_A_right = t.empty(model.cfg.d_model, model.cfg.d_head)
    W_B_right = t.empty(model.cfg.d_model, model.cfg.d_head)

    for W in [W_A_left, W_B_left, W_A_right, W_B_right]:
        nn.init.kaiming_uniform_(W, a=np.sqrt(5))

    W_A = W_A_left @ W_A_right.T
    W_B = W_B_left @ W_B_right.T

    return get_comp_score(W_A, W_B)
    # END SOLUTION


# HIDE
if MAIN:
    n_samples = 300
    comp_scores_baseline = np.zeros(n_samples)
    for i in tqdm(range(n_samples)):
        comp_scores_baseline[i] = generate_single_random_comp_score()

    print("\nMean:", comp_scores_baseline.mean())
    print("Std:", comp_scores_baseline.std())

    hist(
        comp_scores_baseline,
        nbins=50,
        width=800,
        labels={"x": "Composition score"},
        title="Random composition scores",
    )
# END HIDE

# FILTERS: ~
# hist(
#     comp_scores_baseline,
#     nbins=50,
#     width=800,
#     labels={"x": "Composition score"},
#     title="Random composition scores",
#     return_fig=True,
# ).write_html(str(section_dir / "1216.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1216.html" width="820" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can re-plot our above graphs with this baseline set to white. Look for interesting things in this graph!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

baseline = comp_scores_baseline.mean()
for comp_type, comp_scores in composition_scores.items():
    plot_comp_scores(model, comp_scores, f"{comp_type} Composition Scores", baseline=baseline)
    # FILTERS: ~
    # plot_comp_scores(
    #     model,
    #     comp_scores,
    #     f"{comp_type} Composition Scores",
    #     baseline=baseline,
    #     filename=str(section_dir / f"1217-{comp_type}.html"),
    # )
    # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1217-Q.html" width="570" height="470"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1217-K.html" width="570" height="470"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-12/1217-V.html" width="570" height="470"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Some interesting things to observe:</summary>

The most obvious thing that jumps out (when considered in the context of all the analysis we've done so far) is the K-composition scores. `0.7` (the prev token head) is strongly composing with `1.4` and `1.10` (the two attention heads). This is what we expect, and is a good indication that our composition scores are working as intended.

Another interesting thing to note is that the V-composition scores for heads `1.4` and `1.10` with all other heads in layer 0 are very low. In the context of the induction circuit, this is a good thing - the OV circuits of our induction heads should be operating on the **embeddings**, rather than the outputs of the layer-0 heads. (If our repeating sequence is `A B ... A B`, then it's the QK circuit's job to make sure the second `A` attends to the first `B`, and it's the OV circuit's job to project the residual vector at that position onto the **embedding space** in order to extract the `B`-information, while hopefully ignoring anything else that has been written to that position by the heads in layer 0). So once again, this is a good sign for our composition scores.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small_comp_diagram_last.png" width="900">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Theory + Efficient Implementation

So, what's up with that metric? The key is a cute linear algebra result that the squared Frobenius norm is equal to the sum of the squared singular values.

<details>
<summary>Proof</summary>

We'll give three different proofs:

---

##### Short sketch of proof

Clearly $\|M\|_F^2$ equals the sum of squared singular values when $M$ is diagonal. The singular values of $M$ don't change when we multiply it by an orthogonal matrix (only the matrices $U$ and $V$ will change, not $S$), so it remains to show that the Frobenius norm also won't change when we multiply $M$ by an orthogonal matrix. But this follows from the fact that the Frobenius norm is the sum of the squared $l_2$ norms of the column vectors of $M$, and orthogonal matrices preserve $l_2$ norms. (If we're right-multiplying $M$ by an orthogonal matrix, then we instead view this as performing orthogonal operations on the row vectors of $M$, and the same argument holds.)

---

##### Long proof

$$
\begin{aligned}
\|M\|_F^2 &= \sum_{ij}M_{ij}^2 \\
&= \sum_{ij}((USV^T)_{ij})^2 \\
&= \sum_{ij}\bigg(\sum_k U_{ik}S_{kk}V_{jk}\bigg)^2 \\
&= \sum_{ijk_1 k_2}S_{k_1 k_1} S_{k_2 k_2} U_{i k_1} U_{i k_2} V_{j k_2} V_{j k_2} \\
&= \sum_{k_1 k_2}S_{k_1 k_1} S_{k_2 k_2} \bigg(\sum_i U_{i k_1} U_{i k_2}\bigg)\bigg(\sum_j V_{j k_2} V_{j k_2}\bigg) \\
\end{aligned}
$$

Each of the terms in large brackets is actually the dot product of columns of $U$ and $V$ respectively. Since these are orthogonal matrices, these terms evaluate to 1 when $k_1=k_2$ and 0 otherwise. So we are left with:

$$
\|M\|_F^2 = \sum_{k}S_{k k}^2
$$

---

##### Cute proof which uses the fact that the squared Frobenius norm $|M|^2$ is the same as the trace of $MM^T$

$$
\|M\|_F^2 = \text{Tr}(MM^T) = \text{Tr}(USV^TVSU^T) = \text{Tr}(US^2U^T) = \text{Tr}(S^2 U^T U) = \text{Tr}(S^2) = \|S\|_F^2
$$

where we used the cyclicity of trace, and the fact that $U$ is orthogonal so $U^TU=I$ (and same for $V$). We finish by observing that $\|S\|_F^2$ is precisely the sum of the squared singular values.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
So if $W_A=U_AS_AV_A^T$, $W_B=U_BS_BV_B^T$, then $\|W_A\|_F=\|S_A\|_F$, $\|W_B\|_F=\|S_B\|_F$ and $\|W_AW_B\|_F=\|S_AV_A^TU_BS_B\|_F$. In some sense, $V_A^TU_B$ represents how aligned the subspaces written to and read from are, and the $S_A$ and $S_B$ terms weights by the importance of those subspaces.

<details>
<summary>Click here, if this explanation still seems confusing.</summary>

$U_B$ is a matrix of shape `[d_model, d_head]`. It represents **the subspace being read from**, i.e. our later head reads from the residual stream by projecting it onto the `d_head` columns of this matrix.

$V_A$ is a matrix of shape `[d_model, d_head]`. It represents **the subspace being written to**, i.e. the thing written to the residual stream by our earlier head is a linear combination of the `d_head` column-vectors of $V_A$.

$V_A^T U_B$ is a matrix of shape `[d_head, d_head]`. Each element of this matrix is formed by taking the dot product of two vectors of length `d_model`:

* $v_i^A$, a column of $V_A$ (one of the vectors our earlier head embeds into the residual stream)
* $u_j^B$, a column of $U_B$ (one of the vectors our later head projects the residual stream onto)

Let the singular values of $S_A$ be $\sigma_1^A, ..., \sigma_k^A$ and similarly for $S_B$. Then:

$$
\|S_A V_A^T U_B S_B\|_F^2 = \sum_{i,j=1}^k (\sigma_i^A \sigma_j^B)^2 \|v^A_i \cdot u^B_j\|_F^2
$$

This is a weighted sum of the squared cosine similarity of the columns of $V_A$ and $U_B$ (i.e. the output directions of the earlier head and the input directions of the later head). The weights in this sum are given by the singular values of both $S_A$ and $S_B$ - i.e. if $v^A_i$ is an important output direction, **and** $u_B^i$ is an important input direction, then the composition score will be much higher when these two directions are aligned with each other.

---

To build intuition, let's consider a couple of extreme examples.

* If there was no overlap between the spaces being written to and read from, then $V_A^T U_B$ would be a matrix of zeros (since every $v_i^A \cdot u_j^B$ would be zero). This would mean that the composition score would be zero.
* If there was perfect overlap, i.e. the span of the $v_i^A$ vectors and $u_j^B$ vectors is the same, then the composition score is large. It is as large as possible when the most important input directions and most important output directions line up (i.e. when the singular values $\sigma_i^A$ and $\sigma_j^B$ are in the same order).
* If our matrices $W_A$ and $W_B$ were just rank 1 (i.e. $W_A = \sigma_A u_A v_A^T$, and $W_B = \sigma_B u_B v_B^T$), then the composition score is $|v_A^T u_B|$, in other words just the cosine similarity of the single output direction of $W_A$ and the single input direction of $W_B$.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - batching, and using the `FactoredMatrix` class

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> This exercise is optional, and not a super important part of this section conceptually.
> It's also quite messy to rearrange our tensors in the right way! You are invited to skip it if you want.
> ```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can also use this insight to write a more efficient way to calculate composition scores - this is extremely useful if you want to do this analysis at scale! The key is that we know that our matrices have a low rank factorisation, and it's much cheaper to calculate the SVD of a narrow matrix than one that's large in both dimensions. See the [algorithm described at the end of the paper](https://transformer-circuits.pub/2021/framework/index.html#induction-heads:~:text=Working%20with%20Low%2DRank%20Matrices) (search for SVD).

So we can work with the `FactoredMatrix` class. This also provides the method `.norm()` which returns the Frobenium norm. This is also a good opportunity to bring back batching - this will sometimes be useful in our analysis. In the function below, `W_As` and `W_Bs` are both >2D factored matrices (e.g. they might represent the OV circuits for all heads in a particular layer, or across multiple layers), and the function's output should be a tensor of composition scores for each pair of matrices `(W_A, W_B)` in the >2D tensors `(W_As, W_Bs)`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def get_batched_comp_scores(W_As: FactoredMatrix, W_Bs: FactoredMatrix) -> Tensor:
    """
    Computes the compositional scores from indexed factored matrices W_As and W_Bs.

    Each of W_As and W_Bs is a FactoredMatrix object which is indexed by all but its last 2
    dimensions, i.e.:
        W_As.shape == (*A_idx, A_in, A_out)
        W_Bs.shape == (*B_idx, B_in, B_out)
        A_out == B_in

    Return: tensor of shape (*A_idx, *B_idx) where the [*a_idx, *b_idx]th element is the
    compositional score from W_As[*a_idx] to W_Bs[*b_idx].
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    # Flatten W_As into (single_A_idx, 1, A_in, A_out)
    W_As = FactoredMatrix(
        W_As.A.reshape(-1, 1, *W_As.A.shape[-2:]),
        W_As.B.reshape(-1, 1, *W_As.B.shape[-2:]),
    )
    # Flatten W_Bs into (1, single_B_idx, B_in(=A_out), B_out)
    W_Bs = FactoredMatrix(
        W_Bs.A.reshape(1, -1, *W_Bs.A.shape[-2:]),
        W_Bs.B.reshape(1, -1, *W_Bs.B.shape[-2:]),
    )

    # Compute the product, with shape (single_A_idx, single_B_idx, A_in, B_out)
    W_ABs = W_As @ W_Bs

    # Compute the norms, and return the metric
    return W_ABs.norm() / (W_As.norm() * W_Bs.norm())
    # END SOLUTION


# HIDE
if MAIN:
    W_QK = FactoredMatrix(model.W_Q, model.W_K.transpose(-1, -2))
    W_OV = FactoredMatrix(model.W_V, model.W_O)

    composition_scores_batched = dict()
    composition_scores_batched["Q"] = get_batched_comp_scores(W_OV[0], W_QK[1])
    composition_scores_batched["K"] = get_batched_comp_scores(
        W_OV[0], W_QK[1].T
    )  # Factored matrix: .T is interpreted as transpose of the last two axes
    composition_scores_batched["V"] = get_batched_comp_scores(W_OV[0], W_OV[1])

    t.testing.assert_close(composition_scores_batched["Q"], composition_scores["Q"])
    t.testing.assert_close(composition_scores_batched["K"], composition_scores["K"])
    t.testing.assert_close(composition_scores_batched["V"], composition_scores["V"])
    print("Tests passed - your `get_batched_comp_scores` function is working!")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

Suppose `W_As` has shape `(A1, A2, ..., Am, A_in, A_out)` and `W_Bs` has shape `(B1, B2, ..., Bn, B_in, B_out)` (where `A_out == B_in`).

It will be helpful to reshape these two tensors so that:

```python
W_As.shape == (A1*A2*...*Am, 1, A_in, A_out)
W_Bs.shape == (1, B1*B2*...*Bn, B_in, B_out)
```

since we can then multiply them together as `W_As @ W_Bs` (broadcasting will take care of this for us!).

To do the reshaping, the easiest way is to reshape `W_As.A` and `W_As.B`, and define a new `FactoredMatrix` from these reshaped tensors (and same for `W_Bs`).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Targeted Ablations

We can refine the ablation technique to detect composition by looking at the effect of the ablation on the attention pattern of an induction head, rather than the loss. Let's implement this!

Gotcha - by default, `run_with_hooks` removes any existing hooks when it runs. If you want to use caching, set the `reset_hooks_start` flag to False.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

seq_len = 50


def ablation_induction_score(prev_head_index: int | None, ind_head_index: int) -> float:
    """
    Takes as input the index of the L0 head and the index of the L1 head, and then runs with the
    previous token head ablated and returns the induction score for the ind_head_index now.
    """

    def ablation_hook(v, hook):
        if prev_head_index is not None:
            v[:, :, prev_head_index] = 0.0
        return v

    def induction_pattern_hook(attn, hook):
        hook.ctx[prev_head_index] = attn[0, ind_head_index].diag(-(seq_len - 1)).mean()

    model.run_with_hooks(
        rep_tokens,
        fwd_hooks=[
            (utils.get_act_name("v", 0), ablation_hook),
            (utils.get_act_name("pattern", 1), induction_pattern_hook),
        ],
    )
    return model.blocks[1].attn.hook_pattern.ctx[prev_head_index].item()


if MAIN:
    baseline_induction_score = ablation_induction_score(None, 4)
    print(f"Induction score for no ablations: {baseline_induction_score:.5f}\n")
    for i in range(model.cfg.n_heads):
        new_induction_score = ablation_induction_score(i, 4)
        induction_score_change = new_induction_score - baseline_induction_score
        print(f"Ablation score change for head {i:02}: {induction_score_change:+.5f}")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what is the interpretation of the results you're getting?</summary>

You should have found that the induction score without any ablations is about 0.68, and that most other heads don't change the induction score by much when they are ablated, except for head 7 which reduces the induction score to nearly zero.

This is another strong piece of evidence that head `0.7` is the prev token head in this induction circuit.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Bonus

### Looking for Circuits in Real LLMs

A particularly cool application of these techniques is looking for real examples of circuits in large language models. Fortunately, there's a bunch of open source ones you can play around with in the `TransformerLens` library! Many of the techniques we've been using for our 2L transformer carry over to ones with more layers.

This library should make it moderately easy to play around with these models - I recommend going wild and looking for interesting circuits!

Some fun things you might want to try:

- Look for induction heads - try repeating all of the steps from above. Do they follow the same algorithm?
- Look for neurons that erase info
    - i.e. having a high negative cosine similarity between the input and output weights
- Try to interpret a position embedding.

<details>
<summary>Positional Embedding Hint</summary>

Look at the singular value decomposition `t.svd` and plot the principal components over position space. High ones tend to be sine and cosine waves of different frequencies.
</details>

- Look for heads with interpretable attention patterns: e.g. heads that attend to the same word (or subsequent word) when given text in different languages, or the most recent proper noun, or the most recent full-stop, or the subject of the sentence, etc.
    - Pick a head, ablate it, and run the model on a load of text with and without the head. Look for tokens with the largest difference in loss, and try to interpret what the head is doing.
- Try replicating some of Kevin's work on indirect object identification.
- Inspired by the [ROME paper](https://rome.baulab.info/), use the causal tracing technique of patching in the residual stream - can you analyse how the network answers different facts?

Note: I apply several simplifications to the resulting transformer - these leave the model mathematically equivalent and doesn't change the output log probs, but does somewhat change the structure of the model and one change translates the output logits by a constant.

<details>
<summary>Model simplifications</summary>

#### Centering $W_U$

The output of $W_U$ is a $d_{vocab}$ vector (or tensor with that as the final dimension) which is fed into a softmax

#### LayerNorm Folding

LayerNorm is only applied at the start of a linear layer reading from the residual stream (eg query, key, value, mlp_in or unembed calculations)

Each LayerNorm has the functional form $LN:\mathbb{R}^n\to\mathbb{R}^n$,
$LN(x)=s(x) * w_{ln} + b_{ln}$, where $*$ is element-wise multiply and $s(x)=\frac{x-\bar{x}}{|x-\bar{x}|}$, and $w_{ln},b_{ln}$ are both vectors in $\mathbb{R}^n$

The linear layer has form $l:\mathbb{R}^n\to\mathbb{R}^m$, $l(y)=Wy+b$ where $W\in \mathbb{R}^{m\times n},b\in \mathbb{R}^m,y\in\mathbb{R}^n$

So $f(LN(x))=W(w_{ln} * s(x)+b_{ln})+b=(W * w_{ln})s(x)+(Wb_{ln}+b)=W_{eff}s(x)+b_{eff}$, where $W_{eff}$ is the elementwise product of $W$ and $w_{ln}$ (showing that elementwise multiplication commutes like this is left as an exercise) and $b_{eff}=Wb_{ln}+b\in \mathbb{R}^m$.

From the perspective of interpretability, it's much nicer to interpret the folded layer $W_{eff},b_{eff}$ - fundamentally, this is the computation being done, and there's no reason to expect $W$ or $w_{ln}$ to be meaningful on their own.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Training Your Own Toy Models

A fun exercise is training models on the minimal task that'll produce induction heads - predicting the next token in a sequence of random tokens with repeated subsequences. You can get a small 2L Attention-Only model to do this.

<details>
<summary>Tips</summary>

* Make sure to randomise the positions that are repeated! Otherwise the model can just learn the boring algorithm of attending to fixed positions
* It works better if you *only* evaluate loss on the repeated tokens, this makes the task less noisy.
* It works best with several repeats of the same sequence rather than just one.
* If you do things right, and give it finite data + weight decay, you *should* be able to get it to grok - this may take some hyper-parameter tuning though.
* When I've done this I get weird franken-induction heads, where each head has 1/3 of an induction stripe, and together cover all tokens.
* It'll work better if you only let the queries and keys access the positional embeddings, but *should* work either way.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Interpreting Induction Heads During Training

A particularly striking result about induction heads is that they consistently [form very abruptly in training as a phase change](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#argument-phase-change), and are such an important capability that there is a [visible non-convex bump in the loss curve](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-22-08-00---VmlldzoyNTI2MDM0?accessToken=r6v951q0e1l4q4o70wb2q67wopdyo3v69kz54siuw7lwb4jz6u732vo56h6dr7c2) (in this model, approx 2B to 4B tokens). I have a bunch of checkpoints for this model, you can try re-running the induction head detection techniques on intermediate checkpoints and see what happens. (Bonus points if you have good ideas for how to efficiently send a bunch of 300MB checkpoints from Wandb lol)
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Further discussion / investigation

Anthropic has written a post on [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) which goes into much deeper discussion on induction heads. The post is structured around six different points of evidence for the hypothesis that **induction heads are the main source of in-context learning in transformer models**, even large ones. Briefly, these are:

1. Transformers undergo a "phase change" where they suddenly become much better at in-context learning, and this is around the same time induction heads appear.
2. When we change the transformer's architecture to make it easier for induction heads to form, we get a corresponding improvement in in-context learning.
3. When we ablate induction heads at runtime, in-context learning gets worse.
4. We have specific examples of induction heads performing more complex in-context learning algorithms (you'll have the opportunity to investigate one of these later - **indirect object identification**).
5. We have a mechanistic explanation of induction heads, which suggests natural extensions to more general forms of in-context learning.
6. In-context learning-related behaviour is generally smoothly continuous between small and large models, suggesting that the underlying mechanism is also the same.

Here are a few questions for you:

* How compelling do you find this evidence? Discuss with your partner.
    * Which points do you find most compelling?
    * Which do you find least compelling?
    * Are there any subset of these which would be enough to convince you of the hypothesis, in the absence of others?
* In point 3, the paper observes that in-context learning performance degrades when you ablate induction heads. While we measured this by testing the model's ability to copy a duplicated random sequence, the paper used **in-context learning score** (the loss of the 500th token in the context, minus the loss on the 50th token).
    * Can you see why this is a reasonable metric?
    * Can you replicate these results (maybe on a larger model than the 2-layer one we've been using)?
* In point 4 (more complex forms of in-context learning), the paper suggests the natural extension of "fuzzy induction heads", which match patterns like `[A*][B*]...[A][B]` rather than `[A][B]...[A][B]` (where the `*` indicates some form of linguistic similarity, not necessarily being the same token).
    * Can you think of any forms this might take, i.e. any kinds of similarity which induction heads might pick up on? Can you generate examples?
'''




---
File: /infrastructure/master_files/master_1_3_1.py
---

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
[
    {"title": "TMS: Superposition in a Nonprivileged Basis", "icon": "1-circle-fill", "subtitle": "(25%)"},
    {"title": "TMS: Superposition in a Privileged Basis", "icon": "2-circle-fill", "subtitle": "(10%)"},
    {"title": "Feature Geometry", "icon": "3-circle-fill", "subtitle": "(10%)"},
    {"title": "Superposition & Deep Double Descent", "icon": "4-circle-fill", "subtitle": "(10%)"},
    {"title": "Sparse Autoencoders in Toy Models", "icon": "5-circle-fill", "subtitle": "(45%)"},
    {"title": "Bonus", "icon": "star", "subtitle": ""},
]
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# [1.3.1] Toy Models of Superposition & Sparse Autoencoders
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-13-1.png" width="350">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Superposition is a crucially important concept for understanding how transformers work. A definition from Neel Nanda's glossary:

> Superposition is when a model represents more than n features in an $n$-dimensional activation space. That is, features still correspond to directions, but **the set of interpretable directions is larger than the number of dimensions**.

Why should we expect something like this to happen? In general, the world has way more features than the model has dimensions of freedom, and so we can't have a one-to-one mapping between features and values in our model. But the model has to represent these features somehow. Hence, it comes up with techniques for cramming multiple features into fewer dimensions (at the cost of adding noise and interference between features).

In these exercises, feel free to skip sections liberally. Specifically, we recommend skipping sections 3️⃣ and 4️⃣ unless you're particularly interested in exploring those topics in depth. If your goal is to get quickly to the cutting edge of SAE research without spending too much time wading through the theory, you can jump directly to section 0️⃣ in 1.3.2 (which offers a compressed version of sections 1️⃣, 2️⃣ and 5️⃣ in this material, before moving on to studying SAEs in actual LLMs).

Unlike many other topics in this chapter, there's quite a bit of theory which needs to be understood before we start making inferences from the results of our coding experiments. A key point to make here is that, perhaps more so than any other section in this chapter, we really don't understand superposition that well at all! It's hard to point to the seminal work in this field because we don't really know what the critical new insights will look like. That being said, we hope this material gives you enough directions to pursue when you're finished!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Reading Material

* [200 COP in MI: Exploring Polysemanticity and Superposition](https://www.alignmentforum.org/posts/o6ptPu7arZrqRCxyz/200-cop-in-mi-exploring-polysemanticity-and-superposition), <b>15 mins</b>
    * Read the post, up to and including "Tips" (although some parts of it might make more sense after you've read the other things here).
* Neel Nanda's [Dynalist notes on superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2), <b>10 mins</b>
    * These aren't long, you should skim through them, and also use them as a reference during these exercises.
* Anthropic's [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html), <b>20 mins</b>
    * You should read up to & including the "Summary: A Hierarchy of Feature Properties" section.
    * The first few sections ("Key Results", "Definitions and Motivation", and "Empirical Phenomena" are particularly important).
    * We'll also be going through other parts of this paper as we work through the exercises.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Content & Learning Objectives

### 1️⃣ Toy Models of Superposition: Superposition in a Nonprivileged Basis

In this section, you'll cover the basics of Anthropic's toy models of superposition. You'll learn through examples what superposition is, why it happens, and why it presents a problem for interpreting neural networks.

> ##### Learning Objectives
>
> - Understand the concept of superposition, and how it helps models represent a larger set of features
> - Understand the difference between superposition and polysemanticity
> - Learn how sparsity contributes to superposition
> - Understand the idea of the feature importance curve
> - Learn how feature correlation changes the nature and degree of superposition

### 2️⃣ Toy Models of Superposition: Superposition in a Privileged Basis

Superposition in a privileged basis is a bit different than a nonprivileged basis. Firstly, the model has a tendency to align more with the basis directions (even though it often needs some degree of misalignment in order to represent all features). Secondly, many privileged bases are privileged because they are performing some kind of computation on their inputs, rather than just storing representations, so it's important to understand how our model is able to perform this computation.

> ##### Learning Objectives
>
> - Understand the difference between neuron vs bottleneck superposition (or computational vs representational superposition)
> - Learn how models can perform computation in superposition, via the example `f(x) = abs(x)`

### 3️⃣ Feature Geometry

The section takes a slightly deeper dive into the geometry of superposition. It's not as essential as any exercises from the previous three sections, but should still be of interest and worth doing if you have time, or just want to dive more deeply into superposition. We cover **dimensionality** (a measure of how much capacity the model is affording to a single feature), and relate this metric to the forming of increasingly complex geometric structures.

> ##### Learning Objectives
>
> - Learn about *dimensionality*, which essentially measures what fraction of a dimension is allocated to a specific feature
> - Understand the geometric intuitions behind superposition, and how they relate to the more general ideas of superposition in larger models

### 4️⃣ Superposition & Deep Double Descent

Deep Double Descent is a long observed phenomena in deep learning, wherein the model's performance on a task first improves, then plateaus, then starts to improve again with further increases in model size, data size, or training time. The plateau is in line with the predictions of classical statistics (i.e. model overfitting), but the further decrease in loss isn't. Anthropic's paper on deep double descent connects this phenomenon to the idea of superposition, essentially arguing that the two scaling phases represent a memorizing solution and a generalizing solution respectively, with the first characterized by representing *datapoints* in superposition, the second by representing *features* in superposition.

This section guides you through a replication of this paper's key results. Relative to the other material here, this is quite unguided, so we recommend you treat it more as an optional extension than an exercise on the core pathway for today.

> ##### Learning Objectives
> 
> - Understand and characterise the deep double descent phenomena
> - Relate the different phases of double descent to the idea of superposition
> - Practice replicating results from a paper in a more unguided way

### 5️⃣ Sparse Autoencoders in Toy Models

In this last section, you'll learn about sparse autoencoders, and how they might help us resolve problems of superposition. You'll train a sparse autoencoder on the toy model setup from earlier sections, and you'll also implement techniques like neuron resampling and different architectures (e.g. the Gated architecture from [DeepMind's paper](https://deepmind.google/research/publications/88147/)).

> ##### Learning Objectives
>
> - Learn about sparse autoencoders, and how they might be used to disentangle features represented in superposition
> - Train your own SAEs on the toy models from earlier sections, and visualise the feature reconstruction process
> - Understand important SAE training strategies (e.g. resampling) and architecture variants (e.g. Gated, Jump ReLU)

### ☆ Bonus

We end with a section of suggested bonus material & paper replications, like usual.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Questions

Here are a set of questions (with some brief answers provided) which you should be able to answer for yourself after reading the above material. Search for them on Neel's Dynalist notes if you didn't come across them during your reading.

What is a **privileged basis**? Why should we expect neuron activations to be privileged by default? Why *shouldn't* we expect the residual stream to be privileged?

<details>
<summary>Answer</summary>

A privileged basis is one where the **standard basis directions are meaningful** (due to the structure of computation being done on that basis). This doesn't necessarily mean that the basis is interpretable.

**Neurons**

Neuron activations are privileged because of the **elementwise nonlinear function that gets applied**. ReLU is easily described in the standard basis, e.g. in 2D: 

$$\begin{bmatrix} x \\ y \end{bmatrix} \to \begin{bmatrix} \max(x, 0) \\ \max(y, 0) \end{bmatrix}$$

but if you redefine a basis $x' = (x+y)/\sqrt{2}$, $y' = (x-y)/\sqrt{2}$, then describing ReLU in this new basis becomes really messy. More importantly, we now get interference between the components $x'$ and $y'$, i.e. the ReLU is no longer acting on them independently.

$$\begin{bmatrix} x' \\ y' \end{bmatrix} \to \frac{1}{\sqrt{2}} \begin{bmatrix} \max(x, 0) + \max(y, 0) \\ \max(x, 0) - \max(y, 0) \end{bmatrix} = \frac{1}{2} \begin{bmatrix} \max(x'+y', 0) + \max(x'-y', 0) \\ \max(x'+y', 0) - \max(x'-y', 0) \end{bmatrix}$$

**Residual stream**

The residual stream is not privileged because anything that reads from it and writes to it uses a linear map. As a thought experiment, if we changed all the writing matrices (i.e. $W_{out}$ in the MLP layers and $W_O$ in the attention layers) to $W \to W R$, and all the reading matrices (i.e. $W_{in}$ in the MLP layers and $W_Q$, $W_K$, $W_V$ in the attention layers) to $W \to W R^{-1}$ where $R$ is some arbitrary rotation matrix, then the model's computation would be unchanged. Since the matrix $R$ is arbitrary, it can change the basis in any way it wants, so that basis can't be privileged.

To put this another way - if you claimed "I think the 47th element of the residual stream encoded some special information e.g. the plurality of the noun at that sequence position", I could call bullshit on your claim, because this thought experiment shows that any basis direction could just as easily be rotated & distributed as a linear combination of several different basis directions without fundamentally changing the computation done by the transformer. The same does not apply to neurons, because a rotation / change of basis would change the nature of computation done on them.

**Summary**

**Something is a privileged basis if it is not rotation-independent**, i.e. the nature of computation done on it means that the **basis directions have some special significance.**

Common misconception: privileged basis is equivalent to interpretable basis. This is **NOT true** (although it is the case that a basis must be privileged if the individual basis directions have some interpretable meaning; this is necessary but not sufficient).

</details>

What is the difference between **superposition** and **polysemanticity**?

<details>
<summary>Answer</summary>

Polysemanticity happens when one neuron corresponds to multiple features (see [here](https://distill.pub/2020/circuits/zoom-in/#:~:text=lot%20of%20effort.-,Polysemantic%20Neurons,-This%20essay%20may) for more discussion & examples). If we only had polysemanticity, this wouldn't really be a problem for us (there might exist a basis for features s.t. each basis vector corresponds to a single feature).

Superposition is when there are **more features than dimensions**. So it implies polysemanticity (because we must have dimensions representing more than one feature), but the converse is not true.

</details>


What are the **importance** and **sparsity** of features? Do you expect more or less polysemantic neurons if sparsity is larger?

<details>
<summary>Answer</summary>

**Importance** = how useful is this feature for achieving lower loss?

**Sparsity** = how frequently is it in the input data?

If sparsity is larger, then we expect more polysemantic neurons. This is because a single neuron can afford to represent several different sparse features (usually it'll only be representing one of them at any given time, so there won't be interference).
</details>

How would you define a **feature**?

<details>
<summary>Answer</summary>

There's no single correct answer to this. Many of the definitions are unfortunately circular (e.g. "a feature is a thing which could be represented by a neuron"). A few possible definitions are this one from Neel's [Dynalist notes](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#q=feature):

> A feature is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image).

or this similar one from Chris Olah's [Distill circuits Thread](https://distill.pub/2020/circuits/zoom-in/):

> A feature is a a scalar function of the input. In this essay, neural network features are directions, and often simply individual neurons. We claim such features in neural networks are typically meaningful features which can be rigorously studied. A **meaningful feature** is one that genuinely responds to an articulable property of the input, such as the presence of a curve or a floppy ear.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup (don't read, just run)
'''

# ! CELL TYPE: code
# ! FILTERS: [~]
# ! TAGS: []

from IPython import get_ipython

ipython = get_ipython()
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: [master-comment]

# import os
# import sys
# from pathlib import Path

# IN_COLAB = "google.colab" in sys.modules

# chapter = "chapter1_transformer_interp"
# repo = "ARENA_3.0"
# branch = "main"

# # Install dependencies
# try:
#     import transformer_lens
# except:
#     %pip install einops datasets jaxtyping "sae_lens>=3.23.1" tabulate eindex-callum transformer_lens==2.11.0
#     %pip install --force-reinstall numpy pandas

# # Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
# root = (
#     "/content"
#     if IN_COLAB
#     else "/root"
#     if repo not in os.getcwd()
#     else str(next(p for p in Path.cwd().parents if p.name == repo))
# )

# if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
#     if not IN_COLAB:
#         !sudo apt-get install unzip
#         %pip install jupyter ipython --upgrade

#     if not os.path.exists(f"{root}/{chapter}"):
#         !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
#         !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
#         !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
#         !rm {root}/{branch}.zip
#         !rmdir {root}/{repo}-{branch}


# if f"{root}/{chapter}/exercises" not in sys.path:
#     sys.path.append(f"{root}/{chapter}/exercises")

# os.chdir(f"{root}/{chapter}/exercises")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import sys
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Literal

import einops
import numpy as np
import pandas as pd
import plotly.express as px
import torch as t
from IPython.display import HTML, display
from jaxtyping import Float
from torch import Tensor, nn
from torch.distributions.categorical import Categorical
from torch.nn import functional as F
from tqdm.auto import tqdm

device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part31_superposition_and_saes"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
# FILTERS: ~colab
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))
# END FILTERS

import part31_superposition_and_saes.tests as tests
import part31_superposition_and_saes.utils as utils
from plotly_utils import imshow, line

MAIN = __name__ == "__main__"

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 1️⃣ TMS: Superposition in a Nonprivileged Basis
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Toy Model setup

In this section, we'll be examining & running experiments on the toy model studied in [Anthropic's paper](https://transformer-circuits.pub/2022/toy_model/index.html).

You can follow along with the paper from the [Demonstrating Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating) section onwards; it will approximately follow the order of the sections in this notebook.

This paper presented a very rudimentary model for **bottleneck superposition** - when you try and represent more than $n$ features in a vector space of dimension $n$. The model is as follows:

* We take a 5-dimensional input $x$
* We map it down into 2D space
* We map it back up into 5D space (using the transpose of the first matrix)
* We add a bias and ReLU

$$
\begin{aligned}
h &= W x \\
x' &= \operatorname{ReLU}(W^T h + b)
\end{aligned}
$$

### What's the motivation for this setup?

The input $x$ represents our five features (they're uniformly sampled between 0 and 1).

Each feature can have **importance** and **sparsity**. Recall our earlier definitions:

* **Importance** = how useful is this feature for achieving lower loss?
* **Sparsity** = how frequently is it in the input data?

This is realised in our toy model as follows:

* **Importance** = the coefficient on the weighted mean squared error between the input and output, which we use for training the model
    * In other words, our loss function is $L = \sum_x \sum_i I_i (x_i - x_i^\prime)^2$, where $I_i$ is the importance of feature $i$.
* **Sparsity** = the probability of the corresponding element in $x$ being zero
    * In other words, this affects the way our training data is generated (see the method `generate_batch` in the `Module` class below)
    * We often refer to **feature probability** (1 minus sparsity) rather than sparsity

The justification for using $W^T W$ is as follows: we can think of $W$ (which is a matrix of shape `(2, 5)`) as a grid of "overlap values" between the features and bottleneck dimensions. The values of the 5x5 matrix $W^T W$ are the dot products between the 2D representations of each pair of features. To make this intuition clearer, imagine each of the columns of $W$ were unit vectors, then $W^T W$ would be a matrix of cosine similarities between the features (with diagonal elements equal to 1, because the similarity of a feature with itself is 1). To see this for yourself:
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

t.manual_seed(2)

W = t.randn(2, 5)
W_normed = W / W.norm(dim=0, keepdim=True)

imshow(
    W_normed.T @ W_normed,
    title="Cosine similarities of each pair of 2D feature embeddings",
    width=600,
)
# FILTERS: ~
# fig = imshow(
#     W_normed.T @ W_normed, title="Cosine similarities of each pair of 2D feature embeddings", width=600, return_fig=True
# )
# fig.write_html(section_dir / "13110.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-A.html" width="620" height="500">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To put it another way - if the columns of $W$ were orthogonal, then $W^T W$ would be the identity. This can't actually be the case because $W$ is a 2x5 matrix, but its columns can be "nearly orthgonal" in the sense of having pairwise cosine similarities close to 0.

<details>
<summary>Question - can you prove that <code>W.T @ W</code> can't be the identity when <code>W</code> has more columns than rows (or alternatively, when the hidden dimension is strictly smaller than the input dimension)?</summary>

Proof #1: the rank of a matrix product $AB$ is upper-bounded by the maximum of the two factors $A$ and $B$. In the case of $W^T W$, both matrices have rank at most 2, so the product has rank at most 2.

Proof #2: for any vector $x$, $W^T W x = W^T (Wx)$ is in the span of the columns of $W^T$, which is vector space with rank 2.

</details>

Another nice thing about using two bottleneck dimensions is that we get to visualise our output! We've got a few helper functions for this purpose.
'''

# ! CELL TYPE: code
# ! FILTERS: [~py]
# ! TAGS: []

utils.plot_features_in_2d(
    W_normed.unsqueeze(0),  # shape [instances=1 d_hidden=2 features=5]
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/13202.png" width="250">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Compare this plot to the `imshow` plot above, and make sure you understand what's going on here (and how the two plots relate to each other). A lot of the subsequent exercises run with this idea of a geometric interpretation of the model's features and bottleneck dimensions.

<details>
<summary>Help - I'm confused about how these plots work.</summary>

As mentioned, you can view $W$ as being a set of five 2D vectors, one for each of our five features. The heatmap shows us the cosine similarities between each pair of these vectors, and the second plot shows us these five vectors in 2D space.

In the example above, we can see two pairs of vectors (the 1st & 2nd, and the 0th & 4th) have very high cosine similarity. This is reflected in the 2D plot, where these features are very close to each other (the 0th feature is the darkest color, the 4th feature is the lightest).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Defining our model

Below is some code for your model (with most methods not filled out yet). It should be familiar to you if you've already built simple neural networks earlier in this course.

Some notes on the initialization method, which is filled out for you:

#### Weights & instances

The `Config` class has an `n_inst` class. This is so we can optimize multiple models at once in a single training loop (this'll be useful later on). You should treat this as basically like a batch dimension for your weights: each of your weights/biases will actually be `n_inst` separate weights/biases stacked along the zeroth dimension, and each of these will be trained independently, on different data, in parallel (using the same optimizer).

We initialize weights `W` and `b_final`, which correspond to $W$ and $b$ in the Anthropic paper.

#### Sparsity & Importance

The `feature_probability` argument tells us the probability that any given feature will be active. We have the relation  `feature_probability = 1 - sparsity`. We'll often be dealing with very small feature probabilities $p = 1 - S \approx 0$, i.e. sparsities close to 1. The feature probability is used to generate our training data; the importance is used in our loss function (see later for both of these). The default is `feature_probability = 0.01`, i.e. each feaure is present with probability 1%.

The `importance` argument is used when calculating loss (see later exercise). The default is `importance = None` which results in uniform importance.

In the `__init__` method, we have code to broadcast `feature_probability` and `importance`, so that by the end they both always have shape `(n_inst, n_features)`.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `forward`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

For now, you just need to fill in the `forward` method. As the exercises go on, you'll fill in some more of these functions, but for now you can ignore the others.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def linear_lr(step, steps):
    return 1 - (step / steps)


def constant_lr(*_):
    return 1.0


def cosine_decay_lr(step, steps):
    return np.cos(0.5 * np.pi * step / (steps - 1))


@dataclass
class ToyModelConfig:
    # We optimize n_inst models in a single training loop to let us sweep over sparsity or importance
    # curves efficiently. You should treat the number of instances `n_inst` like a batch dimension,
    # but one which is built into our training setup. Ignore the latter 3 arguments for now, they'll
    # return in later exercises.
    n_inst: int
    n_features: int = 5
    d_hidden: int = 2
    n_correlated_pairs: int = 0
    n_anticorrelated_pairs: int = 0
    feat_mag_distn: Literal["unif", "normal"] = "unif"


class ToyModel(nn.Module):
    W: Float[Tensor, "inst d_hidden feats"]
    b_final: Float[Tensor, "inst feats"]

    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)

    def __init__(
        self,
        cfg: ToyModelConfig,
        feature_probability: float | Tensor = 0.01,
        importance: float | Tensor = 1.0,
        device=device,
    ):
        super(ToyModel, self).__init__()
        self.cfg = cfg

        if isinstance(feature_probability, float):
            feature_probability = t.tensor(feature_probability)
        self.feature_probability = feature_probability.to(device).broadcast_to(
            (cfg.n_inst, cfg.n_features)
        )
        if isinstance(importance, float):
            importance = t.tensor(importance)
        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))

        self.W = nn.Parameter(
            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))
        )
        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))
        self.to(device)

    def forward(
        self,
        features: Float[Tensor, "... inst feats"],
    ) -> Float[Tensor, "... inst feats"]:
        """
        Performs a single forward pass. For a single instance, this is given by:
            x -> ReLU(W.T @ W @ x + b_final)
        """
        # SOLUTION
        h = einops.einsum(features, self.W, "... inst feats, inst hidden feats -> ... inst hidden")
        out = einops.einsum(h, self.W, "... inst hidden, inst hidden feats -> ... inst feats")
        return F.relu(out + self.b_final)
        # END SOLUTION
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE

    def generate_batch(self, batch_size: int) -> Float[Tensor, "batch inst feats"]:
        """
        Generates a batch of data of shape (batch_size, n_instances, n_features).
        """
        # You'll fill this in later
        raise NotImplementedError()

    def calculate_loss(
        self,
        out: Float[Tensor, "batch inst feats"],
        batch: Float[Tensor, "batch inst feats"],
    ) -> Float[Tensor, ""]:
        """
        Calculates the loss for a given batch (as a scalar tensor), using this loss described in the
        Toy Models of Superposition paper:

            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss

        Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.
        """
        # You'll fill this in later
        raise NotImplementedError()

    def optimize(
        self,
        batch_size: int = 1024,
        steps: int = 5_000,
        log_freq: int = 50,
        lr: float = 1e-3,
        lr_scale: Callable[[int, int], float] = constant_lr,
    ):
        """
        Optimizes the model using the given hyperparameters.
        """
        optimizer = t.optim.Adam(self.parameters(), lr=lr)

        progress_bar = tqdm(range(steps))

        for step in progress_bar:
            # Update learning rate
            step_lr = lr * lr_scale(step, steps)
            for group in optimizer.param_groups:
                group["lr"] = step_lr

            # Optimize
            optimizer.zero_grad()
            batch = self.generate_batch(batch_size)
            out = self(batch)
            loss = self.calculate_loss(out, batch)
            loss.backward()
            optimizer.step()

            # Display progress bar
            if step % log_freq == 0 or (step + 1 == steps):
                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)


if MAIN:
    tests.test_model(ToyModel)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details><summary>Solution</summary>

```python
def forward(
    self,
    features: Float[Tensor, "... inst feats"],
) -> Float[Tensor, "... inst feats"]:
    """
    Performs a single forward pass. For a single instance, this is given by:
        x -> ReLU(W.T @ W @ x + b_final)
    """
    h = einops.einsum(
        features, self.W, "... inst feats, inst hidden feats -> ... inst hidden"
    )
    out = einops.einsum(
        h, self.W, "... inst hidden, inst hidden feats -> ... inst feats"
    )
    return F.relu(out + self.b_final)
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `generate_batch`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

Next, you should implement the function `generate_batch` above. This should return a tensor of shape `(n_batch, instances, features)`, where:

* The `instances` and `features` values are taken from the model config,
* Each feature is present with probability `self.feature_probability`,
* For each present feature, its **magnitude** is sampled from a uniform distribution between 0 and 1.

Make sure you understand this function well (we recommend looking at the solutions even after you pass the tests), because we'll be making more complicated versions of this function in the section on correlations.

Remember, you can assume `model.feature_probability` has shape `(n_inst, n_features)`.

When you've implemented this function, run the code below to test it.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def generate_batch(self: ToyModel, batch_size: int) -> Float[Tensor, "batch inst feats"]:
    """
    Generates a batch of data of shape (batch_size, n_instances, n_features).
    """
    batch_shape = (batch_size, self.cfg.n_inst, self.cfg.n_features)
    feat_mag = t.rand(batch_shape, device=self.W.device)
    feat_seeds = t.rand(batch_shape, device=self.W.device)
    return t.where(feat_seeds <= self.feature_probability, feat_mag, 0.0)


ToyModel.generate_batch = generate_batch
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToyModel.generate_batch` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_generate_batch(ToyModel)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Training our model

The details of training aren't very conceptually important, so we've given you most of the code for this (in the `optimize` method). We use **learning rate schedulers** to control the learning rate as the model trains - you'll use this later on during the RL chapter.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `calculate_loss`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 5-10 minutes on this exercise.
> ```

You should fill in the `calculate_loss` function below. The loss function **for a single instance** is given by:

$$
L=\frac{1}{BF}\sum_x \sum_i I_i\left(x_i-x_i^{\prime}\right)^2
$$

where:

* $B$ is the batch size,
* $F$ is the number of features,
* $x_i$ are the inputs and $x_i'$ are the model's outputs,
* $I_i$ is the importance of feature $i$,
* $\sum_i$ is a sum over features,
* $\sum_x$ is a sum over the elements in the batch.

For the general case, we sum this formula over all instances.

<details>
<summary>Question - why do you think we take the mean over the feature and batch dimensions, but we sum over the instances dimension?</summary>

We take the mean over batch size because this is standard for loss functions (and means we don't have to use a different learning rate for different batch sizes).

We take the mean over the feature dimension because that's [normal for MSE loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).

We sum over the instances dimension because we want to train each instance independently, and at the same rate as we would train a single instance.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def calculate_loss(
    self: ToyModel,
    out: Float[Tensor, "batch inst feats"],
    batch: Float[Tensor, "batch inst feats"],
) -> Float[Tensor, ""]:
    """
    Calculates the loss for a given batch, using this loss described in the Toy Models paper:

        https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss

    Remember, `self.importance` will always have shape (n_inst, n_features).
    """
    error = self.importance * ((batch - out) ** 2)
    loss = einops.reduce(error, "batch inst feats -> inst", "mean").sum()
    return loss


ToyModel.calculate_loss = calculate_loss
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToyModel.calculate_loss` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_calculate_loss(ToyModel)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, we'll reproduce a version of the figure from the introduction. A few notes:

* The `importance` argument is the same for all instances. It takes values between 1 and ~0.66 for each feature (so for every instance, there will be some features which are more important than others).
* The `feature_probability` is the same for all features, but it varies across instances. In other words, we're runnning several different experiments at once, and we can compare the effect of having larger feature sparsity in these experiments.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=8, n_features=5, d_hidden=2)

# importance varies within features for each instance
importance = 0.9 ** t.arange(cfg.n_features)

# sparsity is the same for all features in a given instance, but varies over instances
feature_probability = 50 ** -t.linspace(0, 1, cfg.n_inst)

line(
    importance,
    width=600,
    height=400,
    title="Importance of each feature (same over all instances)",
    labels={"y": "Feature importance", "x": "Feature"},
)
line(
    feature_probability,
    width=600,
    height=400,
    title="Feature probability (varied over instances)",
    labels={"y": "Probability", "x": "Instance"},
)

model = ToyModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability[:, None],
)
model.optimize()

# COLAB-SPLIT

utils.plot_features_in_2d(
    model.W,
    colors=model.importance,
    title=f"Superposition: {cfg.n_features} features represented in 2D space",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability.squeeze()],
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output for the <code>plot_features_in_2d</code> function]]

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/13205.png" width="1300">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - interpret these diagrams

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

Remember that for all these diagrams, the darker colors have lower importance and the lighter colors have higher importance. Also, the sparsity of all features is increasing as we move from left to right (at the far left there is no sparsity, at the far right feature probability is 5% for all features, i.e. sparsity of 95%).

<details>
<summary>Hint</summary>

For low sparsity, think about what the model would learn to do if all 5 features were present all the time. What's the best our model could do in this case, and how does that relate to the **importance** values?

For high sparsity, think about what the model would learn to do if there was always exactly one feature present. Does this make interference between features less of a problem?
</details>

<details>
<summary>Answer (intuitive)</summary>

When there is no sparsity, the model can never represent more than 2 features faithfully, so it makes sense for it to only represent the two most important features. It stores them orthogonally in 2D space, and sets the other 3 features to zero. This way, it can reconstruct these two features perfectly, and ignores all the rest.

When there is high sparsity, we get a pentagon structure. Most of the time at most one of these five features will be active, which helps avoid **interference** between features. When we try to recover our initial features by projecting our point in 2D space onto these five directions, most of the time when feature $i$ is present, we can be confident that our projection onto the $i$-th feature direction only captures this feature, rather than being affected by the presence of other features. We omit the mathematical details here.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/img/ch13-sparsity-diagram-tms.png" width="900">

The key idea here is that two forces are competing in our model: **feature benefit** (representing more thing is good!), and **interference** (representing things non-orthogonally is bad). The higher the sparsity, the more we can reduce the negative impact of interference, and so the trade-off skews towards "represent more features, non-orthogonally".

</details>


We can also generate a batch and visualise its embedding. Most interestingly, you should see that in the plots with high sparsity (to the right), we very rarely have interference between the five features, because most often $\leq 1$ of those features is present, and the model can recover it by projecting along the corresponding feature dimension without losing any information.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

with t.inference_mode():
    batch = model.generate_batch(200)
    hidden = einops.einsum(
        batch,
        model.W,
        "batch instances features, instances hidden features -> instances hidden batch",
    )

utils.plot_features_in_2d(hidden, title="Hidden state representation of a random batch of data")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/13206.png" width="1400">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Visualizing features across varying sparsity

Now that we've got our pentagon plots and started to get geometric intuition for what's going on, let's scale things up! We're now operating in dimensions too large to visualise, but hopefully our intuitions will carry over.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=10, n_features=100, d_hidden=20)

importance = 100 ** -t.linspace(0, 1, cfg.n_features)
feature_probability = 20 ** -t.linspace(0, 1, cfg.n_inst)

line(
    importance,
    width=600,
    height=400,
    title="Feature importance (same over all instances)",
    labels={"y": "Importance", "x": "Feature"},
)
line(
    feature_probability,
    width=600,
    height=400,
    title="Feature probability (varied over instances)",
    labels={"y": "Probability", "x": "Instance"},
)

model = ToyModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability[:, None],
)
model.optimize(steps=10_000)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Because we can't plot features in 2D anymore, we're going to use a different kind of visualisation:

* The **bottom row plots** shows a bar graph of all the features and their corresponding embedding norms $||W_i||$.
    * As we increase sparsity, the model is able to represent more features (i.e. we have more features with embedding norms close to 1).
    * We also color the bars according to whether they're orthogonal to other features (purple) or not (yellow). So we can see that for low sparsity most features are represented orthogonally (like our left-most plots above) but as we increase sparsity we transition to all features being represented non-orthogonally (like our right-most pentagon plots above).
* The **top row plots** show us the dot products between all pairs of feature vectors (kinda like the heatmaps we plotted at the start of this section).
    * This is another way of visualising the increasing interference between features as we increase sparsity.
    * Note that all these right hand plots represent **matrices with rank at most `d_hidden=20`**. The first few are approximately submatrices of the identity (because we perfectly reconstruct 20 features and delete the rest), but the later plots start to display inference as we plot more than 20 values (the diagonals of these matrices have more than 20 non-zero elements).

See the section [Basic Results](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results) for more of an explanation of this graph and what you should interpret from it.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

utils.plot_features_in_Nd(
    model.W,
    height=800,
    width=1600,
    title="ReLU output model: n_features = 100, d_hidden = 20, I<sub>i</sub> = 0.9<sup>i</sup>",
    subplot_titles=[f"Feature prob = {i:.3f}" for i in feature_probability],
    # FILTERS: ~
    # filename=str(section_dir / "1320-C3.html"),
    # END FILTERS
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-C3.html" width="1620" height="820">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Superposition with correlation

> Note, if you're aiming to progress quickly through these exercises in order to just cover the key ideas behind superposition, this is probably the point at which you can jump to the next section! The key idea here is essentially that negative correlation between features leads to more superposition, because the model suffers less from interference (the cases when both features are active at once). If you're interested in the details and actually performing the replication, read on!

One major thing we haven't considered in our experiments is **correlation**. We could guess that superposition is even more common when features are **anticorrelated** (for a similar reason as why it's more common when features are sparse). Most real-world features are anticorrelated (e.g. the feature "this is a sorted Python list" and "this is some text in an edgy teen vampire romance novel" are probably anticorrelated - that is, unless you've been reading some pretty weird fanfics).

In this section, you'll define a new data-generating function for correlated features, and run the same experiments as in the first section.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `generate_correlated_batch`

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 20-40 minutes on this exercise.
> The exercise itself is a bit fiddly / delicate, so you should definitely look at the solutions if you get stuck.
> ```

You should now fill in the three methods `generate_correlated_features`, `generate_anticorrelated_features` and `generate_uncorrelated_features` in the `Model` class, which are created to generate correlated / anticorrelated data. We've given you a new `generate_batch` function which returns the aggregation from all of these methods.

Note, in the correlated & anticorrelated cases you can assume that the feature probability is the same for all features in each instance. We start these functions by asserting this for you, and creating a vector `p` which contains this feature probability for each instance (which is what you should use instead of `model.feature_probability`). The same is also true for the uncorrelated case, when the number of uncorrelated features we're generating is less than `cfg.n_features` (since if not, it's fine to use the full `self.feature_probability` tensor).

You'll also need to be careful with your probabilities in the anticorrelated case. For example, if you do the following for your pair of features 1 & 2:

```python
feat1_is_present = t.rand() < p
feat2_is_present = t.rand() < p & ~feat1_is_present
```

then your `feat2` probability will actually be `p * (1 - p)` rather than the intended `p`. You want to try and make both features have probability `p`, while _also_ ensuring that they are never both active at the same time! The hints provide some guidance on how you can implement this (it's a bit fiddly and not very conceptually important!).

For more details, you can read the [experimental details in Anthropic's paper](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-correlated-setup), where they describe how they setup correlated and anticorrelated sets.

<details>
<summary>Help - I'm confused about how to implement the correlated features function.</summary>

Try first creating a boolean mask of shape `(batch_size, n_inst, n_correlated_pairs)` representing whether the pair is present, then repeating that mask across feature pairs with `einops.repeat`.

</details>

<details>
<summary>Help - I'm confused about how to implement the anticorrelated features function.</summary>

Here are 2 suggested methods:

1. Create a boolean mask of shape `(batch_size, n_inst, n_anticorrelated_pairs)` with probability $2p$, which represents whether *either* feature is present - and where true, we choose the present feature uniform randomly from the pair. This works because both features will have probability $2p \times 0.5 = p$.
2. Create 2 boolean masks `M1, M2` both of shape `(batch_size, n_inst, n_anticorrelated_pairs)` with probability $p$ and $p / (1 - p)$ respectively. Set the first feature to be present where `M1` is true, and the second feature to be present where `~M1 && M2` is true. This works because the first feature will have probability $p$, and the second will have probability $\frac{(1 - p)p}{(1 - p)} = p$.

The solutions use a method like (2), but either is valid.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def generate_correlated_features(
    self: ToyModel, batch_size: int, n_correlated_pairs: int
) -> Float[Tensor, "batch inst 2*n_correlated_pairs"]:
    """
    Generates a batch of correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, one of
    them is non-zero if and only if the other is non-zero.
    """
    assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
    p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

    # EXERCISE
    # # YOUR CODE HERE!
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    feat_mag = t.rand((batch_size, self.cfg.n_inst, 2 * n_correlated_pairs), device=self.W.device)
    feat_set_seeds = t.rand((batch_size, self.cfg.n_inst, n_correlated_pairs), device=self.W.device)
    feat_set_is_present = feat_set_seeds <= p
    feat_is_present = einops.repeat(
        feat_set_is_present,
        "batch instances features -> batch instances (features pair)",
        pair=2,
    )
    return t.where(feat_is_present, feat_mag, 0.0)
    # END SOLUTION


def generate_anticorrelated_features(
    self: ToyModel, batch_size: int, n_anticorrelated_pairs: int
) -> Float[Tensor, "batch inst 2*n_anticorrelated_pairs"]:
    """
    Generates a batch of anti-correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, each
    of them can only be non-zero if the other one is zero.
    """
    assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
    p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

    assert p.max().item() <= 0.5, "For anticorrelated features, must have 2p < 1"

    # EXERCISE
    # # YOUR CODE HERE!
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    feat_mag = t.rand(
        (batch_size, self.cfg.n_inst, 2 * n_anticorrelated_pairs), device=self.W.device
    )
    seed = t.rand((batch_size, self.cfg.n_inst, n_anticorrelated_pairs), device=self.W.device)
    mask = (
        einops.rearrange(t.stack([seed, 1 - seed], dim=-1), "... feat pair -> ... (feat pair)") <= p
    )
    return feat_mag * mask
    # END SOLUTION


def generate_uncorrelated_features(self: ToyModel, batch_size: int, n_uncorrelated: int) -> Tensor:
    """
    Generates a batch of uncorrelated features.
    """
    if n_uncorrelated == self.cfg.n_features:
        p = self.feature_probability
    else:
        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

    # EXERCISE
    # # YOUR CODE HERE!
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    if n_uncorrelated == self.cfg.n_features:
        p = self.feature_probability
    else:
        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

    feat_mag = t.rand((batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device)
    feat_seeds = t.rand((batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device)
    return t.where(feat_seeds <= p, feat_mag, 0.0)
    # END SOLUTION


# HIDE
def generate_batch(self: ToyModel, batch_size) -> Float[Tensor, "batch inst feats"]:
    """
    Generates a batch of data, with optional correlated & anticorrelated features.
    """
    n_corr_pairs = self.cfg.n_correlated_pairs
    n_anti_pairs = self.cfg.n_anticorrelated_pairs
    n_uncorr = self.cfg.n_features - 2 * n_corr_pairs - 2 * n_anti_pairs

    data = []
    if n_corr_pairs > 0:
        data.append(generate_correlated_features(self, batch_size, n_corr_pairs))
    if n_anti_pairs > 0:
        data.append(generate_anticorrelated_features(self, batch_size, n_anti_pairs))
    if n_uncorr > 0:
        data.append(generate_uncorrelated_features(self, batch_size, n_uncorr))
    batch = t.cat(data, dim=-1)
    return batch


ToyModel.generate_batch = generate_batch
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The code below tests your function, by generating a large number of batches and measuring them statistically.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(
    n_inst=30, n_features=4, d_hidden=2, n_correlated_pairs=1, n_anticorrelated_pairs=1
)

feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_inst).to(device)

model = ToyModel(cfg=cfg, device=device, feature_probability=feature_probability[:, None])

# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated
batch = model.generate_batch(batch_size=100_000)
corr0, corr1, anticorr0, anticorr1 = batch.unbind(dim=-1)

assert ((corr0 != 0) == (corr1 != 0)).all(), "Correlated features should be active together"
assert ((corr0 != 0).float().mean(0) - feature_probability).abs().mean() < 0.002, (
    "Each correlated feature should be active with probability `feature_probability`"
)

assert not ((anticorr0 != 0) & (anticorr1 != 0)).any(), (
    "Anticorrelated features should never be active together"
)
assert ((anticorr0 != 0).float().mean(0) - feature_probability).abs().mean() < 0.002, (
    "Each anticorrelated feature should be active with probability `feature_probability`"
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can also visualise these features, in the form of a bar chart. You should see the correlated features always co-occurring, and the anticorrelated features never co-occurring.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated
batch = model.generate_batch(batch_size=1)
correlated_feature_batch, anticorrelated_feature_batch = batch.split(2, dim=-1)

# Plot correlated features
utils.plot_correlated_features(
    correlated_feature_batch,
    title="Correlated feature pairs: should always co-occur",
    # FILTERS: ~
    # filename=str(section_dir / "1320-E1.html"),
    # END FILTERS
)
utils.plot_correlated_features(
    anticorrelated_feature_batch,
    title="Anti-correlated feature pairs: should never co-occur",
    # FILTERS: ~
    # filename=str(section_dir / "1320-E2.html"),
    # END FILTERS
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-E1.html" width="1020" height="420">
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-E2.html" width="1020" height="420">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, let's try training our model & visualising features in 2D, when we have 2 pairs of correlated features (matching the [first row of the correlation figure](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-organization) in the Anthropic paper).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=5, n_features=4, d_hidden=2, n_correlated_pairs=2)

# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)
feature_probability = 400 ** -t.linspace(0.5, 1, cfg.n_inst)

model = ToyModel(
    cfg=cfg,
    device=device,
    feature_probability=feature_probability[:, None],
)
model.optimize(steps=10_000)

# COLAB-SPLIT

utils.plot_features_in_2d(
    model.W,
    colors=["blue"] * 2 + ["limegreen"] * 2,
    title="Correlated feature sets are represented in local orthogonal bases",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability],
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/1320-E3.png" width="1000">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - generate more correlated feature plots

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> It should just involve changing the parameters in your code above.
> ```

You should now reproduce the second and third rows from the paper's [correlation figure](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-organization). You may not get exactly the same results as the paper, but they should still roughly match (e.g. you should see no antipodal pairs in the code above, but you should see at least some when you test the anticorrelated sets, even if not all of them are antipodal). You can look at the solutions colab to see some examples.

<details>
<summary>Question - for the anticorrelated feature plots, you'll have to increase the feature probability to something like ~10%, or else you won't always form antipodal pairs. Why do you think this is?</summary>

The reason antipodal pairs are better for handling anticorrelated features is that the model can be sure only one of these antipodal pairs will be active at a time, so they won't interfere with each other. So effectively we can be sure that a maximum of 2 directions will be non-zero at a time, and those 2 directions are guaranteed to be orthogonal if they co-occur (because they're from 2 different orthogonal pairs, which lie in orthogonal subspaces to each other). So we can get zero loss. If we don't have antipodal pairs, then we'll sometimes get interference between features from different feature pairs (since their directions might be antipodal).

The key point here - antipodal pairs are only better because they handle interference better, i.e. the cases where both feature pairs are active. This happens with $O(p^2)$ probability (where $p$ is the feature probability). So for very small values of $p$, the edge that the antipodal solution has over the non-antipodal solution is much smaller, and it may end up just settling on whichever solution it finds first. 

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# # YOUR CODE HERE - generate more correlated feature plots
# END EXERCISE
# SOLUTION
# Anticorrelated feature pairs
cfg = ToyModelConfig(n_inst=5, n_features=4, d_hidden=2, n_anticorrelated_pairs=2)

# All same importance, not-super-low feature probabilities (all >10%)
feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_inst)

model = ToyModel(cfg=cfg, device=device, feature_probability=feature_probability[:, None])
model.optimize(steps=10_000)

utils.plot_features_in_2d(
    model.W,
    colors=["red"] * 2 + ["orange"] * 2,
    title="Anticorrelated feature sets are frequently represented as antipodal pairs",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability],
)

# 3 correlated feature pairs
cfg = ToyModelConfig(n_inst=5, n_features=6, d_hidden=2, n_correlated_pairs=3)

# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)
feature_probability = 100 ** -t.linspace(0.5, 1, cfg.n_inst)

model = ToyModel(cfg=cfg, device=device, feature_probability=feature_probability[:, None])
model.optimize(steps=10_000)

utils.plot_features_in_2d(
    model.W,
    colors=["blue"] * 2 + ["limegreen"] * 2 + ["purple"] * 2,
    title="Correlated feature sets are side by side if they can't be orthogonal (and sometimes we get collapse)",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability],
)
# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution (example code, and what you should find)</summary>

```python
# Anticorrelated feature pairs
cfg = ToyModelConfig(n_inst=5, n_features=4, d_hidden=2, n_anticorrelated_pairs=2)

# All same importance, not-super-low feature probabilities (all >10%)
feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_inst)

model = ToyModel(cfg=cfg, device=device, feature_probability=feature_probability[:, None])
model.optimize(steps=10_000)

utils.plot_features_in_2d(
    model.W,
    colors=["red"] * 2 + ["orange"] * 2,
    title="Anticorrelated feature sets are frequently represented as antipodal pairs",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability],
)
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/1320-E4.png" width="950">

```python
# 3 correlated feature pairs
cfg = ToyModelConfig(n_inst=5, n_features=6, d_hidden=2, n_correlated_pairs=3)

# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)
feature_probability = 100 ** -t.linspace(0.5, 1, cfg.n_inst)

model = ToyModel(cfg=cfg, device=device, feature_probability=feature_probability[:, None])
model.optimize(steps=10_000)

utils.plot_features_in_2d(
    model.W,
    colors=["blue"] * 2 + ["limegreen"] * 2 + ["purple"] * 2,
    title="Correlated feature sets are side by side if they can't be orthogonal (and sometimes we get collapse)",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability],
)
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/media-1320/1320-E5.png" width="950">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 2️⃣ TMS: Superposition in a Privileged Basis
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Introduction

So far, we've explored superposition in a model without a privileged basis. We can rotate the hidden activations arbitrarily and, as long as we rotate all the weights, have the exact same model behavior. That is, for any ReLU output model with weights
$W$, we could take an arbitrary orthogonal matrix $O$ and consider the model $W' = OW$. Since $(OW)^T(OW) = W^T W$, the result would be an identical model!

Models without a privileged basis are elegant, and can be an interesting analogue for certain neural network representations which don't have a privileged basis – word embeddings, or the transformer residual stream. But we'd also (and perhaps primarily) like to understand neural network representations where there are neurons which do impose a privileged basis, such as transformer MLP layers or conv net neurons.

Our goal in this section is to explore the simplest toy model which gives us a privileged basis. There are at least two ways we could do this: we could add an activation function or apply $L_1$ regularization to the hidden layer. We'll focus on adding an activation function, since the representation we are most interested in understanding is hidden layers with neurons, such as the transformer MLP layer.

This gives us the following "ReLU hidden layer" model. It's the simplest one we can use which is still likely to give us a privileged basis; we just take our previous setup and apply ReLU to the hidden layer.

$$
\begin{aligned}
h & =\operatorname{ReLU}(W x) \\
x^{\prime} & =\operatorname{ReLU}\left(W^T h+b\right)
\end{aligned}
$$
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `NeuronModel`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> ```

In this section, you'll replicate the [first set of results](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss:~:text=model%20and%20a-,ReLU%20hidden%20layer%20model,-%3A) in the Anthropic paper on studying superposition in a privileged basis. To do this, you'll need a new `NeuronModel` class. It can inherit most methods from the `Model` class, but you'll need to redefine the `forward` method to include an intermediate ReLU.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class NeuronModel(ToyModel):
    def forward(self, features: Float[Tensor, "... inst feats"]) -> Float[Tensor, "... inst feats"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        activations = F.relu(
            einops.einsum(
                features, self.W, "... inst feats, inst d_hidden feats -> ... inst d_hidden"
            )
        )
        out = F.relu(
            einops.einsum(
                activations, self.W, "... inst d_hidden, inst d_hidden feats -> ... inst feats"
            )
            + self.b_final
        )
        return out
        # END SOLUTION


# HIDE
if MAIN:
    tests.test_neuron_model(NeuronModel)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed these tests, you can run the cells below to train the model in the same way as before. We use 7 instances, each with 10 features (with probability ranging between $0.75$ and $0.01$ across instances), and feature importance within each instance decaying as $0.75^{i}$.

We also visualize the matrix $W$. In these plots, we make it so the top-row visualisation is of $W$ rather than $W^T W$ - we can get away with this now because (unlike before) the individual elements of $W$ *are* meaningful. We're working with a **privileged basis**, and $W$ connects features to basis-aligned neurons.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=7, n_features=10, d_hidden=5)

importance = 0.75 ** t.arange(1, 1 + cfg.n_features)
feature_probability = t.tensor([0.75, 0.35, 0.15, 0.1, 0.06, 0.02, 0.01])

model = NeuronModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability[:, None],
)
model.optimize(steps=10_000)

# COLAB-SPLIT

utils.plot_features_in_Nd(
    model.W,
    height=600,
    width=1000,
    title=f"Neuron model: {cfg.n_features=}, {cfg.d_hidden=}, I<sub>i</sub> = 0.75<sup>i</sup>",
    subplot_titles=[f"1 - S = {i:.2f}" for i in feature_probability.squeeze()],
    neuron_plot=True,
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-F1.html" width="1020" height="620">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - interpret these plots

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

The first row shows plots of $W$. The rows are features, the columns are hidden dimensions (neurons).

The second row shows stacked weight plots: in other words, each column is a neuron, and the values in a column are the exposures of the features to that particular neuron. In these plots, each feature is colored differently based on its interference with other features (dark blue means the feature is orthogonal to all other features, and lighter colors means the sum of squared dot products with other features is large).

What is your interpretation of these plots? You should discuss things like monosemanticity / polysemanticity and how this changes with increasing sparsity.

<details>
<summary>Explanation for some of these plots</summary>

**Low sparsity / high feature probability**

With very low sparsity (feature prob $\approx 1$), we get no superposition: every feature is represented faithfully by a different one of the model's neurons, or not represented at all. In other words, we have **pure monosemanticity**.

In the heatmaps, we see a diagonal plot (up to rearrangement of neurons), i.e. each of the 5 most important features has a corresponding neuron which detects that particular feature, and no other.

In the bar charts, we see this monosemanticity represented: each neuron has just one feature exposed to it.

**Medium sparsity / medium feature probability**

At intermediate values, we get some monosemantic neurons, and some polysemantic ones. You should see reoccurring block patterns like these (up to rearrangements of rows and/or columns):

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/three_two2.png" width="130">

Can you see what geometric arrangements these correspond to? The answer is in the nested dropdown below.

<details>
<summary>Answer</summary>

The 3x2 block shows 3 features embedded in 2D space. Denoting the 3 features $i, j, k$ respectively, we can see that $j$ is represented along the direction $(1, 1)$ (orthogonal to the other two), and $i, k$ are represented as $(-1, 1)$ and $(1, -1)$ respectively (antipodal pairs).

As for the 3x3 block, it's actually 3 of the 4 points from a regular tetrahedron! This hints at an important fact which we'll explore in the next (optional) set of exercises: **superposition results in features organizing themselves into geometric structures**, which often represent uniform polyhedra.

</details>

The bar chart shows some neurons are starting to become polysemantic, with exposures to more than one feature.

**High sparsity / low feature probability**

With high sparsity, all neurons are polysemantic, and most / all features are represented in some capacity. The neurons aren't orthogonal (since we have way more features than neurons), but they don't need to be orthogonal: we saw in earlier sections how high sparsity can allow us to represent more features than we had dimensions. The same is true in this case.

Note - Anthropic [finds](https://transformer-circuits.pub/2022/toy_model/index.html#privileged-basis:~:text=The%20solutions%20are%20visualized%20below) that with very high sparsity, each feature will correspond to a pair of neurons. However, you may not find this for your own plots (I didn't!). This is because - as Anthropic mention - they trained many separate instances and took the ones with smallest loss, since these models proved more difficult to optimize than others in their toy model setup.

Overall, it looks a great deal like there are **neuron-level phase changes from monosemantic to polysemantic** as we increase the sparsity, mirroring the feature phase changes we saw earlier.

</details>

Try playing around with different settings (sparsity, importance). What kind of results do you get?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise (optional) - replicate plots more faithfully

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵⚪⚪⚪⚪
> 
> You should spend up to 10-25 minutes on this exercise, if you choose to do it.
> ```

Anthropic mention in their paper that they trained 1000 instances and chose the ones which achieved lowest loss. This is why your results might have differed from theirs, especially when the sparsity is very high / feature probability is very low.

Can you implement this "choose lowest loss" method in your own class? Some suggestions:

* The most basic way would be to modify the `optimize` function to return the loss per instance, and also use a for loop to run several `optimize` calls & at the end give you the best instances for each different level of sparsity.
* A much better way would be to train more instances at once (e.g. `N` instances per level of sparsity), then for each level of sparsity you can argmax over `N` at the end to get a single instance. This will be much faster (although you'll have to be careful not to train 1000 instances at once; your GPU might not support it!).
* To get very fancy, you could even add another dimension to the weight matrices, corresponding to this `N` dimension you argmax over. Then this "taking lowest-loss instance" behavior will be automatic.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Computation in superposition

The example above was interesting, but in some ways it was also limited. The key problem here is that **the model doesn't benefit from the ReLU hidden layer**. Adding a ReLU does encourage the model to have a privileged basis, but since the model is trying to reconstruct the input (i.e. the identity, which is a linear function) it doesn't actually need to use the ReLU, and it will try anything it can to circumvent it - including learning biases which shift all the neurons into a positive regime where they behave linearly. This is a mark against using this toy model to study superposition.

To extend this point: we don't want to study boring linear functions like the identity, we want to study **how models perform (nonlinear) computation in superposition**. The MLP layer in a transformer isn't just a way to represent information faithfully and recover it; it's a way to perform computation on that information. So for this next section, we'll train a model to perform some non-linear computation. Specifically, we'll train our model to **compute the absolute value of inputs $x$**.

Our data $x$ are now sampled from the range $[-1, 1]$ rather than $[0, 1]$ (otherwise calculating the absolute value would be equivalent to reconstructing the input). This is about as simple as a nonlinear function can get, since $abs(x)$ is equivalent to $\operatorname{ReLU}(x) + \operatorname{ReLU}(-x)$. But since it's nonlinear, we can be sure that the model has to use the hidden layer ReLU.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `NeuronComputationModel`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 20-30 minutes on this exercise.
> ```

You should fill in the `NeuronComputationModel` class below. Specifically, you'll need to fill in the `forward`, `generate_batch` and `calculate_loss` methods. Some guidance:

* The model's **forward function** is different - it has a ReLU hidden layer in its forward function (as described above & in the paper).
* The model's **data** is different - see the discussion above. Your `generate_batch` function should be rewritten - it will be the same as the first version of this function you wrote (i.e. without correlations) except for one difference: the value is sampled uniformly from the range $[-1, 1]$ rather than $[0, 1]$.
* The model's **loss function** is different. Rather than computing the importance-weighted $L_2$ error between the input $x$ and output $x'$, we're computing the importance-weighted $L_2$ error between $\operatorname{abs}(x)$ and $x'$. This should just require changing one line. The `optimize` function can stay the same, but it will now be optimizing this new loss function.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class NeuronComputationModel(ToyModel):
    W1: Float[Tensor, "inst d_hidden feats"]
    W2: Float[Tensor, "inst feats d_hidden"]
    b_final: Float[Tensor, "inst feats"]

    def __init__(
        self,
        cfg: ToyModelConfig,
        feature_probability: float | Tensor = 1.0,
        importance: float | Tensor = 1.0,
        device=device,
    ):
        super(ToyModel, self).__init__()
        self.cfg = cfg

        if isinstance(feature_probability, float):
            feature_probability = t.tensor(feature_probability)
        self.feature_probability = feature_probability.to(device).broadcast_to(
            (cfg.n_inst, cfg.n_features)
        )
        if isinstance(importance, float):
            importance = t.tensor(importance)
        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))

        self.W1 = nn.Parameter(
            nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))
        )
        self.W2 = nn.Parameter(
            nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.n_features, cfg.d_hidden)))
        )
        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))
        self.to(device)

    def forward(self, features: Float[Tensor, "... inst feats"]) -> Float[Tensor, "... inst feats"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        activations = F.relu(
            einops.einsum(
                features, self.W1, "... inst feats, inst d_hidden feats -> ... inst d_hidden"
            )
        )
        out = F.relu(
            einops.einsum(
                activations, self.W2, "... inst d_hidden, inst feats d_hidden -> ... inst feats"
            )
            + self.b_final
        )
        return out
        # END SOLUTION

    def generate_batch(self, batch_size) -> Float[Tensor, "batch instances features"]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        feat_mag = (
            2 * t.rand((batch_size, self.cfg.n_inst, self.cfg.n_features), device=self.W1.device)
            - 1
        )
        feat_seed = t.rand(
            (batch_size, self.cfg.n_inst, self.cfg.n_features),
            device=self.W1.device,
        )
        batch = t.where(feat_seed < self.feature_probability, feat_mag, 0.0)
        return batch
        # END SOLUTION

    def calculate_loss(
        self,
        out: Float[Tensor, "batch instances features"],
        batch: Float[Tensor, "batch instances features"],
    ) -> Float[Tensor, ""]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        error = self.importance * ((batch.abs() - out) ** 2)
        loss = einops.reduce(error, "batch inst feats -> inst", "mean").sum()
        return loss
        # END SOLUTION


if MAIN:
    tests.test_neuron_computation_model(NeuronComputationModel)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution for <code>forward</code></summary>

```python
def forward(self, features: Float[Tensor, "... inst feats"]) -> Float[Tensor, "... inst feats"]:
    activations = F.relu(
        einops.einsum(features, self.W1, "... inst feats, inst d_hidden feats -> ... inst d_hidden")
    )
    out = F.relu(
        einops.einsum(activations, self.W2, "... inst d_hidden, inst feats d_hidden -> ... inst feats")
        + self.b_final
    )
    return out
```

</details>

<details>
<summary>Solution for <code>generate_batch</code></summary>

```python
def generate_batch(self, batch_size) -> Float[Tensor, "batch instances features"]:
    feat_mag = 2 * t.rand((batch_size, self.cfg.n_inst, self.cfg.n_features), device=self.W1.device) - 1
    feat_seed = t.rand(
        (batch_size, self.cfg.n_inst, self.cfg.n_features),
        device=self.W1.device,
    )
    batch = t.where(feat_seed < self.feature_probability, feat_mag, 0.0)
    return batch
```

</details>

<details>
<summary>Solution for <code>calculate_loss</code></summary>

```python
def calculate_loss(
    self,
    out: Float[Tensor, "batch instances features"],
    batch: Float[Tensor, "batch instances features"],
) -> Float[Tensor, ""]:
    error = self.importance * ((batch.abs() - out) ** 2)
    loss = einops.reduce(error, "batch inst feats -> inst", "mean").sum()
    return loss
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed these tests, you can run the code below to make the same visualisation as above.

You should see similar patterns: with very low sparsity most/all neurons are monosemantic, but more polysemantic neurons appear as sparsity increases (until all neurons are polysemantic). Another interesting observation: in the monosemantic (or mostly monosemantic) cases, for any given feature there will be some neurons which have positive exposures to that feature and others with negative exposure. This is because some neurons are representing the value $\operatorname{ReLU}(x_i)$ and others are representing the value of $\operatorname{ReLU}(-x_i)$ (as discussed above, we require both of these to compute the absolute value).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=7, n_features=100, d_hidden=40)

importance = 0.8 ** t.arange(1, 1 + cfg.n_features)
feature_probability = t.tensor([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001])

model = NeuronComputationModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability[:, None],
)
model.optimize()

# COLAB-SPLIT

utils.plot_features_in_Nd(
    model.W1,
    height=800,
    width=1600,
    title=f"Neuron computation model: n_features = {cfg.n_features}, d_hidden = {cfg.d_hidden}, I<sub>i</sub> = 0.75<sup>i</sup>",
    subplot_titles=[f"1 - S = {i:.3f}" for i in feature_probability.squeeze()],
    neuron_plot=True,
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-F2.html" width="1620" height="820">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To further confirm that this is happening, we can color the values in the bar chart discretely by feature, rather than continuously by the polysemanticity of that feature. We'll use a feature probability of 50% for this visualisation, which is high enough to make sure each neuron is monosemantic. You should find that the input weights $W_1$ form pairs of antipodal neurons (i.e. ones with positive / negative exposures to that feature direction), but both of these neurons have positive output weights $W_2$ for that feature.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=6, n_features=20, d_hidden=10)

importance = 0.8 ** t.arange(1, 1 + cfg.n_features)
feature_probability = 0.5

model = NeuronComputationModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability,
)
model.optimize()

# COLAB-SPLIT

utils.plot_features_in_Nd_discrete(
    W1=model.W1,
    W2=model.W2,
    title="Neuron computation model (colored discretely, by feature)",
    legend_names=[
        f"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}" for i in range(cfg.n_features)
    ],
    # FILTERS: ~
    # filename=str(section_dir / "1320-F3-b.html"),
    # END FILTERS
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-F3-b.html" width="1420" height="620">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Bonus - the asymmetric superposition motif

In the [Asymmetric Superposition Motif](https://transformer-circuits.pub/2022/toy_model/index.html#computation-asymmetric-motif) section from Anthropic's paper, they discuss a particular quirk of this toy model in detail. Their section explains it in more detail than we will here (including some visual explanations), but we'll provide a relatively brief explanation here.

> When we increase sparsity in our model & start to get superposed features, we don't always have monosemantic neurons which each calculate either $\operatorname{ReLU}(x_i)$ or $\operatorname{ReLU}(-x_i)$ for some feature $i$. Instead, we sometimes have **asymmetric superposition**, where a single neuron detects two different features $i$ and $j$, and stores these features with different magnitudes (assume the $W_1$ vector for feature $i$ is much larger). The $W_2$ vectors have flipped magnitudes (i.e. the vector for $j$ is much larger). When $i$ is present and $j$ is not, there's no problem, because the output for feature $i$ is `large * small` (correct size) and for $j$ is `small * small` (near zero). But when $j$ is present and $i$ is not, the output for feature $j$ is `small * large` (correct size) and for $i$ is `large * large` (much larger than it should be). In particular, this is bad when the sign of output for $i$ is positive. The model fixes this by repurposing another neuron to correct for the case when $j$ is present and $i$ is not. We omit the exact mechanism, but it takes advantage of the fact that the model has a ReLU at the very end, so it doesn't matter if output for a feature is very large and negative (the loss will be truncated at zero), but being large and positive is very bad.

You should read the linked section of the Anthropic paper for details. We've given you code below to replicate the results of this plot - note that some plots will display the kind of asymmetric superposition described above, whereas others will simply have a single pair of neurons for each feature - you might have to fun a few random seeds to observe something exactly resembling Anthropic's plots. Can you understand what the output represents? Can you play around with the hyperparameters to see how this behaviour varies (e.g. with different feature probability or importance)?
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_inst=6, n_features=10, d_hidden=10)

importance = 0.8 ** t.arange(1, 1 + cfg.n_features)
feature_probability = (
    0.35  # slightly lower feature probability, to encourage a small degree of superposition
)

model = NeuronComputationModel(
    cfg=cfg,
    device=device,
    importance=importance[None, :],
    feature_probability=feature_probability,
)
model.optimize()

# COLAB-SPLIT

utils.plot_features_in_Nd_discrete(
    W1=model.W1,
    W2=model.W2,
    title="Neuron computation model (colored discretely, by feature)",
    legend_names=[
        f"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}" for i in range(cfg.n_features)
    ],
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-F4-b.html" width="1420" height="620">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Summary - what have we learned?

With toy models like this, it's important to make sure we take away generalizable lessons, rather than just details of the training setup.

The core things to take away form this paper are:

* What superposition is
* How it varies over feature importance and sparsity
* How it varies when we have correlated or anticorrelated features
* The difference between neuron and bottleneck superposition (or equivalently "computational and representational supervision")
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 3️⃣ Feature Geometry
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
> Note - this section is optional, since it goes into quite extreme detail about the specific problem setup we're using here. If you want, you can jump to the next section.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Dimensionality

We've seen that superposition can allow a model to represent extra features, and that the number of extra features increases as we increase sparsity. In this section, we'll investigate this relationship in more detail, discovering an unexpected geometric story: features seem to organize themselves into geometric structures such as pentagons and tetrahedrons!

The code below runs a third experiment, with all importances the same. We're first interested in the number of features the model has learned to represent. This is well represented with the squared **Frobenius norm** of the weight matrix $W$, i.e. $||W||_F^2 = \sum_{ij}W_{ij}^2$.

<details>
<summary>Question - can you see why this is a good metric for the number of features represented?</summary>

By reordering the sums, we can show that the squared Frobenius norm is the sum of the squared norms of each of the 2D embedding vectors:

$$
\big\|W\big\|_F^2 = \sum_j \left(\sum_i W_{ij}^2\right) = \sum_{j}\big\|W_{[:, j]}\big\|^2
$$

Each of these embedding vectors has squared norm approximately $1$ if a feature is represented, and $0$ if it isn't. So this is roughly the total number of represented features.
</details>

If you run the code below, you'll also plot the total number of "dimensions per feature", $m/\big\|W\big\|_F^2$.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

cfg = ToyModelConfig(n_features=200, d_hidden=20, n_inst=20)

# For this experiment, use constant importance across features (but still vary sparsity across instances)
feature_probability = 20 ** -t.linspace(0, 1, cfg.n_inst)

model = ToyModel(
    cfg=cfg,
    device=device,
    feature_probability=feature_probability[:, None],
)
model.optimize(steps=10_000)

# COLAB-SPLIT

utils.plot_feature_geometry(model)

# FILTERS: ~
# utils.plot_feature_geometry(model, filename=str(section_dir / "1320-G1.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-G1.html" width="1020" height="620"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Surprisingly, we find that this graph is "sticky" at $1$ and $1/2$. On inspection, the $1/2$ "sticky point" seems to correspond to a precise geometric arrangement where features come in "antipodal pairs", each being exactly the negative of the other, allowing two features to be packed into each hidden dimension. It appears that antipodal pairs are so effective that the model preferentially uses them over a wide range of the sparsity regime.

It turns out that antipodal pairs are just the tip of the iceberg. Hiding underneath this curve are a number of extremely specific geometric configurations of features.

How can we discover these geometric configurations? Consider the following metric, which the authors named the **dimensionality** of a feature:

$$
D_i = \frac{\big\|W_i\big\|^2}{\sum_{j} \big( \hat{W_i} \cdot W_j \big)^2}
$$

Intuitively, this is a measure of what "fraction of a dimension" a specific feature gets. Let's try and get a few intuitions for this metric:

* It's never less than zero.
    * It's equal to zero if and only if the vector is the zero vector, i.e. the feature isn't represented.
* It's never greater than one (because when $j = i$, the term in the denominator sum is equal to the numerator).
    * It's equal to one if and only if the $i$-th feature vector $W_i$ is orthogonal to all other features (because then $j=i$ is the only term in the denominator sum).
    * Intuitively, in this case the feature has an entire dimension to itself.
* If there are $k$ features which are all parallel to each other, and orthogonal to all others, then they "share" the dimensionality equally, i.e. $D_i = 1/k$ for each of them.
* The sum of all $D_i$ can't be greater than the total number of features $m$, with equality if and only if all the vectors are orthogonal.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute dimensionality

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

Remember, $W$ has shape `(n_inst, d_hidden, n_features)`. The vectors $W_i$ refer to the feature vectors (i.e. they have length `d_hidden`), and you should broadcast your calculations over the `n_inst` dimension.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

@t.inference_mode()
def compute_dimensionality(
    W: Float[Tensor, "n_inst d_hidden n_features"],
) -> Float[Tensor, "n_inst n_features"]:
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    W_norms = W.norm(dim=1, keepdim=True)
    numerator = W_norms.squeeze() ** 2

    # Compute denominator terms
    W_normalized = W / (W_norms + 1e-8)
    denominator = einops.einsum(W_normalized, W, "i h f1, i h f2 -> i f1 f2").pow(2).sum(-1)

    return numerator / denominator
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_compute_dimensionality(compute_dimensionality)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The code below plots the fractions of dimensions, as a function of increasing levels of sparsity across our instances.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

W = model.W.detach()
dim_fracs = compute_dimensionality(W)

utils.plot_feature_geometry(model, dim_fracs=dim_fracs)

# FILTERS: ~
# utils.plot_feature_geometry(model, dim_fracs=dim_fracs, filename=str(section_dir / "1320-G2.html"))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-G2.html" width="1020" height="620"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What's going on here? It turns out that the model likes to create specific weight geometries and kind of jumps between the different configurations. For example:

* With zero (or very small) sparsity, the feature basis isn't privileged by anything, and so the model represents features with arbitrary directions instead. There's no reason for some features to be represented faithfully and others not to be.
* When we get to higher levels of sparsity, the feature basis becomes privileged. So the model phase-transitions to representing some features in antipodal pairs, and the rest aren't interpreted.
* With further increases in sparsity, we transition to different geometries (see diagram below).

The moral? Superposition is very hard to pin down! There are many points between a dimensionality of 0 (not learning a feature) and 1 (dedicating a dimension to a feature). As an analogy, we often think of water as only having three phases: ice, water and steam. But this is a simplification: there are actually many phases of ice, often corresponding to different crystal structures (eg. hexagonal vs cubic ice). In a vaguely similar way, neural network features seem to also have many other phases within the general category of "superposition."

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/grid_all.png" width="900">

Note that we should take care not to read too much significance into these results. A lot of it depends delicately on the details of our experimental setup (e.g. we used $W^T W$, a positive semidefinite matrix, and there's a correspondence between low-dimensional symmetric pos-semidef matrices like these and the kinds of polytopes that we've seen in the plots above). But hopefully this has given you a sense of the relevant considerations when it comes to packing features into fewer dimensions.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 4️⃣ Superposition & Deep Double Descent
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
> Note - this is less of a structured exercise set, and more of a guided replication. If you're interested in improving your ability to replicate papers (especially those concerning toy models and some low level maths & ML) then we recommend you try them. If you're more interested in moving through exercise sets with fast feedback loops & learning all you can about superposition & SAEs, then you might just want to read the key results from this section (or skip it altogether).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
For this suggested replication, we'll look at the [Anthropic paper](https://transformer-circuits.pub/2023/toy-double-descent/index.html) on Double Descent & superposition. This paper ties the phenomena of [double descent](https://openai.com/research/deep-double-descent) to models of superposition. The theory posed by this paper goes roughly as follows:

* Initially, the model learns a **memorising solution** where datapoints are represented in superposition. This doesn't generalize, so we get low training loss but high test loss.
* Later, the model learns a **generalizing solution** where features are learned and represented in superposition. This generalizes, so we get low training loss and low test loss.
* The spike in loss between these two happens when the model transitions between the memorising and generalizing solutions.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ddd-superposn.png" width="700">

What does it mean to represent datapoints in superposition? If you've done the exercises on correlated / anticorrelated features in an earlier section, you'll know that anticorrelated features are easier to represent in superposition because they don't interfere with each other. This is especially true if features aren't just anticorrelated but are **mutually exclusive**. From the Anthropic paper:

> Consider the case of a language model which verbatim memorizes text. How can it do this? One naive idea is that it might use neurons to create a lookup table mapping sequences to arbitrary continuations. For every sequence of tokens it wishes to memorize, it could dedicate one neuron to detecting that sequence, and then implement arbitrary behavior when it fires. The problem with this approach is that it's extremely inefficient – but it seems like a perfect candidate for superposition, since each case is mutually exclusive and can't interfere.

We'll study this theory in the context of a toy model. Specifically, we'll use the toy model that we worked with in the first section of this paper, but we'll train it in a very particular way: by generating a random batch of data, and then using that same batch for the entire training process. We'll see what happens when the batch sizes change, but the number of features change. According to our theory, the model should represent datapoints in superposition when the batch size is smaller than the number of features, and it should represent features in superposition when the batch size is larger than the number of features.

Rather than giving you a set of exercises to complete, we're leaving this section open-ended. You should consider it more as a paper replication than a set of structured exercises. However, we will give you a few tips:

* Rather than using the Adam optimizer, the paper recommends AdamW, with a default weight decay of `WEIGHT_DECAY = 1e-2`.
    * Weight decay constrains the norm of weights, so that they don't grow too large. With no weight decay, we could in theory memorize an arbitrarily large number of datapoints and represent them evenly spaced around the unit circle; then we can perfectly reconstruct them as long as we have a large enough weight vector to project them onto.
* The paper recommends a learning rate consisting of a linear warmup up to `NUM_WARMUP_STEPS = 2500` (i.e. we increase the learning rate linearly from zero up to `LEARNING_RATE = 1e-3`), followed by cosine decay until the end of training at `NUM_BATCH_UPDATES = 50_000`.
* The paper recommends using a sparsity of 0.999 for the features, and 10,000 features total. However, we recommend instead using `SPARSITY = 0.99` and `N_FEATURES = 1000` (following the replication by Marius Hobbhahn). This will cause our model to train faster, while still observing fundamentally the same patterns.
* When generating the batch of data, you should normalize it (so each vector for a given batch index & instance has unit norm). The rest of the data generation process should be the same as in the first section of this notebook.
* Technically you only need one instance. However, we recommend using a few (e.g. 5-10) so you can pick the instance with lowest loss at the end of training. This is because (thanks to our best frend randomness) not all instances will necessarily learn the optimal solution. In our implementation (code below), we rewrite the `optimize` function to return `(batch_inst, W_inst)` at the end, where `batch_inst` is the batch which had the lowest loss by the end of training, and `W_inst` are the learned weights for that same instance. This is precisely the data you'll need to make the 2D feature plots featured in the paper.
* You can repurpose the function to calculate **dimensionality** from the section on feature geometry. See the paper for a discussion of a generalized dimensionality function, which doesn't just measure dimensionality of features, but also of datapoints.

To get you started, here are some constants which you might find useful:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

NUM_WARMUP_STEPS = 2500
NUM_BATCH_UPDATES = 50_000

WEIGHT_DECAY = 1e-2
LEARNING_RATE = 1e-3

BATCH_SIZES = [3, 5, 6, 8, 10, 15, 30, 50, 100, 200, 500, 1000, 2000]

N_FEATURES = 1000
N_INSTANCES = 5
N_HIDDEN = 2
SPARSITY = 0.99
FEATURE_PROBABILITY = 1 - SPARSITY

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Also, if you want some help with the visualisation, the code below will produce the 2D feature visualisations like those found at the bottom of [this figure](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/fig-2d.png), for all batch sizes stacked horizontally, assuming:

* `features_list` is a list of detached `W`-matrices for single instances, i.e. each has shape `(2, n_features)` (these will be used to produce the blue plots on the first row)
* `data_list` is a list of the projections of our batch of data onto the hidden directions of that same instance, i.e. each has shape `(2, batch_size)` (these will be used to produce the red plots on the second row)

A demonstration is given below (the values are meaningless, they've just been randomly generated to illustrate how to use this function).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

features_list = [t.randn(size=(2, 100)) for _ in BATCH_SIZES]
hidden_representations_list = [t.randn(size=(2, batch_size)) for batch_size in BATCH_SIZES]

utils.plot_features_in_2d(
    features_list + hidden_representations_list,
    colors=[["blue"] for _ in range(len(BATCH_SIZES))] + [["red"] for _ in range(len(BATCH_SIZES))],
    title="Double Descent & Superposition (num features = 100)",
    subplot_titles=[f"Features (batch={bs})" for bs in BATCH_SIZES]
    + [f"Data (batch={bs})" for bs in BATCH_SIZES],
    allow_different_limits_across_subplots=True,
    n_rows=2,
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We've provided an implementation below, although we recommend that you give it a go yourself first before looking at too much of the code. You can use the different dropdowns to get different degrees of hints, if you're struggling.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# EXERCISE
# # YOUR CODE HERE - replicate the results from the Superposition & Deep Double Descent paper!
# END EXERCISE
# SOLUTION
import math
from typing import Any

import pandas as pd
import plotly.express as px

NUM_WARMUP_STEPS = 2500
NUM_BATCH_UPDATES = 50_000
# EVAL_N_DATAPOINTS = 1_000

WEIGHT_DECAY = 1e-2
LEARNING_RATE = 1e-3

BATCH_SIZES = [3, 4, 5, 6, 8, 10, 15, 20, 30, 50, 100, 200, 300, 500, 1000, 2000, 3000]
# SMALLER_BATCH_SIZES = [3, 6, 10, 30, 100, 500, 2000]

N_FEATURES = 1000
N_INSTANCES = 10
D_HIDDEN = 2
SPARSITY = 0.99
FEATURE_PROBABILITY = 1 - SPARSITY


def linear_warmup_lr(step, steps):
    """Increases linearly from 0 to 1."""
    return step / steps


def anthropic_lr(step, steps):
    """As per the description in the paper: 2500 step linear warmup, followed by cosine decay to zero."""
    if step < NUM_WARMUP_STEPS:
        return linear_warmup_lr(step, NUM_WARMUP_STEPS)
    else:
        return cosine_decay_lr(step - NUM_WARMUP_STEPS, steps - NUM_WARMUP_STEPS)


class DoubleDescentModel(ToyModel):
    W: Float[Tensor, "inst d_hidden feats"]
    b_final: Float[Tensor, "inst feats"]
    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)

    @classmethod
    def dimensionality(
        cls, data: Float[Tensor, "... batch d_hidden"]
    ) -> Float[Tensor, "... batch"]:
        """
        Calculates dimensionalities of data. Assumes data is of shape ... batch d_hidden, i.e. if
        it's 2D then it's a batch of vectors of length `d_hidden` and we return the dimensionality
        as a 1D tensor of length `batch`. If it has more dimensions at the start, we assume this
        means separate calculations for each of these dimensions (i.e. they are independent batches
        of vectors).
        """
        # Compute the norms of each vector (this will be the numerator)
        squared_norms = einops.reduce(data.pow(2), "... batch d_hidden -> ... batch", "sum")
        # Compute the denominator (i.e. get the dot product then sum over j)
        data_normed = data / data.norm(dim=-1, keepdim=True)
        interference = einops.einsum(
            data_normed, data, "... batch_i d_hidden, ... batch_j d_hidden -> ... batch_i batch_j"
        )
        polysemanticity = einops.reduce(
            interference.pow(2), "... batch_i batch_j -> ... batch_i", "sum"
        )
        assert squared_norms.shape == polysemanticity.shape

        return squared_norms / polysemanticity

    def generate_batch(self, batch_size: int) -> Float[Tensor, "batch inst feats"]:
        """
        New function for generating batch, so we can normalize it.
        """
        # Get batch from prev method
        batch = super().generate_batch(batch_size)

        # Normalize the batch (i.e. so each vector for a particular batch & instance has norm 1)
        # (need to be careful about vectors with norm zero)
        norms = batch.norm(dim=-1, keepdim=True)
        norms = t.where(norms.abs() < 1e-6, t.ones_like(norms), norms)
        batch_normed = batch / norms
        return batch_normed

    def calculate_loss(
        self,
        out: Float[Tensor, "batch inst feats"],
        batch: Float[Tensor, "batch inst feats"],
        per_inst: bool = False,
    ) -> Float[Tensor, "inst"]:
        """
        New function to calculate loss, because we need a "loss per instance" option to find the
        best instance at the end of our optimization.
        """
        error = self.importance * ((batch - out) ** 2)
        loss = einops.reduce(error, "batch inst feats -> inst", "mean")
        return loss if per_inst else loss.sum()

    def optimize(
        self,
        batch_size: int,
        steps: int = NUM_BATCH_UPDATES,
        log_freq: int = 100,
        lr: float = LEARNING_RATE,
        lr_scale: Callable[[int, int], float] = anthropic_lr,
        weight_decay: float = WEIGHT_DECAY,
    ) -> tuple[Tensor, Tensor]:
        optimizer = t.optim.AdamW(list(self.parameters()), lr=lr, weight_decay=weight_decay)

        progress_bar = tqdm(range(steps))

        # Same batch for each step
        batch = self.generate_batch(batch_size)  # [batch_size inst n_features]

        for step in progress_bar:
            # Update learning rate
            step_lr = lr * lr_scale(step, steps)
            for group in optimizer.param_groups:
                group["lr"] = step_lr

            # Optimize
            optimizer.zero_grad()
            out = self.forward(batch)
            loss = self.calculate_loss(out, batch)
            loss.backward()
            optimizer.step()

            # Display progress bar
            if (step % log_freq == 0) or (step + 1 == steps):
                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)

        # Generate one final batch to compute the loss (we want only the best instance!)
        with t.inference_mode():
            out = self.forward(batch)
            loss_per_inst = self.calculate_loss(out, batch, per_inst=True)
            best_inst = loss_per_inst.argmin()
            print(f"Best instance = #{best_inst}, with loss {loss_per_inst[best_inst].item():.4e}")

        return batch[:, best_inst], self.W[best_inst].detach()


if MAIN:
    # ! Results, part 1/2

    features_list = []
    hidden_representations_list = []

    for batch_size in tqdm(BATCH_SIZES):
        # Define our model
        cfg = ToyModelConfig(n_features=N_FEATURES, n_inst=N_INSTANCES, d_hidden=D_HIDDEN)
        model = DoubleDescentModel(cfg, feature_probability=FEATURE_PROBABILITY).to(device)

        # Optimize, and return the best batch & weight matrix
        batch_inst, W_inst = model.optimize(steps=15_000, batch_size=batch_size)

        # Calculate the hidden feature representations, and add both this and weight matrix to our
        # lists of data
        with t.inference_mode():
            hidden = einops.einsum(
                batch_inst, W_inst, "batch features, hidden features -> hidden batch"
            )
        features_list.append(W_inst.cpu())
        hidden_representations_list.append(hidden.cpu())

    utils.plot_features_in_2d(
        features_list + hidden_representations_list,
        colors=[["blue"] for _ in range(len(BATCH_SIZES))]
        + [["red"] for _ in range(len(BATCH_SIZES))],
        title="Double Descent & Superposition (num features = 1000)",
        subplot_titles=[f"Features (batch={bs})" for bs in BATCH_SIZES]
        + [f"Data (batch={bs})" for bs in BATCH_SIZES],
        allow_different_limits_across_subplots=True,
        n_rows=2,
    )

    # ! Results, part 2/2

    df_data = {"Batch size": [], "Dimensionality": [], "Data": []}

    for batch_size, model_W, hidden in zip(BATCH_SIZES, features_list, hidden_representations_list):
        # Get x-axis data (batch size), and color (blue or red)
        df_data["Batch size"].extend([batch_size] * (N_FEATURES + batch_size))
        df_data["Data"].extend(["features"] * N_FEATURES + ["hidden"] * batch_size)

        # Calculate dimensionality of model.W[inst].T, which has shape [d_hidden=2 N_FEATURES]
        feature_dim = DoubleDescentModel.dimensionality(model_W.T)
        assert feature_dim.shape == (N_FEATURES,)

        # Calculate dimensionality of model's batch data hidden representation.
        # This has shape [d_hidden=2 batch_size]
        data_dim = DoubleDescentModel.dimensionality(hidden.T)
        assert data_dim.shape == (batch_size,)

        # Add them both to the data
        df_data["Dimensionality"].extend(feature_dim.tolist() + data_dim.tolist())

    df = pd.DataFrame(df_data)
    eps = 0.01
    xline1, xline2 = (100 * 200) ** 0.5, (500 * 1000) ** 0.5
    vrect_kwargs: dict[str, Any] = dict(opacity=0.5, layer="below", line_width=0)
    xrange = [math.log10(1.5), math.log10(5000)]
    fig = (
        px.strip(
            df,
            x="Batch size",
            y="Dimensionality",
            color="Data",
            color_discrete_sequence=["rgba(0,0,255,0.3)", "rgba(255,0,0,0.3)"],
            log_x=True,
            template="simple_white",
            width=1000,
            height=600,
            title="Dimensionality of features & hidden representation of training examples",
        )
        .update_traces(marker=dict(opacity=0.5))
        .update_layout(
            xaxis=dict(range=xrange, tickmode="array", tickvals=BATCH_SIZES),
            yaxis=dict(range=[-0.05, 1.0]),
        )
        .add_vrect(x0=1, x1=(1 - eps) * xline1, fillcolor="#ddd", **vrect_kwargs)
        .add_vrect(x0=(1 + eps) * xline1, x1=(1 - eps) * xline2, fillcolor="#ccc", **vrect_kwargs)
        .add_vrect(x0=(1 + eps) * xline2, x1=10_000, fillcolor="#bbb", **vrect_kwargs)
        .add_scatter(
            x=BATCH_SIZES,
            y=[2 / b for b in BATCH_SIZES],
            mode="lines",
            line=dict(shape="spline", dash="dot", color="#333", width=1),
            name="d_hidden / batch_size",
        )
    )

    fig.show()

# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The first 4 hints give you specific bits of code, if there's any particular part of the implementation you're struggling with:

<details>
<summary>Hint (basic setup code)</summary>

Basic imports and constants:

```python
import math
from typing import Any
import pandas as pd
import plotly.express as px

NUM_WARMUP_STEPS = 2500
NUM_BATCH_UPDATES = 50_000
# EVAL_N_DATAPOINTS = 1_000

WEIGHT_DECAY = 1e-2
LEARNING_RATE = 1e-3

BATCH_SIZES = [3, 4, 5, 6, 8, 10, 15, 20, 30, 50, 100, 200, 300, 500, 1000, 2000, 3000]
# SMALLER_BATCH_SIZES = [3, 6, 10, 30, 100, 500, 2000]

N_FEATURES = 1000
N_INSTANCES = 10
D_HIDDEN = 2
SPARSITY = 0.99
FEATURE_PROBABILITY = 1 - SPARSITY
```

Our new schedulers, in line with Anthropic's writeup:

```python
def linear_warmup_lr(step, steps):
    """Increases linearly from 0 to 1."""
    return step / steps

def anthropic_lr(step, steps):
    """As per the description in the paper: 2500 step linear warmup, followed by cosine decay to zero."""
    if step < NUM_WARMUP_STEPS:
        return linear_warmup_lr(step, NUM_WARMUP_STEPS)
    else:
        return cosine_decay_lr(step - NUM_WARMUP_STEPS, steps - NUM_WARMUP_STEPS)
```

</details>

<details>
<summary>Hint (code for new version of <code>ToyModel</code> class)</summary>

```python
class DoubleDescentModel(ToyModel):
    W: Float[Tensor, "inst d_hidden feats"]
    b_final: Float[Tensor, "inst feats"]
    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)

    @classmethod
    def dimensionality(
        cls, data: Float[Tensor, "... batch d_hidden"]
    ) -> Float[Tensor, "... batch"]:
        """
        Calculates dimensionalities of data. Assumes data is of shape ... batch d_hidden, i.e. if it's 2D then
        it's a batch of vectors of length `d_hidden` and we return the dimensionality as a 1D tensor of length
        `batch`. If it has more dimensions at the start, we assume this means separate calculations for each
        of these dimensions (i.e. they are independent batches of vectors).
        """
        # Compute the norms of each vector (this will be the numerator)
        squared_norms = einops.reduce(data.pow(2), "... batch d_hidden -> ... batch", "sum")
        # Compute the denominator (i.e. get the dot product then sum over j)
        data_normed = data / data.norm(dim=-1, keepdim=True)
        interference = einops.einsum(
            data_normed, data, "... batch_i d_hidden, ... batch_j d_hidden -> ... batch_i batch_j"
        )
        polysemanticity = einops.reduce(
            interference.pow(2), "... batch_i batch_j -> ... batch_i", "sum"
        )
        assert squared_norms.shape == polysemanticity.shape

        return squared_norms / polysemanticity

    def generate_batch(self, batch_size: int) -> Float[Tensor, "batch inst feats"]:
        """
        New function for generating batch, so we can normalize it.
        """
        # Get batch from prev method
        batch = super().generate_batch(batch_size)

        # Normalize the batch (i.e. so each vector for a particular batch & instance has norm 1)
        # (need to be careful about vectors with norm zero)
        norms = batch.norm(dim=-1, keepdim=True)
        norms = t.where(norms.abs() < 1e-6, t.ones_like(norms), norms)
        batch_normed = batch / norms
        return batch_normed

    def calculate_loss(
        self,
        out: Float[Tensor, "batch inst feats"],
        batch: Float[Tensor, "batch inst feats"],
        per_inst: bool = False,
    ) -> Float[Tensor, "inst"]:
        """
        New function to calculate loss, because we need a "loss per instance" option to find the best
        instance at the end of our optimization.
        """
        error = self.importance * ((batch - out) ** 2)
        loss = einops.reduce(error, "batch inst feats -> inst", "mean")
        return loss if per_inst else loss.sum()

    def optimize(
        self,
        batch_size: int,
        steps: int = NUM_BATCH_UPDATES,
        log_freq: int = 100,
        lr: float = LEARNING_RATE,
        lr_scale: Callable[[int, int], float] = anthropic_lr,
        weight_decay: float = WEIGHT_DECAY,
    ) -> tuple[Tensor, Tensor]:
        optimizer = t.optim.AdamW(list(self.parameters()), lr=lr, weight_decay=weight_decay)

        progress_bar = tqdm(range(steps))

        # Same batch for each step
        batch = self.generate_batch(batch_size)  # [batch_size inst n_features]

        for step in progress_bar:
            # Update learning rate
            step_lr = lr * lr_scale(step, steps)
            for group in optimizer.param_groups:
                group["lr"] = step_lr

            # Optimize
            optimizer.zero_grad()
            out = self.forward(batch)
            loss = self.calculate_loss(out, batch)
            loss.backward()
            optimizer.step()

            # Display progress bar
            if (step % log_freq == 0) or (step + 1 == steps):
                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)

        # Generate one final batch to compute the loss (we want only the best instance!)
        with t.inference_mode():
            out = self.forward(batch)
            loss_per_inst = self.calculate_loss(out, batch, per_inst=True)
            best_inst = loss_per_inst.argmin()
            print(f"Best instance = #{best_inst}, with loss {loss_per_inst[best_inst].item():.4e}")

        return batch[:, best_inst], self.W[best_inst].detach()
```

</details>

<details>
<summary>Hint (code to train models & replicate 2D feature plots)</summary>

```python
features_list = []
hidden_representations_list = []

for batch_size in tqdm(BATCH_SIZES):
    # Define our model
    cfg = ToyModelConfig(n_features=N_FEATURES, n_inst=N_INSTANCES, d_hidden=D_HIDDEN)
    model = DoubleDescentModel(cfg, feature_probability=FEATURE_PROBABILITY).to(device)

    # Optimize, and return the best batch & weight matrix
    batch_inst, W_inst = model.optimize(steps=15_000, batch_size=batch_size)

    # Calculate the hidden feature representations, and add both this and weight matrix to our lists of data
    with t.inference_mode():
        hidden = einops.einsum(
            batch_inst, W_inst, "batch features, hidden features -> hidden batch"
        )
    features_list.append(W_inst.cpu())
    hidden_representations_list.append(hidden.cpu())
```

Visualising the 2D feature plots:

```python
utils.plot_features_in_2d(
    features_list + hidden_representations_list,
    colors=[["blue"] for _ in range(len(BATCH_SIZES))] + [["red"] for _ in range(len(BATCH_SIZES))],
    title="Double Descent & Superposition (num features = 1000)",
    subplot_titles=[f"Features (batch={bs})" for bs in BATCH_SIZES] + [f"Data (batch={bs})" for bs in BATCH_SIZES],
    allow_different_limits_across_subplots=True,
    n_rows=2,
)
```

You should get something like this:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ddd_fig1.png" width="1400">

</details>

<details>
<summary>Hint (code to replicate the dimensionality plot)</summary>

```python
df_data = {"Batch size": [], "Dimensionality": [], "Data": []}

for batch_size, model_W, hidden in zip(BATCH_SIZES, features_list, hidden_representations_list):
    # Get x-axis data (batch size), and color (blue or red)
    df_data["Batch size"].extend([batch_size] * (N_FEATURES + batch_size))
    df_data["Data"].extend(["features"] * N_FEATURES + ["hidden"] * batch_size)

    # Calculate dimensionality of model.W[inst].T, which has shape [d_hidden=2 N_FEATURES]
    feature_dim = DoubleDescentModel.dimensionality(model_W.T)
    assert feature_dim.shape == (N_FEATURES,)

    # Calculate dimensionality of model's batch data hidden representation. This has shape [d_hidden=2 batch_size]
    data_dim = DoubleDescentModel.dimensionality(hidden.T)
    assert data_dim.shape == (batch_size,)

    # Add them both to the data
    df_data["Dimensionality"].extend(feature_dim.tolist() + data_dim.tolist())


df = pd.DataFrame(df_data)
eps = 0.01
xline1, xline2 = (100 * 200) ** 0.5, (500 * 1000) ** 0.5
vrect_kwargs: dict[str, Any] = dict(opacity=0.5, layer="below", line_width=0)
xrange = [math.log10(1.5), math.log10(5000)]
fig = (
    px.strip(
        df,
        x="Batch size",
        y="Dimensionality",
        color="Data",
        color_discrete_sequence=["rgba(0,0,255,0.3)", "rgba(255,0,0,0.3)"],
        log_x=True,
        template="simple_white",
        width=1000,
        height=600,
        title="Dimensionality of features & hidden representation of training examples",
    )
    .update_traces(marker=dict(opacity=0.5))
    .update_layout(
        xaxis=dict(range=xrange, tickmode="array", tickvals=BATCH_SIZES),
        yaxis=dict(range=[-0.05, 1.0]),
    )
    .add_vrect(x0=1, x1=(1 - eps) * xline1, fillcolor="#ddd", **vrect_kwargs)
    .add_vrect(x0=(1 + eps) * xline1, x1=(1 - eps) * xline2, fillcolor="#ccc", **vrect_kwargs)
    .add_vrect(x0=(1 + eps) * xline2, x1=10_000, fillcolor="#bbb", **vrect_kwargs)
    .add_scatter(
        x=BATCH_SIZES,
        y=[2 / b for b in BATCH_SIZES],
        mode="lines",
        line=dict(shape="spline", dash="dot", color="#333", width=1),
        name="d_hidden / batch_size",
    )
)

fig.show()
```

You should get something like this:

FILTERS: st
<embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-D.html" width="1020" height="620" />
END FILTERS
FILTERS: colab
<img src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-D.png" width="1000">
END FILTERS

</details>

Lastly, you get the full solution code here:

<details>
<summary>Solution (full)</summary>

```python
SOLUTION
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 5️⃣ Sparse Autoencoders in Toy Models
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We now move on to sparse autoencoders, a recent line of work that has been explored by Anthropic in their [recent paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html), and is currently one of the most interesting areas of research in mechanistic interpretability.

In the following set of exercises, you will:

- Build your own sparse autoencoder, writing its architecture & loss function,
- Train your SAE on the hidden activations of the `Model` class which you defined earlier (note the difference between this and the Anthropic paper's setup, since the latter trained SAEs on the MLP layer, whereas we're training it on a non-privileged basis),
- Extract the features from your SAE, and verify that these are the same as your model's learned features.

You should read Anthropic's dictionary learning paper (linked above): the introduction and first section (problem setup) up to and including the "Sparse Autoencoder Setup" section. Make sure you can answer at least the following questions:

<details>
<summary>What is an autoencoder, and what is it trained to do?</summary>

Autoencoders are a type of neural network which learns efficient encodings / representations of unlabelled data. It is trained to compress the input in some way to a **latent representation**, then map it back into the original input space. It is trained by minimizing the reconstruction loss between the input and the reconstructed input.

The "encoding" part usually refers to the latent space being lower-dimensional than the input. However, that's not always the case, as we'll see with sparse autoencoders.

<img src="https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/sae-diagram-2.png" width="900">

</details>

<details>
<summary>Why is the hidden dimension of our autoencoder larger than the number of activations, when we train an SAE on an MLP layer?</summary>

As mentioned in the previous dropdown, usually the latent vector is a compressed representation of the input because it's lower-dimensional. However, it can still be a compressed representation even if it's higher dimensional, if we enforce a sparsity constraint on the latent vector (which in some sense reduces its effective dimensionality).

As for why we do this specifically for our autoencoder use case, it's because we're trying to recover features from superposition, in cases where there are **more features than neurons**. We're hoping our autoencoder learns an **overcomplete feature basis**.

</details>

<details>
<summary>Why does the L1 penalty encourage sparsity? (This isn't specifically mentioned in this paper, but it's an important thing to understand.)</summary>

Unlike $L_2$ penalties, the $L_1$ penalty actually pushes values towards zero. This is a well-known result in statistics, best illustrated below:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/l1-viz.png" width="450">

See [this Google ML page](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization) for more of an explanation (it also has a nice out-of-context animation!).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Problem setup

Recall the setup of our previous model:

$$
\begin{aligned}
h &= W x \\
x' &= \operatorname{ReLU}(W^T h + b)
\end{aligned}
$$

We're going to train our autoencoder to just take in the hidden state activations $h$, map them to a larger (overcomplete) hidden state $z$, then reconstruct the original hidden state $h$ from $z$.

$$
\begin{aligned}
z &= \operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc}) \\
h' &= W_{dec}z + b_{dec}
\end{aligned}
$$

Note the choice to have a different encoder and decoder weight matrix, rather than having them tied - we'll discuss this more later.

It's important not to get confused between the autoencoder and model's notation. Remember - the model takes in features $x$, maps them to **lower-dimensional** vectors $h$, and then reconstructs them as $x'$. The autoencoder takes in these hidden states $h$, maps them to a **higher-dimensional but sparse** vector $z$, and then reconstructs them as $h'$. Our hope is that the elements of $z$ correspond to the features of $x$.

Another note - the use of $b_{dec}$ here might seem weird, since we're subtracting it at the start then adding it back at the end. The way we're treating this term is as a **centralizing term for the hidden states**. It subtracts some learned mean vector from them so that $W_{enc}$ can act on centralized vectors, and then this term gets added back to the reconstructed hidden states at the end of the model.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Notation

The autoencoder's hidden activations go by many names. Sometimes they're called **neurons** (since they do have an activation function applied to them which makes them a privileged basis, like the neurons in an MLP layer). Sometimes they're called **features**, since the idea with SAEs is that these hidden activations are meant to refer to specific features in the data. However, the word feature is a bit [overloaded](https://www.lesswrong.com/posts/9Nkb389gidsozY9Tf/lewis-smith-s-shortform#fd64ALuWK8rXdLKz6) - ideally we want to use "feature" to refer to the attributes of the data itself - if our SAE's weights are randomly initialized, is it fair to call this a feature?!

For this reason, we'll be referring to the autoencoder's hidden activations as **SAE latents**. However, it's worth noting that people sometimes use "SAE features" or "neurons" instead, so try not to get confused (e.g. often people use "neuron resampling" to refer to the resampling of the weights in the SAE).

The new notation we'll adopt in this section is:

- `d_sae`, which is the number of activations in the SAE's hidden layer (i.e. the latent dimension). Note that we want the SAE latents to correspond to the original data features, which is why we'll need `d_sae >= n_features` (usually we'll have equality in this section).
- `d_in`, which is the SAE input dimension. This is the same as `d_hidden` from the previous sections because the SAE is reconstructing the model's hidden activations, however calling it `d_hidden` in the context of an SAE would be confusing. Usually in this section, we'll have `d_in = d_hidden = 2`, so we can visualize the results.

<details>
<summary>Question - in the formulas above (in the "Problem setup" section), what are the shapes of x, x', z, h, and h' ?</summary>

Ignoring batch and instance dimensions:

- `x` and `x'` are vectors of shape `(n_features,)`
- `z` is a vector of shape `(d_sae,)`
- `h` and `h'` are vectors of shape `(d_in,)`, which is equal to `d_hidden` from previous sections

Including batch and instance dimensions, all shapes have extra leading dimensions `(batch_size, n_inst, d)`.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### SAE class

We've provided the `ToySAEConfig` class below. Its arguments are as follows (we omit the ones you'll only need to work with in later exercises):

- `n_inst`, which means the same as it does in your `ToyModel` class
- `d_in`, the input size to your SAE (equal to `d_hidden` of your `ToyModel` class)
- `d_sae`, the SAE's latent dimension size
- `sparsity_coeff`, which is used in your loss function
- `weight_normalize_eps`, which is added to the denominator whenever you normalize weights
- `tied_weights`, which is a boolean determining whether your encoder and decoder weights are tied
- `ste_epsilon`, which is only relevant for JumpReLU SAEs later on

We've also given you the `ToySAE` class. Your job over the next 4 exercises will be to fill in the `__init__`, `W_dec_normalized`, `generate_batch` and `forward` methods.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

@dataclass
class ToySAEConfig:
    n_inst: int
    d_in: int
    d_sae: int
    sparsity_coeff: float = 0.2
    weight_normalize_eps: float = 1e-8
    tied_weights: bool = False
    ste_epsilon: float = 0.01


class ToySAE(nn.Module):
    W_enc: Float[Tensor, "inst d_in d_sae"]
    _W_dec: Float[Tensor, "inst d_sae d_in"] | None
    b_enc: Float[Tensor, "inst d_sae"]
    b_dec: Float[Tensor, "inst d_in"]

    def __init__(self, cfg: ToySAEConfig, model: ToyModel) -> None:
        super(ToySAE, self).__init__()

        assert cfg.d_in == model.cfg.d_hidden, "Model's hidden dim doesn't match SAE input dim"
        self.cfg = cfg
        self.model = model.requires_grad_(False)
        self.model.W.data[1:] = self.model.W.data[0]
        self.model.b_final.data[1:] = self.model.b_final.data[0]

        raise NotImplementedError()

        self.to(device)

    @property
    def W_dec(self) -> Float[Tensor, "inst d_sae d_in"]:
        return self._W_dec if self._W_dec is not None else self.W_enc.transpose(-1, -2)

    @property
    def W_dec_normalized(self) -> Float[Tensor, "inst d_sae d_in"]:
        """
        Returns decoder weights, normalized over the autoencoder input dimension.
        """
        # You'll fill this in later
        raise NotImplementedError()

    def generate_batch(self, batch_size: int) -> Float[Tensor, "batch inst d_in"]:
        """
        Generates a batch of hidden activations from our model.
        """
        # You'll fill this in later
        raise NotImplementedError()

    def forward(
        self, h: Float[Tensor, "batch inst d_in"]
    ) -> tuple[
        dict[str, Float[Tensor, "batch inst"]],
        Float[Tensor, "batch inst"],
        Float[Tensor, "batch inst d_sae"],
        Float[Tensor, "batch inst d_in"],
    ]:
        """
        Forward pass on the autoencoder.

        Args:
            h: hidden layer activations of model

        Returns:
            loss_dict:       dict of different loss terms, each having shape (batch_size, n_inst)
            loss:            total loss (i.e. sum over terms of loss dict), same shape as loss terms
            acts_post:       autoencoder latent activations, after applying ReLU
            h_reconstructed: reconstructed autoencoder input
        """
        # You'll fill this in later
        raise NotImplementedError()

    def optimize(
        self,
        batch_size: int = 1024,
        steps: int = 10_000,
        log_freq: int = 100,
        lr: float = 1e-3,
        lr_scale: Callable[[int, int], float] = constant_lr,
        resample_method: Literal["simple", "advanced", None] = None,
        resample_freq: int = 2500,
        resample_window: int = 500,
        resample_scale: float = 0.5,
        hidden_sample_size: int = 256,
    ) -> list[dict[str, Any]]:
        """
        Optimizes the autoencoder using the given hyperparameters.

        Args:
            model:              we reconstruct features from model's hidden activations
            batch_size:         size of batches we pass through model & train autoencoder on
            steps:              number of optimization steps
            log_freq:           number of optimization steps between logging
            lr:                 learning rate
            lr_scale:           learning rate scaling function
            resample_method:    method for resampling dead latents
            resample_freq:      number of optimization steps between resampling dead latents
            resample_window:    number of steps needed for us to classify a neuron as dead
            resample_scale:     scale factor for resampled neurons
            hidden_sample_size: size of hidden value sample we add to the logs (for visualization)

        Returns:
            data_log:           dictionary containing data we'll use for visualization
        """
        assert resample_window <= resample_freq

        optimizer = t.optim.Adam(self.parameters(), lr=lr)  # betas=(0.0, 0.999)
        frac_active_list = []
        progress_bar = tqdm(range(steps))

        # Create lists of dicts to store data we'll eventually be plotting
        data_log = []

        for step in progress_bar:
            # Resample dead latents
            if (resample_method is not None) and ((step + 1) % resample_freq == 0):
                frac_active_in_window = t.stack(frac_active_list[-resample_window:], dim=0)
                if resample_method == "simple":
                    self.resample_simple(frac_active_in_window, resample_scale)
                elif resample_method == "advanced":
                    self.resample_advanced(frac_active_in_window, resample_scale, batch_size)

            # Update learning rate
            step_lr = lr * lr_scale(step, steps)
            for group in optimizer.param_groups:
                group["lr"] = step_lr

            # Get a batch of hidden activations from the model
            with t.inference_mode():
                h = self.generate_batch(batch_size)

            # Optimize
            loss_dict, loss, acts, _ = self.forward(h)
            loss.mean(0).sum().backward()
            optimizer.step()
            optimizer.zero_grad()

            # Normalize decoder weights by modifying them directly (if not using tied weights)
            if not self.cfg.tied_weights:
                self.W_dec.data = self.W_dec_normalized.data

            # Calculate the mean sparsities over batch dim for each feature
            frac_active = (acts.abs() > 1e-8).float().mean(0)
            frac_active_list.append(frac_active)

            # Display progress bar, and log a bunch of values for creating plots / animations
            if step % log_freq == 0 or (step + 1 == steps):
                progress_bar.set_postfix(
                    lr=step_lr,
                    loss=loss.mean(0).sum().item(),
                    frac_active=frac_active.mean().item(),
                    **{k: v.mean(0).sum().item() for k, v in loss_dict.items()},  # type: ignore
                )
                with t.inference_mode():
                    loss_dict, loss, acts, h_r = self.forward(
                        h := self.generate_batch(hidden_sample_size)
                    )
                data_log.append(
                    {
                        "steps": step,
                        "frac_active": (acts.abs() > 1e-8).float().mean(0).detach().cpu(),
                        "loss": loss.detach().cpu(),
                        "h": h.detach().cpu(),
                        "h_r": h_r.detach().cpu(),
                        **{name: param.detach().cpu() for name, param in self.named_parameters()},
                        **{name: loss_term.detach().cpu() for name, loss_term in loss_dict.items()},
                    }
                )

        return data_log

    @t.no_grad()
    def resample_simple(
        self,
        frac_active_in_window: Float[Tensor, "window inst d_sae"],
        resample_scale: float,
    ) -> None:
        """
        Resamples dead latents, by modifying the model's weights and biases inplace.

        Resampling method is:
            - For each dead neuron, generate a random vector of size (d_in,), and normalize these vecs
            - Set new values of W_dec and W_enc to be these normalized vecs, at each dead neuron
            - Set b_enc to be zero, at each dead neuron
        """
        raise NotImplementedError()

    @t.no_grad()
    def resample_advanced(
        self,
        frac_active_in_window: Float[Tensor, "window inst d_sae"],
        resample_scale: float,
        batch_size: int,
    ) -> None:
        """
        Resamples latents that have been dead for `dead_feature_window` steps, according to `frac_active`.

        Resampling method is:
            - Compute the L2 reconstruction loss produced from the hidden state vecs `h`
            - Randomly choose values of `h` with probability proportional to their reconstruction loss
            - Set new values of W_dec & W_enc to be these centered & normalized vecs, at each dead neuron
            - Set b_enc to be zero, at each dead neuron
        """
        raise NotImplementedError()

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `__init__`

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 5-15 minutes on this exercise.
> ```

You should implement the `__init__` method below. This should define the weights `b_enc`, `b_dec`, `W_enc` and `_W_dec`. Use [Kaiming uniform](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) for weight initialization, and initialize the biases at zero.

Note, we use `_W_dec` to handle the case of tied weights: it should be `None` if we have tied weights, and a proper parameter if we don't have tied weights. The property `W_dec` we've given you in the class above will deal with both cases for you.

<details>
<summary>Why might we want / not want to tie our weights?</summary>

In our `Model` implementations, we used a weight and its transpose. You might think it also makes sense to have the encoder and decoder weights be transposed copies of each other, since intuitively both the encoder and decoder's latent vectors meant to represent some feature's "direction in the original model's hidden dimension".

The reason we might not want to tie weights is pretty subtle. The job of the encoder is in some sense to recover features from superposition, whereas the job of the decoder is just to represent that feature faithfully if present (since the goal of our SAE is to write the input as a linear combination of `W_dec` vectors) - this is why we generally see the decoder weights as the "true direction" for a feature, when weights are untied.

The diagram below might help illustrate this concept (if you want, you can replicate the results in this diagram using our toy model setup!).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/w-dec-explained.png" width="700">

In simple settings like this toy model we might not benefit much from untying weights, and tying weights can actually help us avoid finding annoying local minima in our optimization. However, for most of these exercises we'll use untied weights in order to illustrate SAE concepts more clearly.

</details>

Also, note that we've defined `self.cfg` and `self.model` for you in the init function - in the latter case, we've frozen the model's weights (because when you train your SAE you don't want to track gradients in your base model), and we've also modified the model's weights so they all match the first instance (this is so we can more easily interpret our SAE plots we'll create when we finish training).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def __init__(self: ToySAE, cfg: ToySAEConfig, model: ToyModel) -> None:
    super(ToySAE, self).__init__()

    assert cfg.d_in == model.cfg.d_hidden, "Model's hidden dim doesn't match SAE input dim"
    self.cfg = cfg
    self.model = model.requires_grad_(False)
    self.model.W.data[1:] = self.model.W.data[0]
    self.model.b_final.data[1:] = self.model.b_final.data[0]

    self.W_enc = nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_in, cfg.d_sae))))
    self._W_dec = (
        None
        if self.cfg.tied_weights
        else nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_sae, cfg.d_in))))
    )
    self.b_enc = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))
    self.b_dec = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_in))

    self.to(device)


ToySAE.__init__ = __init__
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.__init__` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_sae_init(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `W_dec_normalized`

> ```yaml
> Difficulty: 🔴⚪⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend 5-10 minutes on this exercise.
> ```

You should now fill in the `W_dec_normalized` property, which returns the decoder weights, normalized (with L2 norm) over the autoencoder input dimension. Note that the existence of the `W_dec` property means you can safety refer to this attribute, without having to worry about `_W_dec` any more. Also, remember to add `cfg.weight_normalize_eps` to your denominator (this helps avoid divide-by-zero errors).

<details>
<summary>Why do we need <code>W_dec_normalized</code>?</summary>

We normalize `W_dec` to stop the model from cheating! Imagine if we didn't normalize `W_dec` - the model could make `W_enc` 10 times smaller, and make `W_dec` 10 times larger. The outputs would be the same (keeping the reconstruction error constant), but the latent activations would be 10 times smaller, letting the model shrink the sparsity penalty (the L1 loss term) without learning anything useful.

L2-normalizing the columns of `W_dec` also makes the magnitude of our latent activations more clearly interpretable: with normalization, they answer the question "how much of each unit-length feature is present?"

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
@property
def W_dec_normalized(self: ToySAE) -> Float[Tensor, "inst d_sae d_in"]:
    """Returns decoder weights, normalized over the autoencoder input dimension."""
    return self.W_dec / (self.W_dec.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps)


ToySAE.W_dec_normalized = W_dec_normalized
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.W_dec_normalized` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_sae_W_dec_normalized(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `generate_batch`

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend 5-15 minutes on this exercise.
> ```

As mentioned, our data no longer comes directly from `ToyModel.generate_batch`. Instead, we use `Model.generate_batch` to get our model input $x$, and then apply our model's `W` matrix to get its hidden activations $h=Wx$. Note that we're working with the model from the "Superposition in a Nonprivileged Basis" model, meaning there's no ReLU function to apply to get $h$.

You should fill in the `generate_batch` method now, then run the test. Note - remember to use `self.model` rather than `model`!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def generate_batch(self: ToySAE, batch_size: int) -> Float[Tensor, "batch inst d_in"]:
    """
    Generates a batch of hidden activations from our model.
    """
    return einops.einsum(
        self.model.generate_batch(batch_size),
        self.model.W,
        "batch inst feats, inst d_in feats -> batch inst d_in",
    )


ToySAE.generate_batch = generate_batch
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.generate_batch` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_sae_generate_batch(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `forward`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 25-40 minutes on this exercise.
> ```

You should calculate the autoencoder's hidden state activations as $z = \operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc})$, and then reconstruct the output as $h' = W_{dec}z + b_{dec}$. A few notes:

- The **first variable** we return is a `loss_dict`, which contains the loss tensors of shape `(batch_size, n_inst)` for both terms in our loss function (before multiplying by the L1 coefficient). This is used for logging, and it'll also be used later in our neuron resampling methods. For this architecture, your keys should be `"L_reconstruction"` and `"L_sparsity"`.
- The **second variable** we return is the `loss` term, which also has shape `(batch_size, n_inst)`, and is created by summing the losses in `loss_dict` (with sparsity loss multiplied by `cfg.sparsity_coeff`). When doing gradient descent, we'll average over the batch dimension & sum over the instance dimension (since we're training our instances independently & in parallel).
- The **third variable** we return is the hidden state activations `acts`, which are also used later for neuron resampling (as well as logging how many latents are active).
- The **fourth variable** we return is the reconstructed hidden states `h_reconstructed`, i.e. the autoencoder's actual output.

An important note regarding our loss term - the reconstruction loss is the squared difference between input & output **averaged** over the `d_in` dimension, but the sparsity penalty is the L1 norm of the hidden activations **summed** over the `d_sae` dimension. Can you see why we average one but sum the other?

<details>
<summary>Hint</summary>

Suppose we averaged L1 loss too. Consider the gradients a single latent receives from the reconstruction loss and sparsity penalty - what do they look like in the limit of very large `d_sae`?

</details>

<details>
<summary>Answer - why we average L2 loss over <code>d_in</code> but sum L1 loss over <code>d_sae</code></summary>

Suppose for sake of argument we averaged L1 loss too. Imagine if we doubled the latent dimension, but kept all other SAE hyperparameters the same. The per-hidden-unit gradient from the reconstruction loss would still be the same (because changing a single hidden unit's encoder or decoder vector would have the same effect on the output as before), but the per-hidden-unit gradient from the sparsity penalty would have halved (because we're averaging the sparsity penalty over `d_sae`). This means that in the limit, the sparsity penalty wouldn't matter at all, and the only important thing would be getting zero reconstruction loss.

</details>

Note - make sure you're using `self.W_dec_normalized` rather than `self.W_dec` in your forward function. This is because if we're using tied weights then we won't be able to manually normalize `W_dec` inplace, but we still want to use the normalized version.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def forward(
    self: ToySAE, h: Float[Tensor, "batch inst d_in"]
) -> tuple[
    dict[str, Float[Tensor, "batch inst"]],
    Float[Tensor, "batch inst"],
    Float[Tensor, "batch inst d_sae"],
    Float[Tensor, "batch inst d_in"],
]:
    """
    Forward pass on the autoencoder.

    Args:
        h: hidden layer activations of model

    Returns:
        loss_dict:       dict of different loss terms, each dict value having shape (batch_size, n_inst)
        loss:            total loss (i.e. sum over terms of loss dict), same shape as loss_dict values
        acts_post:       autoencoder latent activations, after applying ReLU
        h_reconstructed: reconstructed autoencoder input
    """
    h_cent = h - self.b_dec

    # Compute latent (hidden layer) activations
    acts_pre = (
        einops.einsum(h_cent, self.W_enc, "batch inst d_in, inst d_in d_sae -> batch inst d_sae")
        + self.b_enc
    )
    acts_post = F.relu(acts_pre)

    # Compute reconstructed input
    h_reconstructed = (
        einops.einsum(
            acts_post, self.W_dec_normalized, "batch inst d_sae, inst d_sae d_in -> batch inst d_in"
        )
        + self.b_dec
    )

    # Compute loss terms
    L_reconstruction = (h_reconstructed - h).pow(2).mean(-1)
    L_sparsity = acts_post.abs().sum(-1)
    loss_dict = {"L_reconstruction": L_reconstruction, "L_sparsity": L_sparsity}
    loss = L_reconstruction + self.cfg.sparsity_coeff * L_sparsity

    return loss_dict, loss, acts_post, h_reconstructed


ToySAE.forward = forward
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.forward` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_sae_forward(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Training your SAE

The `optimize` method has been given to you. A few notes on how it differs from your previous model:

- Before each optimization step, we implement **neuron resampling** - we'll get to this later.
- We have more logging, via the `data_log` dictionary - we'll use this for visualization.
- We've used `betas=(0.0, 0.999)`, to match the description in [Anthropic's Feb 2024 update](https://transformer-circuits.pub/2024/feb-update/index.html#dict-learning-loss) - although they document it to work better specifically for large models, we may as well match it here.

First, let's define and train our model, and visualize model weights and the data returned from `sae.generate_batch` (which are the hidden state representations of our trained model, and will be used for training our SAE).

Note that we'll use a feature probability of 2.5% (and assume independence between features) for all subsequent exercises.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

d_hidden = d_in = 2
n_features = d_sae = 5
n_inst = 16

# Create a toy model, and train it to convergence
cfg = ToyModelConfig(n_inst=n_inst, n_features=n_features, d_hidden=d_hidden)
model = ToyModel(cfg=cfg, device=device, feature_probability=0.025)
model.optimize()

sae = ToySAE(cfg=ToySAEConfig(n_inst=n_inst, d_in=d_in, d_sae=d_sae), model=model)

h = sae.generate_batch(512)

utils.plot_features_in_2d(model.W[:8], title="Base model")
utils.plot_features_in_2d(
    einops.rearrange(h[:, :8], "batch inst d_in -> inst d_in batch"),
    title="Hidden state representation of a random batch of data",
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, let's train our SAE, and visualize the instances with lowest loss! We've also created a function `animate_features_in_2d` which creates an animation of the training over time. If the inline displaying doesn't work, you might have to open the saved HTML file in your browser to see it.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

data_log = sae.optimize(steps=20_000)

utils.animate_features_in_2d(
    data_log,
    instances=list(range(8)),  # only plot the first 8 instances
    rows=["W_enc", "_W_dec"],
    filename=str(section_dir / "animation-training.html"),
    title="SAE on toy model",
)

# If this display code doesn't work, try saving & opening animation in your browser
with open(section_dir / "animation-training.html") as f:
    display(HTML(f.read()))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-animation-training-b.html" width="2100" height="650" style="background-color:white;">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In other words, the autoencoder is generally successful at reconstructing the model's hidden states, and maybe sometimes it learns the fully monosemantic solution (one latent per feature), but more often it learns a combination of **polysemantic latents** and **dead latents** (which never activate). These are a big problem because they don't receive any gradients during training, so they're not a problem which fixes itself over time. You can check the presence of dead latents by graphing the feature probabilities over training, in the code below. You should find that:

1. Some latents are dead for most or all of training (with "fraction of datapoints active" being zero),
2. Some latents fire more frequently than the target feature prob of 2.5% (these are usually polysemantic, i.e. they fire on more than one different feature),
3. Some latents fire approximately at or slightly below the target probability (these are usually monosemantic). If any of your instances above learned the full monosemantic solution (i.e. latents uniformly spaced around the 2D hidden dimension) then you should find that all 5 latents in that instance fall into this third category.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

utils.frac_active_line_plot(
    frac_active=t.stack([data["frac_active"] for data in data_log]),
    title="Probability of sae features being active during training",
    avg_window=20,
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-line-b.html" width="1020" height="600">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Resampling

From Anthropic's paper (replacing terminology "dead neurons" with "dead latents" in accordance with how we're using the term):

> Second, we found that over the course of training some latents cease to activate, even across a large number of datapoints. We found that “resampling” these dead latents during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension. Our resampling procedure is detailed in [Autoencoder Resampling](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling), but in brief we periodically check for latents which have not fired in a significant number of steps and reset the encoder weights on the dead latents to match data points that the autoencoder does not currently represent well.

Your next task is to implement this resampling procedure.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `resample_simple`

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 20-30 minutes on this exercise.
> ```

The process Anthropic describes for resampling SAE latents is pretty involved, so we'll start by implementing a simpler version of it. Specifically, we'll implement the following algorithm for each instance `inst`:

* Find all the dead latents (i.e. the values `(inst, d)` where `frac_active_in_window[:, inst, d]` are all zero).
* For each of these, do the following:
    * Generate a new random vector `v` of length `d_in`.
    * Set the decoder weights `W_dec[inst, d, :]` to this new vector `v`, normalized.
    * Set the encoder weights `W_enc[inst, :, d]` to this new vector `v`, scaled to have norm `resample_scale`.
    * Set the encoder biases `b_enc[inst, d]` to zero.

The test function we've given you will check that your function replaces / zeros the correct weights.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
@t.no_grad()
def resample_simple(
    self: ToySAE,
    frac_active_in_window: Float[Tensor, "window inst d_sae"],
    resample_scale: float,
) -> None:
    """
    Resamples dead latents, by modifying the model's weights and biases inplace.

    Resampling method is:
        - For each dead neuron, generate a random vector of size (d_in,), and normalize these vecs
        - Set new values of W_dec and W_enc to be these normalized vectors, at each dead neuron
        - Set b_enc to be zero, at each dead neuron

    This function performs resampling over all instances at once, using batched operations.
    """
    # Get a tensor of dead latents
    dead_latents_mask = (frac_active_in_window < 1e-8).all(dim=0)  # [instances d_sae]
    n_dead = int(dead_latents_mask.int().sum().item())

    # Get our random replacement values of shape [n_dead d_in], and scale them
    replacement_values = t.randn((n_dead, self.cfg.d_in), device=self.W_enc.device)
    replacement_values_normed = replacement_values / (
        replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
    )

    # Change the corresponding values in W_enc, W_dec, and b_enc
    self.W_enc.data.transpose(-1, -2)[dead_latents_mask] = (
        resample_scale * replacement_values_normed
    )
    self.W_dec.data[dead_latents_mask] = replacement_values_normed
    self.b_enc.data[dead_latents_mask] = 0.0


ToySAE.resample_simple = resample_simple
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.resample_simple` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_resample_simple(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed the tests, train your model again, and watch the animation to see how the neuron resampling has helped the training process. You should be able to see the resampled neurons in red.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

resampling_sae = ToySAE(cfg=ToySAEConfig(n_inst=n_inst, d_in=d_in, d_sae=d_sae), model=model)

resampling_data_log = resampling_sae.optimize(steps=20_000, resample_method="simple")

utils.animate_features_in_2d(
    resampling_data_log,
    rows=["W_enc", "_W_dec"],
    instances=list(range(8)),  # only plot the first 8 instances
    filename=str(section_dir / "animation-training-resampling.html"),
    color_resampled_latents=True,
    title="SAE on toy model (with resampling)",
)

utils.frac_active_line_plot(
    frac_active=t.stack([data["frac_active"] for data in resampling_data_log]),
    title="Probability of sae features being active during training",
    avg_window=20,
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-animation-training-resample-c.html" width="2100" height="650" style="background-color:white;"><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-line-resampling.html" width="1020" height="600">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Much better!

Now that we have pretty much full reconstruction on our features, let's visualize that reconstruction! The `animate_features_in_2d` function also offers features to plot hidden state reconstructions and how they evolve over time. Examining how the hidden state reconstructions evolve over time can help you understand what's going on, for example:

- The SAE often learns a non-sparse solution (e.g. 4 uniformly spaced polysemantic latents & 1 dead latent) before converging to the ideal solution.
    - Note, we also see something similar when training SAEs on LLMs: they first find a non-sparse solution with small reconstruction loss, before learning a more sparse solution (L0 goes down).
- Hovering over hidden states, you should observe some things:
    - Low-magnitude hidden states are often reconstructed as zero, this is because the SAE can't separate them from interference from other features.
    - Even for correctly reconstructed features, the hidden state magnitude is generally smaller than the true hidden states - this is called **shrinkage**, and we'll discuss it extensively in the next section.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

utils.animate_features_in_2d(
    resampling_data_log,
    rows=["W_enc", "h", "h_r"],
    instances=list(range(4)),  # plotting fewer instances for a smaller animation file size
    color_resampled_latents=True,
    filename=str(section_dir / "animation-training-reconstructions.html"),
    title="SAE on toy model (showing hidden states & reconstructions)",
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-animation-training-resample-2.html" width="1280" height="930" style="background-color:white;">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `resample_advanced`


> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 20-40 minutes on this exercise, if you choose to do it.
> ```

This section can be considered optional if you've already implemented the simpler version of `resample` above. However, if you're interested in a version of it which hues close to [Anthropic's methodology](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling), then you might still be interested in this exercise.

The main difference we'll make is in how the resampled values are chosen. Rather than just drawing them randomly from a distribution and normalizing them, we'll be **sampling them with replacement from a set of input activations $h$, with sampling probabilities weighted by the squared $L_2$ loss of the autoencoder on each input**. Intuitively, this will make it more likely that our resampled neurons will represent feature directions that the autoencoder is currently doing a bad job of representing.

The new resampling algorithm looks like the following - for each instance we:

* Generate a batch of hidden data `h` from your SAE and compute its squared reconstruction loss `l2_squared`. It should have shape `(batch_size, n_inst)`. If the L2 loss for this instance `l2_squared[:, inst]` is zero everywhere, we can skip this instance.
* Find the dead latents for this instance (i.e. the instances `inst` and latent indices `d` where `frac_active_in_window[:, inst, d]` are all zero).
* For each of these, do the following:
    * Randomly sample a vector `v = h[x, inst, :]`, where `0 <= x < batch_size` is chosen according to the distribution with probabilities proportional to `l2_squared[:, inst]`.
    * Set the decoder weights `W_dec[inst, d, :]` to this new vector `v`, normalized.
    * Set the encoder weights `W_enc[inst, :, d]` to this new vector `v`, scaled to have norm `resample_scale * avg_W_enc_alive_norm` (where the term `avg_W_enc_alive_norm` is the mean norm of the encoder weights of alive neurons for that particular instance).
    * Set the encoder biases `b_enc[inst, d]` to zero.

So we really have just 2 changes: the added use of `avg_W_enc_alive_norm` for the encoder weights, and the sampling from the L2-based distribution to get our vectors `v`. Because this function can get a bit messy, we recommend you iterate through the instances rather than trying to resample them all at once.

For the sampling, we recommend that you use `torch.distributions.categorical.Categorical` to define a probability distribution, which can then be sampled from using the `sample` method. We've included an example of how to use this function below.

<details>
<summary>Example of using <code>Categorical</code>.</summary>

```python
from torch.distributions.categorical import Categorical

# Define a prob distn over (0, 1, 2, 3, 4) with probs proportional to (4, 3, 2, 1, 0)
values = t.arange(5).flip(0)
probs = values.float() / values.sum()
distribution = Categorical(probs = probs)

# Sample a single value from it
distribution.sample() # tensor(1)

# Sample multiple values with replacement (values will mostly be in the lower end of the range)
distribution.sample((10,)) # tensor([1, 1, 3, 0, 0, 1, 0, 3, 2, 2])
```

When you're sampling multiple times, make sure to pass a 1D tensor rather than a scalar.

</details>

Once you've implemented this resampling method, run the tests:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
@t.no_grad()
def resample_advanced(
    self: ToySAE,
    frac_active_in_window: Float[Tensor, "window inst d_sae"],
    resample_scale: float,
    batch_size: int,
) -> None:
    """
    Resamples latents that have been dead for 'dead_feature_window' steps, according to `frac_active`.

    Resampling method is:
        - Compute the L2 reconstruction loss produced from the hidden state vectors `h`
        - Randomly choose values of `h` with probability proportional to their reconstruction loss
        - Set new values of W_dec & W_enc to be these centered & normalized vecs, at each dead neuron
        - Set b_enc to be zero, at each dead neuron

    Returns colors and titles (useful for creating the animation: resampled neurons appear in red).
    """
    h = self.generate_batch(batch_size)
    l2_loss = self.forward(h)[0]["L_reconstruction"]

    for instance in range(self.cfg.n_inst):
        # Find the dead latents in this instance. If all latents are alive, continue
        is_dead = (frac_active_in_window[:, instance] < 1e-8).all(dim=0)
        dead_latents = t.nonzero(is_dead).squeeze(-1)
        n_dead = dead_latents.numel()
        if n_dead == 0:
            continue  # If we have no dead features, then we don't need to resample

        # Compute L2 loss for each element in the batch
        l2_loss_instance = l2_loss[:, instance]  # [batch_size]
        if l2_loss_instance.max() < 1e-6:
            continue  # If we have zero reconstruction loss, we don't need to resample

        # Draw `d_sae` samples from [0, 1, ..., batch_size-1], with probabilities proportional to
        # the values of l2_loss
        distn = Categorical(probs=l2_loss_instance.pow(2) / l2_loss_instance.pow(2).sum())
        replacement_indices = distn.sample((n_dead,))  # type: ignore

        # Index into the batch of hidden activations to get our replacement values
        replacement_values = (h - self.b_dec)[replacement_indices, instance]  # [n_dead d_in]
        replacement_values_normalized = replacement_values / (
            replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
        )

        # Get the norm of alive neurons (or 1.0 if there are no alive neurons)
        W_enc_norm_alive_mean = (
            self.W_enc[instance, :, ~is_dead].norm(dim=0).mean().item() if (~is_dead).any() else 1.0
        )

        # Lastly, set the new weights & biases (W_dec is normalized, W_enc needs specific scaling,
        # b_enc is zero)
        self.W_dec.data[instance, dead_latents, :] = replacement_values_normalized
        self.W_enc.data[instance, :, dead_latents] = (
            replacement_values_normalized.T * W_enc_norm_alive_mean * resample_scale
        )
        self.b_enc.data[instance, dead_latents] = 0.0


ToySAE.resample_advanced = resample_advanced
# END SOLUTION
# EXERCISE
# # Go back up and edit your `ToySAE.resample_advanced` method, then run the test below
# END EXERCISE

# HIDE
if MAIN:
    tests.test_resample_advanced(ToySAE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
After passing the tests, you can try training & visualizing your SAE again. You might not spot a lot of improvement with this resampling method in 2 dimensions, but for much higher-dimensional spaces it becomes highly beneficial to resample neurons in a more targeted way.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Gated & JumpReLU SAEs

In these sections, we'll discuss two alternative SAE architectures that seem to offer performance improvement over standard models. Both of them have similar intuitions (and are actually close to being mathematically equivalent under certain assumptions), although we'll focus on Gated SAEs first before moving to JumpReLU. This isn't necessarily because they're conceptually simpler (there's an argument that JumpReLU is simpler), it's more because they're easier to train. However, it's worth remembering during this section that both architectures are important and effective, and the intuitions from one often carry over to the other.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Gated SAEs

There are many different SAE architecture variants being explored at the moment. One especially exciting one is the **Gated SAE**, described in detail in this paper from [DeepMind](https://arxiv.org/pdf/2404.16014). We can motivate this architecture by starting with two observations

1. **Empirically, features usually seem to want to be binary.** For instance, we often see features like "is this about a basketball" which are better thought of as "off" or "on" than occupying some continuous range from 0 to 1. In practice reconstructing the precise coefficients does matter, and they often seem important for indicating something like the model's confidence in a particular feature being present. But still, we'd ideally like an architecture which can learn this discontinuity.

One easy option would be to have a discontinuous activation function in the hidden layer of our SAE, such as a **Jump ReLU**. This activation has a jump at some value $\theta$, and could allow us to represent this nonlinearity.

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wZqqQysfLrt2CFx4T/zzrdot3xexvcz3mqghn8" width="240">

However, there's another problem which Jump ReLUs alone** won't solve:

2. **SAEs suffer from [shrinkage](https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes).** Recall that the actual objective we want is that the L0 "norm" (the number of non-zero elements) of the hidden layer is small, and we use the L1 norm as a proxy for this. The two loss term in the SAE loss function have conflicting goals: the reconstruction term wants to make the autoencoder good at reconstructing the input, and the sparsity term wants to shrink the magnitude of the hidden layer. This means that even when perfect reconstruction is possible with only a single hidden unit activated, the sparsity loss will bias the magnitude of this hidden unit to zero, and the reconstruction will be worse.  

**_Note, JumpReLUs alone don't fix shrinkage, but JumpReLUs plus L0 penalty <u>does</u> fix shrinkage - we'll discuss this later in the chapter._

This brings us to **Gated SAEs**, which seem to fix both problems by having a Heaviside term which applies a discontinuity, and decoupling this term from the magnitude term. Instead of our standard function for computing SAE activations:

$$
\mathbf{f}(\mathbf{x}):=\operatorname{ReLU}\left(\mathbf{W}_{\mathrm{enc}}\left(\mathbf{x}-\mathbf{b}_{\mathrm{dec}}\right)+\mathbf{b}_{\mathrm{enc}}\right)
$$

we instead use:

$$
\tilde{\mathbf{f}}(\mathbf{x}):=\underbrace{\mathbf{1} [\overbrace{\mathbf{W}_{\text {gate }}\left(\mathbf{x}-\mathbf{b}_{\text {dec }}\right)+\mathbf{b}_{\text {gate }}}^{\pi_{\text {gate }}(\mathbf{x})}>0]}_{\mathbf{f}_{\text {gate }}(\mathbf{x})} \odot \underbrace{\operatorname{ReLU}\left(\mathbf{W}_{\text {mag }}\left(\mathbf{x}-\mathbf{b}_{\text {dec }}\right)+\mathbf{b}_{\text {mag }}\right)}_{\mathbf{f}_{\text {mag }}(\mathbf{x})}
$$
where $\mathbf{1}[\cdot > 0]$ is the pointwise Heaviside step function and $\odot$ is elementwise multiplication. The features' gate and activation magnitudes are computed by weight matrices, $W_{\text{mag}}$ and $W_{\text{gate}}$. Interestingly, if we tie the gated and magnitude weights as $\left(\mathbf{W}_{\text {mag }}\right)_{i j}:=\left(\exp \left(\mathbf{r}_{\text {mag }}\right)\right)_i \cdot\left(\mathbf{W}_{\text {gate }}\right)_{i j}$, then we can show that this is basically equivalent to a Jump ReLU activation function with a parameterized threshold value $\theta$ (left as an exercise to the reader!).

You might be wondering, how can we train this SAE? Ideally we'd place a sparsity penalty on the term $f_{\text{gate}}(\mathbf{x})$, since that's the thing which determines whether our activations will be zero or not. Unfortunately we can't do that, because gradients won't propagate through the Heaviside function (it's discontinuous). Instead, we apply a sparsity penalty to the preactivation $\pi_{\text {gate }}(\mathbf{x})$. So we have our loss function:

$$
\mathcal{L}_{\text {gated }}(\mathbf{x}):=\underbrace{\|\mathbf{x}-\hat{\mathbf{x}}(\tilde{\mathbf{f}}(\mathbf{x}))\|_2^2}_{\mathcal{L}_{\text {reconstruct }}}+\underbrace{\lambda\left\|\operatorname{ReLU}\left(\boldsymbol{\pi}_{\text {gate }}(\mathbf{x})\right)\right\|_1}_{\mathcal{L}_{\text {sparsity }}}
$$

However, there's a problem here. As long as the preactivation values $\pi_{\text {gate }}(\mathbf{x})$ are positive, reducing them will reduce the sparsity penalty without changing the reconstruction loss (all that matters for reconstruction is whether the preactivation values are positive or negative). So eventually they'll hit zero, and won't receive any more gradients (because the model's output will just always be zero from that point onwards). To combat this, we add an auxiliary loss term equal to the reconstruction loss when we swap out the true latent activations for the preactivation values $\pi_{\text {gate }}(\mathbf{x})$. This will add a gradient for the preactivations which pushes them up, offsetting the sparsity loss function which will only push those values down towards zero. We now have our final loss function:

$$
\mathcal{L}_{\text {gated }}(\mathbf{x}):=\underbrace{\|\mathbf{x}-\hat{\mathbf{x}}(\tilde{\mathbf{f}}(\mathbf{x}))\|_2^2}_{\mathcal{L}_{\text {reconstruct }}}+\underbrace{\lambda\left\|\operatorname{ReLU}\left(\boldsymbol{\pi}_{\text {gate }}(\mathbf{x})\right)\right\|_1}_{\mathcal{L}_{\text {sparsity }}}+\underbrace{\left\|\mathbf{x}-\hat{\mathbf{x}}_{\text {frozen }}\left(\operatorname{ReLU}\left(\boldsymbol{\pi}_{\text {gate }}(\mathbf{x})\right)\right)\right\|_2^2}_{\mathcal{L}_{\text {aux }}}
$$
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement Gated SAEs

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 60 minutes on this exercise.
> ```

Now, you have all the information you need to implement a Gated SAE and compare it to the standard model. Below we've given you the `GatedToySAE` class which should have modified versions of the `ToySAE` methods, in accordance with the descriptions above. 

*Note - an alternative way of implementing this would be to modify your `ToySAE` class to support both gated and standard architectures, e.g. by introducing an `architecture` argument in your SAE config class. You're encouraged to try this as a bonus exercise if you think it would be good practice for you!*

Some tips:

- For the forward pass and the loss function, you can reference Appendix G in the [DeepMind paper](https://arxiv.org/pdf/2404.16014), on page 34. We recommend sticking to the naming convention used by that appendix, as you'll probably find this easiest.
- Remember to create _and resample_ different weights if you're using the Gated architecture. For instance, if Gated then you should be zeroing `b_mag`, `b_gate` and `r_mag` at all dead latents.
- We recommend you tie the gate and magnitude weights by default, i.e. as $\left(\mathbf{W}_{\text {mag }}\right)_{i j}:=\exp \left(\mathbf{r}_{\text {mag }}\right)_i \times \left(\mathbf{W}_{\text {gate }}\right)_{i j}$ like they do in the paper. This kind of tying is arguably a lot less unnatural than tying encoder & decoder weights. If you're *also* tying weights, then you can interpret that as $W_{\text{dec}} = W_{\text{gate}}$.

<details>
<summary>Help - I'm not sure how I should implement this weight tying.</summary>

We recommend using a property, like this:

```python
@property
def W_mag(self) -> Float[Tensor, "inst d_in d_sae"]:
    assert self.cfg.architecture == "gated", "W_mag only available for gated model"
    return self.r_mag.exp().unsqueeze(1) * self.W_gate
```

Then you only have to define `r_mag` and `W_gate`. Note, this means you should be careful when you're resampling, because you can't set the values of `W_mag` directly.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class GatedToySAE(ToySAE):
    W_gate: Float[Tensor, "inst d_in d_sae"]
    b_gate: Float[Tensor, "inst d_sae"]
    r_mag: Float[Tensor, "inst d_sae"]
    b_mag: Float[Tensor, "inst d_sae"]
    _W_dec: Float[Tensor, "inst d_sae d_in"] | None
    b_dec: Float[Tensor, "inst d_in"]

    def __init__(self, cfg: ToySAEConfig, model: ToyModel):
        super(ToySAE, self).__init__()

        # EXERCISE
        # # YOUR CODE HERE - initialize the Gated model's weights & biases
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        assert cfg.d_in == model.cfg.d_hidden, "ToyModel's hidden dim doesn't match SAE input dim"
        self.cfg = cfg
        self.model = model.requires_grad_(False)
        self.model.W.data[1:] = self.model.W.data[0]
        self.model.b_final.data[1:] = self.model.b_final.data[0]

        self._W_dec = (
            None
            if self.cfg.tied_weights
            else nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_sae, cfg.d_in))))
        )
        self.b_dec = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_in))

        self.W_gate = nn.Parameter(
            nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_in, cfg.d_sae)))
        )
        self.b_gate = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))
        self.r_mag = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))
        self.b_mag = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))
        # END SOLUTION

        self.to(device)

    @property
    def W_dec(self) -> Float[Tensor, "inst d_sae d_in"]:
        # EXERCISE
        # # YOUR CODE HERE - return the decoder weights. Depending on what you name your
        # # weights in __init__, this may not differ from the `ToySAE` implementation.
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return self._W_dec if self._W_dec is not None else self.W_gate.transpose(-1, -2)
        # END SOLUTION

    @property
    def W_mag(self) -> Float[Tensor, "inst d_in d_sae"]:
        # EXERCISE
        # # YOUR CODE HERE - implement the magnitude weights getter (tied as described above).
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        return self.r_mag.exp().unsqueeze(1) * self.W_gate
        # END SOLUTION

    def forward(
        self, h: Float[Tensor, "batch inst d_in"]
    ) -> tuple[
        dict[str, Float[Tensor, "batch inst"]],
        Float[Tensor, ""],
        Float[Tensor, "batch inst d_sae"],
        Float[Tensor, "batch inst d_in"],
    ]:
        """
        Same as previous forward function, but allows for gated case as well (in which case we have
        different functional form, as well as a new term "L_aux" in the loss dict).
        """
        # EXERCISE
        # # YOUR CODE HERE - implement the Gated forward function. This will be similar
        # # to the standard forward function, but with the gating mechanism included
        # # (plus a new loss term "L_aux" in the loss dict).
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        h_cent = h - self.b_dec

        # Compute the gating terms (pi_gate(x) and f_gate(x) in the paper)
        gating_pre_activation = (
            einops.einsum(
                h_cent, self.W_gate, "batch inst d_in, inst d_in d_sae -> batch inst d_sae"
            )
            + self.b_gate
        )
        active_features = (gating_pre_activation > 0).float()

        # Compute the magnitude term (f_mag(x) in the paper)
        magnitude_pre_activation = (
            einops.einsum(
                h_cent, self.W_mag, "batch inst d_in, inst d_in d_sae -> batch inst d_sae"
            )
            + self.b_mag
        )
        feature_magnitudes = F.relu(magnitude_pre_activation)

        # Compute the hidden activations (f˜(x) in the paper)
        acts_post = active_features * feature_magnitudes

        # Compute reconstructed input
        h_reconstructed = (
            einops.einsum(
                acts_post, self.W_dec, "batch inst d_sae, inst d_sae d_in -> batch inst d_in"
            )
            + self.b_dec
        )

        # Compute loss terms
        gating_post_activation = F.relu(gating_pre_activation)
        via_gate_reconstruction = (
            einops.einsum(
                gating_post_activation,
                self.W_dec.detach(),
                "batch inst d_sae, inst d_sae d_in -> batch inst d_in",
            )
            + self.b_dec.detach()
        )
        loss_dict = {
            "L_reconstruction": (h_reconstructed - h).pow(2).mean(-1),
            "L_sparsity": gating_post_activation.sum(-1),
            "L_aux": (via_gate_reconstruction - h).pow(2).sum(-1),
        }

        loss = (
            loss_dict["L_reconstruction"]
            + self.cfg.sparsity_coeff * loss_dict["L_sparsity"]
            + loss_dict["L_aux"]
        )
        # END SOLUTION

        assert sorted(loss_dict.keys()) == ["L_aux", "L_reconstruction", "L_sparsity"]
        return loss_dict, loss, acts_post, h_reconstructed

    @t.no_grad()
    def resample_simple(
        self, frac_active_in_window: Float[Tensor, "window inst d_sae"], resample_scale: float
    ) -> None:
        # EXERCISE
        # # YOUR CODE HERE - implement the resample_simple function for the Gated SAE.
        # # This will be identical to the ToySAE implementation, except that it will
        # # apply to different weights & biases.
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        dead_latents_mask = (frac_active_in_window < 1e-8).all(dim=0)  # [instances d_sae]
        n_dead = int(dead_latents_mask.int().sum().item())

        replacement_values = t.randn((n_dead, self.cfg.d_in), device=self.W_gate.device)
        replacement_values_normed = replacement_values / (
            replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
        )

        # New names for weights & biases to resample
        self.W_gate.data.transpose(-1, -2)[dead_latents_mask] = (
            resample_scale * replacement_values_normed
        )
        self.W_dec.data[dead_latents_mask] = replacement_values_normed
        self.b_mag.data[dead_latents_mask] = 0.0
        self.b_gate.data[dead_latents_mask] = 0.0
        self.r_mag.data[dead_latents_mask] = 0.0
        # END SOLUTION

    @t.no_grad()
    def resample_advanced(
        self,
        frac_active_in_window: Float[Tensor, "window inst d_sae"],
        resample_scale: float,
        batch_size: int,
    ) -> None:
        # EXERCISE
        # # YOUR CODE HERE - implement the resample_advanced function for the Gated SAE.
        # # This will be identical to the ToySAE implementation, except that it will
        # # apply to different weights & biases.
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        h = self.generate_batch(batch_size)
        l2_loss = self.forward(h)[0]["L_reconstruction"]

        for instance in range(self.cfg.n_inst):
            is_dead = (frac_active_in_window[:, instance] < 1e-8).all(dim=0)
            dead_latents = t.nonzero(is_dead).squeeze(-1)
            n_dead = dead_latents.numel()
            if n_dead == 0:
                continue

            l2_loss_instance = l2_loss[:, instance]  # [batch_size]
            if l2_loss_instance.max() < 1e-6:
                continue

            distn = Categorical(probs=l2_loss_instance.pow(2) / l2_loss_instance.pow(2).sum())
            replacement_indices = distn.sample((n_dead,))  # type: ignore

            replacement_values = (h - self.b_dec)[replacement_indices, instance]  # [n_dead d_in]
            replacement_values_normalized = replacement_values / (
                replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
            )

            W_gate_norm_alive_mean = (
                self.W_gate[instance, :, ~is_dead].norm(dim=0).mean().item()
                if (~is_dead).any()
                else 1.0
            )

            # New names for weights & biases to resample
            self.W_dec.data[instance, dead_latents, :] = replacement_values_normalized
            self.W_gate.data[instance, :, dead_latents] = (
                replacement_values_normalized.T * W_gate_norm_alive_mean * resample_scale
            )
            self.b_mag.data[instance, dead_latents] = 0.0
            self.b_gate.data[instance, dead_latents] = 0.0
            self.r_mag.data[instance, dead_latents] = 0.0
        # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, you can run the code below to train a Gated SAE and visualize the results. Note that we're only plotting the best 4/16 instances (ranked according to loss averaged over the last 10 sampled batches), since generally SAEs with thresholding on toy models tend to more easily collapse into local minima (I suspect this is because thresholding flattens the loss landscape and allows more exploration & finding of local minima, whereas simple SAE architectures are more directly funnelled towards the global minimum).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

gated_sae = GatedToySAE(
    cfg=ToySAEConfig(
        n_inst=n_inst,
        d_in=d_in,
        d_sae=d_sae,
        sparsity_coeff=1.0,
    ),
    model=model,
)
gated_data_log = gated_sae.optimize(steps=20_000, resample_method="advanced")

# Animate the best instances, ranked according to average loss near the end of training
n_inst_to_plot = 4
n_batches_for_eval = 10
avg_loss = t.concat([d["loss"] for d in gated_data_log[-n_batches_for_eval:]]).mean(0)
best_instances = avg_loss.topk(n_inst_to_plot, largest=False).indices.tolist()

utils.animate_features_in_2d(
    gated_data_log,
    rows=["W_gate", "_W_dec", "h", "h_r"],
    instances=best_instances,
    filename=str(section_dir / "animation-training-gated.html"),
    color_resampled_latents=True,
    title="SAE on toy model",
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-animation-training-gated.html" width="1190" height="1215" style="background-color:white;">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - demonstrate advantage of Gated models

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> This is a quite long and unguided exercise, we recommend you come back to it after you've gone through the other content in this notebook.
> ```

When thinking about how thresholding models like Gated & JumpReLU can outperform standard SAEs, the plot to have in your head is the one below, from the appendix of DeepMind's Gated SAEs paper. The left histogram shows the distribution along a particular feature direction - the blue represents the distribution from interference when the feature is off but other non-orthogonal features are on, and the red represents the distribution then the feature is on. The distributions form a clearly bimodal pattern, and we can see in the figure on the right how a jump discontinuity (like the one provided by ReLU or by Gated models) can better model this discontinuity, by correctly reconstructing more of the interference cases (blue) as zero.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/distn-gated.png" width="900">

Although our data distribution isn't exactly the same as the one here, it is still bimodal: the histogram of "projection along feature direction $f$ conditional on $f$ being active" will have a significantly greater mean than the histogram of "projection along feature direction $f$ conditional on $f$ being inactive". In fact, you can try replicating this exact plot yourself and showing exactly how your Gated model outperforms the standard model.

We've left this exercise relatively open-ended rather than just being a function to fill in. If you want to attempt it, we recommend you get help from Claude or ChatGPT to create the visualization - the important part is understanding the plot well enough to know what data you need to gather in order to replicate it. Also, note that our toy model setup is slightly different from the paper's - we're using 5 independent features and so the "X off" distribution is down to interference from the other features, whereas the paper only considers a single feature and predefines an "X on" and "X off" distribution. The docstring should help you better understand what plot we're making here.

If you want, you can also extend the function `generate_batch` so that it supports a normal distribution with most of its probability mass in the range `[0, 1]` (this is what the `feat_mag_distn` field in the `ToyModelConfig` class is for) so that it more closely matches the distribution in the paper's toy model setup. However, you shouldn't have to do this to replicate the key result.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def generate_batch(self: ToyModel, batch_size: int) -> Float[Tensor, "batch inst feats"]:
    """
    Generates a batch of data of shape (batch_size, n_instances, n_features).

    This is optional, we just provide the function for you here to use for completeness (the code
    run below will not use "normal" distribution mode to generate the data), it'll use the same
    "unif" mode we've used so far.)
    """
    assert self.cfg.feat_mag_distn in ["unif", "normal"], (
        f"Unknown feature distribution: {self.cfg.feat_mag_distn}"
    )

    batch_shape = (batch_size, self.cfg.n_inst, self.cfg.n_features)
    feat_seeds = t.rand(batch_shape, device=self.W.device)
    feat_mag = (
        t.rand(batch_shape, device=self.W.device)
        if self.cfg.feat_mag_distn == "unif"
        else t.clip(0.5 + 0.2 * t.randn(batch_shape, device=self.W.device), min=0.0, max=1.0)
    )
    return t.where(feat_seeds <= self.feature_probability, feat_mag, 0.0)


ToyModel.generate_batch = generate_batch


@t.inference_mode()
def replicate_figure_15(sae_tuples: list[tuple[str, ToySAE, list[dict[str, Any]]]]) -> None:
    """
    This function should replicate figure 15 from the DeepMind paper, in a way which conforms to our
    toy model setup. It should create 2 plots:

        (1) A histogram of activation distributions projected along some chosen feature direction,
            color coded according to whether that feature is active or inactive. You should find
            that the distribution when active is almost always positive, and the distribution when
            not active has mean below zero.

        (2) A scatter plot of SAE reconstructions. In other words, the x-axis values should be the
            original feature values, and the y-axis should be the SAE's reconstructions of those
            features (i.e. the post-ReLU activations of the SAE). You should use different colors
            for different SAE architectures.
    """
    # ! (1) Histogram of activation projections

    # Generate a batch of features (with at least one feature in our instance being non-zero). Note,
    # our choice of model and instance / feature idx here is arbitrary, since we've verified all
    # models learn the uniform solution (we're only using models for this plot, not the saes)
    data = defaultdict(list)
    data = defaultdict(list)
    model = sae_tuples[0][1].model
    instance_idx = feature_idx = 0
    feature_idx = 1
    n_samples = 10_000
    feats = t.empty((0, model.cfg.n_features), device=device)
    while feats.shape[0] < n_samples:
        new_feats = model.generate_batch(n_samples)[:, instance_idx]
        new_feats = new_feats[(new_feats > 1e-4).any(dim=-1)]  # shape [batch, feats]
        feats = t.cat([feats, new_feats], dim=0)[:n_samples]

    # Map these to hidden activations, then project them back to feature directions for the 0th
    # feature, and plot them
    h = feats @ model.W[instance_idx].T
    h_proj = h @ model.W[instance_idx, :, feature_idx]
    is_active = feats[:, feature_idx] > 1e-4
    px.histogram(
        pd.DataFrame(
            {
                "x": h_proj.tolist(),
                "Feature": ["on" if active else "off" for active in is_active.tolist()],
            }
        ).sort_values(by="Feature", inplace=False),
        color="Feature",
        marginal="box",
        barmode="overlay",
        width=800,
        height=500,
        opacity=0.6,
        title="Distribution of activation projection",
    ).update_layout(bargap=0.02).show()

    # ! (2) Scatter plot of SAE reconstructions

    for mode, sae, data in sae_tuples:
        # Get repeated version of `h` to use in our fwd pass
        h = feats @ sae.model.W[instance_idx].T
        h_repeated = einops.repeat(h, "batch d_in -> batch inst d_in", inst=sae.cfg.n_inst)

        # Get the best instance, and get activations for this instance
        n_batches_for_eval = 10
        best_inst = (
            t.concat([d["loss"] for d in data_log[-n_batches_for_eval:]]).mean(0).argmin().item()
        )
        acts = sae.forward(h_repeated)[2][:, best_inst]  # shape [batch, d_sae]

        # Find the SAE latent that corresponds to this 0th feature (we're assuming here that there
        # actually is one!)
        latent_idx = acts[feats[:, feature_idx] > 1e-4].mean(0).argmax().item()

        # Add data for the second histogram. In this context we scale our activations by the norm of
        # model.W. This is because our activations `acts` are defined as the coefficients of unit
        # vecs whose sparse combination equals the true features, but our features `feats` weren't
        # defined this same way because model.W isn't normalized.
        data["Act"].extend(feats[:, feature_idx].tolist())
        data["Reconstructed act"].extend(
            (acts[:, latent_idx] / sae.model.W[best_inst, :, feature_idx].norm()).tolist()
        )
        data["SAE function"].extend([mode for _ in range(len(feats))])

    # Second histogram: comparison of activation projection & reconstructed activation projection
    px.scatter(
        pd.DataFrame(data),
        width=800,
        height=500,
        title=f"Act vs Reconstructed Act for {' & '.join(m.capitalize() for m, _, _ in sae_tuples)}",
        color="SAE function",
        x="Act",
        opacity=0.25,
        y="Reconstructed act",
        marginal_y="histogram",
        render_mode="webgl",
    ).add_shape(
        type="line",
        x0=0,
        y0=0,
        x1=1.1,
        y1=1.1,
        layer="below",
        line=dict(color="#666", width=2, dash="dash"),
    ).update_layout(
        xaxis=dict(range=[0, 1.1]), xaxis2=dict(range=[0, int(0.01 * n_samples)])
    ).show()


if MAIN:
    replicate_figure_15(
        [
            ("standard", resampling_sae, resampling_data_log),
            ("gated", gated_sae, gated_data_log),
        ],
    )
# END SOLUTION
# EXERCISE
# # YOUR CODE HERE - replicate figure 15a & 15b from the paper
# END EXERCISE

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-fig15a-gated-NEW.html" width="820" height="520" style="background-color:white;">
<br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-fig15b-gated-NEW.html" width="820" height="520" style="background-color:white;">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
If you do this correctly, you should observe a figure 15b plot that's similar to the one in the paper, except for 2 differences. One of them is the extra noise (i.e. datapoints which aren't on the monotonically increasing line) in both SAEs; this is because our toy model setup differs from DeepMind's (these points correspond to cases where more than one of our 5 features is active at once). However, there is another interesting difference too - can you spot it, and can you explain why it's there?

Note, if you've not been able to generate the plot, you can look at the solutions Colab or Streamlit dropdown, and then try to answer this question.

<details>
<summary>What the difference is</summary>

The line for the Gated model is the same as the paper, but the line for the standard model sits lower. It doesn't cross above the Gated line, like it does in the paper's diagram.

</details>

<details>
<summary>Explanation for the difference (hint)</summary>

Look at the section on the toy model in the DeepMind paper. How did they actually generate the data for that plot? Are there any particular phenomena we might experience in our plot which they wouldn't?

</details>

<details>
<summary>Explanation for the difference (answer)</summary>

The answer is **shrinkage**. 

The DeepMind paper didn't generate their figures by actually training SAEs with reconstruction loss & sparsity penalties; they analytically solved the problem by finding the projection (and bias / thresholding) that led to the smallest reconstruction loss. This meant that their standard SAE didn't suffer from shrinkage. But we trained ours on an L1 penalty, which means we do suffer from shrinkage - hence the line for the standard SAE falls below the gated line.

Note that the gated line (the non-zero piece of it) does approximately go through the line `x=y` i.e. it doesn't suffer from shrinkage - this is in line with what we expect (we discussed earlier how thresholding allows models to avoid the problem of shrinkage).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### JumpReLU SAEs

> Note - this section is a bit mathematically dense, and so you might want to skip it if you're not comfortable with this.

JumpReLU SAEs offer many of the same advantages as Gated SAEs, but they don't also require a detached forward pass to compute the auxiliary loss function like Gated SAEs do. Furthermore, evidence from the Gated SAEs paper (specifically the section on ablation studies) suggests that Gated SAEs don't benefit from the ability to untie the magnitude and gating weights, meaning we might just be better off working with JumpReLU SAEs! The only downside is that some groups have found them a bit harder to train, however for our simple models here we should be able to train them without much trouble.

The JumpReLU architecture is identical to regular SAEs, except we have an extra parameter $\theta$ (which is a vector of length `d_sae` representing the threshold for each latent), and our activation function is $\operatorname{JumpReLU}_\theta(z) = z H(z - \theta)$, where $z$ are the pre-activation SAE hidden values and $H$ is the Heaviside step function (i.e. value of 1 if $z > \theta$ and 0 otherwise). The function looks like:

<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wZqqQysfLrt2CFx4T/zzrdot3xexvcz3mqghn8" width="240">

We train JumpReLU SAEs against the following loss function:

$$
\mathcal{L}(\mathbf{x}):=\underbrace{\|\mathbf{x}-\hat{\mathbf{x}}(\mathbf{f}(\mathbf{x}))\|_2^2}_{\mathcal{L}_{\text {reconstruct }}}+\underbrace{\lambda\|\mathbf{f}(\mathbf{x})\|_0}_{\mathcal{L}_{\text {sparsity }}}
$$

This is just like the standard SAE loss function, except we penalize the L0 norm of the hidden activations directly, rather than L1. The question remains - how do we backprop these terms wrt $\theta$, since the heaviside function and L0 norm are both discontinuous? The answer comes from **straight-through-estimators** (STEs), which are a method for approximating gradients of non-differentiable functions. Specifically, we first rewrite the L0 term in terms of the Heaviside step function  $\|\mathbf{f}(\mathbf{x})\|_0 = \sum_{i=1}^{d_{\text{sae}}} H(\pi_i(\mathbf{x}) - \theta_i)$ where $\pi_i(\mathbf{x})$ are the pre-JumpReLU SAE hidden values. Next, since we've reduced the problem to just thinking about the Heaviside and JumpReLU functions, we can use the following estimates:

$$
\begin{aligned}
\frac{ð}{ð \theta} \operatorname{JumpReLU}_\theta(z) & :=-\frac{\theta}{\varepsilon} K\left(\frac{z-\theta}{\varepsilon}\right) \\
\frac{ð}{ð \theta} H(z-\theta) & :=-\frac{1}{\varepsilon} K\left(\frac{z-\theta}{\varepsilon}\right)
\end{aligned}
$$

where $K$ is some **valid kernel function** (i.e. must satisfy the properties of a centered, finite-variance probability density function). In the GDM experiments, they use the **rectangle function** $H(z+\frac{1}{2}) - H(z-\frac{1}{2})$. 

We provide 2 intuitions for why this works below - one functional/visual, and one probability-based. If you really don't care about this, you can skip to the exercise section (although we do encourage you to read at least one of these).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Functional / visual intuition

What we're effectively doing here is approximating discontinuous functions with sharp cumulative distribution functions. For example, take the heaviside function $H(z) = \mathbf{1}(z > 0)$. We can approximate this with a cdf $F$ which is sharp around the discontinuity (i.e. $F(z) = 0$ for all slightly negative $z$, and $F(z) = 1$ for all slightly positive $z$). The reason our derivative approximations above involve probability density functions $K$ is that the derivative of a cumulative distribution function $F$ is its probability density function.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/jumprelu-1.png" width="560">

If you're interested, the dropdown below derives this result using actual calculus (i.e. showing that the integral of these approximate derivatives over a sufficiently large region equals the size of the jump discontinuity). Note that this isn't crucial and we don't necessarily recommend it unless you're especially curious.

<details>
<summary>Derivation of this integral result (less important)</summary>

Suppose $F$ is the cumulative distribution function of $K$, so we have $F'(z) = K(z)$ and $F(-\infty) = 0, F(\infty) = 1$. Then let's compute the integral of the approximated Heaviside function over a region with centre $z$ and radius $\epsilon C$. Note we're computing the integral over a negative range, because it's moving $\theta$ from above $z$ to below $z$ that causes the output to jump from 0 to 1.

$$
\int\limits_{z+\epsilon C}^{z-\epsilon C} -\frac{1}{\epsilon} K\left(\frac{z-\theta}{\epsilon}\right) d\theta = \int\limits_{-C}^{C} K(\theta)\; d\theta = F(C) - F(-C) \xrightarrow[C \to \infty]{} 1 - 0 = 1
$$

which is the size of the jump discontinuity. Note that for our choice of the rectangle function $H(z+\frac{1}{2}) - H(z-\frac{1}{2})$ as the kernel function, this result holds even when we integrate over the small region with $C=\frac{1}{2}$, i.e. $\theta \in [z - \frac{\epsilon}{2}, z + \frac{\epsilon}{2}]$. It makes sense that we'd want a property like this, because the effect on our $\theta$ values should be largest when we're close to the jump discontinuity, and zero in most other regions.

For the JumpReLU term, after applying the reparametrization above, we can recognize the integral of $\theta K(\theta)$ as being the expected value of a variable with pdf $K$ (which is zero by our choice of $K$**), meaning we get:

$$
\int\limits_{z+\epsilon C}^{z-\epsilon C} -\frac{\theta}{\epsilon} K\left(\frac{z-\theta}{\epsilon}\right) d\theta = \int\limits_{-C}^{C} (z - \theta) K(\theta)\; d\theta = \int\limits_{-C}^{C} z K(\theta)\; d\theta \xrightarrow[C \to \infty]{} z
$$

which once again equals the size of the jump discontinuity, and once again is also a result that holds if we just take the region $\theta \in [z - \frac{\epsilon}{2}, z + \frac{\epsilon}{2}]$ for our chosen kernel $K$.

**Technically it's only zero if we integrate over the entire domain. But our choice of $K$ (as well as most reasonable choices for $K$) are not only centered at zero but also symmetric around zero and decay rapidly as we move away from zero, meaning we can make this assumption.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Probability-based intuition

Another way to think about this is that our inputs $x$ have some element of randomness. So our loss function values $\mathcal{L}_\theta(x)$ are themselves random variables which approximate the expected loss $\mathbb{E}_x\left[\mathcal{L}_\theta(x)\right]$. And it turns out that even if we can't compute the gradient of the loss directly if the loss contains a non-continuous term, we can compute the gradient of the expected loss. For example, consider the sparsity term $\|\mathbf{f}(\mathbf{x})\|_0 = \sum_{i=1}^{d_{\text{sae}}} H(z_i - \theta_i)$ (where $z_i$ are the pre-JumpReLU hidden values). This is not differentiable at zero, but its expected value is $\mathbb{E}_x \|\mathbf{f}(\mathbf{x})\|_0 = \sum_{i=1}^{d_{\text{sae}}} \mathbb{P}(z_i > \theta_i)$ which is differentiable - the derivative wrt $\theta_i$ is $-\mathbb{E}_x\left[p_i(z_i-\theta_i)\right]$, where $p_i$ are the probability density functions for $z_i$. 

Okay, so we know what we want our derivatives to be in expectation, but why does our choice $\frac{ð}{ð \theta} H(z-\theta) :=-\frac{1}{\varepsilon} K\left(\frac{z-\theta}{\varepsilon}\right)$ satisfy this? The answer is that this expression is a form of **kernel density estimation** (KDE), i.e. it approximates the pdf for a variable by smoothing out its empirical distribution. 

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/jumprelu-2.png" width="620">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Some final notes about JumpReLU SAEs, before we move on to the actual exercises:

- The nice thing about using L0 rather than L1 as a penalty is that we can target specific sparsity values. Rather than just using L0 as a penalty, we can use the squared difference between L0 and some target level: $\mathcal{L}_{\text {sparsity }}(\mathbf{x})=\lambda\left(\|\mathbf{f}(\mathbf{x})\|_0 / L_0^{\text {target }}-1\right)^2$. We won't implement this in these exercises, but you're welcome to try implementing it once you've got the standard version working.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement custom gradient functions

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 15-30 minutes on this and the next exercise.
> ```

We're going to start by implementing custom `jumprelu` and `heaviside` functions, roughly in line with the way DeepMind implements them in their appendix. PyTorch provides a helpful way to create custom functions with different behaviours in their forward and backward passes. For example, below is one with forward behaviour $f(x) = x^n$, and backward behaviour $f'(x) = nx^{n-1}$.

Note, we need to return `n * (input ** (n - 1)) * grad_output` from our backward function, rather than just `n * (input ** (n - 1))`, since we're actually computing $\frac{dL}{dx} = \frac{dL}{df(x)} \times f'(x)$ via the chain rule (where $x$ is `input` and $\frac{dL}{df(x)}$ is `grad_output`) - if you're confused here, you might want to revisit the ARENA material from the fundamentals chapter, on building your own backprop.

Also note that the `backward` function actually returns a tuple, which consists of all gradients wrt each of the `forward` arguments in the order they were in for `forward` (this includes the integer `n`). We return `None` since we don't need to track gradients wrt this variable.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class CustomFunction(t.autograd.Function):
    @staticmethod
    def forward(ctx: Any, input: Tensor, n: int) -> Tensor:
        # Save any necessary information for backward pass
        ctx.save_for_backward(input)
        ctx.n = n  # Save n as it will be needed in the backward pass
        # Compute the output
        return input**n

    @staticmethod
    def backward(ctx: Any, grad_output: Tensor) -> tuple[Tensor, None]:
        # Retrieve saved tensors and n
        (input,) = ctx.saved_tensors
        n = ctx.n
        # Return gradient for input and None for n (as it's not a Tensor)
        return n * (input ** (n - 1)) * grad_output, None


# Test our function, and its gradient
input = t.tensor(3.0, requires_grad=True)
output = CustomFunction.apply(input, 2)
output.backward()

t.testing.assert_close(output, t.tensor(9.0))
t.testing.assert_close(input.grad, t.tensor(6.0))

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should now implement your own `jumprelu` and `heaviside` functions. Note that both functions take 2 tensor inputs $z$ and $\theta$ as well as one float $\epsilon$. We're using the following conventions for our Heaviside function:

$$
\begin{aligned}
H(z, \theta; \epsilon) & := \boldsymbol{\mathbb{1}}[z - \theta > 0] \\
\frac{ð}{ð z} H(z, \theta; \epsilon) & := 0 \\
\frac{ð}{ð \theta} H(z, \theta; \epsilon) & := -\frac{1}{\epsilon} K\left(\frac{z-\theta}{\epsilon}\right) \\
\end{aligned}
$$

and for our JumpReLU:

$$
\begin{aligned}
\operatorname{JumpReLU}(z, \theta; \epsilon) & := z \cdot \boldsymbol{\mathbb{1}}[z - \theta > 0] \\
\frac{ð}{ð z} \operatorname{JumpReLU}(z, \theta; \epsilon) & := \boldsymbol{\mathbb{1}}[z - \theta > 0] \\
\frac{ð}{ð \theta} \operatorname{JumpReLU}(z, \theta; \epsilon) & :=-\frac{\theta}{\epsilon} K\left(\frac{z-\theta}{\epsilon}\right)
\end{aligned}
$$

where $K(x) = \boldsymbol{\mathbb{1}}\left[|x| < \frac{1}{2}\right]$ is the rectangle kernel function.

Note that in both cases we use the STE estimator for derivatives wrt $\theta$, but ignore STE estimates for $z$, i.e. we differentiate wrt $z$ pretending that $\frac{ð}{ð z} \boldsymbol{\mathbb{1}}[z - \theta > 0] = 0$. This is so that our parameter $\theta$ is the only one that implements the thresholding behaviour. Essentially, you can think of the other parameters being updated by gradient descent under the assumption that the output is a locally continuous function of those parameters.

A few final notes before you get started:

- We've given you the `rectangle` helper function which you can use in both implementations.
- You don't have to worry about broadcasting issues in this exercise, since PyTorch's autograd mechanism will handle this for you (for example if the gradient for `theta` you return from `backward` has a leading batch dimension meaning it's not the same shape as `theta`, it will automatically be summed over that dimension before being added to `theta.grad`). However, if you want to exactly match DeepMind's pseudocode in their paper appendix then you're certainly welcome to make this summing explicit. For more on the subtleties of summing & broadcasting over dimensions during backprop, see the first ARENA chapter!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def rectangle(x: Tensor, width: float = 1.0) -> Tensor:
    """
    Returns the rectangle function value, i.e. K(x) = 1[|x| < width/2], as a float.
    """
    return (x.abs() < width / 2).float()


class Heaviside(t.autograd.Function):
    """
    Implementation of the Heaviside step function, using straight through estimators for the derivative.

        forward:
            H(z,θ,ε) = 1[z > θ]

        backward:
            dH/dz := None
            dH/dθ := -1/ε * K(z/ε)

            where K is the rectangle kernel function with width 1, centered at 0: K(u) = 1[|u| < 1/2]
    """

    @staticmethod
    def forward(ctx: Any, z: Tensor, theta: Tensor, eps: float) -> Tensor:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Save any necessary information for backward pass
        ctx.save_for_backward(z, theta)
        ctx.eps = eps
        # Compute the output
        return (z > theta).float()
        # END SOLUTION

    @staticmethod
    def backward(ctx: Any, grad_output: Tensor) -> tuple[Tensor, Tensor, None]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Retrieve saved tensors & values
        (z, theta) = ctx.saved_tensors
        eps = ctx.eps
        # Compute gradient of the loss with respect to z (no STE) and theta (using STE)
        grad_z = 0.0 * grad_output
        grad_theta = -(1.0 / eps) * rectangle((z - theta) / eps) * grad_output
        grad_theta_agg = grad_theta.sum(dim=0)  # note, sum over batch dim isn't strictly necessary

        return grad_z, grad_theta_agg, None
        # END SOLUTION


# HIDE
if MAIN:
    # Test our Heaviside function, and its pseudo-gradient
    z = t.tensor([[1.0, 1.4, 1.6, 2.0]], requires_grad=True)
    theta = t.tensor([1.5, 1.5, 1.5, 1.5], requires_grad=True)
    eps = 0.5
    output = Heaviside.apply(z, theta, eps)
    output.backward(t.ones_like(output))  # equiv to backprop on each elem of z independently

    # Test values
    t.testing.assert_close(output, t.tensor([[0.0, 0.0, 1.0, 1.0]]))  # expect H(θ,z,ε) = 1[z > θ]
    t.testing.assert_close(
        theta.grad, t.tensor([0.0, -2.0, -2.0, 0.0])
    )  # expect dH/dθ = -1/ε * K((z-θ)/ε)
    t.testing.assert_close(z.grad, t.tensor([[0.0, 0.0, 0.0, 0.0]]))  # expect dH/dz = zero

    # Test handling of batch dimension
    theta.grad = None
    output_stacked = Heaviside.apply(t.concat([z, z]), theta, eps)
    output_stacked.backward(t.ones_like(output_stacked))
    t.testing.assert_close(theta.grad, 2 * t.tensor([0.0, -2.0, -2.0, 0.0]))

    print("All tests for `Heaviside` passed!")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I don't understand the expected values for the Heaviside function.</summary>

This diagram should help:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/jumprelu-3b.png" width="700">

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class JumpReLU(t.autograd.Function):
    """
    Implementation of the JumpReLU function, using straight through estimators for the derivative.

        forward:
            J(z,θ,ε) = z * 1[z > θ]

        backward:
            dJ/dθ := -θ/ε * K((z - θ)/ε)
            dJ/dz := 1[z > θ]

            where K is the rectangle kernel function with width 1, centered at 0: K(u) = 1[|u| < 1/2]
    """

    @staticmethod
    def forward(ctx: Any, z: Tensor, theta: Tensor, eps: float) -> Tensor:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Save any necessary information for backward pass
        ctx.save_for_backward(z, theta)
        ctx.eps = eps
        # Compute the output
        return z * (z > theta).float()
        # END SOLUTION

    @staticmethod
    def backward(ctx: Any, grad_output: Tensor) -> tuple[Tensor, Tensor, None]:
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Retrieve saved tensors & values
        (z, theta) = ctx.saved_tensors
        eps = ctx.eps
        # Compute gradient of the loss with respect to z (no STE) and theta (using STE)
        grad_z = (z > theta).float() * grad_output
        grad_theta = -(theta / eps) * rectangle((z - theta) / eps) * grad_output
        grad_theta_agg = grad_theta.sum(dim=0)  # note, sum over batch dim isn't strictly necessary
        return grad_z, grad_theta_agg, None
        # END SOLUTION


# HIDE
if MAIN:
    # Test our JumpReLU function, and its pseudo-gradient
    z = t.tensor([[1.0, 1.4, 1.6, 2.0]], requires_grad=True)
    theta = t.tensor([1.5, 1.5, 1.5, 1.5], requires_grad=True)
    eps = 0.5
    output = JumpReLU.apply(z, theta, eps)
    output.backward(
        t.ones_like(output)
    )  # equiv to backprop on each of the 5 elements of z independently

    # Test values
    t.testing.assert_close(
        output, t.tensor([[0.0, 0.0, 1.6, 2.0]])
    )  # expect J(θ,z,ε) = z * 1[z > θ]
    t.testing.assert_close(
        theta.grad, t.tensor([0.0, -3.0, -3.0, 0.0])
    )  # expect dJ/dθ = -θ/ε * K((z-θ)/ε)
    t.testing.assert_close(z.grad, t.tensor([[0.0, 0.0, 1.0, 1.0]]))  # expect dJ/dz = 1[z > θ]

    print("All tests for `JumpReLU` passed!")
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - I don't understand the expected values for the JumpReLU function.</summary>

This diagram should help. Remember that the STE is just meant to be an estimator for the discontinuous part of JumpReLU, not a continuous approximation to the whole function.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/jumprelu-4.png" width="700">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement JumpReLU SAEs

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 40 minutes on this exercise.
> ```

Now that you've implemented both these functions, you should have enough pieces to assemble the full JumpReLU SAE. We recommend that you build it in the same way as you built the Gated SAE in the previous exercise, i.e. creating a different class with the following differences from the standard SAE architecture:

- Add the parameter `log_theta`, which has shape `(n_instances, d_sae)` and produces your vectors `theta` which are used in your JumpReLU / Heaviside functions.
    - We use `log_theta` rather than `theta` because our threshold values should always be positive.
    - Both when initializing and resampling, we recommend taking `theta = 0.1` rather than the paper's value of `0.001` (this is because small values take a long time to increase, thanks to the small gradients in the `log` function). You'll need to convert these values to log-space when setting `log_theta`.
- SAE hidden values now use the JumpReLU activation function rather than standard ReLU, i.e. the i-th hidden value is $\operatorname{JumpReLU}_\theta(\pi_i(x))$, where $\pi_i(x) = (W_{enc}x + b_{enc})_i$ are the pre-JumpReLU activations.
    - In the DeepMind appendix, they suggest passing $\operatorname{ReLU}(\pi_i(x))$ rather than $\pi_i(x)$ into the ReLU and JumpReLU functions (this is so that negative values of $\pi_i(x)$ don't affect the gradient, in edge-case situations where $\theta_i$ has gotten small enough that we can have $0 > \pi_i(x) > \theta_i - \epsilon/2$). We recommend this too.
- The sparsity loss term is no longer the L1 norm, instead it's $\lambda \|\mathbf{f}(\mathbf{x})\|_0 = \sum_{i=1}^{d_{\text{sae}}} H(\pi_i(x) - \theta_i)$, where $\lambda$ is the sparsity coefficient.
    - We recommend starting with a value of `0.1` for the sparsity coefficient; this is given to you in the example code below.
    - Note that we still sum this L0 penalty term over `d_sae` rather than averaging it, for the same reasons as we summed over `d_sae` for our L1 penalty term.
- We recommend a default value of `ste_epsilon=0.01` for the STE, rather than the DeepMind paper's value of `0.001` (this is the default used by your `ToySAEConfig`).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

THETA_INIT = 0.1


class JumpReLUToySAE(ToySAE):
    W_enc: Float[Tensor, "inst d_in d_sae"]
    _W_dec: Float[Tensor, "inst d_sae d_in"] | None
    b_enc: Float[Tensor, "inst d_sae"]
    b_dec: Float[Tensor, "inst d_in"]
    log_theta: Float[Tensor, "inst d_sae"]
    # EXERCISE

    # # YOUR CODE HERE - write the methods of your new SAE, which should support all 3 modes
    # END EXERCISE
    # SOLUTION
    def __init__(self, cfg: ToySAEConfig, model: ToyModel):
        super(ToySAE, self).__init__()

        assert cfg.d_in == model.cfg.d_hidden, "ToyModel's hidden dim doesn't match SAE input dim"
        self.cfg = cfg
        self.model = model.requires_grad_(False)
        self.model.W.data[1:] = self.model.W.data[0]
        self.model.b_final.data[1:] = self.model.b_final.data[0]

        self._W_dec = (
            None
            if self.cfg.tied_weights
            else nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_sae, cfg.d_in))))
        )
        self.b_dec = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_in))

        self.W_enc = nn.Parameter(
            nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_in, cfg.d_sae)))
        )
        self.b_enc = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))
        self.log_theta = nn.Parameter(t.full((cfg.n_inst, cfg.d_sae), t.log(t.tensor(THETA_INIT))))

        self.to(device)

    @property
    def theta(self) -> Float[Tensor, "inst d_sae"]:
        return self.log_theta.exp()

    def forward(
        self, h: Float[Tensor, "batch inst d_in"]
    ) -> tuple[
        dict[str, Float[Tensor, "batch inst"]],
        Float[Tensor, ""],
        Float[Tensor, "batch inst d_sae"],
        Float[Tensor, "batch inst d_in"],
    ]:
        """
        Same as previous forward function, but allows for gated case as well (in which case we have different
        functional form, as well as a new term "L_aux" in the loss dict).
        """
        h_cent = h - self.b_dec

        acts_pre = (
            einops.einsum(
                h_cent, self.W_enc, "batch inst d_in, inst d_in d_sae -> batch inst d_sae"
            )
            + self.b_enc
        )
        # print(self.theta.mean(), self.theta.std(), self.theta.min(), self.theta.max())
        acts_relu = F.relu(acts_pre)
        acts_post = JumpReLU.apply(acts_relu, self.theta, self.cfg.ste_epsilon)

        h_reconstructed = (
            einops.einsum(
                acts_post, self.W_dec, "batch inst d_sae, inst d_sae d_in -> batch inst d_in"
            )
            + self.b_dec
        )

        loss_dict = {
            "L_reconstruction": (h_reconstructed - h).pow(2).mean(-1),
            "L_sparsity": Heaviside.apply(acts_relu, self.theta, self.cfg.ste_epsilon).sum(-1),
        }

        loss = loss_dict["L_reconstruction"] + self.cfg.sparsity_coeff * loss_dict["L_sparsity"]

        return loss_dict, loss, acts_post, h_reconstructed

    @t.no_grad()
    def resample_simple(
        self,
        frac_active_in_window: Float[Tensor, "window inst d_sae"],
        resample_scale: float,
    ) -> None:
        dead_latents_mask = (frac_active_in_window < 1e-8).all(dim=0)  # [instances d_sae]
        n_dead = int(dead_latents_mask.int().sum().item())

        replacement_values = t.randn((n_dead, self.cfg.d_in), device=self.W_enc.device)
        replacement_values_normed = replacement_values / (
            replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
        )

        # New names for weights & biases to resample
        self.W_enc.data.transpose(-1, -2)[dead_latents_mask] = (
            resample_scale * replacement_values_normed
        )
        self.W_dec.data[dead_latents_mask] = replacement_values_normed
        self.b_enc.data[dead_latents_mask] = 0.0
        self.log_theta.data[dead_latents_mask] = t.log(t.tensor(THETA_INIT))

    @t.no_grad()
    def resample_advanced(
        self,
        frac_active_in_window: Float[Tensor, "window inst d_sae"],
        resample_scale: float,
        batch_size: int,
    ) -> None:
        h = self.generate_batch(batch_size)
        l2_loss = self.forward(h)[0]["L_reconstruction"]

        for instance in range(self.cfg.n_inst):
            is_dead = (frac_active_in_window[:, instance] < 1e-8).all(dim=0)
            dead_latents = t.nonzero(is_dead).squeeze(-1)
            n_dead = dead_latents.numel()
            if n_dead == 0:
                continue

            l2_loss_instance = l2_loss[:, instance]  # [batch_size]
            if l2_loss_instance.max() < 1e-6:
                continue

            distn = Categorical(probs=l2_loss_instance.pow(2) / l2_loss_instance.pow(2).sum())
            replacement_indices = distn.sample((n_dead,))  # type: ignore

            replacement_values = (h - self.b_dec)[replacement_indices, instance]  # [n_dead d_in]
            replacement_values_normalized = replacement_values / (
                replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps
            )

            W_enc_norm_alive_mean = (
                self.W_enc[instance, :, ~is_dead].norm(dim=0).mean().item()
                if (~is_dead).any()
                else 1.0
            )

            # New names for weights & biases to resample
            self.b_enc.data[instance, dead_latents] = 0.0
            self.log_theta.data[instance, dead_latents] = t.log(t.tensor(THETA_INIT))
            self.W_dec.data[instance, dead_latents, :] = replacement_values_normalized
            self.W_enc.data[instance, :, dead_latents] = (
                replacement_values_normalized.T * W_enc_norm_alive_mean * resample_scale
            )

    # END SOLUTION


if MAIN:
    jumprelu_sae = JumpReLUToySAE(
        cfg=ToySAEConfig(
            n_inst=n_inst, d_in=d_in, d_sae=d_sae, tied_weights=True, sparsity_coeff=0.1
        ),
        model=model,
    )
    jumprelu_data_log = jumprelu_sae.optimize(
        steps=20_000, resample_method="advanced"
    )  # batch_size=4096?

    # Animate the best instances, ranked according to average loss near the end of training
    n_inst_to_plot = 4
    n_batches_for_eval = 10
    avg_loss = t.concat([d["loss"] for d in jumprelu_data_log[-n_batches_for_eval:]]).mean(0)
    best_instances = avg_loss.topk(n_inst_to_plot, largest=False).indices.tolist()

    utils.animate_features_in_2d(
        jumprelu_data_log,
        rows=["W_enc", "h", "h_r"],
        instances=best_instances,
        filename=str(section_dir / "animation-training-jumprelu.html"),
        color_resampled_latents=True,
        title="JumpReLU SAE on toy model",
    )

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Replicate figure 15 for jumprelu SAE (should get same results as for gated)
replicate_figure_15(
    [
        ("standard", resampling_sae, resampling_data_log),
        # ("gated", gated_sae, gated_data_log), # you can uncomment this to compare all 3!
        ("jumprelu", jumprelu_sae, jumprelu_data_log),
    ]
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-animation-training-jumprelu-2.html" width="1200" height="950" style="background-color:white;">
<br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-1320/1320-fig15b-jumprelu-v2.html" width="820" height="520" style="background-color:white;">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# ☆ Bonus
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Suggested paper replications
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#phase-change)

There are several aspects of this paper which we didn't cover in these exercises. In particular, **superposition as a phase change** studies the interaction between sparsity and relative feature importance, and finds a phase change in the optimal weight configuration as these inputs are varied. Some examples can be found in [this notebook](https://github.com/wattenberg/superposition/blob/main/Exploring_Exact_Toy_Models.ipynb)

This might be a good replication for you if:

* You enjoy diving into the specific mathematical details of superposition
* You find theoretical work interesting, as well as empirical work
* You've enjoyed the first ~3 exercise sets in this section

<br>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf)

This is a paper by Redwood Research, which builds on the ideas we discussed in the first three four of this paper (toy models of superposition, and the results on feature geometry).

They deeply study a measure called capacity, which is the same as what we called dimensionality above. Their results suggest an explanation for why features are often sharply "pinned" to either 0 or 1 capacity (i.e. not represented at all, or represented orthogonally to all other features).

This might be a good replication for you if:

* You enjoy diving into the specific mathematical details of superposition
* You're comfortable with mathematical topics like linear algebra and calculus
* You find theoretical work interesting, as well as empirical work
* You've enjoyed the first ~4 exercise sets in this section

<!-- ### [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/abs/2305.01610)

The authors train a set of sparse linear probes on neuron activations to predict the presence of certain input features. They manage to find **sparse combinations of neurons which represent many features in superposition**, e.g. a neuron which activates on the bigram phrase "social security" but not either word individually (see image below).

Note that this paper is slightly less relevant now that dictionary learning with SAEs has superceded its methodology - but it still represents a large step forward for the goal of extracting features from superposition in MLPs.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/socialsecurity.png" width="750"> -->
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Suggested topics for further exploration
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### [Softmax Linear Units](https://transformer-circuits.pub/2022/solu/index.html)

This is a proposed architectural change which appears to increase the number of interpretable MLPs with low performance cost. In particular, it may reduce the instance of superposition.

TL;DR: SOLU is an activation function $\vec{x} \to \vec{x} * \operatorname{softmax}(\vec{x})$ which encourages sparsity in activations in the same way that softmax encourages sparsity (often softmax'ed probability distributions have one probability close to one and the others close to zero). Encouraging activation sparsity might make it harder for neurons to be polysemantic.

Replication of the results of this paper might not be a practical final week project. However, several transformers in the TransformerLens library have been trained with SOLU (see the [model page](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html) for more details), which makes them a good candidate for closer study. Some questions you might want to explore:

- How do Neel's SoLU and GELU models compare in [neuroscope](https://neuroscope.io/) under the polysemanticity metric used in the SoLU paper? (what fraction of neurons seem monosemantic when looking at the top 10 activating dataset examples for 1 minute)
- The SoLU metrics for polysemanticity are somewhat limited, since they only provide information about whether a neuron is monosemantic when activating strongly (and this may not be corrrelated to whether it is monosemantic in general - see [this caveat](https://transformer-circuits.pub/2022/solu/index.html#:~:text=be%20reverse%2Dengineered.-,CAVEAT,-Since%20publication%2C%20we%27ve) in the paper). Can you find any better metrics? Can you be more reliable, or more scalable?
- The paper [speculates](https://transformer-circuits.pub/2022/solu/index.html#section-4-3) that the LayerNorm after the SoLU activations lets the model "smuggle through" superposition, by smearing features across many dimensions, having the output be very small, and letting the LayerNorm scale it up. Can you find any evidence of this in solu-1l?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)

There are many other interesting topics from Anthropic's dictionary learning paper which we didn't have time to dive into here, such as automated interpretability, feature motifs, and finite-state automata.

There's also a [Future Work](https://transformer-circuits.pub/2023/monosemantic-features/index.html#discussion-future-work) section at the end, which readers might find interesting for any project ideas!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### [Exciting Open Problems In Mech Interp v2](https://docs.google.com/document/d/1lIIzMjenXh-U0j5jkuqSDTawCoMNW4TqUlxk7mmbmRg/edit)

This document was written by Neel, and it collates a bunch of interesting open problems in mech interp (with a strong focus on SAE-related ones). Again, many of these could make great capstone projects! We encourage you to pick more achievable, less ambitious projects from this list though.

If any of the projects you're interested in involve training a sparse autoencoder, we *strongly* recommend [this post](https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes) by Arthur Conmy, which collates a bunch of different techniques for training SAEs well (most of which we didn't cover in these exercises).
'''




---
File: /infrastructure/master_files/master_1_4_1.py
---

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
[
    {"title": "Model & Task Setup", "icon": "1-circle-fill", "subtitle": "(5%)"},
    {"title": "Logit Attribution", "icon": "2-circle-fill", "subtitle": "(10%)"},
    {"title": "Activation Patching", "icon": "3-circle-fill", "subtitle": "(30%)"},
    {"title": "Path Patching", "icon": "4-circle-fill", "subtitle": "(30%)"},
    {"title": "Paper Replication", "icon": "5-circle-fill", "subtitle": "(25%)"},
    {"title": "Bonus", "icon": "star", "subtitle": ""},
]
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# [1.4.1] Indirect Object Identification
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-1.png" width="350">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This notebook / document is built around the [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper, in which the authors aim to understand the **indirect object identification circuit** in GPT-2 small. This circuit is resposible for the model's ability to complete sentences like `"John and Mary went to the shops, John gave a bag to"` with the correct token "`" Mary"`.

It is loosely divided into different sections, each one with their own flavour. Sections 1, 2 & 3 are derived from Neel Nanda's notebook [Exploratory_Analysis_Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=WXktSe0CvBdh). The flavour of these exercises is experimental and loose, with a focus on demonstrating what exploratory analysis looks like in practice with the transformerlens library. They skimp on rigour, and instead try to speedrun the process of finding suggestive evidence for this circuit. The code and exercises are simple and generic, but accompanied with a lot of detail about what each stage is doing, and why (plus several optional details and tangents). Section 4 introduces you to the idea of **path patching**, which is a more rigorous and structured way of analysing the model's behaviour. Here, you'll be replicating some of the results of the paper, which will serve to rigorously validate the insights gained from earlier sections. It's the most technically dense of all five sections. Lastly, sections 5 & 6 are much less structured, and have a stronger focus on open-ended exercises & letting you go off and explore for yourself.

Which exercises you want to do will depend on what you're hoping to get out of these exercises. For example:

* You want to understand activation patching - **1, 2, 3**
* You want to get a sense of how to do exploratory analysis on a model - **1, 2, 3**
* You want to understand activation and path patching - **1, 2, 3, 4**
* You want to understand the IOI circuit fully, and replicate the paper's key results - **1, 2, 3, 4, 5**
* You want to understand the IOI circuit fully, and replicate the paper's key results (but you already understand activation patching) - **1, 2, 4, 5**
* You want to understand IOI, and then dive deeper e.g. by looking for more circuits in models or investigating anomalies - **1, 2, 3, 4, 5, 6**

*Note - if you find yourself getting frequent CUDA memory errors, you can periodically call `torch.cuda.empty_cache()` to [free up some memory](https://stackoverflow.com/questions/57858433/how-to-clear-gpu-memory-after-pytorch-model-training-without-restarting-kernel).*

Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## The purpose / structure of these exercises

At a surface level, these exercises are designed to take you through the indirect object identification circuit. But it's also designed to make you a better interpretability researcher! As a result, most exercises will be doing a combination of:

1. Showing you some new feature/component of the circuit, and
2. Teaching you how to use tools and interpret results in a broader mech interp context.

A key idea to have in mind during these exercises is the **spectrum from simpler, more exploratory tools to more rigoruous, complex tools**. On the simpler side, you have something like inspecting attention patterns, which can give a decent (but sometimes misleading) picture of what an attention head is doing. These should be some of the first tools you reach for, and you should be using them a lot even before you have concrete hypotheses about a circuit. On the more rigorous side, you have something like path patching, which is a pretty rigorous and effortful tool that is best used when you already have reasonably concrete hypotheses about a circuit. As we go through the exercises, we'll transition from left to right along this spectrum.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## The IOI task

The first step when trying to reverse engineer a circuit in a model is to identify *what* capability we want to reverse engineer. Indirect Object Identification is a task studied in Redwood Research's excellent [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper (see [Neel Nanda's interview with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo) or [Kevin Wang's Twitter thread](https://threadreaderapp.com/thread/1587601532639494146.html) for an overview). The task is to complete sentences like "When Mary and John went to the store, John gave a drink to" with " Mary" rather than " John".

In the paper they rigorously reverse engineer a 26 head circuit, with 7 separate categories of heads used to perform this capability. The circuit they found roughly breaks down into three parts:

1. Identify what names are in the sentence
2. Identify which names are duplicated
3. Predict the name that is *not* duplicated

Why was this task chosen? The authors give a very good explanation for their choice in their [video walkthrough of their paper](https://www.youtube.com/watch?v=gzwj0jWbvbo), which you are encouraged to watch. To be brief, some of the reasons were:

* This is a fairly common grammatical structure, so we should expect the model to build some circuitry for solving it quite early on (after it's finished with all the more basic stuff, like n-grams, punctuation, induction, and simpler grammatical structures than this one).
* It's easy to measure: the model always puts a much higher probability on the IO and S tokens (i.e. `" Mary"` and `" John"`) than any others, and this is especially true once the model starts being stripped down to the core part of the circuit we're studying. So we can just take the logit difference between these two tokens, and use this as a metric for how well the model can solve the task.
* It is a crisp and well-defined task, so less likely to be solved in terms of memorisation of a large bag of heuristics (unlike e.g. tasks like "predict that the number `n+1` will follow `n`, which as Neel mentions in the video walkthrough is actually much more annoying and subtle than it first seems!).

A terminology note: `IO` will refer to the indirect object (in the example, `" Mary"`), `S1` and `S2` will refer to the two instances of the subject token (i.e. `" John"`), and `end` will refer to the end token `" to"` (because this is the position we take our prediction from, and we don't care about any tokens after this point). We will also sometimes use `S` to refer to the identity of the subject token (rather than referring to the first or second instance in particular).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Keeping track of your guesses & predictions

There's a lot to keep track of in these exercises as we work through them. You'll be exposed to new functions and modules from transformerlens, new ways to causally intervene in models, all the while building up your understanding of how the IOI task is performed. The notebook starts off exploratory in nature (lots of plotting and investigation), and gradually moves into more technical details, refined analysis, and replication of the paper's results, as we improve our understanding of the IOI circuit. You are recommended to keep a document or page of notes nearby as you go through these exercises, so you can keep track of the main takeaways from each section, as well as your hypotheses for how the model performs the task, and your ideas for how you might go off and test these hypotheses on your own if the notebook were to suddenly end.

If you are feeling extremely confused at any point, you can come back to the dropdown below, which contains diagrams explaining how the circuit works. There is also an accompanying intuitive explanation which you might find more helpful. However, I'd recommend you try and go through the notebook unassisted before looking at these.

<details>
<summary>Intuitive explanation of IOI circuit</summary>

First, let's start with an analogy for how transformers work (you can skip this if you've already read [my post](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)). Imagine a line of people, who can only look forward. Each person has a token written on their chest, and their goal is to figure out what token the person in front of them is holding. Each person is allowed to pass a question backwards along the line (not forwards), and anyone can choose to reply to that question by passing information forwards to the person who asked. In this case, the sentence is `"When Mary and John went to the store, John gave a drink to Mary"`. You are the person holding the `" to"` token, and your goal is to figure out that the person in front of him has the `" Mary"` token.

To be clear about how this analogy relates to transformers:
* Each person in the line represents a vector in the residual stream. Initially they just store their own token, but they accrue more information as they ask questions and receive answers (i.e. as components write to the residual stream)
* The operation of an attention head is represented by a question & answer:
    * The person who asks is the destination token, the people who answer are the source tokens
    * The question is the query vector
    * The information *which determines who answers the question* is the key vector
    * The information *which gets passed back to the original asker* is the value vector

Now, here is how the IOI circuit works in this analogy. Each bullet point represents a class of attention heads.

* The person with the second `" John"` token asks the question "does anyone else hold the name `" John"`?". They get a reply from the first `" John"` token, who also gives him their location. So he now knows that `" John"` is repeated, and he knows that the first `" John"` token is 4th in the sequence.
    * These are *Duplicate Token Heads*
* You ask the question "which names are repeated?", and you get an answer from the person holding the second `" John"` token. You now also know that `" John"` is repeated, and where the first `" John"` token is.
    * These are *S-Inhibition Heads*
* You ask the question "does anyone have a name that isn't `" John"`, and isn't at the 4th position in the sequence?". You get a reply from the person holding the `" Mary"` token, who tells you that they have name `" Mary"`. You use this as your prediction.
    * These are *Name Mover Heads*

This is a fine first-pass understanding of how the circuit works. A few other features:

* The person after the first `" John"` (holding `" went"`) had previously asked about the identity of the person behind him. So he knows that the 4th person in the sequence holds the `" John"` token, meaning he can also reply to the question of the person holding the second `" John"` token. *(previous token heads / induction heads)*
    * This might not seem necessary, but since previous token heads / induction heads are just a pretty useful thing to have in general, it makes sense that you'd want to make use of this information!
* If for some reason you forget to ask the question "does anyone have a name that isn't `" John"`, and isn't at the 4th position in the sequence?", then you'll have another chance to do this.
    * These are *(Backup Name Mover Heads)*
    * Their existance might be partly because transformers are trained with **dropout**. This can make them "forget" things, so it's important to have a backup method for recovering that information!
* You want to avoid overconfidence, so you also ask the question "does anyone have a name that isn't `" John"`, and isn't at the 4th position in the sequence?" another time, in order to ***anti-***predict the response that you get from this question. *(negative name mover heads)*
    * Yes, this is as weird as it sounds! The authors speculate that these heads "hedge" the predictions, avoiding high cross-entropy loss when making mistakes.

</details>

<details>
<summary>Diagram 1 (simple)</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-simple-a.png" width="1000">

</details>

<details>
<summary>Diagram 2 (complex)</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-full-d.png" width="1250">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Content & Learning Objectives

### 1️⃣ Model & Task Setup

In this section you'll set up your model, and see how to analyse its performance on the IOI task. You'll also learn how to measure it's performance using tools like logit difference.

> ##### Learning Objectives
>
> * Understand the IOI task, and why the authors chose to study it
> * Build functions to demonstrate the model's performance on this task

### 2️⃣ Logit Attribution

Next, you'll move on to some component attribution: evaluating the importance of each model component for the IOI task. However, this type of analysis is limited to measuring a component's direct effect, as opposed to indirect effect - we'll measure the latter in future sections.

> ##### Learning Objectives
>
> * Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way
> * Learn how to use different transformerlens helper functions, which decompose the residual stream in different ways

### 3️⃣ Activation Patching

We introduce one of the two important patching tools you'll use during this section: **activation patching**. This can be used to discover which components of a model are important for a particular task, by measuring the changes in our previously-defined task metrics when you patch into a particular component with corrupted input.

> ##### Learning Objectives
>
> * Understand the idea of activation patching, and how it can be used
>     * Implement some of the activation patching helper functinos in transformerlens from scratch (i.e. using hooks)
> * Use activation patching to track the layers & sequence positions in the residual stream where important information is stored and processed
> * By the end of this section, you should be able to draw a rough sketch of the IOI circuit

### 4️⃣ Path Patching

Next, we move to path patching, a more refined form of activation patching which examines the importance of particular paths between model components. This will give us a more precise picture of how our circuit works.

> ##### Learning Objectives
>
> * Understand the idea of path patching, and how it differs from activation patching
> * Implement path patching from scratch (i.e. using hooks)
> * Replicate several of the results in the [IOI paper](https://arxiv.org/abs/2211.00593)

### 5️⃣ Full Replication: Minimial Circuits and more

Lastly, we'll do some cleaning up, by replicating some other results from the IOI paper. This includes implementing a complex form of ablation which removes every component from the model except for the ones we've identified from previous analysis, and showing that the performace is recovered. This section is more open-ended and less structured.

> ##### Learning Objectives
>
> * Replicate most of the other results from the [IOI paper](https://arxiv.org/abs/2211.00593)
> * Practice more open-ended, less guided coding

### ☆ Bonus / exploring anomalies

We end with a few suggested bonus exercises for this particular circuit, as well as ideas for capstone projects / paper replications.

> ##### Learning Objectives
>
> * Explore other parts of the model (e.g. negative name mover heads, and induction heads)
> * Understand the subtleties present in model circuits, and the fact that there are often more parts to a circuit than seem obvious after initial investigation
> * Understand the importance of the three quantitative criteria used by the paper: **faithfulness**, **completeness** and **minimality**
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup code
'''

# ! CELL TYPE: code
# ! FILTERS: [~]
# ! TAGS: []

from IPython import get_ipython

ipython = get_ipython()
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: [master-comment]

# import os
# import sys
# from pathlib import Path

# IN_COLAB = "google.colab" in sys.modules

# chapter = "chapter1_transformer_interp"
# repo = "ARENA_3.0"
# branch = "main"

# # Install dependencies
# try:
#     import transformer_lens
# except:
#     %pip install transformer_lens==2.11.0 einops jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python

# # Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
# root = (
#     "/content"
#     if IN_COLAB
#     else "/root"
#     if repo not in os.getcwd()
#     else str(next(p for p in Path.cwd().parents if p.name == repo))
# )

# if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
#     if not IN_COLAB:
#         !sudo apt-get install unzip
#         %pip install jupyter ipython --upgrade

#     if not os.path.exists(f"{root}/{chapter}"):
#         !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
#         !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
#         !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
#         !rm {root}/{branch}.zip
#         !rmdir {root}/{repo}-{branch}


# if f"{root}/{chapter}/exercises" not in sys.path:
#     sys.path.append(f"{root}/{chapter}/exercises")

# os.chdir(f"{root}/{chapter}/exercises")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import re
import sys
from functools import partial
from itertools import product
from pathlib import Path
from typing import Callable, Literal

import circuitsvis as cv
import einops
import numpy as np
import plotly.express as px
import torch as t
from IPython.display import HTML, display
from jaxtyping import Bool, Float, Int
from rich import print as rprint
from rich.table import Column, Table
from torch import Tensor
from tqdm.notebook import tqdm
from transformer_lens import ActivationCache, HookedTransformer, utils
from transformer_lens.components import MLP, Embed, LayerNorm, Unembed
from transformer_lens.hook_points import HookPoint

t.set_grad_enabled(False)
device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part41_indirect_object_identification"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
# FILTERS: ~colab
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))
# END FILTERS

import part41_indirect_object_identification.tests as tests
from plotly_utils import bar, imshow, line, scatter

MAIN = __name__ == "__main__"

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 1️⃣ Model & Task Setup
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Loading our model
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The first step is to load in our model, GPT-2 Small, a 12 layer and 80M parameter transformer with `HookedTransformer.from_pretrained`. The various flags are simplifications that preserve the model's output but simplify its internals.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model = HookedTransformer.from_pretrained(
    "gpt2-small",
    center_unembed=True,
    center_writing_weights=True,
    fold_ln=True,
    refactor_factored_attn_matrices=True,
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Note on <code>refactor_factored_attn_matrices</code> (optional)</summary>

This argument means we redefine the matrices $W_Q$, $W_K$, $W_V$ and $W_O$ in the model (without changing the model's actual behaviour).

For example, we know that instead of working with $W_Q$ and $W_K$ individually, the only matrix we actually need to use in the model is the low-rank matrix $W_Q W_K^T$ (note that I'm using the convention of matrix multiplication on the right, which matches the code in transformerlens and previous exercises in this series, but doesn't match Anthropic's Mathematical Frameworks paper). So if we perform singular value decomposition $W_Q W_K^T = U S V^T$, then we see that we can just as easily define $W_Q = U \sqrt{S}$ and $W_K = V \sqrt{S}$ and use these instead. This means that $W_Q$ and $W_K$ both have orthogonal columns with matching norms. You can investigate this yourself (e.g. using the code below). This is arguably a more interpretable setup, because now there's no obvious asymmetry between the keys and queries.

There's also some fiddlyness with how biases are handled in this factorisation, which is why the comments above don't hold absolutely (see the documentation for more info).

```python
# Show column norms are the same (except first few, for fiddly bias reasons)
line([model.W_Q[0, 0].pow(2).sum(0), model.W_K[0, 0].pow(2).sum(0)])
# Show columns are orthogonal (except first few, again)
W_Q_dot_products = einops.einsum(
    model.W_Q[0, 0], model.W_Q[0, 0], "d_model d_head_1, d_model d_head_2 -> d_head_1 d_head_2"
)
imshow(W_Q_dot_products)
```

In a similar way, since $W_{OV} = W_V W_O = U S V^T$, we can define $W_V = U S$ and $W_O = V^T$. This is arguably a more interpretable setup, because now $W_O$ is just a rotation, and doesn't change the norm, so $z$ has the same norm as the result of the head.
</details>

<details>
<summary>Note on <code>fold_ln</code>, <code>center_unembed</code> and <code>center_writing_weights</code> (optional)</summary>

See link [here](https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln) for comments.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The next step is to verify that the model can *actually* do the task! Here we use `utils.test_prompt`, and see that the model is significantly better at predicting Mary than John!

<details><summary>Asides</summary>

Note: If we were being careful, we'd want to run the model on a range of prompts and find the average performance. We'll do more stuff like this in the fourth section (when we try to replicate some of the paper's results, and take a more rigorous approach).

`prepend_bos` is a flag to add a BOS (beginning of sequence) to the start of the prompt. GPT-2 was not trained with this, but I find that it often makes model behaviour more stable, as the first token is treated weirdly.
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Here is where we test on a single prompt
# Result: 70% probability on Mary, as we expect

example_prompt = "After John and Mary went to the store, John gave a bottle of milk to"
example_answer = " Mary"
utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']
Tokenized answer: [' Mary']

Performance on answer token:
<span style="font-weight: bold">Rank: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="font-weight: bold">        Logit: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18.09</span><span style="font-weight: bold"> Prob: </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">70.07</span><span style="font-weight: bold">% Token: | Mary|</span>

Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|
Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|
Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|
Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|
Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|
Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|
Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|
Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|
Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|
Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|

<span style="font-weight: bold">Ranks of the answer tokens:</span> <span style="font-weight: bold">[(</span><span style="color: #008000; text-decoration-color: #008000">' Mary'</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="font-weight: bold">)]</span>
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We now want to find a reference prompt to run the model on. Even though our ultimate goal is to reverse engineer how this behaviour is done in general, often the best way to start out in mechanistic interpretability is by zooming in on a concrete example and understanding it in detail, and only *then* zooming out and verifying that our analysis generalises. In section 3, we'll work with a dataset similar to the one used by the paper authors, but this probably wouldn't be the first thing we reached for if we were just doing initial investigations.

We'll run the model on 4 instances of this task, each prompt given twice - one with the first name as the indirect object, one with the second name. To make our lives easier, we'll carefully choose prompts with single token names and the corresponding names in the same token positions.

<details><summary>Aside on tokenization</summary>

We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency!

Tokens are a *massive* headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`

**Exercise:** I recommend using `model.to_str_tokens` to explore how the model tokenizes different strings. In particular, try adding or removing spaces at the start, or changing capitalization - these change tokenization!</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

prompt_format = [
    "When John and Mary went to the shops,{} gave the bag to",
    "When Tom and James went to the park,{} gave the ball to",
    "When Dan and Sid went to the shops,{} gave an apple to",
    "After Martin and Amy went to the park,{} gave a drink to",
]
name_pairs = [
    (" Mary", " John"),
    (" Tom", " James"),
    (" Dan", " Sid"),
    (" Martin", " Amy"),
]

# Define 8 prompts, in 4 groups of 2 (with adjacent prompts having answers swapped)
prompts = [
    prompt.format(name)
    for (prompt, names) in zip(prompt_format, name_pairs)
    for name in names[::-1]
]
# Define the answers for each prompt, in the form (correct, incorrect)
answers = [names[::i] for names in name_pairs for i in (1, -1)]
# Define the answer tokens (same shape as the answers)
answer_tokens = t.concat([model.to_tokens(names, prepend_bos=False).T for names in answers])

rprint(prompts)
rprint(answers)
rprint(answer_tokens)

table = Table("Prompt", "Correct", "Incorrect", title="Prompts & Answers:")

for prompt, answer in zip(prompts, answers):
    table.add_row(prompt, repr(answer[0]), repr(answer[1]))

rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
    <span style="color: #008000; text-decoration-color: #008000">'When John and Mary went to the shops, John gave the bag to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'When John and Mary went to the shops, Mary gave the bag to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'When Tom and James went to the park, James gave the ball to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'When Tom and James went to the park, Tom gave the ball to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'When Dan and Sid went to the shops, Sid gave an apple to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'When Dan and Sid went to the shops, Dan gave an apple to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'After Martin and Amy went to the park, Amy gave a drink to'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'After Martin and Amy went to the park, Martin gave a drink to'</span>
<span style="font-weight: bold">]</span>

<span style="font-weight: bold">[</span>
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Mary'</span>, <span style="color: #008000; text-decoration-color: #008000">' John'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' John'</span>, <span style="color: #008000; text-decoration-color: #008000">' Mary'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Tom'</span>, <span style="color: #008000; text-decoration-color: #008000">' James'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' James'</span>, <span style="color: #008000; text-decoration-color: #008000">' Tom'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Dan'</span>, <span style="color: #008000; text-decoration-color: #008000">' Sid'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Sid'</span>, <span style="color: #008000; text-decoration-color: #008000">' Dan'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Martin'</span>, <span style="color: #008000; text-decoration-color: #008000">' Amy'</span><span style="font-weight: bold">)</span>,
    <span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">' Amy'</span>, <span style="color: #008000; text-decoration-color: #008000">' Martin'</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">]</span>

<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5335</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1757</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1757</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5335</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4186</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3700</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3700</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4186</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6035</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15686</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15686</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6035</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5780</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14235</span><span style="font-weight: bold">]</span>,
        <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14235</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5780</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">device</span>=<span style="color: #008000; text-decoration-color: #008000">'cuda:0'</span><span style="font-weight: bold">)</span>

<span style="font-style: italic">                                   Prompts &amp; Answers:                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                                        </span>┃<span style="font-weight: bold"> Correct   </span>┃<span style="font-weight: bold"> Incorrect </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩
│ When John and Mary went to the shops, John gave the bag to    │ ' Mary'   │ ' John'   │
│ When John and Mary went to the shops, Mary gave the bag to    │ ' John'   │ ' Mary'   │
│ When Tom and James went to the park, James gave the ball to   │ ' Tom'    │ ' James'  │
│ When Tom and James went to the park, Tom gave the ball to     │ ' James'  │ ' Tom'    │
│ When Dan and Sid went to the shops, Sid gave an apple to      │ ' Dan'    │ ' Sid'    │
│ When Dan and Sid went to the shops, Dan gave an apple to      │ ' Sid'    │ ' Dan'    │
│ After Martin and Amy went to the park, Amy gave a drink to    │ ' Martin' │ ' Amy'    │
│ After Martin and Amy went to the park, Martin gave a drink to │ ' Amy'    │ ' Martin' │
└───────────────────────────────────────────────────────────────┴───────────┴───────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - the <code>rich</code> library</summary>

The outputs above were created by `rich`, a fun library which prints things in nice formats. It has functions like `rich.table.Table`, which are very easy to use but can produce visually clear outputs which are sometimes useful.

You can also color the columns of a table, by using the `rich.table.Column` argument with the `style` parameter:

```python
cols = [
    "Prompt",
    Column("Correct", style="rgb(0,200,0) bold"),
    Column("Incorrect", style="rgb(255,0,0) bold"),
]
table = Table(*cols, title="Prompts & Answers:")

for prompt, answer in zip(prompts, answers):
    table.add_row(prompt, repr(answer[0]), repr(answer[1]))

rprint(table)
```
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We now run the model on these prompts and use `run_with_cache` to get both the logits and a cache of all internal activations for later analysis.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

tokens = model.to_tokens(prompts, prepend_bos=True)
# Move the tokens to the GPU
tokens = tokens.to(device)
# Run the model and cache all activations
original_logits, cache = model.run_with_cache(tokens)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the indirect object's name and the subject's name (eg, `logit(Mary) - logit(John)`).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement the performance evaluation function

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> It's important to understand exactly what this function is computing, and why it matters.
> ```

This function should take in your model's logit output (shape `(batch, seq, d_vocab)`), and the array of answer tokens (shape `(batch, 2)`, containing the token ids of correct and incorrect answers respectively for each sequence), and return the logit difference as described above. If `per_prompt` is False, then it should take the mean over the batch dimension, if not then it should return an array of length `batch`.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def logits_to_ave_logit_diff(
    logits: Float[Tensor, "batch seq d_vocab"],
    answer_tokens: Int[Tensor, "batch 2"] = answer_tokens,
    per_prompt: bool = False,
) -> Float[Tensor, "*batch"]:
    """
    Returns logit difference between the correct and incorrect answer.

    If per_prompt=True, return the array of differences rather than the average.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    # Only the final logits are relevant for the answer
    final_logits: Float[Tensor, "batch d_vocab"] = logits[:, -1, :]
    # Get the logits corresponding to the indirect object / subject tokens respectively
    answer_logits: Float[Tensor, "batch 2"] = final_logits.gather(dim=-1, index=answer_tokens)
    # Find logit difference
    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)
    answer_logit_diff = correct_logits - incorrect_logits
    return answer_logit_diff if per_prompt else answer_logit_diff.mean()
    # END SOLUTION


# HIDE
tests.test_logits_to_ave_logit_diff(logits_to_ave_logit_diff)

original_per_prompt_diff = logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)
print("Per prompt logit difference:", original_per_prompt_diff)
original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)
print("Average logit difference:", original_average_logit_diff)

cols = [
    "Prompt",
    Column("Correct", style="rgb(0,200,0) bold"),
    Column("Incorrect", style="rgb(255,0,0) bold"),
    Column("Logit Difference", style="bold"),
]
table = Table(*cols, title="Logit differences")

for prompt, answer, logit_diff in zip(prompts, answers, original_per_prompt_diff):
    table.add_row(prompt, repr(answer[0]), repr(answer[1]), f"{logit_diff.item():.3f}")

rprint(table)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                             Logit differences                                              </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                                        </span>┃<span style="font-weight: bold"> Correct   </span>┃<span style="font-weight: bold"> Incorrect </span>┃<span style="font-weight: bold"> Logit Difference </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ When John and Mary went to the shops, John gave the bag to    │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Mary'   </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' John'   </span>│<span style="font-weight: bold"> 3.337            </span>│
│ When John and Mary went to the shops, Mary gave the bag to    │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' John'   </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Mary'   </span>│<span style="font-weight: bold"> 3.202            </span>│
│ When Tom and James went to the park, James gave the ball to   │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Tom'    </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' James'  </span>│<span style="font-weight: bold"> 2.709            </span>│
│ When Tom and James went to the park, Tom gave the ball to     │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' James'  </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Tom'    </span>│<span style="font-weight: bold"> 3.797            </span>│
│ When Dan and Sid went to the shops, Sid gave an apple to      │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Dan'    </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Sid'    </span>│<span style="font-weight: bold"> 1.720            </span>│
│ When Dan and Sid went to the shops, Dan gave an apple to      │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Sid'    </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Dan'    </span>│<span style="font-weight: bold"> 5.281            </span>│
│ After Martin and Amy went to the park, Amy gave a drink to    │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Martin' </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Amy'    </span>│<span style="font-weight: bold"> 2.601            </span>│
│ After Martin and Amy went to the park, Martin gave a drink to │<span style="color: #00c800; text-decoration-color: #00c800; font-weight: bold"> ' Amy'    </span>│<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold"> ' Martin' </span>│<span style="font-weight: bold"> 5.767            </span>│
└───────────────────────────────────────────────────────────────┴───────────┴───────────┴──────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Brainstorm What's Actually Going On
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Before diving into running experiments, it's often useful to spend some time actually reasoning about how the behaviour in question could be implemented in the transformer. **This is optional, and you'll likely get the most out of engaging with this section if you have a decent understanding already of what a transformer is and how it works!**

You don't have to do this and forming hypotheses after exploration is also reasonable, but I think it's often easier to explore and interpret results with some grounding in what you might find. In this particular case, I'm cheating somewhat, since I know the answer, but I'm trying to simulate the process of reasoning about it!

Note that often your hypothesis will be wrong in some ways and often be completely off. We're doing science here, and the goal is to understand how the model *actually* works, and to form true beliefs! There are two separate traps here at two extremes that it's worth tracking:
* Confusion: Having no hypotheses at all, getting a lot of data and not knowing what to do with it, and just floundering around
* Dogmatism: Being overconfident in an incorrect hypothesis and being unwilling to let go of it when reality contradicts you, or flinching away from running the experiments that might disconfirm it.

**Exercise:** Spend some time thinking through how you might imagine this behaviour being implemented in a transformer. Try to think through this for yourself before reading through my thoughts!

<details> <summary>(*) <b>My reasoning</b></summary>

<h3>Brainstorming:</h3>

So, what's hard about the task? Let's focus on the concrete example of the first prompt, `"When John and Mary went to the shops, John gave the bag to" -> " Mary"`.

A good starting point is thinking though whether a tiny model could do this, e.g. a <a href="https://transformer-circuits.pub/2021/framework/index.html">1L Attn-Only model</a>. I'm pretty sure the answer is no! Attention is really good at the primitive operations of looking nearby, or copying information. I can believe a tiny model could figure out that at `to` it should look for names and predict that those names came next (e.g. the skip trigram " John...to -> John"). But it's much harder to tell how <i>many</i> of each previous name there are - attending to each copy of John will look exactly the same as attending to a single John token. So this will be pretty hard to figure out on the " to" token!

The natural place to break this symmetry is on the second `" John"` token - telling whether there is an earlier copy of the <i>current</i> token should be a much easier task. So I might expect there to be a head which detects duplicate tokens on the second `" John"` token, and then another head which moves that information from the second `" John` token to the `" to"` token.

The model then needs to learn to predict `" Mary"` and <i>not</i> `" John"`. I can see two natural ways to do this:
1. Detect all preceding names and move this information to " to" and then delete the any name corresponding to the duplicate token feature. This feels easier done with a non-linearity, since precisely cancelling out vectors is hard, so I'd imagine an MLP layer deletes the `" John"` direction of the residual stream.
2. Have a head which attends to all previous names, but where the duplicate token features <i>inhibit</i> it from attending to specific names. So this only attends to Mary. And then the output of this head maps to the logits.

<details>
<summary>Spoiler - which one of these two is correct</summary>

It's the second one.
</details>

<h3>Experiment Ideas</h3>

A test that could distinguish these two is to look at which components of the model add directly to the logits - if it's mostly attention heads which attend to `" Mary"` and to neither `" John"` it's probably hypothesis 2, if it's mostly MLPs it's probably hypothesis 1.

And we should be able to identify duplicate token heads by finding ones which attend from `" John"` to `" John"`, and whose outputs are then moved to the `" to"` token by V-Composition with another head (Spoiler: It's more complicated than that!)

Note that all of the above reasoning is very simplistic and could easily break in a real model! There'll be significant parts of the model that figure out whether to use this circuit at all (we don't want to inhibit duplicated names when, e.g. figuring out what goes at the start of the <i>next</i> sentence), and may be parts towards the end of the model that do "post-processing" just before the final output. But it's a good starting point for thinking about what's going on.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 2️⃣ Logit Attribution
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Direct Logit Attribution
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards (you will have seen this if you went through the balanced bracket classifier task, and in fact if you did then this section will probably be quite familiar to you and you should feel free to just skim through it). The main technique used to do this is called **direct logit attribution**

**Background:** The central object of a transformer is the **residual stream**. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).

The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called **direct logit attribution**. Here we look at the direct attribution to the logit difference!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Background and motivation of the logit difference

Logit difference is actually a *really* nice and elegant metric and is a particularly nice aspect of the setup of Indirect Object Identification. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities).

The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged.

But we have:

```
log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)
```

and because they differ by a constant, we have:

```
log_probs(" Mary") - log_probs(" John") = logits(" Mary") - logits(" John")
```

- the ability to add an arbitrary constant cancels out!

<details>
<summary>Technical details (if this equivalence doesn't seem obvious to you)</summary>

Let $\vec{\textbf{x}}$ be the logits, $\vec{\textbf{L}}$ be the log probs, and $\vec{\textbf{p}}$ be the probs. Then we have the following relations:

$$
p_i = \operatorname{softmax}(\vec{\textbf{x}})_i = \frac{e^{x_i}}{\sum_{i=1}^n e^{x_i}}
$$

and:

$$
L_i = \log p_i
$$

Combining these, we get:

$$
L_i = \log \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} = x_i - \log \sum_{j=1}^n e^{x_j}
$$

Notice that the sum term on the right hand side is the same for all $i$, so we get:

$$
L_i - L_j = x_i - x_j
$$

in other words, the logit diff $x_i - x_j$ is the same as the log prob diff. This motivates the choice of logit diff as our choice of metric (since the model is directly training to make the log prob of the correct token large, and all other log probs small).

</details>

Further, the metric helps us isolate the precise capability we care about - figuring out *which* name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.

Our metric is further refined, because each prompt is repeated twice, for each possible indirect object. This controls for irrelevant behaviour such as the model learning that John is a more frequent token than Mary (this actually happens! The final layernorm bias increases the John logit by 1 relative to the Mary logit). Another way to handle this would be to use a large enough dataset (with names randomly chosen) that this effect is averaged out, which is what we'll do in section 3.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details> <summary>Ignoring LayerNorm</summary>

LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is *almost* a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts).

But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of *all* components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the `cache` to apply the global layernorm scaling factor to each constant. See [my clean GPT-2 implementation](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Clean_Transformer_Implementation) for more on LayerNorm.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Logit diff directions
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
***Getting an output logit is equivalent to projecting onto a direction in the residual stream, and the same is true for getting the logit diff.***

<details>
<summary>If it's not clear what is meant by this statement, read this dropdown.</summary>

Suppose our final value in the residual stream for a single sequence and a position within that sequence is $x$ (i.e. $x$ is a vector of length $d_{model}$). Then (ignoring layernorm - see the point above for why it's okay to do this), we get logits by multiplying by the unembedding matrix $W_U$ (which has shape $(d_{model}, d_{vocab})$):

$$
\text{output} = x^T W_U
$$

Now, remember that we want the logit diff, which is $\text{output}_{IO} - \text{output}_{S}$ (the difference between the logits for our indirect object and subject). We can write this as:

$$
\text{logit diff} = (x^T W_U)_{IO} - (x^T W_U)_{S} = x^T (u_{IO} - u_{S})
$$

where $u_{IO}$ and $u_S$ are the **columns of the unembedding matrix** $W_U$ corresponding to the indirect object and subject tokens respectively.

To summarize, we've written the logit diff as a dot product between the vector in the residual stream and a constant vector (which is a function of the model's unembedding matrix). We call this vector $u_{IO} - u_{S}$ the **logit difference direction** (because it *"points in the direction of largest logit difference"*). To put it another way, if $x$ is a vector of fixed magnitude, then it maximises the logit difference when it is pointing in the same direction as the vector $u_{IO} - u_{S}$. We use the term "projection" synonymously with "dot product" here.

(If you've completed the exercise where we interpret a transformer on balanced / unbalanced bracket strings, this is basically the same principle. The only difference here is that we actually have a much larger unembedding vocabulary than just the classifications `{balanced, unbalanced}`, but since we're only interested in comparing the model's prediction for IO vs S, and the logits for these two tokens are usually larger than most others, this method is still well-justified).
</details>

We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)  # [batch 2 d_model]
print("Answer residual directions shape:", answer_residual_directions.shape)

correct_residual_directions, incorrect_residual_directions = answer_residual_directions.unbind(
    dim=1
)
logit_diff_directions = (
    correct_residual_directions - incorrect_residual_directions
)  # [batch d_model]
print("Logit difference directions shape:", logit_diff_directions.shape)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer.

<details> <summary>Technical details</summary>

`logits = Unembed(LayerNorm(final_residual_stream))`, so we technically need to account for the centering, and then learned translation and scaling of the layernorm, not just the variance 1 scaling.

The centering is accounted for with the preprocessing flag `center_writing_weights` which ensures that every weight matrix writing to the residual stream has mean zero.

The learned scaling is folded into the unembedding weights `model.unembed.W_U` via `W_U_fold = layer_norm.weights[:, None] * unembed.W_U`

The learned translation is folded to `model.unembed.b_U`, a bias added to the logits (note that GPT-2 is not trained with an existing `b_U`). This roughly represents unigram statistics. But we can ignore this because each prompt occurs twice with names in the opposite order, so this perfectly cancels out.

Note that rather than using layernorm scaling we could just study cache["ln_final.hook_normalised"]

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The code below does the following:

* Gets the final residual stream values from the `cache` object (which you should already have defined above).
* Apply layernorm scaling to these values.
    * This is done by `cache.apply_to_ln_stack`, a helpful function which takes a stack of residual stream values (e.g. a batch, or the residual stream decomposed into components), treats them as the input to a specific layer, and applies the layer norm scaling of that layer to them.
    * The keyword arguments here indicate that our input is the residual stream values for the last sequence position, and we want to apply the final layernorm in the model.
* Project them along the unembedding directions (you've already defined these above, as `logit_diff_directions`).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Cache syntax: resid_post is the residual stream at the end of the layer, -1 gets the final layer.
# The general syntax is [activation_name, layer_index, sub_layer_type].
final_residual_stream: Float[Tensor, "batch seq d_model"] = cache["resid_post", -1]
print(f"Final residual stream shape: {final_residual_stream.shape}")
final_token_residual_stream: Float[Tensor, "batch d_model"] = final_residual_stream[:, -1, :]

# Apply LayerNorm scaling (to just the final sequence position)
# pos_slice is the subset of the positions we take - here the final token of each prompt
scaled_final_token_residual_stream = cache.apply_ln_to_stack(
    final_token_residual_stream, layer=-1, pos_slice=-1
)

average_logit_diff = einops.einsum(
    scaled_final_token_residual_stream, logit_diff_directions, "batch d_model, batch d_model ->"
) / len(prompts)

print(f"Calculated average logit diff: {average_logit_diff:.10f}")
print(f"Original logit difference:     {original_average_logit_diff:.10f}")

t.testing.assert_close(average_logit_diff, original_average_logit_diff)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Logit Lens
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `residual_stack_to_logit_diff`

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> Again, make sure you understand what the output of this function represents.
> ```

This function should look a lot like your code immediately above. `residual_stack` is a tensor of shape `(..., batch, d_model)` containing the residual stream values for the final sequence position. You should apply the final layernorm to these values, then project them in the logit difference directions.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def residual_stack_to_logit_diff(
    residual_stack: Float[Tensor, "... batch d_model"],
    cache: ActivationCache,
    logit_diff_directions: Float[Tensor, "batch d_model"] = logit_diff_directions,
) -> Float[Tensor, "..."]:
    """
    Gets the avg logit difference between the correct and incorrect answer for a given stack of
    components in the residual stream.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    batch_size = residual_stack.size(-2)
    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)
    return (
        einops.einsum(
            scaled_residual_stack, logit_diff_directions, "... batch d_model, batch d_model -> ..."
        )
        / batch_size
    )
    # END SOLUTION


# HIDE
# Test function by checking that it gives the same result as the original logit difference
t.testing.assert_close(
    residual_stack_to_logit_diff(final_token_residual_stream, cache), original_average_logit_diff
)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you have the solution, you can plot your results.

<details> <summary>Details on <code>accumulated_resid</code></summary>

Key for the plot below: `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)

* `layer` is the layer for which we input the residual stream (this is used to identify *which* layer norm scaling factor we want)
* `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention & before MLP
* `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.
* `return_labels` is whether to return the labels for each component returned (useful for plotting)
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

accumulated_residual, labels = cache.accumulated_resid(
    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True
)
# accumulated_residual has shape (component, batch, d_model)

logit_lens_logit_diffs: Float[Tensor, "component"] = residual_stack_to_logit_diff(
    accumulated_residual, cache
)

line(
    logit_lens_logit_diffs,
    hovermode="x unified",
    title="Logit Difference From Accumulated Residual Stream",
    labels={"x": "Layer", "y": "Logit Diff"},
    xaxis_tickvals=labels,
    width=800,
)
# FILTERS: ~
# line(
#     logit_lens_logit_diffs,
#     hovermode="x unified",
#     title="Logit Difference From Accumulated Residual Stream",
#     labels={"x": "Layer", "y": "Logit Diff"},
#     xaxis_tickvals=labels,
#     width=800,
#     return_fig=True,
# ).write_html(section_dir / "14101.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14101.html" width="820" height="460"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what is the interpretation of this plot? What does this tell you about how the model solves this task?</summary>

Fascinatingly, we see that the model is utterly unable to do the task until layer 7, almost all performance comes from attention layer 9, and performance actually *decreases* from there.

This tells us that there must be something going on (primarily in layers 7, 8 and 9) which writes to the residual stream in the correct way to solve the IOI task. This allows us to narrow in our focus, and start asking questions about what kind of computation is going on in those layers (e.g. the contribution of attention layers vs MLPs, and which attention heads are most important).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Layer Attribution
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can repeat the above analysis but for each layer (this is equivalent to the differences between adjacent residual streams)

Note: Annoying terminology overload - layer k of a transformer means the kth **transformer block**, but each block consists of an **attention layer** (to move information around) *and* an **MLP layer** (to process information).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)
per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)

line(
    per_layer_logit_diffs,
    hovermode="x unified",
    title="Logit Difference From Each Layer",
    labels={"x": "Layer", "y": "Logit Diff"},
    xaxis_tickvals=labels,
    width=800,
)

# FILTERS: ~
# line(
#     per_layer_logit_diffs,
#     hovermode="x unified",
#     title="Logit Difference From Each Layer",
#     labels={"x": "Layer", "y": "Logit Diff"},
#     xaxis_tickvals=labels,
#     width=800,
#     return_fig=True,
# ).write_html(section_dir / "14102.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14102.html" width="820" height="460"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what is the interpretation of this plot? What does this tell you about how the model solves this task?</summary>

We see that only attention layers matter, which makes sense! The IOI task is about moving information around (i.e. moving the correct name and not the incorrect name), and less about processing it. And again we note that attention layer 9 improves things a lot, while attention 10 and attention 11 *decrease* performance.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Head Attribution
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can further break down the output of each attention layer into the sum of the outputs of each attention head. Each attention layer consists of 12 heads, which each act independently and additively.

<details> <summary>Decomposing attention output into sums of heads</summary>

The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer).
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)
per_head_residual = einops.rearrange(
    per_head_residual, "(layer head) ... -> layer head ...", layer=model.cfg.n_layers
)
per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)

imshow(
    per_head_logit_diffs,
    labels={"x": "Head", "y": "Layer"},
    title="Logit Difference From Each Head",
    width=600,
)

# FILTERS: ~
# fig = imshow(
#     per_head_logit_diffs,
#     labels={"x": "Head", "y": "Layer"},
#     title="Logit Difference From Each Head",
#     width=600,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14103.html")
# fig.show()
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14103.html" width="620" height="460"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We see that only a few heads really matter - heads 9.6 and 9.9 contribute a lot positively (explaining why attention layer 9 is so important), while heads 10.7 and 11.10 contribute a lot negatively (explaining why attention layer 10 and layer 11 are actively harmful). These correspond to (some of) the name movers and negative name movers discussed in the paper. There are also several heads that matter positively or negatively but less strongly (other name movers and backup name movers).

There are a few meta observations worth making here - our model has 144 heads, yet we could localise this behaviour to a handful of specific heads, using straightforward, general techniques. This supports the claim in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) that attention heads are the right level of abstraction to understand attention. It also really surprising that there are *negative* heads - eg 10.7 makes the incorrect logit 7x *more* likely. I'm not sure what's going on there, though the paper discusses some possibilities.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Recap of useful functions from this section

Here, we take stock of all the functions from transformerlens which you might not have seen previously.

* `cache.apply_ln_to_stack`
    * Apply layernorm scaling to a stack of residual stream values.
    * We used this to help us go from "final value in residual stream" to "projection of logits in logit difference directions", without getting the code too messy!
* `cache.accumulated_resid(layer=None)`
    * Returns the accumulated residual stream up to layer `layer` (or up to the final value of residual stream if layer is None), i.e. a stack of previous residual streams up to that layer's input.
    * Useful when studying the **logit lens**.
    * First dimension of output is `(0_pre, 0_mid, 1_pre, 1_mid, ..., final_post)`
* `cache.decompose_resid(layer)`.
    * Decomposes the residual stream input to layer `layer` into a stack of the output of previous layers. The sum of these is the input to layer `layer`.
    * First dimension of output is `(embed, pos_embed, 0_attn_out, 0_mlp_out, ...)`.
* `cache.stack_head_results(layer)`
    * Returns a stack of all head results (i.e. residual stream contribution) up to layer `layer`
    * (i.e. like `decompose_resid` except it splits each attention layer by head rather than splitting each layer by attention/MLP)
    * First dimension of output is `layer * head` (we needed to rearrange to `(layer, head)` to plot it).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Attention Analysis

Attention heads are particularly fruitful to study because we can look directly at their attention patterns and study from what positions they move information from and to. This is particularly useful here as we're looking at the direct effect on the logits so we need only look at the attention patterns from the final token.

We use the `circuitsvis` library (developed from Anthropic's PySvelte library) to visualize the attention patterns! We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt (as an illustration).

<details> <summary>Interpreting Attention Patterns</summary>

A common mistake to make when looking at attention patterns is thinking that they must convey information about the *token* looked at (maybe accounting for the context of the token). But actually, all we can confidently say is that it moves information from the *residual stream position* corresponding to that input token. Especially later on in the model, there may be components in the residual stream that are nothing to do with the input token! Eg the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in ".", "!" or "?"
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def topk_of_Nd_tensor(tensor: Float[Tensor, "rows cols"], k: int):
    """
    Helper function: does same as tensor.topk(k).indices, but works over 2D tensors.
    Returns a list of indices, i.e. shape [k, tensor.ndim].

    Example: if tensor is 2D array of values for each head in each layer, this will
    return a list of heads.
    """
    i = t.topk(tensor.flatten(), k).indices
    return np.array(np.unravel_index(utils.to_numpy(i), tensor.shape)).T.tolist()


if MAIN:
    k = 3

    for head_type in ["Positive", "Negative"]:
        # Get the heads with largest (or smallest) contribution to the logit difference
        top_heads = topk_of_Nd_tensor(
            per_head_logit_diffs * (1 if head_type == "Positive" else -1), k
        )

        # Get all their attention patterns
        attn_patterns_for_important_heads: Float[Tensor, "head q k"] = t.stack(
            [cache["pattern", layer][:, head][0] for layer, head in top_heads]
        )

        # Display results
        display(HTML(f"<h2>Top {k} {head_type} Logit Attribution Heads</h2>"))
        display(
            cv.attention.attention_patterns(
                attention=attn_patterns_for_important_heads,
                tokens=model.to_str_tokens(tokens[0]),
                attention_head_names=[f"{layer}.{head}" for layer, head in top_heads],
            )
        )
        # FILTERS: ~
        # html = f"<h2 style='font-family: system-ui;'>Top {k} {head_type} Logit Attribution Heads</h2>\n\n"
        # html += str(
        #     cv.attention.attention_patterns(
        #         attention=attn_patterns_for_important_heads,
        #         tokens=model.to_str_tokens(tokens[0]),
        #         attention_head_names=[f"{layer}.{head}" for layer, head in top_heads],
        #     )
        # )
        # with open(section_dir / f"14104-{head_type.lower()}.html", "w") as f:
        #     f.write(html)
        # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14104-positive.html" width="700" height="450" style="background-color: white"></div>
<br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14104-negative.html" width="700" height="450" style="background-color: white"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Reminder - you can use `attention_patterns` or `attention_heads` for these visuals. The former lets you see the actual values, the latter lets you hover over tokens in a printed sentence (and it provides other useful features like locking on tokens, or a superposition of all heads in the display). Both can be useful in different contexts (although I'd recommend usually using `attention_patterns`, it's more useful in most cases for quickly getting a sense of attention patterns).

Try replacing `attention_patterns` above with `attention_heads`, and compare the output.

<details>
<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>

This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.

Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:

```python
attn_heads = cv.attention.attention_heads(
    attention = attn_patterns_for_important_heads,
    tokens = model.to_str_tokens(tokens[0]),
    attention_head_names = [f"{layer}.{head}" for layer, head in top_heads],
)

path = "attn_heads.html"

with open(path, "w") as f:
    f.write(str(attn_heads))

webbrowser.open(path)
```

To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.
</details>

From these plots, you might want to start thinking about the algorithm which is being implemented. In particular, for the attention heads with high positive attribution scores, where is `" to"` attending to? How might this head be affecting the logit diff score?

We'll save a full hypothesis for how the model works until the end of the next section.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 3️⃣ Activation Patching
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The obvious limitation to the techniques used above is that they only look at the very end of the circuit - the parts that directly affect the logits. Clearly this is not sufficient to understand the circuit! We want to understand how things compose together to produce this final output, and ideally to produce an end-to-end circuit fully explaining this behaviour.

The technique we'll use to investigate this is called **activation patching**. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.

The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.

We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to *localise* which activations matter.

In other words, this is a **noising** algorithm (unlike last section which was mostly **denoising**).

The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The diagrams below demonstrate activation patching on an abstract neural network (the nodes represent activations, and the arrows between them are weight connections).

A regular forward pass on the clean input looks like:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-1c.png" width="300">

And activation patching from a corrupted input (green) into a forward pass for the clean input (black) looks like:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-2c.png" width="440">

where the dotted line represents patching in a value (i.e. during the forward pass on the clean input, we replace node $D$ with the value it takes on the corrupted input). Nodes $H$, $G$ and $F$ are colored orange, to represent that they now follow a distribution which is not the same as clean or corrupted.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can patch into a transformer in many different ways (e.g. values of the residual stream, the MLP, or attention heads' output - see below). We can also get even more granular by patching at particular sequence positions (not shown in diagram).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-examples.png" width="840">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Noising vs denoising

We might call this algorithm a type of **noising**, since we're running the model on a clean input and adding noise by patching in from the corrupted input. We can also consider the opposite algorithm, **denoising**, where we run the model on a corrupted input and remove noise by patching in from the clean input.

When would you use noising vs denoising? It depends on your goals. The results of denoising are much stronger, because showing that a component or set of components is sufficient for a task is a big deal. On the other hand, the complexity of transformers and interdependence of components means that noising a model can have unpredictable consequences. If loss goes up when we ablate a component, it doesn't necessarily mean that this component was necessary for the task. As an example, ablating MLP0 in gpt2-small seems to make performance much worse on basically any task (because it acts as a kind of extended embedding; more on this later in these exercises), but it's not doing anything important which is *specfic* for the IOI task.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Example: denoising the residual stream

The above was all fairly abstract, so let's zoom in and lay out a concrete example to understand Indirect Object Identification. We'll start with an exercise on denoising, but we'll move onto noising later in this section (and the next section, on path patching).

Here our clean input will be the original sentences (e.g. <code>"When Mary and John went to the store, John gave a drink to"</code>) and our corrupted input will have the subject token flipped (e.g. <code>"When Mary and John went to the store, <u>Mary</u> gave a drink to"</code>). Patching by replacing corrupted residual stream values with clean values is a causal intervention which will allow us to understand precisely which parts of the network are identifying the indirect object. If a component is important, then patching in (replacing that component's corrupted output with its clean output) will reverse the signal that this component produces, hence making performance much better.

Note - the noising and denoising terminology doesn't exactly fit here, since the "noised dataset" actually **reverses** the signal rather than erasing it. The reason we're describing this as denoising is more a matter of framing - we're trying to figure out which components / activations are **sufficient** to recover performance, rather than which are **necessary**. If you're ever confused, this is a useful framing to have - **noising tells you what is necessary, denoising tells you what is sufficient.**
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Question - we could instead have our corrupted sentence be <code>"When <u>John</u> and <u>Mary</u> went to the store, <u>Mary</u> gave a drink to"</code> (i.e. flip all 3 occurrences of names in the sentence). Why do you think we don't do this?

<details>
<summary>Hint</summary>

What if, at some point during the model's forward pass on the prompt `"When Mary and John went to the store, John gave a drink to"`, it contains some representation of the information **"the indirect object is the fourth token in this sequence"**?
</details>

<details>
<summary>Answer</summary>

The model could point to the indirect object `' Mary'` in two different ways:

* Via **token information**, i.e. **"the indirect object is the token `' Mary'`"**.
* Via **positional information**, i.e. **"the indirect object is the fourth token in this sequence"**.

We want the corrupted dataset to reverse both these signals when it's patched into the clean dataset. But if we corrupted the dataset by flipping all three names, then:

* The token information is flipped, because the corresponding information in the model for the corrupted prompt will be **"the indirect object is the token `' Mary'`"**.
* The positional information is ***not*** flipped, because the corresponding information will still be **"the indirect object is the fourth token in this sequence"**.

In fact, in the bonus section we'll take advantage of this fact to try and disentangle whether token or positional information is being used by the model (i.e. by flipping the token information but not the positional information, and vice-versa). Spoiler alert - it turns out to be using a bit of both!
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
One natural thing to patch in is the residual stream at a specific layer and specific position. For example, the model is likely intitially doing some processing on the `S2` token to realise that it's a duplicate, but then uses attention to move that information to the `end` token. So patching in the residual stream at the `end` token will likely matter a lot in later layers but not at all in early layers.

We can zoom in much further and patch in specific activations from specific layers. For example, we think that the output of head 9.9 on the final token is significant for directly connecting to the logits, so we predict that just patching the output of this head will significantly affect performance.

Note that this technique does *not* tell us how the components of the circuit connect up, just what they are.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
TransformerLens has helpful built-in functions to perform activation patching, but in order to understand the process better, you're now going to implement some of these functions from first principles (i.e. just using hooks). You'll be able to test your functions by comparing their output to the built-in functions.

If you need a refresher on hooks, you can return to the exercises on induction heads (which take you through how to use hooks, as well as how to cache activations).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

from transformer_lens import patching

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Creating a metric
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Before we patch, we need to create a metric for evaluating a set of logits. Since we'll be running our **corrupted prompts** (with `S2` replaced with the wrong name) and patching in our **clean prompts**, it makes sense to choose a metric such that:

* A value of zero means no change (from the performance on the corrupted prompt)
* A value of one means clean performance has been completely recovered

For example, if we patched in the entire clean prompt, we'd get a value of one. If our patching actually makes the model even better at solving the task than its regular behaviour on the clean prompt then we'd get a value greater than 1, but generally we expect values between 0 and 1.

It also makes sense to have the metric be a linear function of the logit difference. This is enough to uniquely specify a metric.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

clean_tokens = tokens
# Swap each adjacent pair to get corrupted tokens
indices = [i + 1 if i % 2 == 0 else i - 1 for i in range(len(tokens))]
corrupted_tokens = clean_tokens[indices]

print(
    "Clean string 0:    ",
    model.to_string(clean_tokens[0]),
    "\nCorrupted string 0:",
    model.to_string(corrupted_tokens[0]),
)

clean_logits, clean_cache = model.run_with_cache(clean_tokens)
corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)

clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)
print(f"Clean logit diff: {clean_logit_diff:.4f}")

corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)
print(f"Corrupted logit diff: {corrupted_logit_diff:.4f}")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - create a metric

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> ```

Fill in the function `ioi_metric` below, to create the required metric. Note that we can afford to use default arguments in this function, because we'll be using the same dataset for this whole section.

**Important note** - this function needs to return a scalar tensor, rather than a float. If not, then some of the patching functions later on won't work. The type signature of this is `Float[Tensor, ""]`.

**Second important note** - we've defined this to be 0 when performance is the same as on corrupted input, and 1 when it's the same as on clean input. This is because we're performing a **denoising algorithm**; we're looking for activations which are sufficient for recovering a model's performance (i.e. activations which have enough information to recover the correct answer from the corrupted input). Our "null hypothesis" is that the component isn't sufficient, and so patching it by replacing corrupted with clean values doesn't recover any performance. In later sections we'll be doing noising, and we'll define a new metric function for that.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def ioi_metric(
    logits: Float[Tensor, "batch seq d_vocab"],
    answer_tokens: Int[Tensor, "batch 2"] = answer_tokens,
    corrupted_logit_diff: float = corrupted_logit_diff,
    clean_logit_diff: float = clean_logit_diff,
) -> Float[Tensor, ""]:
    """
    Linear function of logit diff, calibrated so that it equals 0 when performance is same as on
    corrupted input, and 1 when performance is same as on clean input.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)
    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)
    # END SOLUTION


t.testing.assert_close(ioi_metric(clean_logits).item(), 1.0)
t.testing.assert_close(ioi_metric(corrupted_logits).item(), 0.0)
t.testing.assert_close(ioi_metric((clean_logits + corrupted_logits) / 2).item(), 0.5)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Residual Stream Patching
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Lets begin with a simple example: we patch in the residual stream at the start of each layer and for each token position. Before you write your own function to do this, let's see what this looks like with TransformerLens' `patching` module. Run the code below.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

act_patch_resid_pre = patching.get_act_patch_resid_pre(
    model=model,
    corrupted_tokens=corrupted_tokens,
    clean_cache=clean_cache,
    patching_metric=ioi_metric,
)

labels = [f"{tok} {i}" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))]

# FILTERS: st,py
imshow(
    act_patch_resid_pre,
    labels={"x": "Position", "y": "Layer"},
    x=labels,
    title="resid_pre Activation Patching",
    width=700,
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     act_patch_resid_pre,
#     labels={"x": "Position", "y": "Layer"},
#     x=labels,
#     title="resid_pre Activation Patching",
#     width=700,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14105.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_resid_pre,
#     labels={"x": "Position", "y": "Layer"},
#     x=labels,
#     title="resid_pre Activation Patching",
#     width=600
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14105.html" width="720" height="500"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Question - what is the interpretation of this graph? What significant things does it tell you about the nature of how the model solves this task?

<details>
<summary>Hint</summary>

Think about locality of computation.
</details>

<details>
<summary>Answer</summary>

Originally all relevant computation happens on `S2`, and at layers 7 and 8, the information is moved to `END`. Moving the residual stream at the correct position near *exactly* recovers performance!

To be clear, the striking thing about this graph isn't that the first row is zero everywhere except for `S2` where it is 1, or that the rows near the end trend to being zero everywhere except for `END` where they are 1; both of these are exactly what we'd expect. The striking things are:

* The computation is highly localized; the relevant information for choosing `IO` over `S` is initially stored in `S2` token and then moved to `END` token without taking any detours.
* The model is basically done after layer 8, and the rest of the layers actually slightly impede performance on this particular task.

(Note - for reference, tokens and their index from the first prompt are on the x-axis. In an abuse of notation, note that the difference here is averaged over *all* 8 prompts, while the labels only come from the *first* prompt.)
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement head-to-residual patching

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 20-25 minutes on this exercise.
> 
> It's very important to understand how patching works. Many subsequent exercises will build on this one.
> ```

Now, you should implement the `get_act_patch_resid_pre` function below, which should give you results just like the code you ran above. A quick refresher on how to use hooks in this way:

* Hook functions take arguments `tensor: torch.Tensor` and `hook: HookPoint`. It's often easier to define a hook function taking more arguments than these, and then use `functools.partial` when it actually comes time to add your hook.
* The function `model.run_with_hooks` takes arguments:
    * The tokens to run (as first argument)
    * `fwd_hooks` - a list of `(hook_name, hook_fn)` tuples. Remember that you can use `utils.get_act_name` to get hook names.
* Tip - it's good practice to have `model.reset_hooks()` at the start of functions which add and run hooks. This is because sometimes hooks fail to be removed (if they cause an error while running). There's nothing more frustrating than fixing a hook error only to get the same error message, not realising that you've failed to clear the broken hook!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def patch_residual_component(
    corrupted_residual_component: Float[Tensor, "batch pos d_model"],
    hook: HookPoint,
    pos: int,
    clean_cache: ActivationCache,
) -> Float[Tensor, "batch pos d_model"]:
    """
    Patches a given sequence position in the residual stream, using the value
    from the clean cache.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]
    return corrupted_residual_component
    # END SOLUTION


def get_act_patch_resid_pre(
    model: HookedTransformer,
    corrupted_tokens: Float[Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable[[Float[Tensor, "batch pos d_vocab"]], float],
) -> Float[Tensor, "3 layer pos"]:
    """
    Returns an array of results of patching each position at each layer in the residual
    stream, using the value from the clean cache.

    The results are calculated using the patching_metric function, which should be
    called on the model's logit output.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    model.reset_hooks()
    seq_len = corrupted_tokens.size(1)
    results = t.zeros(model.cfg.n_layers, seq_len, device=device, dtype=t.float32)

    for layer in tqdm(range(model.cfg.n_layers)):
        for position in range(seq_len):
            hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)
            patched_logits = model.run_with_hooks(
                corrupted_tokens,
                fwd_hooks=[(utils.get_act_name("resid_pre", layer), hook_fn)],
            )
            results[layer, position] = patching_metric(patched_logits)

    return results
    # END SOLUTION


# HIDE
if MAIN:
    act_patch_resid_pre_own = get_act_patch_resid_pre(
        model, corrupted_tokens, clean_cache, ioi_metric
    )

    t.testing.assert_close(act_patch_resid_pre, act_patch_resid_pre_own)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Once you've passed the tests, you can plot your results.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

imshow(
    act_patch_resid_pre_own,
    x=labels,
    title="Logit Difference From Patched Residual Stream",
    labels={"x": "Sequence Position", "y": "Layer"},
    width=700,
)

# FILTERS: ~
# fig = imshow(
#     act_patch_resid_pre_own,
#     x=labels,
#     title="Logit Difference From Patched Residual Stream",
#     labels={"x": "Sequence Position", "y": "Layer"},
#     width=700,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14106.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14106.html" width="720" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Patching in residual stream by block
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Rather than just patching to the residual stream in each layer, we can also patch just after the attention layer or just after the MLP. This gives is a slightly more refined view of which tokens matter and when.

The function `patching.get_act_patch_block_every` works just like `get_act_patch_resid_pre`, but rather than just patching to the residual stream, it patches to `resid_pre`, `attn_out` and `mlp_out`, and returns a tensor of shape `(3, n_layers, seq_len)`.

One important thing to note - we're cycling through the `resid_pre`, `attn_out` and `mlp_out` and only patching one of them at a time, rather than patching all three at once.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

act_patch_block_every = patching.get_act_patch_block_every(
    model, corrupted_tokens, clean_cache, ioi_metric
)

# FILTERS: st,py
imshow(
    act_patch_block_every,
    x=labels,
    facet_col=0,  # This argument tells plotly which dimension to split into separate plots
    facet_labels=["Residual Stream", "Attn Output", "MLP Output"],  # Subtitles of separate plots
    title="Logit Difference From Patched Attn Head Output",
    labels={"x": "Sequence Position", "y": "Layer"},
    width=1200,
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     act_patch_block_every,
#     x=labels,
#     facet_col=0,  # This argument tells plotly which dimension to split into separate plots
#     facet_labels=["Residual Stream", "Attn Output", "MLP Output"],  # Subtitles of separate plots
#     title="Logit Difference From Patched Attn Head Output",
#     labels={"x": "Sequence Position", "y": "Layer"},
#     width=1200,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14107.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_block_every,
#     x=labels,
#     facet_col=0, # This argument tells plotly which dimension to split into separate plots
#     facet_labels=["Residual Stream", "Attn Output", "MLP Output"], # Subtitles of separate plots
#     title="Logit Difference From Patched Attn Head Output",
#     labels={"x": "Sequence Position", "y": "Layer"},
#     width=1200,
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14107.html" width="1220" height="500"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what is the interpretation of the second two plots?</summary>

We see that several attention layers are significant but that, matching the residual stream results, early layers matter on `S2`, and later layers matter on `END`, and layers essentially don't matter on any other token. Extremely localised!

As with direct logit attribution, layer 9 is positive and layers 10 and 11 are not, suggesting that the late layers only matter for direct logit effects, but we also see that layers 7 and 8 matter significantly. Presumably these are the heads that move information about which name is duplicated from `S2` to `END`.

In contrast, the MLP layers do not matter much. This makes sense, since this is more a task about moving information than about processing it, and the MLP layers specialise in processing information. The one exception is MLP0, which matters a lot, but I think this is misleading and just a generally true statement about MLP0 rather than being about the circuit on this task. Read won for an interesting aside about MLP0!

</details>


<details>
<summary>An aside on knowledge storage in the MLP</summary>

We may have mentioned at some point that "facts" or "knowledge" are stored in the MLP layers.
Here's an example using our previous function to investigate this claim:
Given the prompt `The White House is where the`, we would expect that gpt2 would guess ` president` as the answer (as part of the completion `The White House is where the president lives.`)
and given the prompt `The Haunted House is where the`, we would expect that gpt2 would guess `ghosts` as the answer (as part of the completion `The Haunted House is where the ghosts live.`)

Indeed this is the case (mostly because I cherry-picked these prompts to get a clean example where I can swap out a single-token word, and get a different single token answer).
How does the model do this? Somewhere it has to have the association between White House/President and Haunted House/Ghosts.

We can see this by feeding the two prompts through, and using as our metric the logit difference between the tokens ` president` and ` ghosts`.

```python
clean_prompt, clean_answer = "The White House is where the", " president" #Note the space in the answer!
corrupted_prompt, corrupted_answer = "The Haunted House is where the", " ghosts"

clean_tokens = model.to_tokens(clean_prompt)
corrupted_tokens = model.to_tokens(corrupted_prompt)

assert clean_tokens.shape == corrupted_tokens.shape, "clean and corrupted tokens must have same shape"

clean_token = model.to_single_token(clean_answer)
corrupted_token = model.to_single_token(corrupted_answer)

utils.test_prompt(clean_prompt, clean_answer, model)
utils.test_prompt(corrupted_prompt, corrupted_answer, model)

clean_logits, clean_cache = model.run_with_cache(clean_tokens)

def answer_metric(
    logits: Float[Tensor, "batch seq d_vocab"],
    clean_token: Int = clean_token,
    corrupted_token: Int = corrupted_token,
) -> Float[Tensor, "batch"]:
    return logits[:, -1, clean_token] - logits[:, -1, corrupted_token]

act_patch_block_every = patching.get_act_patch_block_every(model, corrupted_tokens, clean_cache, answer_metric)

imshow(
    act_patch_block_every,
    x=["<endoftext>","The", "White/Haunted", "House", "is", "where", "the"],
    facet_col=0,  # This argument tells plotly which dimension to split into separate plots
    facet_labels=["Residual Stream", "Attn Output", "MLP Output"],  # Subtitles of separate plots
    title="Logit Difference (president - ghosts)",
    labels={"x": "Sequence Position", "y": "Layer"},
    width=1200,
)
```

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Tied embeddings (what MLP0 is doing)

It's often observed on GPT-2 Small that MLP0 matters a lot, and that ablating it utterly destroys performance. The current accepted hypothesis is that the first MLP layer is essentially acting as an **extension of the embedding**, and that when later layers want to access the input tokens they mostly read in the output of the first MLP layer, rather than the token embeddings. Within this frame, the first attention layer doesn't do much.

In this framing, it makes sense that MLP0 matters on `S2`, because that's the one position with a different input token (i.e. it has a different extended embedding between the two prompt versions, and all other tokens will have basically the same extended embeddings).

Why does this happen? It seems like most of the effect comes from the fact that **the embedding and unembedding matrices in GPT2-Small are tied**, i.e. the one equals the transpose of the other. On one hand this seems principled - if two words mean similar things (e.g. "big" and "large") then they should be substitutable, i.e. have similar embeddings and unembeddings. This would seem to suggest that the geometric structure of the embedding and unembedding spaces should be related. On the other hand, there's one major reason why this isn't as principled as it seems - the embedding and the unembedding together form the direct path (if we had no other components then the transformer would just be the linear map $x \to x^T W_E W_U$), and we do not want this to be symmetric because bigram prediction isn't symmetric! As an example, if $W_E = W_U^T$ then in order to predict "Barack Obama" as a probable bigram, we'd also have to predict "Obama Barack" with equally high probability, which obviously shouldn't happen. So it makes sense that the first MLP layer might be used in part to overcome this asymmetry: we now think of $\operatorname{MLP}_0(x^T W_E) W_U$ as the direct path, which is no longer symmetric when $W_E$ and $W_U$ are tied.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise (optional) - implement head-to-block patching

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> 
> Most code can be copied from the last exercise.
> ```

If you want, you can implement the `get_act_patch_resid_pre` function for fun, although it's similar enough to the previous exercise that doing this isn't compulsory.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def get_act_patch_block_every(
    model: HookedTransformer,
    corrupted_tokens: Float[Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable[[Float[Tensor, "batch pos d_vocab"]], float],
) -> Float[Tensor, "3 layer pos"]:
    """
    Returns an array of results of patching each position at each layer in the residual stream,
    using the value from the clean cache.

    The results are calculated using the patching_metric function, which should be called on the
    model's logit output.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    model.reset_hooks()
    results = t.zeros(3, model.cfg.n_layers, tokens.size(1), device=device, dtype=t.float32)

    for component_idx, component in enumerate(["resid_pre", "attn_out", "mlp_out"]):
        for layer in tqdm(range(model.cfg.n_layers)):
            for position in range(corrupted_tokens.shape[1]):
                hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)
                patched_logits = model.run_with_hooks(
                    corrupted_tokens,
                    fwd_hooks=[(utils.get_act_name(component, layer), hook_fn)],
                )
                results[component_idx, layer, position] = patching_metric(patched_logits)

    return results
    # END SOLUTION


# HIDE
if MAIN:
    act_patch_block_every_own = get_act_patch_block_every(
        model, corrupted_tokens, clean_cache, ioi_metric
    )

    t.testing.assert_close(act_patch_block_every, act_patch_block_every_own)

    # FILTERS: st,py
    imshow(
        act_patch_block_every_own,
        x=labels,
        facet_col=0,
        facet_labels=["Residual Stream", "Attn Output", "MLP Output"],
        title="Logit Difference From Patched Attn Head Output",
        labels={"x": "Sequence Position", "y": "Layer"},
        width=1200,
    )
    # END FILTERS

    # FILTERS: ~
    # fig = imshow(
    #     act_patch_block_every_own,
    #     x=labels,
    #     facet_col=0,
    #     facet_labels=["Residual Stream", "Attn Output", "MLP Output"],
    #     title="Logit Difference From Patched Attn Head Output",
    #     labels={"x": "Sequence Position", "y": "Layer"},
    #     width=1200,
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / "14108.html")
    # END FILTERS
# END HIDE

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_block_every_own,
#     x=labels,
#     facet_col=0,
#     facet_labels=["Residual Stream", "Attn Output", "MLP Output"],
#     title="Logit Difference From Patched Attn Head Output",
#     labels={"x": "Sequence Position", "y": "Layer"},
#     width=1200
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14108.html" width="1220" height="500"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Head Patching
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We can refine the above analysis by patching in individual heads! This is somewhat more annoying, because there are now three dimensions `(head_index, position and layer)`.

The code below patches a head's output over all sequence positions, and returns the results (for each head in the model).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

act_patch_attn_head_out_all_pos = patching.get_act_patch_attn_head_out_all_pos(
    model, corrupted_tokens, clean_cache, ioi_metric
)

# FILTERS: st,py
imshow(
    act_patch_attn_head_out_all_pos,
    labels={"y": "Layer", "x": "Head"},
    title="attn_head_out Activation Patching (All Pos)",
    width=600,
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     act_patch_attn_head_out_all_pos,
#     labels={"y": "Layer", "x": "Head"},
#     title="attn_head_out Activation Patching (All Pos)",
#     width=600,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14109.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_attn_head_out_all_pos,
#     labels={"y": "Layer", "x": "Head"},
#     title="attn_head_out Activation Patching (All Pos)",
#     width=600
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14109.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what are the interpretations of this graph? Which heads do you think are important?</summary>

We see some of the heads that we observed in our attention plots at the end of last section (e.g. `9.9` having a large positive score, and `10.7` having a large negative score). But we can also see some other important heads, for instance:

* In layers 7-8 there are several important heads. We might deduce that these are the ones responsible for moving information from `S2` to `end`.
* In the earlier layers, there are some more important heads (e.g. `3.0` and `5.5`). We might guess these are performing some primitive logic, e.g. causing the second `" John"` token to attend to previous instances of itself.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement head-to-head patching

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> 
> Again, it should be similar to the first patching exercise (you can copy code).
> ```

You should implement your own version of this patching function below.

You'll need to define a new hook function, but most of the code from the previous exercise should be reusable.

<details>
<summary>Help - I'm not sure what hook name to use for my patching.</summary>

You should patch at:

```python
utils.get_act_name("z", layer)
```

This is the linear combination of value vectors, i.e. it's the thing you multiply by $W_O$ before adding back into the residual stream. There's no point patching after the $W_O$ multiplication, because it will have the same effect, but take up more memory (since `d_model` is larger than `d_head`).
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def patch_head_vector(
    corrupted_head_vector: Float[Tensor, "batch pos head_index d_head"],
    hook: HookPoint,
    head_index: int,
    clean_cache: ActivationCache,
) -> Float[Tensor, "batch pos head_index d_head"]:
    """
    Patches the output of a given head (before it's added to the residual stream) at every sequence
    position, using the value from the clean cache.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    corrupted_head_vector[:, :, head_index] = clean_cache[hook.name][:, :, head_index]
    return corrupted_head_vector
    # END SOLUTION


def get_act_patch_attn_head_out_all_pos(
    model: HookedTransformer,
    corrupted_tokens: Float[Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable,
) -> Float[Tensor, "layer head"]:
    """
    Returns an array of results of patching at all positions for each head in each layer, using the
    value from the clean cache. The results are calculated using the patching_metric function, which
    should be called on the model's logit output.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    model.reset_hooks()
    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)

    for layer in tqdm(range(model.cfg.n_layers)):
        for head in range(model.cfg.n_heads):
            hook_fn = partial(patch_head_vector, head_index=head, clean_cache=clean_cache)
            patched_logits = model.run_with_hooks(
                corrupted_tokens,
                fwd_hooks=[(utils.get_act_name("z", layer), hook_fn)],
                return_type="logits",
            )
            results[layer, head] = patching_metric(patched_logits)

    return results
    # END SOLUTION


# FILTERS: st,py
if MAIN:
    act_patch_attn_head_out_all_pos_own = get_act_patch_attn_head_out_all_pos(
        model, corrupted_tokens, clean_cache, ioi_metric
    )

    t.testing.assert_close(act_patch_attn_head_out_all_pos, act_patch_attn_head_out_all_pos_own)

    imshow(
        act_patch_attn_head_out_all_pos_own,
        title="Logit Difference From Patched Attn Head Output",
        labels={"x": "Head", "y": "Layer"},
        width=600,
    )
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     act_patch_attn_head_out_all_pos_own,
#     labels={"y": "Layer", "x": "Head"},
#     title="Logit Difference From Patched Attn Head Output",
#     width=600,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14110.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# act_patch_attn_head_out_all_pos_own = get_act_patch_attn_head_out_all_pos(
#     model, corrupted_tokens, clean_cache, ioi_metric
# )

# t.testing.assert_close(act_patch_attn_head_out_all_pos, act_patch_attn_head_out_all_pos_own)

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_attn_head_out_all_pos_own,
#     title="Logit Difference From Patched Attn Head Output",
#     labels={"x":"Head", "y":"Layer"},
#     width=600
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14109.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Decomposing Heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Finally, we'll look at one more example of activation patching.

Decomposing attention layers into patching in individual heads has already helped us localise the behaviour a lot. But we can understand it further by decomposing heads. An attention head consists of two semi-independent operations - calculating *where* to move information from and to (represented by the attention pattern and implemented via the QK-circuit) and calculating *what* information to move (represented by the value vectors and implemented by the OV circuit). We can disentangle which of these is important by patching in just the attention pattern *or* the value vectors. See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [Neel's walkthrough video](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
A useful function for doing this is `get_act_patch_attn_head_all_pos_every`. Rather than just patching on head output (like the previous one), it patches on:
* Output (this is equivalent to patching the value the head writes to the residual stream)
* Querys (i.e. the patching the query vectors, without changing the key or value vectors)
* Keys
* Values
* Patterns (i.e. the attention patterns).

Again, note that this function isn't patching multiple things at once. It's looping through each of these five, and getting the results from patching them one at a time.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

act_patch_attn_head_all_pos_every = patching.get_act_patch_attn_head_all_pos_every(
    model, corrupted_tokens, clean_cache, ioi_metric
)

# FILTERS: st,py
imshow(
    act_patch_attn_head_all_pos_every,
    facet_col=0,
    facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
    title="Activation Patching Per Head (All Pos)",
    labels={"x": "Head", "y": "Layer"},
    width=1200,
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     act_patch_attn_head_all_pos_every,
#     facet_col=0,
#     facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
#     title="Activation Patching Per Head (All Pos)",
#     labels={"x": "Head", "y": "Layer"},
#     width=1200,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14111.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_attn_head_all_pos_every,
#     facet_col=0,
#     facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
#     title="Activation Patching Per Head (All Pos)",
#     labels={"x": "Head", "y": "Layer"},
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14111.html" width="1220" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise (optional) - implement head-to-head-input patching

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to ~10 minutes on this exercise.
> Most code can be copied from the last exercise.
> ```

Again, if you want to implement this yourself then you can do so below, but it isn't a compulsory exercise because it isn't conceptually different from the previous exercises. If you don't implement it, then you should still look at the solution to make sure you understand what's going on.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def patch_attn_patterns(
    corrupted_head_vector: Float[Tensor, "batch head_index pos_q pos_k"],
    hook: HookPoint,
    head_index: int,
    clean_cache: ActivationCache,
) -> Float[Tensor, "batch pos head_index d_head"]:
    """
    Patches the attn patterns of a given head at every sequence position, using the value from the
    clean cache.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    corrupted_head_vector[:, head_index] = clean_cache[hook.name][:, head_index]
    return corrupted_head_vector
    # END SOLUTION


def get_act_patch_attn_head_all_pos_every(
    model: HookedTransformer,
    corrupted_tokens: Float[Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable,
) -> Float[Tensor, "layer head"]:
    """
    Returns an array of results of patching at all positions for each head in each layer (using the
    value from the clean cache) for output, queries, keys, values and attn pattern in turn.

    The results are calculated using the patching_metric function, which should be called on the
    model's logit output.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    results = t.zeros(5, model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)
    # Loop over each component in turn
    for component_idx, component in enumerate(["z", "q", "k", "v", "pattern"]):
        for layer in tqdm(range(model.cfg.n_layers)):
            for head in range(model.cfg.n_heads):
                # Get different hook function if we're doing attention probs
                hook_fn_general = (
                    patch_attn_patterns if component == "pattern" else patch_head_vector
                )
                hook_fn = partial(hook_fn_general, head_index=head, clean_cache=clean_cache)
                # Get patched logits
                patched_logits = model.run_with_hooks(
                    corrupted_tokens,
                    fwd_hooks=[(utils.get_act_name(component, layer), hook_fn)],
                    return_type="logits",
                )
                results[component_idx, layer, head] = patching_metric(patched_logits)

    return results
    # END SOLUTION


if MAIN:
    act_patch_attn_head_all_pos_every_own = get_act_patch_attn_head_all_pos_every(
        model, corrupted_tokens, clean_cache, ioi_metric
    )

    t.testing.assert_close(act_patch_attn_head_all_pos_every, act_patch_attn_head_all_pos_every_own)

    # FILTERS: st,py
    imshow(
        act_patch_attn_head_all_pos_every_own,
        facet_col=0,
        facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
        title="Activation Patching Per Head (All Pos)",
        labels={"x": "Head", "y": "Layer"},
        width=1200,
    )
    # END FILTERS

    # FILTERS: ~
    # fig = imshow(
    #     act_patch_attn_head_all_pos_every_own,
    #     facet_col=0,
    #     facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
    #     title="Activation Patching Per Head (All Pos)",
    #     labels={"x": "Head", "y": "Layer"},
    #     width=1200,
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / "14112.html")
    # END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     act_patch_attn_head_all_pos_every_own,
#     facet_col=0,
#     facet_labels=["Output", "Query", "Key", "Value", "Pattern"],
#     title="Activation Patching Per Head (All Pos)",
#     labels={"x": "Head", "y": "Layer"},
#     width=1200
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14112.html" width="1220" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note - we can do this in an even more fine-grained way; the function `patching.get_act_patch_attn_head_by_pos_every` (i.e. same as above but replacing `all_pos` with `by_pos`) will give you the same decomposition, but by sequence position *as well as* by layer, head and component. The same holds for the `patching.get_act_patch_attn_head_out_all_pos` function earlier (replace `all_pos` with `by_pos`). These functions are unsurprisingly pretty slow though!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This plot has some striking features. For instance, this shows us that we have at least three different groups of heads:

* Earlier heads (`3.0`, `5.5`, `6.9`) which matter because of their attention patterns (specifically their query vectors).
* Middle heads in layers 7 & 8 (`7.3`, `7.9`, `8.6`, `8.10`) seem to matter more because of their value vectors.
* Later heads which improve the logit difference (`9.9`, `10.0`), which matter because of their query vectors.

Question - what is the significance of the results for the middle heads (i.e. the important ones in layers 7 & 8)? In particular, how should we interpret the fact that value patching has a much bigger effect than the other two forms of patching?

*Hint - if you're confused, try plotting the attention patterns of heads `7.3`, `7.9`, `8.6`, `8.10`. You can mostly reuse the code from above when we displayed the output of attention heads.*

<details>
<summary>Code to plot attention heads</summary>

```python
# Get the heads with largest value patching
# (we know from plot above that these are the 4 heads in layers 7 & 8)
k = 4
top_heads = topk_of_Nd_tensor(act_patch_attn_head_all_pos_every[3], k=k)

# Get all their attention patterns
attn_patterns_for_important_heads: Float[Tensor, "head q k"] = t.stack([
    cache["pattern", layer][:, head].mean(0)
        for layer, head in top_heads
])

# Display results
display(HTML(f"<h2>Top {k} Logit Attribution Heads (from value-patching)</h2>"))
display(cv.attention.attention_patterns(
    attention = attn_patterns_for_important_heads,
    tokens = model.to_str_tokens(tokens[0]),
    attention_head_names = [f"{layer}.{head}" for layer, head in top_heads],
))
```
</details>

<details>
<summary>Answer</summary>

The attention patterns show us that these heads attend from `END` to `S2`, so we can guess that they're responsible for moving information from `S2` to `END` which is used to determine the answer. This agrees with our earlier results, when we saw that most of the information gets moved over layers 7 & 8.

The fact that value patching is the most important thing for them suggests that the interesting computation goes into **what information they move from `S2` to `end`**, rather than **why `end` attends to `S2`**. See the diagram below if you're confused why we can draw this inference.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/k-vs-v-patching-explained.png" width="900">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Consolidating Understanding
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
OK, let's zoom out and reconsolidate. Here's a recap of the most important observations we have so far:

* Heads `9.9`, `9.6`, and `10.0` are the most important heads in terms of directly writing to the residual stream. In all these heads, the `END` attends strongly to the `IO`.
    * We discovered this by taking the values written by each head in each layer to the residual stream, and projecting them along the logit diff direction by using `residual_stack_to_logit_diff`. We also looked at attention patterns using `circuitsvis`.
    * <span style="color:darkorange">**This suggests that these heads are copying `IO` to `end`, to use it as the predicted next token.**</span>
    * The question then becomes *"how do these heads know to attend to this token, and not attend to `S`?"*

<br>

* All the action is on `S2` until layer 7 and then transitions to `END`. And that attention layers matter a lot, MLP layers not so much (apart from MLP0, likely as an extended embedding).
    * We discovered this by doing **activation patching** on `resid_pre`, `attn_out`, and `mlp_out`.
    * <span style="color:darkorange">**This suggests that there is a cluster of heads in layers 7 & 8, which move information from `S2` to `END`. We deduce that this information is how heads `9.9`, `9.6` and `10.0` know to attend to `IO`.**</span>
    * The question then becomes *"what is this information, how does it end up in the `S2` token, and how does `END` know to attend to it?"*

<br>

* The significant heads in layers 7 & 8 are `7.3`, `7.9`, `8.6`, `8.10`. These heads have high activation patching values for their value vectors, less so for their queries and keys.
    * We discovered this by doing **activation patching** on the value inputs for these heads.
    * <span style="color:darkorange">**This supports the previous observation, and it tells us that the interesting computation goes into *what gets moved* from `S2` to `END`, rather than the fact that `END` attends to `S2`.**</span>.
    * We still don't know: *"what is this information, and how does it end up in the `S2` token?"*

<br>

* As well as the 2 clusters of heads given above, there's a third cluster of important heads: early heads (e.g. `3.0`, `5.5`, `6.9`) whose query vectors are particularly important for getting good performance.
    * We discovered this by doing **activation patching** on the query inputs for these heads.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
With all this in mind, can you come up with a theory for what these three heads are doing, and come up with a simple model of the whole circuit?

*Hint - if you're still stuck, try plotting the attention pattern of head `3.0`. The patterns of `5.5` and `6.9` might seem a bit confusing at first (they add complications to the "simplest possible picture" of how the circuit works); we'll discuss them later so they don't get in the way of understanding the core of the circuit.*

<details>
<summary>Answer (and simple diagram of circuit)</summary>

If you plotted the attention pattern for head `3.0`, you should have seen that `S2` paid attention to `S1`. This suggests that the early heads are detecting when the destination token is a duplicate. So the information that the subject is a duplicate gets stored in `S2`.

How can the information that the subject token is a duplicate help us predict the token after `end`? Well, the correct answer (the `IO` token) is the non-duplicated token. So we can infer that the information that the subject token is a duplicate is used to *inhibit* the attention of the late heads to the duplicated token, and they instead attend to the non-duplicated token.

To summarise the second half of the circuit: information about this duplicated token is then moved from `S2` to `end` by the middle cluster of heads `7.3`, `7.9`, `8.6` and `8.10`, and this information goes into the queries of the late heads `9.9`, `9.6` and `10.0`, making them *inhibit* their attention to the duplicated token. Instead, they attend to `IO` (copying this token directly to the logits).

This picture of the circuit turns out to be mostly right. It misses out on some subtleties which we'll discuss shortly, but it's a good rough picture to have in your head. We might illustrate this as follows:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-simple.png" width="1000">

Explanation:

* We call the early heads **DTH** (duplicate token heads), their job is to detect that `S2` is a duplicate.
* The second group of heads are called **SIH** (S-inhibition heads), their job is to move the duplicated token information from `S2` to `END`. We've illustrated this as them moving the positional information, but in principle this could also be token embedding information (more on this in the final section).
* The last group of heads are called **NMH** (name mover heads), their job is to copy the `IO` token to the `END` token, where it is used as the predicted next token (thanks to the S-inihbition heads, these heads don't pay attention to the `S` token).

Note - if you're still confused about how to interpret this diagram, but you understand induction circuits and how they work, it might help to compare this diagram to one written in the same style which I made for [induction circuits](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ih-simple.png). Also, if you've read my induction heads [LessWrong post](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated) and you're confused about how this style of diagram is different from that one, [here](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ih-compared.png) is an image comparing the two diagrams (for induction heads) and explaining how they differ.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, let's flesh out this picture a bit more by comparing our results to the paper results. Below is a more complicated version of the diagram in the dropdown above, which also labels the important heads. The diagram is based on the paper's [original diagram](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672942728/mirroredImages/3ecs6duLmTfyra3Gp/h5icqzpyuhu4mqvfjhvw.png). Don't worry if you don't understand everything in this diagram; the boundaries of the circuit are fuzzy and the "role" of every head is in this circuit is a leaky abstraction. Rather, this diagram is meant to point your intuitions in the right direction for better understanding this circuit.

<details>
<summary>Diagram of large circuit</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-full-c.png" width="1250">
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Here are the main ways it differs from the one above:
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Induction heads

Rather than just having duplicate token heads in the first cluster of heads, we have two other types of heads as well: previous token heads and induction heads. The induction heads do the same thing as the duplicate token heads, via an induction mechanism. They cause token `S2` to attend to `S1+1` (mediated by the previous token heads), and their output is used as both a pointer to `S1` and as a signal that `S1` is duplicated (more on the distinction between these two in the paragraph "Position vs token information being moved" below).

*(Note - the original paper's diagram implies the induction heads and duplicate token heads compose with each other. This is misleading, and is not the case.)*

Why are induction heads used in this circuit? We'll dig into this more in the bonus section, but one likely possibility is that induction heads are just a thing that forms very early on in training by default, and so it makes sense for the model to repurpose this already-existing machinery for this job. See [this paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) for more on induction heads, and how / why they form.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Negative & Backup name mover heads

Earlier, we saw that some heads in later layers were actually harming performance. These heads turn out to be doing something pretty similar to name mover heads, but in reverse (i.e. they inhibit the correct answer). It's not obvious why the model does this; the paper speculates that these heads might help the model "hedge" so as to avoid high cross-entropy loss when making mistakes.

Backup name mover heads are possibly even weirder. It turns out that when we **ablate** the name mover heads, these ones pick up the slack and do the task anyway (even though they don't seem to do it when the NMHs aren't ablated). This is an example of **built-in redundancy** in the model. One possible explanation is that this resulted from the model being trained with dropout, although this explanation isn't fully satisfying (models trained without dropout still seem to have BNMHs, although they aren't as strong as they are in this model). Like with induction heads, we'll dig into this more in the final section.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### Positional vs token information

There are 2 kinds of S-inhibition heads shown in the diagram - ones that inhibit based on positional information (pink), and ones that inhibit based on token information (purple). It's not clear which heads are doing which (and in fact some heads might be doing both!).

The paper has an ingenious way of teasing apart which type of information is being used by which of the S-inhibition heads, which we'll discuss in the final section.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
#### K-composition in S-inhibition heads

When we did activation patching on the keys and values of S-inhibition heads, we found that the values were important and the keys weren't. We concluded that K-composition isn't really happening in these heads, and `END` must be paying attention to `S2` for reasons other than the duplicate token information (e.g. it might just be paying attention to the closest name, or to any names which aren't separated from it by a comma). Although this is mostly true, it turns out that there is a bit of K-composition happening in these heads. We can think of this as the duplicate token heads writing the "duplicated" flag to the residual stream (without containing any information about the identity and position of this token), and this flag is being used by the keys of the S-inhibition heads (i.e. they make `END` pay attention to `S2`). In the diagram, this is represented by the dark grey boxes (rather than just the light grey boxes we had in the simplified version). We haven't seen any evidence for this happening yet, but we will in the next section (when we look at path patching).

Note - whether the early heads are writing positional information or "duplicate flag" information to the residual stream is not necessarily related to whether the head is an induction head or a duplicate token head. In principle, either type of head could write either type of information.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 4️⃣ Path Patching
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This section will be a lot less conceptual and exploratory than the last two sections, and a lot more technical and rigorous. You'll learn what path patching is and how it works, and you'll use it to replicate many of the paper's results (as well as some other paper results not related to path patching).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Here, we'll be more closely following the setup that the paper's authors used, rather than the rough-and-ready exploration we used in the first few sections. To be clear, a lot of the rigour that we'll be using in the setup here isn't necessary if you're just starting to investigate a model's circuit. This rigour is necessary if you're publishing a paper, but it can take a lot of time and effort!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

from part41_indirect_object_identification.ioi_dataset import NAMES, IOIDataset

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The dataset we'll be using is an instance of `IOIDataset`, which is generated by randomly choosing names from the `NAMES` list (as well as sentence templates and objects from different lists). You can look at the `ioi_dataset.py` file to see details of how this is done.

(Note - you can reduce `N` if you're getting memory errors from running this code. If you're still getting memory errors from `N = 10` then you're recommended to switch to Colab, or to use a virtual machine e.g. via Lambda Labs.)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

N = 25
ioi_dataset = IOIDataset(
    prompt_type="mixed",
    N=N,
    tokenizer=model.tokenizer,
    prepend_bos=False,
    seed=1,
    device=str(device),
)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This dataset has a few useful attributes & methods. Here are the main ones you should be aware of for these exercises:

* `toks` is a tensor of shape `(batch_size, max_seq_len)` containing the token IDs (i.e. this is what you pass to your model)
* `s_tokenIDs` and `io_tokenIDs` are lists containing the token IDs for the subjects and objects
* `sentences` is a list containing the sentences (as strings)
* `word_idx` is a dictionary mapping word types (e.g. `"S1"`, `"S2"`, `"IO"` or `"end"`) to tensors containing the positions of those words for each sequence in the dataset.
    * This is particularly handy for indexing, since the positions of the subject, indirect object, and end tokens are no longer the same in every sentence like they were in previous sections.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Firstly, what dataset should we use for patching? In the previous section we just flipped the subject and indirect object tokens around, which meant the direction of the signal was flipped around. However, what we'll be doing here is a bit more principled - rather than flipping the IOI signal, we'll be erasing it. We do this by constructing a new dataset from `ioi_dataset` which replaces every name with a different random name. This way, the sentence structure stays the same, but all information related to the actual indirect object identification  task (i.e. the identities and positions of repeated names) has been erased.

For instance, given the sentence `"When John and Mary went to the shops, John gave the bag to Mary"`, the corresponding sentence in the ABC dataset might be `"When Edward and Laura went to the shops, Adam gave the bag to Mary"`. We would expect the residual stream for the latter prompt to carry no token or positional information which could help it solve the IOI task (i.e. favouring `Mary` over `John`, or favouring the 2nd token over the 4th token).

We define this dataset below. Note the syntax of the `gen_flipped_prompts` method - the letters tell us how to replace the names in the sequence. For instance, `ABB->XYZ` tells us to take sentences of the form `"When Mary and John went to the store, John gave a drink to Mary"` with `"When [X] and [Y] went to the store, [Z] gave a drink to Mary"` for 3 independent randomly chosen names `[X]`, `[Y]` and `[Z]`. We'll use this function more in the bonus section, when we're trying to disentangle positional and token signals (since we can also do fun things like `ABB->BAB` to swap the first two names, etc).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

abc_dataset = ioi_dataset.gen_flipped_prompts("ABB->XYZ, BAB->XYZ")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's take a look at this dataset. We'll define a helper function `make_table`, which prints out tables after being fed columns rather than rows (don't worry about the syntax, it's not important).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def format_prompt(sentence: str) -> str:
    """Format a prompt by underlining names (for rich print)"""
    return (
        re.sub(
            "(" + "|".join(NAMES) + ")", lambda x: f"[u bold dark_orange]{x.group(0)}[/]", sentence
        )
        + "\n"
    )


def make_table(cols, colnames, title="", n_rows=5, decimals=4):
    """Makes and displays a table, from cols rather than rows (using rich print)"""
    table = Table(*colnames, title=title)
    rows = list(zip(*cols))
    f = lambda x: x if isinstance(x, str) else f"{x:.{decimals}f}"
    for row in rows[:n_rows]:
        table.add_row(*list(map(f, row)))
    rprint(table)


if MAIN:
    make_table(
        colnames=["IOI prompt", "IOI subj", "IOI indirect obj", "ABC prompt"],
        cols=[
            map(format_prompt, ioi_dataset.sentences),
            model.to_string(ioi_dataset.s_tokenIDs).split(),
            model.to_string(ioi_dataset.io_tokenIDs).split(),
            map(format_prompt, abc_dataset.sentences),
        ],
        title="Sentences from IOI vs ABC distribution",
    )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                      Sentences from IOI vs ABC distribution                                       </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> IOI prompt                              </span>┃<span style="font-weight: bold"> IOI subj </span>┃<span style="font-weight: bold"> IOI indirect obj </span>┃<span style="font-weight: bold"> ABC prompt                              </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jane</span> got a snack at   │ Jane     │ Victoria         │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> got a snack at   │
│ the store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jane</span> decided to give it to   │          │                  │ the store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Adam</span> decided to give it to   │
│ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span>                                │          │                  │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span>                                │
│                                         │          │                  │                                         │
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span> got a necklace   │ Sullivan │ Rose             │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Marcus</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Max</span> got a necklace at   │
│ at the garden, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> decided to give │          │                  │ the garden, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jeremy</span> decided to give it   │
│ it to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span>                              │          │                  │ to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span>                                 │
│                                         │          │                  │                                         │
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alex</span> got a drink at the   │ Alex     │ Alan             │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jay</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jason</span> got a drink at the   │
│ store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alex</span> decided to give it to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span>  │          │                  │ store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jacob</span> decided to give it to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span> │
│                                         │          │                  │                                         │
│ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jessica</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span> had a long    │ Jessica  │ Crystal          │ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Joshua</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jack</span> had a long        │
│ argument, and afterwards <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jessica</span> said   │          │                  │ argument, and afterwards <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Grant</span> said to  │
│ to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span>                              │          │                  │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span>                                 │
│                                         │          │                  │                                         │
│ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Kevin</span> were working   │ Kevin    │ Jonathan         │ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alice</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Mark</span> were working at    │
│ at the school. <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Kevin</span> decided to give a  │          │                  │ the school. <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Carter</span> decided to give a    │
│ necklace to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span>                    │          │                  │ necklace to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span>                    │
│                                         │          │                  │                                         │
└─────────────────────────────────────────┴──────────┴──────────────────┴─────────────────────────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Next, we'll define functions similar to the ones from previous sections. We've just given you these, rather than making you repeat the exercise of writing them (although you should compare these functions to the ones you wrote earlier, and make sure you understand how they work).

We'll call these functions something slightly different, so as not to pollute namespace.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def logits_to_ave_logit_diff_2(
    logits: Float[Tensor, "batch seq d_vocab"],
    ioi_dataset: IOIDataset = ioi_dataset,
    per_prompt=False,
) -> Float[Tensor, "*batch"]:
    """
    Returns logit difference between the correct and incorrect answer.

    If per_prompt=True, return the array of differences rather than the average.
    """
    # Only the final logits are relevant for the answer
    # Get the logits corresponding to the indirect object / subject tokens respectively
    io_logits: Float[Tensor, "batch"] = logits[
        range(logits.size(0)), ioi_dataset.word_idx["end"], ioi_dataset.io_tokenIDs
    ]
    s_logits: Float[Tensor, "batch"] = logits[
        range(logits.size(0)), ioi_dataset.word_idx["end"], ioi_dataset.s_tokenIDs
    ]
    # Find logit difference
    answer_logit_diff = io_logits - s_logits
    return answer_logit_diff if per_prompt else answer_logit_diff.mean()


model.reset_hooks(including_permanent=True)

ioi_logits_original, ioi_cache = model.run_with_cache(ioi_dataset.toks)
abc_logits_original, abc_cache = model.run_with_cache(abc_dataset.toks)

ioi_per_prompt_diff = logits_to_ave_logit_diff_2(ioi_logits_original, per_prompt=True)
abc_per_prompt_diff = logits_to_ave_logit_diff_2(abc_logits_original, per_prompt=True)

ioi_average_logit_diff = logits_to_ave_logit_diff_2(ioi_logits_original).item()
abc_average_logit_diff = logits_to_ave_logit_diff_2(abc_logits_original).item()

print(f"Average logit diff (IOI dataset): {ioi_average_logit_diff:.4f}")
print(f"Average logit diff (ABC dataset): {abc_average_logit_diff:.4f}")

make_table(
    colnames=["IOI prompt", "IOI logit diff", "ABC prompt", "ABC logit diff"],
    cols=[
        map(format_prompt, ioi_dataset.sentences),
        ioi_per_prompt_diff,
        map(format_prompt, abc_dataset.sentences),
        abc_per_prompt_diff,
    ],
    title="Sentences from IOI vs ABC distribution",
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Average logit diff (IOI dataset): 2.8052
Average logit diff (ABC dataset): -0.7699

<span style="font-style: italic">                                      Sentences from IOI vs ABC distribution                                       </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> IOI prompt                            </span>┃<span style="font-weight: bold"> IOI logit diff </span>┃<span style="font-weight: bold"> ABC prompt                            </span>┃<span style="font-weight: bold"> ABC logit diff </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jane</span> got a snack at │ 2.4403         │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> got a snack at │ -2.4177        │
│ the store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jane</span> decided to give it to │                │ the store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Adam</span> decided to give it to │                │
│ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span>                              │                │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Victoria</span>                              │                │
│                                       │                │                                       │                │
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span> got a necklace │ 6.5408         │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Marcus</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Max</span> got a necklace at │ 6.1583         │
│ at the garden, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Sullivan</span> decided to    │                │ the garden, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jeremy</span> decided to give it │                │
│ give it to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span>                       │                │ to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Rose</span>                               │                │
│                                       │                │                                       │                │
│ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alex</span> got a drink at the │ 3.3345         │ When <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jay</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jason</span> got a drink at the │ -1.7351        │
│ store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alex</span> decided to give it to     │                │ store, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jacob</span> decided to give it to    │                │
│ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span>                                  │                │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alan</span>                                  │                │
│                                       │                │                                       │                │
│ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jessica</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span> had a long  │ 1.1972         │ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Joshua</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jack</span> had a long      │ -4.2358        │
│ argument, and afterwards <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jessica</span> said │                │ argument, and afterwards <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Grant</span> said   │                │
│ to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span>                            │                │ to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Crystal</span>                            │                │
│                                       │                │                                       │                │
│ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Kevin</span> were working │ 3.0530         │ Then, <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Alice</span> and <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Mark</span> were working at  │ 0.1968         │
│ at the school. <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Kevin</span> decided to give  │                │ the school. <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Carter</span> decided to give a  │                │
│ a necklace to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span>                │                │ necklace to <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline">Jonathan</span>                  │                │
│                                       │                │                                       │                │
└───────────────────────────────────────┴────────────────┴───────────────────────────────────────┴────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note that we're always measuring performance ***with respect to the correct answers for the IOI dataset, not the ABC dataset***, because we want our ABC dataset to carry no information that helps with the IOI task (hence patching it in gives us signals which are totally uncorrelated with the correct answer). For instance, the model will obviously not complete sentences like `"When Max and Victoria got a snack at the store, Clark decided to give it to"` with the name `"Tyler"`.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Finally, let's define a new `ioi_metric` function which works for our new data.

In order to match the paper's results, we'll use a different convention here. 0 means performance is the same as on the IOI dataset (i.e. hasn't been harmed in any way), and -1 means performance is the same as on the ABC dataset (i.e. the model has completely lost the ability to distinguish between the subject and indirect object).

Again, we'll call this function something slightly different.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def ioi_metric_2(
    logits: Float[Tensor, "batch seq d_vocab"],
    clean_logit_diff: float = ioi_average_logit_diff,
    corrupted_logit_diff: float = abc_average_logit_diff,
    ioi_dataset: IOIDataset = ioi_dataset,
) -> float:
    """
    We calibrate this so that the value is 0 when performance isn't harmed (i.e. same as IOI
    dataset), and -1 when performance has been destroyed (i.e. is same as ABC dataset).
    """
    patched_logit_diff = logits_to_ave_logit_diff_2(logits, ioi_dataset)
    return (patched_logit_diff - clean_logit_diff) / (clean_logit_diff - corrupted_logit_diff)


print(f"IOI metric (IOI dataset): {ioi_metric_2(ioi_logits_original):.4f}")
print(f"IOI metric (ABC dataset): {ioi_metric_2(abc_logits_original):.4f}")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## What is path patching?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In the previous section, we looked at activation patching, which answers questions like *what would happen if you took an attention head, and swapped the value it writes to the residual stream with the value it would have written under a different distribution, while keeping everything else the same?*. This proved to be a good way to examine the role of individual components like attention heads, and it allowed us to perform some more subtle analysis like patching keys / queries / values in turn to figure out which of them were more important for which heads.

However, when we're studying a circuit, rather than just swapping out an entire attention head, we might want to ask more nuanced questions like *what would happen if the direct input from attention head $A$ to head $B$ (where $B$ comes after $A$) was swapped out with the value it would have been under a different distribution, while keeping everything else the same?*. Rather than answering the general question of how important attention heads are, this answers the more specific question of how important the circuit formed by connecting up these two attention heads is. Path patching is designed to answer questions like these.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The following diagrams might help explain the difference between activation and path patching in transformers. Recall that activation patching looked like:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-2c.png" width="420">

where the black and green distributions are our clean and corrupted datasets respectively (so this would be `ioi_dataset` and `abc_dataset`). In contrast, path patching involves replacing **edges** rather than **nodes**. In the diagram below, we're replacing the edge $D \to G$ with what it would be on the corrupted distribution. So in our patched run, $G$ is calculated just like it would be on the clean distribution, but as if the **direct** input from $D$ had come from the corrupted distribution instead.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-3c.png" width="560">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Unfortunately, for a transformer, this is easier to describe than to actually implement. This is because the "nodes" are attention heads, and the "edges" are all tangled together in the residual stream (that is to say, it's not clear how one could change the value of one edge without without affecting every path that includes that edge). The solution is to use the 3-step algorithm shown in the diagram below (which reads from right to left).

Terminology note - we call head $D$ the **sender node**, and head $G$ the **receiver node**. Also, by "freezing" nodes, we mean "patch with the value that is the same as the input". For instance, if we didn't freeze head $H$ in step 2 below, it would have a different value because it would be affected by the corrupted value of head $D$.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-4c.png" width="900">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's make this concrete, and take a simple 3-layer transformer with 2 heads per layer. Let's perform path patching on the edge from head `0.0` to `2.0` (terminology note: `0.0` is the **sender**, and `2.0` is the **receiver**). Note that here, we're considering "direct paths" as anything that doesn't go through another attention head (so it can go through any combination of MLPs). Intuitively, the nodes (attention heads) are the only things that can move information around in the model, and this is the thing we want to study. In contrast, MLPs just perform information processing, and they're not as interesting for this task.

Our 3-step process looks like the diagram below (remember green is corrupted, grey is clean).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/path-patching-alg-transformers-6.png" width="700">

(Note - in this diagram, the uncoloured nodes indicate we aren't doing any patching; we're just allowing them to be computed from the values of nodes which are downstream of it.)
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Why does this work? If you stare at the middle picture above for long enough, you'll realise that the contribution from every non-direct path from `0.0` $\to$ `2.0` is the same as it would be on the clean distribution, while all the direct paths' contributions are the same as they would be on the corrupted distribution.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/path-patching-decomp-four.png" width="850">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Why MLPs?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You might be wondering why we're including MLPs as part of our direct path. The short answer is that this is what the IOI paper does, and we're trying to replicate it! The slightly longer answer is that both this method and a method which doesn't count MLPs as the direct path are justifiable.

To take one example, suppose the output of head `0.0` is being used directly by head `2.0`, but one of the MLPs is acting as a mediator. To oversimplify, we might imagine that `0.0` writes the vector $v$ into the residual stream, some neuron detects $v$ and writes $w$ to the residual stream, and `2.0` detects $w$. If we didn't count MLPs as a direct path then we wouldn't catch this causal relationship. The drawback is that things get a bit messier, because now we're essentially passing a "fake input" into our MLPs, and it's dangerous to assume that any operation as clean as the one previously described (with vectors $v$, $w$) would still happen under these new circumstances.

Also, having MLPs as part of the direct path doesn't help us understand what role the MLPs play in the circuit, all it does is tell us that some of them are important! Luckily, in the IOI circuit, MLPs aren't important (except for MLP0), and so doing both these forms of path patching get pretty similar results. As an optional exercise, you can reproduce the results from the following few sections using this different form of path patching. It's actually algorithmically easier to implement, because we only need one forward pass rather than two. Can you see why?

<details>
<summary>Answer</summary>

Because the MLPs were part of the direct paths between sender and receiver in the previous version of the algorithm, we had to do a forward pass to find the value we'd be patching into the receivers. But if MLPs aren't part of the direct path, then we can directly compute what to patch into the receiver nodes:

```
orig_receiver_input <- orig_receiver_input + (new_sender_output - old_sender_output)
```

Diagram with direct paths not including MLPs:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/path-patching-decomp-one.png" width="1200">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Path Patching: Name Mover Heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We'll start with a simple type of path patching - with just one receiver node, which is the final value of the residual stream. We've only discussed receiver nodes being other attention heads so far, but the same priciples hold for any choice of receiver nodes.

<details>
<summary>Question - can you explain the difference between path patching from an attention head to the residual stream, and activation patching on that attention head?</summary>

Activation patching changes the value of that head, and all subsequent layers which depend on that head.

Path patching will answer the question "what if the value written by the head directly to the residual stream was the same as in $x_{new}$, but every non-direct path from this head to the residual stream (i.e. paths going through other heads) the value was the same as it would have been under $x_{orig}$?
</details>

This patching is described at the start of section 3.1 in [the paper](https://arxiv.org/pdf/2211.00593.pdf) (page 5). The 3-step process will look like:

1. Run the model on clean and corrupted input. Cache the head outputs.
2. Run the model on clean input, with the sender head **patched** from the corrupted input, and every other head **frozen** to their values on the clean input. Cache the final value of the residual stream (i.e. `resid_post` in the final layer).
3. Normally we would re-run the model on the clean input and patch in the cached value of the final residual stream, but in this case we don't need to because we can just unembed the final value of the residual stream directly without having to run another forward pass.

Here is an illustration for a 2-layer transformer:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/path-patching-residpost-newest.png" width="680">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement path patching to the final residual stream value

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 30-45 minutes on this exercise.
> Path patching is a very challenging algorithm with many different steps.
> ```

You should implement path patching from heads to the residual stream, as described above (and in the paper).

This exercise is expected to be challenging, with several moving parts. We've purposefully left it very open-ended, including a function & docstring but nothing else. 

Here are a few hints / tips for how to proceed:

* Split your function up into 3 parts (one for each of the steps above), and write each section one at a time.
* You'll need a new hook function: one which performs freezing / patching for step 2 of the algorithm.
* You can reuse a lot of code from your activation patching function.
* When calling `model.run_with_cache`, you can use the keyword argument `names_filter`, which is a function from name to boolean. If you use this argument, your model will only cache activations with a name which passes this filter (e.g. you can use it like `names_filter = lambda name: name.endswith("q")` to only cache query vectors).

You can also look at the dropdowns to get more hints and guidance (e.g. if you want to start from a function docstring).

You'll know you've succeeded if you can plot the results, and replicate Figure 3(b) from [the paper](https://arxiv.org/pdf/2211.00593.pdf) (at the top of page 6).

**Note - if you use `model.add_hook` then `model.run_with_cache`, you might have to pass the argument `level=1` to the `add_hook` method. I don't know why the function sometimes fails unless you do this (this bug only started appearing after the exercises were written). I've not had time to track this down, but extra credit to anyone who can (-:**
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Click here to get a docstring for the main function.</summary>

```python
def get_path_patch_head_to_final_resid_post(
    model: HookedTransformer,
    patching_metric: Callable,
    new_dataset: IOIDataset = abc_dataset,
    orig_dataset: IOIDataset = ioi_dataset,
    new_cache: ActivationCache | None = abc_cache,
    orig_cache: ActivationCache | None = ioi_cache,
) -> Float[Tensor, "layer head"]:
    \'\'\'
    Performs path patching (see algorithm in appendix B of IOI paper), with:

        sender head = (each head, looped through, one at a time)
        receiver node = final value of residual stream

    Returns:
        tensor of metric values for every possible sender head
    \'\'\'
    pass
```
</details>

<details>
<summary>Click here to get a docstring for the main function, plus some annotations and function structure.</summary>

```python
def get_path_patch_head_to_final_resid_post(
    model: HookedTransformer,
    patching_metric: Callable,
    new_dataset: IOIDataset = abc_dataset,
    orig_dataset: IOIDataset = ioi_dataset,
    new_cache: ActivationCache | None = abc_cache,
    orig_cache: ActivationCache | None = ioi_cache,
) -> Float[Tensor, "layer head"]:
    \'\'\'
    Performs path patching (see algorithm in appendix B of IOI paper), with:

        sender head = (each head, looped through, one at a time)
        receiver node = final value of residual stream

    Returns:
        tensor of metric values for every possible sender head
    \'\'\'
    model.reset_hooks()
    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)

    # ========== Step 1 ==========
    # Gather activations on x_orig and x_new

    # YOUR CODE HERE


    # Using itertools to loop gives us a smoother progress bar (using nested for loops is also fine)
    for (sender_layer, sender_head) in tqdm_notebook(list(itertools.product(
        range(model.cfg.n_layers),
        range(model.cfg.n_heads)
    ))):
        pass

        # ========== Step 2 ==========
        # Run on x_orig, with sender head patched from x_new, every other head frozen

        # YOUR CODE HERE


        # ========== Step 3 ==========
        # Unembed the final residual stream value, to get our patched logits

        # YOUR CODE HERE


        # Save the results
        results[sender_layer, sender_head] = patching_metric(patched_logits)


    return results
```
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def patch_or_freeze_head_vectors(
    orig_head_vector: Float[Tensor, "batch pos head_index d_head"],
    hook: HookPoint,
    new_cache: ActivationCache,
    orig_cache: ActivationCache,
    head_to_patch: tuple[int, int],
) -> Float[Tensor, "batch pos head_index d_head"]:
    """
    This helps implement step 2 of path patching. We freeze all head outputs (i.e. set them to their
    values in orig_cache), except for head_to_patch (if it's in this layer) which we patch with the
    value from new_cache.

    head_to_patch: tuple of (layer, head)
    """
    # Setting using ..., otherwise changing orig_head_vector will edit cache value too
    orig_head_vector[...] = orig_cache[hook.name][...]
    if head_to_patch[0] == hook.layer():
        orig_head_vector[:, :, head_to_patch[1]] = new_cache[hook.name][:, :, head_to_patch[1]]
    return orig_head_vector


def get_path_patch_head_to_final_resid_post(
    model: HookedTransformer,
    patching_metric: Callable,
    new_dataset: IOIDataset = abc_dataset,
    orig_dataset: IOIDataset = ioi_dataset,
    new_cache: ActivationCache | None = abc_cache,
    orig_cache: ActivationCache | None = ioi_cache,
) -> Float[Tensor, "layer head"]:
    """
    Performs path patching (see algorithm in appendix B of IOI paper), with:

        sender head = (each head, looped through, one at a time)
        receiver node = final value of residual stream

    Returns:
        tensor of metric values for every possible sender head
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    model.reset_hooks()
    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)

    resid_post_hook_name = utils.get_act_name("resid_post", model.cfg.n_layers - 1)
    resid_post_name_filter = lambda name: name == resid_post_hook_name

    # ========== Step 1 ==========
    # Gather activations on x_orig and x_new

    # Note the use of names_filter for the run_with_cache function. Using it means we
    # only cache the things we need (in this case, just attn head outputs).
    z_name_filter = lambda name: name.endswith("z")
    if new_cache is None:
        _, new_cache = model.run_with_cache(
            new_dataset.toks, names_filter=z_name_filter, return_type=None
        )
    if orig_cache is None:
        _, orig_cache = model.run_with_cache(
            orig_dataset.toks, names_filter=z_name_filter, return_type=None
        )

    # Looping over every possible sender head (the receiver is always the final resid_post)
    for sender_layer, sender_head in tqdm(
        list(product(range(model.cfg.n_layers), range(model.cfg.n_heads)))
    ):
        # ========== Step 2 ==========
        # Run on x_orig, with sender head patched from x_new, every other head frozen

        hook_fn = partial(
            patch_or_freeze_head_vectors,
            new_cache=new_cache,
            orig_cache=orig_cache,
            head_to_patch=(sender_layer, sender_head),
        )
        model.add_hook(z_name_filter, hook_fn)

        _, patched_cache = model.run_with_cache(
            orig_dataset.toks, names_filter=resid_post_name_filter, return_type=None
        )
        # if (sender_layer, sender_head) == (9, 9):
        #     return patched_cache
        assert set(patched_cache.keys()) == {resid_post_hook_name}

        # ========== Step 3 ==========
        # Unembed the final residual stream value, to get our patched logits

        patched_logits = model.unembed(model.ln_final(patched_cache[resid_post_hook_name]))

        # Save the results
        results[sender_layer, sender_head] = patching_metric(patched_logits)

    return results
    # END SOLUTION


path_patch_head_to_final_resid_post = get_path_patch_head_to_final_resid_post(model, ioi_metric_2)
# COLAB-SPLIT

imshow(
    100 * path_patch_head_to_final_resid_post,
    title="Direct effect on logit difference",
    labels={"x": "Head", "y": "Layer", "color": "Logit diff. variation"},
    coloraxis=dict(colorbar_ticksuffix="%"),
    width=600,
)

# FILTERS: ~
# fig = imshow(
#     100 * path_patch_head_to_final_resid_post,
#     title="Direct effect on logit difference",
#     labels={"x": "Head", "y": "Layer", "color": "Logit diff. variation"},
#     coloraxis=dict(colorbar_ticksuffix="%"),
#     width=600,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14113.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14113.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Help - all the values in my heatmap are the same.</summary>

There could be a few possible reasons for this. A common one is that you're changing an actual tensor, rather than just changing its values - this means when one tensor changes, the other one does too. For instance, if you do something like:

```python
x = t.zeros(3)
y = x
x[0] = 1
print(y)
```

then `y` will also be `[1, 0, 0]`. To avoid this, you can use the `...` syntax, which means "set all values in this tensor to the values in this other tensor". For instance, if you do:

```python
x = t.zeros(3)
y = t.zeros(3)
x[...] = y
x[0] = 1
print(y)
```

then `y` will still be `[0, 0, 0]`.

Using `x[:] = y` will also work.

---

Another possible explanation would be passing in the wrong input values / cache at some point in the algorithm, or freezing to the wrong values. Remember that in the diagram, grey represents original values (clean) and blue represents new values (corrupted), so e.g. in step 2 we want to run the model on `orig_dataset` (= IOI dataset) and we also want to freeze all non-sender heads to their values in `orig_cache`.

---

Lastly, make sure you're not freezing your heads in a way that doesn't override the sender patching! If more than one hook function is added to a hook point, they're executed in the order they were added (with the last one possibly overriding the previous ones).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What is the interpretation of this plot? How does it compare to the equivalent plot we got from activation patching? (Remember that our metric is defined in a different way, so we should expect a sign difference between the two results.)

<details>
<summary>Some thoughts</summary>

This plot is actually almost identical to the one we got from activation patching (apart from the results being negated, because of the new metric).

This makes sense; the only reason activation patching would do something different to path patching is if the heads writing in the `Mary - John` direction had their outputs used by a later head (because this would be accounted for in activation patching, whereas path patching isolates the direct effect on the residual stream only). Since attention heads' primary purpose is to move information around the model, it's reasonable to guess that this probably isn't happening.

Don't worry though, in the next set of exercises we'll do some more interesting path patching, and we'll get some results which are meaningfully different from our activation patching results.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Path Patching: S-Inhibition Heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In the first section on path patching, we performed a simple kind of patching - from the output of an attention head to the final value of the residual stream. Here we'll do something a bit more interesting, and patch from the output of one head to the input of a later head. The purpose of this is to examine exactly how two heads are composing, and what effect the composed heads have on the model's output.

We got a hint of this in the previous section, where we patched the values of the S-inhibition heads and found that they were important. But this didn't tell us which inputs to these value vectors were important; we had to make educated guesses about this based on our analysis earlier parts of the model. In path patching, we can perform a more precise test to find which heads are important.

The paper's results from path patching are shown in figure 5(b), on page 7.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement path patching from head to head

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 20-25 minutes on this exercise.
> 
> You'll need a new hook function, but copying code from the previous exercise should make this one easier.
> ```

You should fill in the function `get_path_patch_head_to_head` below. It takes as arguments a list of receiver nodes (as well as the type of input - keys, queries, or values), and returns a tensor of shape\* `(layer, head)` where each element is the result of running the patching metric on the output of the model, after applying the 3-step path patching algorithm from one of the model's heads to all the receiver heads. You should be able to replicate the paper's results (figure 5(b)).

\**Actually, you don't need to return all layers, because the causal effect from any sender head which is on the same or a later layer than the last of your receiver heads will necessarily be zero.*

If you want a bit more guidance, you can use the dropdown below to see the ways in which this function should be different from your first path patching function (in most ways these functions will be similar, so you can start by copying that function).

<details>
<summary>Differences from first path patching function</summary>

Step 1 is identical in both - gather all the observations.

Step 2 is very similar. The only difference is that you'll be caching a different set of activations (your receiver heads).

In section 3, since your receiver nodes are in the middle of the model rather than at the very end, you will have to run the model again with these nodes patched in rather than just calculating the logit output directly from the patched values of the final residual stream. To do this, you'll have to write a new hook function to patch in the inputs to an attention head (if you haven't done this already).
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

def patch_head_input(
    orig_activation: Float[Tensor, "batch pos head_idx d_head"],
    hook: HookPoint,
    patched_cache: ActivationCache,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "batch pos head_idx d_head"]:
    """
    Function which can patch any combination of heads in layers,
    according to the heads in head_list.
    """
    heads_to_patch = [head for layer, head in head_list if layer == hook.layer()]
    orig_activation[:, :, heads_to_patch] = patched_cache[hook.name][:, :, heads_to_patch]
    return orig_activation


def get_path_patch_head_to_heads(
    receiver_heads: list[tuple[int, int]],
    receiver_input: str,
    model: HookedTransformer,
    patching_metric: Callable,
    new_dataset: IOIDataset = abc_dataset,
    orig_dataset: IOIDataset = ioi_dataset,
    new_cache: ActivationCache | None = None,
    orig_cache: ActivationCache | None = None,
) -> Float[Tensor, "layer head"]:
    """
    Performs path patching (see algorithm in appendix B of IOI paper), with:

        sender head = (each head, looped through, one at a time)
        receiver node = input to a later head (or set of heads)

    The receiver node is specified by receiver_heads and receiver_input, for example if
    receiver_input = "v" and receiver_heads = [(8, 6), (8, 10), (7, 9), (7, 3)], we're doing path
    patching from each head to the value inputs of the S-inhibition heads.

    Returns:
        tensor of metric values for every possible sender head
    """
    model.reset_hooks()

    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    assert receiver_input in ("k", "q", "v")
    receiver_layers = set(next(zip(*receiver_heads)))
    receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]
    receiver_hook_names_filter = lambda name: name in receiver_hook_names

    results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=device, dtype=t.float32)

    # ========== Step 1 ==========
    # Gather activations on x_orig and x_new

    # Note the use of names_filter for the run_with_cache function. Using it means we
    # only cache the things we need (in this case, just attn head outputs).
    z_name_filter = lambda name: name.endswith("z")
    if new_cache is None:
        _, new_cache = model.run_with_cache(
            new_dataset.toks, names_filter=z_name_filter, return_type=None
        )
    if orig_cache is None:
        _, orig_cache = model.run_with_cache(
            orig_dataset.toks, names_filter=z_name_filter, return_type=None
        )

    # Note, the sender layer will always be before the final receiver layer, otherwise there will
    # be no causal effect from sender -> receiver. So we only need to loop this far.
    for sender_layer, sender_head in tqdm(
        list(product(range(max(receiver_layers)), range(model.cfg.n_heads)))
    ):
        # ========== Step 2 ==========
        # Run on x_orig, with sender head patched from x_new, every other head frozen

        hook_fn = partial(
            patch_or_freeze_head_vectors,
            new_cache=new_cache,
            orig_cache=orig_cache,
            head_to_patch=(sender_layer, sender_head),
        )
        model.add_hook(z_name_filter, hook_fn, level=1)

        _, patched_cache = model.run_with_cache(
            orig_dataset.toks, names_filter=receiver_hook_names_filter, return_type=None
        )
        # model.reset_hooks(including_permanent=True)
        assert set(patched_cache.keys()) == set(receiver_hook_names)

        # ========== Step 3 ==========
        # Run on x_orig, patching in the receiver node(s) from the previously cached value

        hook_fn = partial(
            patch_head_input,
            patched_cache=patched_cache,
            head_list=receiver_heads,
        )
        patched_logits = model.run_with_hooks(
            orig_dataset.toks,
            fwd_hooks=[(receiver_hook_names_filter, hook_fn)],
            return_type="logits",
        )

        # Save the results
        results[sender_layer, sender_head] = patching_metric(patched_logits)

    return results
    # END SOLUTION


model.reset_hooks()

s_inhibition_value_path_patching_results = get_path_patch_head_to_heads(
    receiver_heads=[(8, 6), (8, 10), (7, 9), (7, 3)],
    receiver_input="v",
    model=model,
    patching_metric=ioi_metric_2,
)
# COLAB-SPLIT

imshow(
    100 * s_inhibition_value_path_patching_results,
    title="Direct effect on S-Inhibition Heads' values",
    labels={"x": "Head", "y": "Layer", "color": "Logit diff.<br>variation"},
    width=600,
    coloraxis=dict(colorbar_ticksuffix="%"),
)

# FILTERS: ~
# fig = imshow(
#     100 * s_inhibition_value_path_patching_results,
#     title="Direct effect on S-Inhibition Heads' values",
#     labels={"x": "Head", "y": "Layer", "color": "Logit diff.<br>variation"},
#     width=600,
#     coloraxis=dict(colorbar_ticksuffix="%"),
#     return_fig=True,
# )
# fig.write_html(section_dir / "14114.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14114.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Question - what is the interpretation of this plot? </summary>

This plot confirms our earlier observations, that the S-inhibition heads' value vectors are the ones which matter. But it does more, by confirming our hypothesis that the S-inhibition heads' value vectors are supplied to them primarily by the outputs of heads `0.1`, `3.0`, `5.5` and `6.9` (which are the heads found by the paper to be the two most important duplicate token heads and two most important induction heads respectively).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 5️⃣ Full Replication: Minimial Circuits and more
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
This section will be a lot more open-ended and challenging. You'll be given somewhat less guidance in the exercises.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Copying & writing direction results
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We'll start this section by replicating the paper's analysis of the **name mover heads** and **negative name mover heads**. Our previous analysis should have pretty much convinced us that these heads are copying / negatively copying our indirect object token, but the results here show this with a bit more rigour.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - replicate writing direction results

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 20-25 minutes on this exercise.
> These exercises are much more challenging than they are conceptually important.
> ```

Let's look at figure 3(c) from the paper. This plots the output of the strongest name mover and negative name mover heads against the attention probabilities for `END` attending to `IO` or `S` (color-coded).

Some clarifications:
* "Projection" here is being used synonymously with "dot product".
* We're projecting onto the name embedding, i.e. the embedding vector for the token which is being paid attention to. This is not the same as the logit diff (which we got by projecting the heads' output onto the difference between the unembedding vectors for `IO` and `S`).
    * We're doing this because the question we're trying to answer is *"does the attention head copy (or anti-copy) the names which it pays attention to?"*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should write code to replicate the paper's results in the cells below. Given four 1D tensors storing the results for a particular head (i.e. the projections and attention probabilities, for the `IO` and `S` tokens respectively), we've given you code to generate a plot which looks like the one in the paper. Again, you'll know that your code has worked if you can get results that resemble those in the paper.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def scatter_embedding_vs_attn(
    attn_from_end_to_io: Float[Tensor, "batch"],
    attn_from_end_to_s: Float[Tensor, "batch"],
    projection_in_io_dir: Float[Tensor, "batch"],
    projection_in_s_dir: Float[Tensor, "batch"],
    layer: int,
    head: int,
):
    scatter(
        x=t.concat([attn_from_end_to_io, attn_from_end_to_s], dim=0),
        y=t.concat([projection_in_io_dir, projection_in_s_dir], dim=0),
        color=["IO"] * N + ["S"] * N,
        title=f"Projection of the output of {layer}.{head} along the name<br>embedding vs attention probability on name",
        title_x=0.5,
        labels={"x": "Attn prob on name", "y": "Dot w Name Embed", "color": "Name type"},
        color_discrete_sequence=["#72FF64", "#C9A5F7"],
        width=650,
    )
    # FILTERS: ~
    # fig = scatter(
    #     x=t.concat([attn_from_end_to_io, attn_from_end_to_s], dim=0),
    #     y=t.concat([projection_in_io_dir, projection_in_s_dir], dim=0),
    #     color=["IO"] * N + ["S"] * N,
    #     title=f"Projection of the output of {layer}.{head} along the name<br>embedding vs attention probability on name",
    #     title_x=0.5,
    #     labels={"x": "Attn prob on name", "y": "Dot w Name Embed", "color": "Name type"},
    #     color_discrete_sequence=["#72FF64", "#C9A5F7"],
    #     width=650,
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / f"14115-L{layer}H{head}.html")
    # END FILTERS


if MAIN:

    def calculate_and_show_scatter_embedding_vs_attn(
        layer: int,
        head: int,
        cache: ActivationCache = ioi_cache,
        dataset: IOIDataset = ioi_dataset,
    ) -> None:
        """
        Creates and plots a figure equivalent to 3(c) in the paper.

        This should involve computing the four 1D tensors:
            attn_from_end_to_io
            attn_from_end_to_s
            projection_in_io_dir
            projection_in_s_dir
        and then calling the scatter_embedding_vs_attn function.
        """
        # EXERCISE
        # raise NotImplementedError()
        # END EXERCISE
        # SOLUTION
        # Get the value written to the residual stream at the end token by this head
        z = cache[utils.get_act_name("z", layer)][:, :, head]  # [batch seq d_head]
        N = z.size(0)
        output = z @ model.W_O[layer, head]  # [batch seq d_model]
        output_on_end_token = output[t.arange(N), dataset.word_idx["end"]]  # [batch d_model]

        # Get the directions we'll be projecting onto
        io_unembedding = model.W_U.T[dataset.io_tokenIDs]  # [batch d_model]
        s_unembedding = model.W_U.T[dataset.s_tokenIDs]  # [batch d_model]

        # Get the value of projections, by multiplying and summing over the d_model dimension
        projection_in_io_dir = (output_on_end_token * io_unembedding).sum(-1)  # [batch]
        projection_in_s_dir = (output_on_end_token * s_unembedding).sum(-1)  # [batch]

        # Get attention probs, and index to get the probabilities from END -> IO / S
        attn_probs = cache["pattern", layer][:, head]  # [batch seqQ seqK]
        attn_from_end_to_io = attn_probs[
            t.arange(N), dataset.word_idx["end"], dataset.word_idx["IO"]
        ]  # [batch]
        attn_from_end_to_s = attn_probs[
            t.arange(N), dataset.word_idx["end"], dataset.word_idx["S1"]
        ]  # [batch]

        # Show scatter plot
        scatter_embedding_vs_attn(
            attn_from_end_to_io,
            attn_from_end_to_s,
            projection_in_io_dir,
            projection_in_s_dir,
            layer,
            head,
        )
        # END SOLUTION

    # COLAB-SPLIT
    calculate_and_show_scatter_embedding_vs_attn(9, 9)  # name mover head 9.9

    calculate_and_show_scatter_embedding_vs_attn(11, 10)  # negative name mover head 11.10

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14115-L9H9.html" width="670" height="480"></div>
<br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14115-L11H10.html" width="670" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - replicate copying score results

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵⚪⚪⚪
> 
> You should spend up to 30-40 minutes on this exercise.
> These exercises are much more challenging than they are conceptually important.
> ```

Now let's do a different kind of test of the name mover heads' copying, by looking directly at the OV circuits.

From page 6 of the paper:

> To check that the Name Mover Heads copy names generally, we studied what values are written via the heads’ OV matrix. Specifically, we first obtained the state of the residual stream at the position of each name token after the first MLP layer. Then, we multiplied this by the OV matrix of a Name Mover Head (simulating what would happen if the head attended perfectly to that token), multiplied by the unembedding matrix, and applied the final layer norm to obtain logit probabilities. We compute the proportion of samples that contain the input name token in the top 5 logits (N = 1000) and call this the copy score. All three Name Mover Heads have a copy score above 95%, compared to less than 20% for an average head.
>
> Negative Name Mover Heads ... have a large negative copy score–the copy score calculated with the negative of the OV matrix (98% compared to 12% for an average head).

Note the similarity between their method and how we studied copying in induction heads, during an earlier set of exercises. However, there are differences (e.g. we're only looking at whether the head copies names, rather than whether it copies tokens in general).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should replicate these results by completing the `get_copying_scores` function below.

You could do this by indexing from the `ioi_cache`, but a much more principled alternative would be to embed all the names in the `NAMES` list and apply operations like MLPs, layernorms and OV matrices manually. This is what the solutions do.

A few notes:

* You can use `model.to_tokens` to convert the names to tokens. Remember to use `prepend_bos=False`, since you just want the tokens of names so you can embed them. Note that this function will treat the list of names as a batch of single-token inputs, which works fine for our purposes.
* You can apply MLPs and layernorms as functions, by just indexing the model's blocks (e.g. use `model.blocks[i].mlp` or `model.blocks[j].ln1` as a function). Remember that `ln1` is the layernorm that comes before attention, and `ln2` comes before the MLP.
* Remember that you need to apply MLP0 before you apply the OV matrix (which is why we omit the 0th layer in our scores). The reason for this is that ablating MLP0 has a strangely large effect in gpt2-small relative to ablating other MLPs, possibly because it's acting as an extended embedding (see [here](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/XNjRwEX9kxbpzWFWd#:~:text=GPT%2D2%20Small%E2%80%99s%20performance%20is%20ruined%20if%20you%20ablate%20MLP0) for an explanation).

Also, you shouldn't expect to get exactly the same results as the paper (because some parts of this experiment have been set up very slightly different), although you probably shouldn't be off by more than 10%.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def get_copying_scores(
    model: HookedTransformer, k: int = 5, names: list = NAMES
) -> Float[Tensor, "2 layer-1 head"]:
    """
    Gets copying scores (both positive and negative) as described in page 6 of the IOI paper, for
    every (layer, head) pair in the model.

    Returns these in a 3D tensor (the first dimension is for positive vs negative).

    Omits the 0th layer, because this is before MLP0 (which we're claiming acts as an extended
    embedding).
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads), device=device)

    # Define components from our model (for typechecking, and cleaner code)
    embed: Embed = model.embed
    mlp0: MLP = model.blocks[0].mlp
    ln0: LayerNorm = model.blocks[0].ln2
    unembed: Unembed = model.unembed
    ln_final: LayerNorm = model.ln_final

    # Get embeddings for the names in our list
    name_tokens: Int[Tensor, "batch 1"] = model.to_tokens(names, prepend_bos=False)
    name_embeddings: Int[Tensor, "batch 1 d_model"] = embed(name_tokens)

    # Get residual stream after applying MLP
    resid_after_mlp1 = name_embeddings + mlp0(ln0(name_embeddings))

    # Loop over all (layer, head) pairs
    for layer in range(1, model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            # Get W_OV matrix
            W_OV = model.W_V[layer, head] @ model.W_O[layer, head]

            # Get residual stream after applying W_OV or -W_OV respectively
            # (note, because of bias b_U, it matters that we do sign flip here, not later)
            resid_after_OV_pos = resid_after_mlp1 @ W_OV
            resid_after_OV_neg = resid_after_mlp1 @ -W_OV

            # Get logits from value of residual stream
            logits_pos = unembed(ln_final(resid_after_OV_pos)).squeeze()  # [batch d_vocab]
            logits_neg = unembed(ln_final(resid_after_OV_neg)).squeeze()  # [batch d_vocab]

            # Check how many are in top k
            topk_logits: Int[Tensor, "batch k"] = t.topk(logits_pos, dim=-1, k=k).indices
            in_topk = (topk_logits == name_tokens).any(-1)
            # Check how many are in bottom k
            bottomk_logits: Int[Tensor, "batch k"] = t.topk(logits_neg, dim=-1, k=k).indices
            in_bottomk = (bottomk_logits == name_tokens).any(-1)

            # Fill in results
            results[:, layer - 1, head] = t.tensor(
                [in_topk.float().mean(), in_bottomk.float().mean()]
            )

    return results
    # END SOLUTION


# FILTERS: st,py
if MAIN:
    copying_results = get_copying_scores(model)

    imshow(
        copying_results,
        facet_col=0,
        facet_labels=["Positive copying scores", "Negative copying scores"],
        title="Copying scores of attention heads' OV circuits",
        width=900,
    )

    heads = {"name mover": [(9, 9), (10, 0), (9, 6)], "negative name mover": [(10, 7), (11, 10)]}

    for i, name in enumerate(["name mover", "negative name mover"]):
        make_table(
            title=f"Copying Scores ({name} heads)",
            colnames=["Head", "Score"],
            cols=[
                list(map(str, heads[name])) + ["[dark_orange bold]Average"],
                [f"{copying_results[i, layer - 1, head]:.2%}" for (layer, head) in heads[name]]
                + [f"[dark_orange bold]{copying_results[i].mean():.2%}"],
            ],
        )
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     copying_results,
#     facet_col=0,
#     facet_labels=["Positive copying scores", "Negative copying scores"],
#     title="Copying scores of attention heads' OV circuits",
#     width=900,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14116.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# copying_results = get_copying_scores(model)

# imshow(
#     copying_results,
#     facet_col=0,
#     facet_labels=["Positive copying scores", "Negative copying scores"],
#     title="Copying scores of attention heads' OV circuits",
#     width=800
# )

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# heads = {"name mover": [(9, 9), (10, 0), (9, 6)], "negative name mover": [(10, 7), (11, 10)]}

# for i, name in enumerate(["name mover", "negative name mover"]):
#     make_table(
#         title=f"Copying Scores ({name} heads)",
#         colnames=["Head", "Score"],
#         cols=[
#             list(map(str, heads[name])) + ["[dark_orange bold]Average"],
#             [f"{copying_results[i, layer-1, head]:.2%}" for (layer, head) in heads[name]] + [f"[dark_orange bold]{copying_results[i].mean():.2%}"]
#         ]
#     )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14116.html" width="920" height="480"></div>

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">Copying Scores (name </span>
<span style="font-style: italic">    mover heads)     </span>
┏━━━━━━━━━┳━━━━━━━━━┓
┃<span style="font-weight: bold"> Head    </span>┃<span style="font-weight: bold"> Score   </span>┃
┡━━━━━━━━━╇━━━━━━━━━┩
│ (9, 9)  │ 100.00% │
│ (10, 0) │ 94.50%  │
│ (9, 6)  │ 94.50%  │
│ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">Average</span> │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">10.11%</span>  │
└─────────┴─────────┘

<span style="font-style: italic">    Copying Scores    </span>
<span style="font-style: italic"> (negative name mover </span>
<span style="font-style: italic">        heads)        </span>
┏━━━━━━━━━━┳━━━━━━━━━┓
┃<span style="font-weight: bold"> Head     </span>┃<span style="font-weight: bold"> Score   </span>┃
┡━━━━━━━━━━╇━━━━━━━━━┩
│ (10, 7)  │ 100.00% │
│ (11, 10) │ 100.00% │
│ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">Average</span>  │ <span style="color: #ff8700; text-decoration-color: #ff8700; font-weight: bold">1.93%</span>   │
└──────────┴─────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Validation of early heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
There are three different kinds of heads which appear early in the circuit, which can be validated by looking at their attention patterns on simple random sequences of tokens. Can you figure out which three types these are, and how to validate them in this way?

<details>
<summary>Answer</summary>

Previous token heads, induction heads, and duplicate token heads.

We can validate them all at the same time, using sequences of `n` random tokens followed by those same `n` random tokens repeated. This works as follows:

* Prev token heads, by measuring the attention patterns with an offset of one (i.e. one below the diagonal).
* Induction heads, by measuring the attention patterns with an offset of `n-1` (i.e. the second instance of a token paying attention to the token after its first instance).
* Duplicate token heads, by measuring the attention patterns with an offset of `n` (i.e. a token paying attention to its previous instance).

In all three cases, if heads score close to 1 on these metrics, it's strong evidence that they are working as this type of head.

Note, it's a leaky abstraction to say things like "head X is an induction head", since we're only observing it on a certain distribution. For instance, it's not clear what the role of induction heads and duplicate token heads is when there are no duplicates (they could in theory do something completely different).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - perform head validation

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 20-30 minutes on this exercise.
> Understanding how to identify certain types of heads by their characteristic attention patterns is important.
> ```

Once you've read the answer in the dropdown above, you should perform this validation. The result should be a replication of Figure 18 in the paper.

We've provided a template for this function. Note use of `typing.Literal`, which is how we indicate that the argument should be one of the following options.

We've also provided a helper function `generate_repeated_tokens` (which is similar to the one you used from exercise set 1.2, except that it has no start token, to match the paper), and a helper function `plot_early_head_validation_results` which calls the `get_attn_scores` function and plots the results (in a way which should look like Figure 18). So it's just the `get_attn_scores` function that you need to fill in.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def generate_repeated_tokens(
    model: HookedTransformer, seq_len: int, batch: int = 1
) -> Float[Tensor, "batch 2*seq_len"]:
    """
    Generates a sequence of repeated random tokens (no start token).
    """
    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.int64)
    rep_tokens = t.cat([rep_tokens_half, rep_tokens_half], dim=-1).to(device)
    return rep_tokens


def get_attn_scores(
    model: HookedTransformer,
    seq_len: int,
    batch: int,
    head_type: Literal["duplicate", "prev", "induction"],
) -> Float[Tensor, "n_layers n_heads"]:
    """
    Returns attention scores for sequence of duplicated tokens, for every head.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    rep_tokens = generate_repeated_tokens(model, seq_len, batch)

    _, cache = model.run_with_cache(
        rep_tokens, return_type=None, names_filter=lambda name: name.endswith("pattern")
    )

    # Get the right indices for the attention scores

    if head_type == "duplicate":
        src_indices = range(seq_len)
        dest_indices = range(seq_len, 2 * seq_len)
    elif head_type == "prev":
        src_indices = range(seq_len)
        dest_indices = range(1, seq_len + 1)
    elif head_type == "induction":
        dest_indices = range(seq_len, 2 * seq_len)
        src_indices = range(1, seq_len + 1)

    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)
    for layer in range(model.cfg.n_layers):
        for head in range(model.cfg.n_heads):
            attn_scores = cache["pattern", layer]  # [batch seqQ seqK]
            avg_attn_on_duplicates = attn_scores[:, head, dest_indices, src_indices].mean().item()
            results[layer, head] = avg_attn_on_duplicates

    return results
    # END SOLUTION


def plot_early_head_validation_results(seq_len: int = 50, batch: int = 50):
    """
    Produces a plot that looks like Figure 18 in the paper.
    """
    head_types = ["duplicate", "prev", "induction"]

    results = t.stack(
        [get_attn_scores(model, seq_len, batch, head_type=head_type) for head_type in head_types]
    )

    imshow(
        results,
        facet_col=0,
        facet_labels=[
            f"{head_type.capitalize()} token attention prob.<br>on sequences of random tokens"
            for head_type in head_types
        ],
        labels={"x": "Head", "y": "Layer"},
        width=1300,
    )
    # FILTERS: ~
    # fig = imshow(
    #     results,
    #     facet_col=0,
    #     facet_labels=[
    #         f"{head_type.capitalize()} token attention prob.<br>on sequences of random tokens"
    #         for head_type in head_types
    #     ],
    #     labels={"x": "Head", "y": "Layer"},
    #     width=1300,
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / "14117.html")
    # END FILTERS


# FILTERS: st,py
if MAIN:
    model.reset_hooks()
    plot_early_head_validation_results()
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# model.reset_hooks()
# plot_early_head_validation_results()

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14117.html" width="1320" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note - these figures suggest that it would be a useful bit of infrastructure to have a "wiki" for the heads of a model, giving their scores according to some metrics re head functions, like the ones we've seen here. HookedTransformer makes this pretty easy to make, as just changing the name input to `HookedTransformer.from_pretrained` gives a different model but in the same architecture, so the same code should work. If you want to make this, I'd love to see it!

As a proof of concept, [I made a mosaic of all induction heads across the 40 models then in HookedTransformer](https://www.neelnanda.io/mosaic).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Minimal Circuit
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Background: faithfulness, completeness, and minimality
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The authors developed three criteria for validating their circuit explanations: faithfulness, completeness and minimality. They are defined as follows:

* **Faithful** = the circuit can perform as well as the whole model
* **Complete** = the circuit contains all nodes used to perform the task
* **Minimal** = the circuit doesn't contain nodes irrelevant to the task

If all three criteria are met, then the circuit is considered a reliable explanation for model behaviour.

Exercise - can you understand why each of these criteria is important? For each pair of criteria, is it possible for a circuit to meet them both but fail the third (and if yes, can you give examples?).

<details>
<summary>Answer</summary>

The naive circuit (containing the entire model) is trivially faithful and complete, but obviously not minimal. In general, the problem with non-minimal circuits is that they may not be mechanistically understandable, which defeats the purpose of this kind of circuit analysis.

Completeness obviously implies faithfulness, because if a node isn't involved in the task, then it can't improve the model's performance on that task.

You might initially think faithfulness implies completeness, but this actually isn't true. Backup name mover heads illustrate this point. They are used in the task, and without understanding the role they play you'll have an incorrect model of reality (e.g. you'll think ablating the name mover heads would destroy performance, which turns out not to be true). So if you define a circuit that doesn't contain backup name mover heads then it will be faithful (the backup name mover heads won't be used) but not complete.

Summary:

* **Faithful & complete, not minimal** = possible (example: naive circuit)
* **Faithful & minimal, not complete** = possible (example: circuit missing backup name mover heads)
* **Complete & minimal, not faithful** = impossible (since completeness implies faithfulness)

In the paper, the authors formalise these concepts. Faithfulness is equivalent to $|F(C) - F(M)|$ being small (where $C$ is our circuit, $M$ is the model, and $F$ is our performance metric function), and completeness is equivalent to $|F(C\backslash K) - F(M\backslash K)|$ being small **for any subset $K \subset C$** (including when $K$ is the empty set, showing that completeness implies faithfulness). Can you see how our circuit missing backup name mover heads violates this condition?

<details>
<summary>Answer</summary>

It violates the condition when $K$ is the set of name mover heads. $C \backslash K$ performs worse than $M \backslash K$, because the latter contains backup name mover heads while the former has lost its name mover heads ***and*** backup name mover heads.
</details>
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now that we've analysed most components of the circuit and we have a rough idea of how they work, the next step is to ablate everything except those core components and verify that the model still performs well.

This ablation is pretty massive - we're ablating everything except for the output of each of our key attention heads (e.g. duplicate token heads or S-inhibition heads) at a single sequence position (e.g. for the DTHs this is the `S2` token, and for SIHs this is the `end` token). Given that our core circuit has 26 heads in total, and our sequences have length around 20 on average, this means we're ablating all but $(26 / 144) / 20 \approx 1\%$ of our attention heads' output (and the number of possible paths through the model is reduced by ***much*** more than this).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
How do we ablate? We could use zero-ablation, but this actually has some non-obvious problems. To explain why intuitively, heads might be "expecting" non-zero input, and setting the input to zero is essentially an arbitrary choice which takes it off distribution. You can think of this as adding a bias term to this head, which might mess up subsequent computation and lead to noisy results. We could also use mean-ablation (i.e. set a head's output to its average output over `ioi_dataset`), but the problem here is that taking the mean over this dataset might contain relevant information for solving the IOI task. For example the `is_duplicated` flag which gets written to `S2` will be present for all sequences, so averaging won't remove this information.

Can you think of a way to solve this problem? After you've considered this, you can use the dropdown below to see how the authors handled this.

<details>
<summary>Answer</summary>

We ablate with the mean of the ABC dataset rather than the IOI dataset. This removes the problem of averages still containing relevant information from solving the IOI task.

</details>

One other complication - the sentences have different templates, and the positions of tokens like `S` and `IO` are not consistent across these templates (we avoided this problem in previous exercises by choosing a very small set of sentences, where all the important tokens had the same indices). An example of two templates with different positions:

```
"Then, [B] and [A] had a long argument and after that [B] said to [A]"
"After the lunch [B] and [A] went to the [PLACE], and [B] gave a [OBJECT] to [A]"
```

Can you guess what the authors did to solve this problem?

<details>
<summary>Answer</summary>

They took the mean over each template rather than over the whole dataset, and used these values to ablate with.

In other words, when they performed ablation by patching in the output of a head (which has shape `(batch, seq, d_model)`), the value patched into the `(i, j, k)`-th element of this tensor would be the average value of the `k`-th element of the vector at sequence position `j`, for all sentences with the same template as the `i`-th sentence in the batch.
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - constructing the minimal circuit

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵⚪⚪⚪
> 
> This exercise is expected to take a long time; at least an hour. It is probably the most challenging exercise in this notebook.
> ```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You now have enough information to perform ablation on your model, to get the minimal circuit. Below, you can try to implement this yourself.

This exercise is very technically challenging, so you're welcome to skip it if it doesn't seem interesting to you. However, I recommend you have a read of the solution, to understand the rough contours of how this ablation works.

If you want to attempt this task, then you can start with the code below. We define two dictionaries, one mapping head types to the heads in the model which are of that type, and the other mapping head types to the sequence positions which we *won't* be ablating for those types of head.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

CIRCUIT = {
    "name mover": [(9, 9), (10, 0), (9, 6)],
    "backup name mover": [(10, 10), (10, 6), (10, 2), (10, 1), (11, 2), (9, 7), (9, 0), (11, 9)],
    "negative name mover": [(10, 7), (11, 10)],
    "s2 inhibition": [(7, 3), (7, 9), (8, 6), (8, 10)],
    "induction": [(5, 5), (5, 8), (5, 9), (6, 9)],
    "duplicate token": [(0, 1), (0, 10), (3, 0)],
    "previous token": [(2, 2), (4, 11)],
}

SEQ_POS_TO_KEEP = {
    "name mover": "end",
    "backup name mover": "end",
    "negative name mover": "end",
    "s2 inhibition": "end",
    "induction": "S2",
    "duplicate token": "S2",
    "previous token": "S1+1",
}

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To be clear, the things that we'll be mean-ablating are:

* Every head not in the `CIRCUIT` dict
* Every sequence position for the heads in `CIRCUIT` dict, except for the sequence positions given by the `SEQ_POS_TO_KEEP` dict

And we'll be mean-ablating by replacing a head's output with the mean output for `abc_dataset`, over all sentences with the same template as the sentence in the batch. You can access the templates for a dataset using the `dataset.groups` attribute, which returns a list of tensors (each one containing the indices of sequences in the batch sharing the same template).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, you can try and complete the following function, which should add a ***permanent hook*** to perform this ablation whenever the model is run on the `ioi_dataset` (note that this hook will only make sense if the model is run on this dataset, so we should reset hooks if we run it on a different dataset).

Permanent hooks are a useful feature of transformerlens. They behave just like regular hooks, except they aren't removed when you run the model (e.g. using `model.run_with_cache` or `model.run_with_hooks`). The only way to remove them is with:

```python
model.reset_hooks(including_permanent=True)
```

You can add permanent hooks as follows:

```python
model.add_hook(hook_name, hook_fn, is_permanent=True)
```

where `hook_name` can be a string or a filter function mapping strings to booleans.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# SOLUTION
def get_heads_and_posns_to_keep(
    means_dataset: IOIDataset,
    model: HookedTransformer,
    circuit: dict[str, list[tuple[int, int]]],
    seq_pos_to_keep: dict[str, str],
) -> dict[int, Bool[Tensor, "batch seq head"]]:
    """
    Returns a dictionary mapping layers to a boolean mask giving the indices of the z output which
    *shouldn't* be mean-ablated.

    The output of this function will be used for the hook function that does ablation.
    """
    heads_and_posns_to_keep = {}
    batch_size, seq_len, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads

    for layer in range(model.cfg.n_layers):
        mask = t.zeros(size=(batch_size, seq_len, n_heads))

        for head_type, head_list in circuit.items():
            seq_pos = seq_pos_to_keep[head_type]
            indices = means_dataset.word_idx[seq_pos]
            for layer_idx, head_idx in head_list:
                if layer_idx == layer:
                    mask[range(batch_size), indices, head_idx] = 1

        heads_and_posns_to_keep[layer] = mask.bool()

    return heads_and_posns_to_keep


def hook_fn_mask_z(
    z: Float[Tensor, "batch seq head d_head"],
    hook: HookPoint,
    heads_and_posns_to_keep: dict[int, Bool[Tensor, "batch seq head"]],
    means: Float[Tensor, "layer batch seq head d_head"],
) -> Float[Tensor, "batch seq head d_head"]:
    """
    Hook function which masks the z output of a transformer head.

    heads_and_posns_to_keep
        dict created with the get_heads_and_posns_to_keep function. This tells us where to mask.

    means
        Tensor of mean z values of the means_dataset over each group of prompts with the same
        template. This tells us
        what values to mask with.
    """
    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly
    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)

    # Set z values to the mean
    z = t.where(mask_for_this_layer, z, means[hook.layer()])

    return z


def compute_means_by_template(
    means_dataset: IOIDataset, model: HookedTransformer
) -> Float[Tensor, "layer batch seq head_idx d_head"]:
    """
    Returns the mean of each head's output over the means dataset. This mean is computed separately
    for each group of prompts with the same template (these are given by means_dataset.groups).
    """
    # Cache the outputs of every head
    _, means_cache = model.run_with_cache(
        means_dataset.toks.long(),
        return_type=None,
        names_filter=lambda name: name.endswith("z"),
    )
    # Create tensor to store means
    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head
    batch, seq_len = len(means_dataset), means_dataset.max_len
    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)

    # Get set of different templates for this data
    for layer in range(model.cfg.n_layers):
        z_for_this_layer = means_cache[utils.get_act_name("z", layer)]  # [batch seq head d_head]
        for template_group in means_dataset.groups:
            z_for_this_template = z_for_this_layer[template_group]
            z_means_for_this_template = einops.reduce(
                z_for_this_template, "batch seq head d_head -> seq head d_head", "mean"
            )
            means[layer, template_group] = z_means_for_this_template

    return means


# END SOLUTION
def add_mean_ablation_hook(
    model: HookedTransformer,
    means_dataset: IOIDataset,
    circuit: dict[str, list[tuple[int, int]]] = CIRCUIT,
    seq_pos_to_keep: dict[str, str] = SEQ_POS_TO_KEEP,
    is_permanent: bool = True,
) -> HookedTransformer:
    """
    Adds a permanent hook to the model, which ablates according to the circuit and seq_pos_to_keep
    dictionaries.

    In other words, when the model is run on ioi_dataset, every head's output will be replaced with
    the mean over means_dataset for sequences with the same template, except for a subset of heads
    and sequence positions as specified by the circuit and seq_pos_to_keep dicts.
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    model.reset_hooks(including_permanent=True)

    # Compute the mean of each head's output on the ABC dataset, grouped by template
    means = compute_means_by_template(means_dataset, model)

    # Convert this into a boolean map
    heads_and_posns_to_keep = get_heads_and_posns_to_keep(
        means_dataset, model, circuit, seq_pos_to_keep
    )

    # Get a hook function which will patch in the mean z values for each head, at
    # all positions which aren't important for the circuit
    hook_fn = partial(hook_fn_mask_z, heads_and_posns_to_keep=heads_and_posns_to_keep, means=means)

    # Apply hook
    model.add_hook(lambda name: name.endswith("z"), hook_fn, is_permanent=is_permanent)

    return model
    # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint (docstrings of some functions which will be useful for your main function)</summary>

```python
def compute_means_by_template(
    means_dataset: IOIDataset,
    model: HookedTransformer
) -> Float[Tensor, "layer batch seq head_idx d_head"]:
    \'\'\'
    Returns the mean of each head's output over the means dataset. This mean is
    computed separately for each group of prompts with the same template (these
    are given by means_dataset.groups).
    \'\'\'
    pass


def get_heads_and_posns_to_keep(
    means_dataset: IOIDataset,
    model: HookedTransformer,
    circuit: dict[str, list[tuple[int, int]]],
    seq_pos_to_keep: dict[str, str],
) -> dict[int, Bool[Tensor, "batch seq head"]]:
    \'\'\'
    Returns a dictionary mapping layers to a boolean mask giving the indices of the
    z output which *shouldn't* be mean-ablated.

    The output of this function will be used for the hook function that does ablation.
    \'\'\'
    pass


def hook_fn_mask_z(
    z: Float[Tensor, "batch seq head d_head"],
    hook: HookPoint,
    heads_and_posns_to_keep: dict[int, Bool[Tensor, "batch seq head"]],
    means: Float[Tensor, "layer batch seq head d_head"],
) -> Float[Tensor, "batch seq head d_head"]:
    \'\'\'
    Hook function which masks the z output of a transformer head.

    heads_and_posns_to_keep
        dict created with the get_heads_and_posns_to_keep function. This tells
        us where to mask.

    means
        Tensor of mean z values of the means_dataset over each group of prompts
        with the same template. This tells us what values to mask with.
    \'\'\'
    pass
```

Once you fill in these three functions, completing the main function is simple. It should:

* Use `compute_means_by_template` to get the means
* Use `get_heads_and_posns_to_keep` to get the boolean mask
* Apply `functools.partial` to `hook_fn_mask_z`, using the outputs of the 2 previous functions, to get your hook function which performs the mean ablation
* Add this hook function to your model, as a permanent hook

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To test whether your function works, you can use the function provided to you, and see if the logit difference from your implementation of the circuit matches this one:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

import part41_indirect_object_identification.ioi_circuit_extraction as ioi_circuit_extraction

model = ioi_circuit_extraction.add_mean_ablation_hook(
    model,
    means_dataset=abc_dataset,
    circuit=CIRCUIT,
    seq_pos_to_keep=SEQ_POS_TO_KEEP,
)
ioi_logits_minimal = model(ioi_dataset.toks)

print(f"""
Avg logit diff (IOI dataset, using entire model): {logits_to_ave_logit_diff_2(ioi_logits_original):.4f}
Avg logit diff (IOI dataset, only using circuit): {logits_to_ave_logit_diff_2(ioi_logits_minimal):.4f}
""")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model = add_mean_ablation_hook(
    model,
    means_dataset=abc_dataset,
    circuit=CIRCUIT,
    seq_pos_to_keep=SEQ_POS_TO_KEEP,
)
ioi_logits_minimal = model(ioi_dataset.toks)

print(f"""
Avg logit diff (IOI dataset, using entire model): {logits_to_ave_logit_diff_2(ioi_logits_original):.4f}
Avg logit diff (IOI dataset, only using circuit): {logits_to_ave_logit_diff_2(ioi_logits_minimal):.4f}
""")

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You should find that the logit difference only drops by a small amount, and is still high enough to represent a high likelihood ratio favouring the IO token over S.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - calculate minimality scores

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵⚪⚪⚪
> 
> This exercise is expected to take a long time; at least an hour.
> It is probably the second most challenging exercise in this notebook.
> ```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We'll conclude this section by replicating figure 7 of the paper, which shows the minimality scores for the model.

Again, this exercise is very challenging and is designed to be done with minimal guidance. You will need to read the relevant sections of the paper which explain this plot: section 4 (experimental validation), from the start up to the end of section 4.2. Note that you won't need to perform the sampling algorithm described on page 10, because we're giving you the set $K$ for each component, in the form of the dictionary below (this is based on the information given in figure 20 of the paper, the "minimality sets" table).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

K_FOR_EACH_COMPONENT = {
    (9, 9): set(),
    (10, 0): {(9, 9)},
    (9, 6): {(9, 9), (10, 0)},
    (10, 7): {(11, 10)},
    (11, 10): {(10, 7)},
    (8, 10): {(7, 9), (8, 6), (7, 3)},
    (7, 9): {(8, 10), (8, 6), (7, 3)},
    (8, 6): {(7, 9), (8, 10), (7, 3)},
    (7, 3): {(7, 9), (8, 10), (8, 6)},
    (5, 5): {(5, 9), (6, 9), (5, 8)},
    (5, 9): {(11, 10), (10, 7)},
    (6, 9): {(5, 9), (5, 5), (5, 8)},
    (5, 8): {(11, 10), (10, 7)},
    (0, 1): {(0, 10), (3, 0)},
    (0, 10): {(0, 1), (3, 0)},
    (3, 0): {(0, 1), (0, 10)},
    (4, 11): {(2, 2)},
    (2, 2): {(4, 11)},
    (11, 2): {(9, 9), (10, 0), (9, 6)},
    (10, 6): {(9, 9), (10, 0), (9, 6), (11, 2)},
    (10, 10): {(9, 9), (10, 0), (9, 6), (11, 2), (10, 6)},
    (10, 2): {(9, 9), (10, 0), (9, 6), (11, 2), (10, 6), (10, 10)},
    (9, 7): {(9, 9), (10, 0), (9, 6), (11, 2), (10, 6), (10, 10), (10, 2)},
    (10, 1): {(9, 9), (10, 0), (9, 6), (11, 2), (10, 6), (10, 10), (10, 2), (9, 7)},
    (11, 9): {(9, 9), (10, 0), (9, 6), (9, 0)},
    (9, 0): {(9, 9), (10, 0), (9, 6), (11, 9)},
}

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Also, given a dictionary `minimality_scores` (which maps heads to their scores), the following code will produce a plot that looks like the one from the paper:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def plot_minimal_set_results(minimality_scores: dict[tuple[int, int], float]):
    """
    Plots the minimality results, in a way resembling figure 7 in the paper.

    minimality_scores:
        dict with elements like (9, 9): minimality score for head 9.9 (as described
        in section 4.2 of the paper)
    """

    CIRCUIT_reversed = {head: k for k, v in CIRCUIT.items() for head in v}
    colors = [CIRCUIT_reversed[head].capitalize() + " head" for head in minimality_scores.keys()]
    color_sequence = [px.colors.qualitative.Dark2[i] for i in [0, 1, 2, 5, 3, 6]] + ["#BAEA84"]

    bar(
        list(minimality_scores.values()),
        x=list(map(str, minimality_scores.keys())),
        labels={"x": "Attention head", "y": "Change in logit diff", "color": "Head type"},
        color=colors,
        template="ggplot2",
        color_discrete_sequence=color_sequence,
        bargap=0.02,
        yaxis_tickformat=".0%",
        legend_title_text="",
        title="Plot of minimality scores (as percentages of full model logit diff)",
        width=800,
        hovermode="x unified",
    )

    # FILTERS: ~
    # fig = bar(
    #     list(minimality_scores.values()),
    #     x=list(map(str, minimality_scores.keys())),
    #     labels={"x": "Attention head", "y": "Change in logit diff", "color": "Head type"},
    #     color=colors,
    #     template="ggplot2",
    #     color_discrete_sequence=color_sequence,
    #     bargap=0.02,
    #     yaxis_tickformat=".0%",
    #     legend_title_text="",
    #     title="Plot of minimality scores (as percentages of full model logit diff)",
    #     width=800,
    #     hovermode="x unified",
    #     return_fig=True,
    # )
    # fig.write_html(section_dir / "14118.html")
    # END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Now, you should create the `minimality_scores` dictionary, and use the plot function given above to plot the results:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# EXERCISE
# minimality_scores = {(9, 9): ...}
# plot_minimal_set_results(minimality_scores)
# # YOUR CODE HERE - create the `minimality_scores` dictionary, to be used in the plot function given above
# END EXERCISE
# SOLUTION
def get_score(
    model: HookedTransformer,
    ioi_dataset: IOIDataset,
    abc_dataset: IOIDataset,
    K: set[tuple[int, int]],
    C: dict[str, list[tuple[int, int]]],
) -> float:
    """
    Returns the value F(C \ K), where F is the logit diff, C is the core circuit, and K is the set
    of circuit components to remove.
    """
    C_excl_K = {k: [head for head in v if head not in K] for k, v in C.items()}
    model = add_mean_ablation_hook(model, abc_dataset, C_excl_K, SEQ_POS_TO_KEEP)
    logits = model(ioi_dataset.toks)
    score = logits_to_ave_logit_diff_2(logits, ioi_dataset).item()

    return score


def get_minimality_score(
    model: HookedTransformer,
    ioi_dataset: IOIDataset,
    abc_dataset: IOIDataset,
    v: tuple[int, int],
    K: set[tuple[int, int]],
    C: dict[str, list[tuple[int, int]]] = CIRCUIT,
) -> float:
    """
    Returns the value | F(C \ K_union_v) - F(C | K) |, where F is the logit diff, C is the core
    circuit, K is the set of circuit components to remove, and v is a head (not in K).
    """
    assert v not in K
    K_union_v = K | {v}
    C_excl_K_score = get_score(model, ioi_dataset, abc_dataset, K, C)
    C_excl_Kv_score = get_score(model, ioi_dataset, abc_dataset, K_union_v, C)

    return abs(C_excl_K_score - C_excl_Kv_score)


if MAIN:

    def get_all_minimality_scores(
        model: HookedTransformer,
        ioi_dataset: IOIDataset = ioi_dataset,
        abc_dataset: IOIDataset = abc_dataset,
        k_for_each_component: dict = K_FOR_EACH_COMPONENT,
    ) -> dict[tuple[int, int], float]:
        """
        Returns dict of minimality scores for every head in the model (as a fraction of F(M), the
        logit diff of the full model).

        Warning - this resets all hooks at the end (including permanent).
        """
        # Get full circuit score F(M), to divide minimality scores by
        model.reset_hooks(including_permanent=True)
        logits = model(ioi_dataset.toks)
        full_circuit_score = logits_to_ave_logit_diff_2(logits, ioi_dataset).item()

        # Get all minimality scores, using the `get_minimality_score` function
        minimality_scores = {}
        for v, K in tqdm(k_for_each_component.items()):
            score = get_minimality_score(model, ioi_dataset, abc_dataset, v, K)
            minimality_scores[v] = score / full_circuit_score

        model.reset_hooks(including_permanent=True)

        return minimality_scores

    minimality_scores = get_all_minimality_scores(model)
    # COLAB-SPLIT
    plot_minimal_set_results(minimality_scores)
# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14118.html" width="820" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint (docstrings of some functions which will be useful for your main function)</summary>

```python
def get_score(
    model: HookedTransformer,
    ioi_dataset: IOIDataset,
    abc_dataset: IOIDataset,
    K: set[tuple[int, int]],
    C: dict[str, list[tuple[int, int]]],
) -> float:
    \'\'\'
    Returns the value F(C \ K), where F is the logit diff, C is the
    core circuit, and K is the set of circuit components to remove.
    \'\'\'
    pass


def get_minimality_score(
    model: HookedTransformer,
    ioi_dataset: IOIDataset,
    abc_dataset: IOIDataset,
    v: tuple[int, int],
    K: set[tuple[int, int]],
    C: dict[str, list[tuple[int, int]]] = CIRCUIT,
) -> float:
    \'\'\'
    Returns the value | F(C \ K_union_v) - F(C | K) |, where F is
    the logit diff, C is the core circuit, K is the set of circuit
    components to remove, and v is a head (not in K).
    \'\'\'
    pass


def get_all_minimality_scores(
    model: HookedTransformer,
    ioi_dataset: IOIDataset = ioi_dataset,
    abc_dataset: IOIDataset = abc_dataset,
    k_for_each_component: dict = K_FOR_EACH_COMPONENT
) -> dict[tuple[int, int], float]:
    \'\'\'
    Returns dict of minimality scores for every head in the model (as
    a fraction of F(M), the logit diff of the full model).

    Warning - this resets all hooks at the end (including permanent).
    \'\'\'
    pass


minimality_scores = get_all_minimality_scores(model)

plot_minimal_set_results(minimality_scores)
```

The output of the third function can be plotted using the plotting function given above.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note - your results won't be exactly the same as the paper's, because of random error (e.g. the order of importanc of heads within each category might not be the same, especially heads with a small effect on the model like the backup name mover heads). But they should be reasonably similar in their important features.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# ☆ Bonus / exploring anomalies
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Here, you'll explore some weirder parts of the circuit which we haven't looked at in detail yet. Specifically, there are three parts to explore:

* Early induction heads
* Backup name mover heads
* Positional vs token information being moved

These three sections are all optional, and you can do as many of them as you want (in whichever order you prefer). There will also be some further directions of investigation at the end of this section, which have been suggested either by the authors or by Neel.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Early induction heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
As we've discussed, a really weird observation is that some of the early heads detecting duplicated tokens are induction heads, not just direct duplicate token heads. This is very weird! What's up with that?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
First off, let's just recap what induction heads are. An induction head is an important type of attention head that can detect and continue repeated sequences. It is the second head in a two head induction circuit, which looks for previous copies of the current token and attends to the token *after* it, and then copies that to the current position and predicts that it will come next. They're enough of a big deal that [Anthropic wrote a whole paper on them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).

The diagram below shows a diagram for how they work to predict that the token `"urs"` follows `" D"`, the second time the word `"Dursley"` appears (note that we assume the model has not been trained on Harry Potter, so this is an example of in-context learning).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ih-simple.png" width="650">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Why is it surprising that induction heads up here? It's surprising because it feels like overkill. The model doesn't care about *what* token comes after the first copy of the subject, just that it's duplicated. And it already has simpler duplicate token heads. My best guess is that it just already had induction heads around and that, in addition to their main function, they *also* only activate on duplicated tokens. So it was useful to repurpose this existing machinery.

This suggests that as we look for circuits in larger models life may get more and more complicated, as components in simpler circuits get repurposed and built upon.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
First, in the cell below, you should visualise the attention patterns of the induction heads (`5.5` and `6.9`) on sequences containg repeated tokens, and verify that they are attending to the token *after* the previous instance of that same token. You might want to repeat code you wrote in the "Validation of duplicate token heads" section.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model.reset_hooks(including_permanent=True)

attn_heads = [(5, 5), (6, 9)]

# Get repeating sequences (note we could also take mean over larger batch)
batch = 1
seq_len = 15
rep_tokens = generate_repeated_tokens(model, seq_len, batch)

# Run cache (we only need attention patterns for layers 5 and 6)
_, cache = model.run_with_cache(
    rep_tokens,
    return_type=None,
    names_filter=lambda name: name.endswith("pattern")
    and any(f".{layer}." in name for layer, head in attn_heads),
)

# Display results
attn = t.stack([cache["pattern", layer][0, head] for (layer, head) in attn_heads])
cv.attention.attention_patterns(
    tokens=model.to_str_tokens(rep_tokens[0]),
    attention=attn,
    attention_head_names=[f"{layer}.{head}" for (layer, head) in attn_heads],
)

# FILTERS: ~
html = cv.attention.attention_patterns(
    tokens=model.to_str_tokens(rep_tokens[0]),
    attention=attn,
    attention_head_names=[f"{layer}.{head}" for (layer, head) in attn_heads],
)
with open(section_dir / "14119.html", "w") as f:
    f.write(str(html))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
One implication of this is that it's useful to categories heads according to whether they occur in simpler circuits, so that as we look for more complex circuits we can easily look for them. This is Hooked to do here! An interesting fact about induction heads is that they work on a sequence of repeated random tokens - notable for being wildly off distribution from the natural language GPT-2 was trained on. Being able to predict a model's behaviour off distribution is a good mark of success for mechanistic interpretability! This is a good sanity check for whether a head is an induction head or not.

We can characterise an induction head by just giving a sequence of random tokens repeated once, and measuring the average attention paid from the second copy of a token to the token after the first copy. At the same time, we can also measure the average attention paid from the second copy of a token to the first copy of the token, which is the attention that the induction head would pay if it were a duplicate token head, and the average attention paid to the previous token to find previous token heads.

Note that this is a superficial study of whether something is an induction head - we totally ignore the question of whether it actually does boost the correct token or whether it composes with a single previous head and how. In particular, we sometimes get anti-induction heads which suppress the induction-y token (no clue why!), and this technique will find those too.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - validate prev token heads via patching

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> This just involves performing a specific kind of patching, with functions you've already written.
> ```

The paper mentions that heads `2.2` and `4.11` are previous token heads. Hopefully you already validated this in the previous section by plotting the previous token scores (in your replication of Figure 18). But this time, you'll perform a particular kind of path patching to prove that these heads are functioning as previous token heads, in the way implied by our circuit diagram.

<details>
<summary>Question - what kind of path patching should you perform?</summary>

To show these heads are acting as prev token heads in an induction circuit, we want to perform key-patching (i.e. patch the path from the output of the prev token heads to the key inputs of the induction heads).

We expect this to worsen performance, because it interrupts the duplicate token signal provided by the induction heads.
</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model.reset_hooks(including_permanent=True)

# EXERCISE
# # YOUR CODE HERE - create `induction_head_key_path_patching_results`
# END EXERCISE
# SOLUTION
induction_head_key_path_patching_results = get_path_patch_head_to_heads(
    receiver_heads=[(5, 5), (6, 9)], receiver_input="k", model=model, patching_metric=ioi_metric_2
)
# END SOLUTION

# COLAB-SPLIT

imshow(
    100 * induction_head_key_path_patching_results,
    title="Direct effect on Induction Heads' keys",
    labels={"x": "Head", "y": "Layer", "color": "Logit diff.<br>variation"},
    coloraxis=dict(colorbar_ticksuffix="%"),
    width=600,
)

# FILTERS: ~
# fig = imshow(
#     100 * induction_head_key_path_patching_results,
#     title="Direct effect on Induction Heads' keys",
#     labels={"x": "Head", "y": "Layer", "color": "Logit diff.<br>variation"},
#     coloraxis=dict(colorbar_ticksuffix="%"),
#     width=600,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14120.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14120.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You might notice that there are two other heads in the induction heads box in the paper's diagram, both in brackets (heads `5.8` and `5.9`). Can you try and figure out what these heads are doing, and why they're in brackets?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Hint</summary>

Recall the path patching section, where you applied patching from attention heads' outputs to the value inputs to the S-inhibition heads (hopefully replicating figure 4b from the paper). Try patching the **keys** of the S-inhibition heads instead. Do heads `5.8` and `5.9` stand out here?
</details>

<details>
<summary>Answer</summary>

After making the plot suggested in the hint, you should find that patching from `5.8` and `5.9` each have around a 5% negative impact on the logit difference if you patch from them to the S-inhibition heads. We'd like to conclude that these are induction heads, and they're acting to increase the attention paid by `end` to `S2`. Unfortunately, they don't seem to be induction heads according to the results of figure 18 in the paper (which we replicated in the section "validation of early heads").

It turns out that they're acting as induction heads on this distribution, but not in general. A simple way we can support this hypothesis is to look at attention patterns (we find that `S2` pays attention to `S1+1` in both head `5.8` and `5.9`).
</details>

<details>
<summary>Aside - a lesson in nonlinearity</summary>

In the dropdown above, I claimed that `5.8` and `5.9` were doing induction on this distribution. A more rigorous way to test this would be to path patch to the keys of these heads, and see if either of the previous token heads have a large effect. If they do, this is very strong evidence for induction. However, it turns out that neither of the prev token heads affect the model's IOI performance via their path through the fuzzy induction heads. Does this invalidate our hypothesis?

In fact, no, because of nonlinearity. The two prev token heads `2.2` and `4.11` will be adding onto the **logits** used to calculate the attention probability from `S2` to `S1+1`, rather than adding onto the probabilities directly. SO it might be the case that one head on its own doesn't increase the attention probability very much, both both heads acting together significantly increase it. (Motivating example: suppose we only had 2 possible source tokens, without either head acting the logits are `[0.0, -8.0]`, and the action of each head is to add `4.0` to the second logit. If one head acts then the probability on the second token goes from basically zero to `0.018` (which will have a very small impact on the attention head's output), but if both heads are acting then the logits are equal and the probability is `0.5` (which obviously has a much larger effect)).

This is in fact what I found here - after modifying the path patching function to allow more than one sender head, I found that patching from both `2.2` and `4.11` to `5.8` and `5.9` had a larger effect on the model's IOI performance than patching from either head alone (about a 1% drop in performance for both vs almost 0% for either individually). The same effect can be found when we patch from these heads to all four induction heads, although it's even more pronounced (a 3% and 15% drop respectively for the heads individually, vs 31% for both).
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Backup name mover heads
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Another fascinating anomaly is that of the **backup name mover heads**. A standard technique to apply when interpreting model internals is ablations, or knock-out. If we run the model but intervene to set a specific head to zero, what happens? If the model is robust to this intervention, then naively we can be confident that the head is not doing anything important, and conversely if the model is much worse at the task this suggests that head was important. There are several conceptual flaws with this approach, making the evidence only suggestive, e.g. that the average output of the head may be far from zero and so the knockout may send it far from expected activations, breaking internals on *any* task. But it's still an Hooked technique to apply to give some data.

But a wild finding in the paper is that models have **built in redundancy**. If we knock out one of the name movers, then there are some backup name movers in later layers that *change their behaviour* and do (some of) the job of the original name mover head. This means that naive knock-out will significantly underestimate the importance of the name movers.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's test this! Let's ablate the most important name mover (which is `9.9`, as the code below verifies for us) on just the `end` token, and compare performance.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model.reset_hooks(including_permanent=True)

ioi_logits, ioi_cache = model.run_with_cache(ioi_dataset.toks)
original_average_logit_diff = logits_to_ave_logit_diff_2(ioi_logits)

s_unembeddings = model.W_U.T[ioi_dataset.s_tokenIDs]
io_unembeddings = model.W_U.T[ioi_dataset.io_tokenIDs]
logit_diff_directions = io_unembeddings - s_unembeddings  # [batch d_model]

per_head_residual, labels = ioi_cache.stack_head_results(layer=-1, return_labels=True)
per_head_residual = einops.rearrange(
    per_head_residual[
        :, t.arange(len(ioi_dataset)).to(device), ioi_dataset.word_idx["end"].to(device)
    ],
    "(layer head) batch d_model -> layer head batch d_model",
    layer=model.cfg.n_layers,
)

per_head_logit_diffs = residual_stack_to_logit_diff(
    per_head_residual, ioi_cache, logit_diff_directions
)

top_layer, top_head = topk_of_Nd_tensor(per_head_logit_diffs, k=1)[0]
print(f"Top Name Mover to ablate: {top_layer}.{top_head}")

# Getting means we can use to ablate
abc_means = ioi_circuit_extraction.compute_means_by_template(abc_dataset, model)[top_layer]


# Define hook function and add to model
def ablate_top_head_hook(z: Float[Tensor, "batch pos head_index d_head"], hook):
    """
    Ablates hook by patching in results
    """
    z[range(len(ioi_dataset)), ioi_dataset.word_idx["end"], top_head] = abc_means[
        range(len(ioi_dataset)), ioi_dataset.word_idx["end"], top_head
    ]
    return z


model.add_hook(utils.get_act_name("z", top_layer), ablate_top_head_hook)

# Run the model, temporarily adds caching hooks and then removes *all* hooks after running,
# including the ablation hook.
ablated_logits, ablated_cache = model.run_with_cache(ioi_dataset.toks)
rprint(
    "\n".join(
        [
            f"{original_average_logit_diff:.4f} = Original logit diff",
            f"{per_head_logit_diffs[top_layer, top_head]:.4f} = Direct Logit Attribution of top name mover head",
            f"{original_average_logit_diff - per_head_logit_diffs[top_layer, top_head]:.4f} = Naive prediction of post ablation logit diff",
            f"{logits_to_ave_logit_diff_2(ablated_logits):.4f} = Logit diff after ablating L{top_layer}H{top_head}",
        ]
    )
)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: []

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Top Name Mover to ablate: 9.9

<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.8052</span> = Original logit diff
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.7086</span> = Direct Logit Attribution of top name mover head
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0966</span> = Naive prediction of post ablation logit diff
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.6290</span> = Logit diff after ablating L9H9
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What's going on here? We calculate the logit diff for our full model, and how much of that is coming directly from head `9.9`. Given this, we come up with an estimate for what the logit diff will fall to when we ablate this head. In fact, performance is **much** better than this naive prediction.

Why is this happening? As before, we can look at the direct logit attribution of each head to get a sense of what's going on.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

per_head_ablated_residual, labels = ablated_cache.stack_head_results(layer=-1, return_labels=True)
per_head_ablated_residual = einops.rearrange(
    per_head_ablated_residual[
        :, t.arange(len(ioi_dataset)).to(device), ioi_dataset.word_idx["end"].to(device)
    ],
    "(layer head) batch d_model -> layer head batch d_model",
    layer=model.cfg.n_layers,
)
per_head_ablated_logit_diffs = residual_stack_to_logit_diff(
    per_head_ablated_residual, ablated_cache, logit_diff_directions
)
per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(
    model.cfg.n_layers, model.cfg.n_heads
)

# COLAB-SPLIT

imshow(
    t.stack(
        [
            per_head_logit_diffs,
            per_head_ablated_logit_diffs,
            per_head_ablated_logit_diffs - per_head_logit_diffs,
        ]
    ),
    title="Direct logit contribution by head, pre / post ablation",
    labels={"x": "Head", "y": "Layer"},
    facet_col=0,
    facet_labels=["No ablation", "9.9 is ablated", "Change in head contribution post-ablation"],
    width=1200,
)

scatter(
    y=per_head_logit_diffs.flatten(),
    x=per_head_ablated_logit_diffs.flatten(),
    hover_name=labels,
    range_x=(-1, 1),
    range_y=(-2, 2),
    labels={"x": "Ablated", "y": "Original"},
    title="Original vs Post-Ablation Direct Logit Attribution of Heads",
    width=600,
    add_line="y=x",
)

# FILTERS: ~
# fig = imshow(
#     t.stack([per_head_logit_diffs, per_head_ablated_logit_diffs, per_head_ablated_logit_diffs - per_head_logit_diffs]),
#     title="Direct logit contribution by head, pre / post ablation",
#     labels={"x": "Head", "y": "Layer"},
#     facet_col=0,
#     facet_labels=["No ablation", "9.9 is ablated", "Change in head contribution post-ablation"],
#     return_fig=True,
#     width=1200,
# )
# fig.write_html(section_dir / "14121.html")

# fig = scatter(
#     y=per_head_logit_diffs.flatten(),
#     x=per_head_ablated_logit_diffs.flatten(),
#     hover_name=labels,
#     range_x=(-1, 1),
#     range_y=(-2, 2),
#     labels={"x": "Ablated", "y": "Original"},
#     title="Original vs Post-Ablation Direct Logit Attribution of Heads",
#     width=600,
#     add_line="y=x",
#     return_fig=True,
# )
# fig.write_html(section_dir / "14122.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14121.html" width="1220" height="480"></div><br>
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14122.html" width="620" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
The first plots show us that, after we ablate head `9.9`, while its direct contribution to the logit diff falls (obviously), a lot of contributions from other heads (particularly in layer 10) actually increase. The second plot shows this in a different way (the distinctive heads in the right hand heatmap are the same as the heads lying well below the y=x line in the scatter plot).

One natural hypothesis is that this is because the final LayerNorm scaling has changed, which can scale up or down the final residual stream. This is slightly true, and we can see that the typical head is a bit off from the x=y line. But the average LN scaling ratio is 1.04, and this should uniformly change *all* heads by the same factor, so this can't be sufficient.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

ln_scaling_no_ablation = ioi_cache["ln_final.hook_scale"][
    t.arange(len(ioi_dataset)), ioi_dataset.word_idx["end"]
].squeeze()
ln_scaling_ablated = ablated_cache["ln_final.hook_scale"][
    t.arange(len(ioi_dataset)), ioi_dataset.word_idx["end"]
].squeeze()

# FILTERS: st,py
scatter(
    y=ln_scaling_ablated,
    x=ln_scaling_no_ablation,
    labels={"x": "No ablation", "y": "Ablation"},
    title=f"Final LN scaling factors compared (ablation vs no ablation)<br>Average ratio = {(ln_scaling_no_ablation / ln_scaling_ablated).mean():.4f}",
    width=700,
    add_line="y=x",
)
# END FILTERS

# FILTERS: ~
# fig = scatter(
#     y=ln_scaling_ablated,
#     x=ln_scaling_no_ablation,
#     labels={"x": "No ablation", "y": "Ablation"},
#     title=f"Final LN scaling factors compared (ablation vs no ablation)<br>Average ratio = {(ln_scaling_no_ablation / ln_scaling_ablated).mean():.4f}",
#     width=700,
#     add_line="y=x",
#     return_fig=True,
# )
# fig.write_html(section_dir / "14123.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# scatter(
#     y=ln_scaling_ablated,
#     x=ln_scaling_no_ablation,
#     labels={"x": "No ablation", "y": "Ablation"},
#     title=f"Final LN scaling factors compared (ablation vs no ablation)<br>Average ratio = {(ln_scaling_no_ablation / ln_scaling_ablated).mean():.4f}",
#     width=700,
#     add_line="y=x"
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14123.html" width="720" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
**Exercise to the reader:** Can you finish off this analysis? What's going on here? Why are the backup name movers changing their behaviour? Why is one negative name mover becoming significantly less important?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Positional vs token information being moved

In section A of the appendix (titled **Disentangling token and positional signal in the output of S-Inhibition Heads**), the authors attempt to figure out whether the S-Inhibition heads are using token or positional information to supress the attention paid to `S1`. This is illustrated in my IOI diagram, by purple vs pink boxes.

The way the authors find out which one is which is ingenious. They construct datasets from the original IOI dataset, with some of the signals erased or flipped. For instance, if they want to examine the effect of inverting the positional information but preserving the token information written by the S-inhibition heads, they can replace sentences like:

```
When Mary and John went to the store, John gave a drink to Mary
```

with:

```
When John and Mary went to the store, John gave a drink to Mary
```

Let's be exactly clear on why this works. The information written to the `end` token position by the S-inhibition heads will be some combination of "don't pay attention to the duplicated token" and "don't pay attention to the token that's in the same position as the duplicated token". If we run our model on the first sentence above but then patch in the second sentence, then:

* The **"don't pay attention to the duplicated token"** signal will be unchanged (because this signal still refers to John)
* The **"don't pay attention to the token that's in the same position as the duplicated token"** signal will flip (because this information points to the position of `Mary` in the second sentence, hence to the position of `John` in the first sentence).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
That was just one example (flipping positional information, keeping token information the same), but we can do any of six different types of flip:
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
| Token signal | Positional signal | Sentence                                                          | ABB -> ? |
| ------------ | ----------------- | ----------------------------------------------------------------- | -------- |
| Same         | Same              | `When Mary and John went to the store, John gave a drink to Mary` | ABB      |
| Random       | Same              | `When Emma and Paul went to the store, Paul gave ...`             | CDD      |
| Inverted     | Same              | `When John and Mary went to the store, Mary gave ...`             | BAA      |
| Same         | Inverted          | `When John and Mary went to the store, John gave ...`             | BAB      |
| Random       | Inverted          | `When Paul and Emma went to the store, Emma gave ...`             | DCD      |
| Inverted     | Inverted          | `When Mary and John went to the store, Mary gave ...`             | ABA      |
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We use the `gen_flipped_prompts` method to generate each of these datasets:
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

datasets: list[tuple[tuple, str, IOIDataset]] = [
    ((0, 0), "original", ioi_dataset),
    ((1, 0), "random token", ioi_dataset.gen_flipped_prompts("ABB->CDD, BAB->DCD")),
    ((2, 0), "inverted token", ioi_dataset.gen_flipped_prompts("ABB->BAA, BAB->ABA")),
    ((0, 1), "inverted position", ioi_dataset.gen_flipped_prompts("ABB->BAB, BAB->ABB")),
    (
        (1, 1),
        "inverted position, random token",
        ioi_dataset.gen_flipped_prompts("ABB->DCD, BAB->CDD"),
    ),
    (
        (2, 1),
        "inverted position, inverted token",
        ioi_dataset.gen_flipped_prompts("ABB->ABA, BAB->BAA"),
    ),
]

results = t.zeros(3, 2).to(device)

s2_inhibition_heads = CIRCUIT["s2 inhibition"]
layers = set(layer for layer, head in s2_inhibition_heads)

names_filter = lambda name: name in [utils.get_act_name("z", layer) for layer in layers]


def patching_hook_fn(
    z: Float[Tensor, "batch seq head d_head"], hook: HookPoint, cache: ActivationCache
):
    heads_to_patch = [head for layer, head in s2_inhibition_heads if layer == hook.layer()]
    z[:, :, heads_to_patch] = cache[hook.name][:, :, heads_to_patch]
    return z


for (row, col), desc, dataset in datasets:
    # Get cache of values from the modified dataset
    _, cache_for_patching = model.run_with_cache(
        dataset.toks, names_filter=names_filter, return_type=None
    )

    # Run model on IOI dataset, but patch S-inhibition heads with signals from modified dataset
    patched_logits = model.run_with_hooks(
        ioi_dataset.toks,
        fwd_hooks=[(names_filter, partial(patching_hook_fn, cache=cache_for_patching))],
    )

    # Get logit diff for patched results
    # Note, we still use IOI dataset for our "correct answers" reference point
    results[row, col] = logits_to_ave_logit_diff_2(patched_logits, ioi_dataset)


# FILTERS: st,py
imshow(
    results,
    labels={"x": "Positional signal", "y": "Token signal"},
    x=["Original", "Inverted"],
    y=["Original", "Random", "Inverted"],
    title="Logit diff after changing all S2 inhibition heads' output signals via patching",
    text_auto=".2f",
    width=700,
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     results,
#     labels={"x": "Positional signal", "y": "Token signal"},
#     x=["Original", "Inverted"],
#     y=["Original", "Random", "Inverted"],
#     title="Logit diff after changing all S2 inhibition heads' output signals via patching",
#     text_auto=".2f",
#     width=700,
#     return_fig=True,
# )
# fig.write_html(section_dir / "14124.html")
# END FILTERS

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     results,
#     labels={"x": "Positional signal", "y": "Token signal"},
#     x=["Original", "Inverted"],
#     y=["Original", "Random", "Inverted"],
#     title="Logit diff after changing all S2 inhibition heads' output signals via patching",
#     text_auto=".2f",
# )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14124.html" width="720" height="480"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
What are your interpretations of this plot?

<details>
<summary>Some thoughts</summary>

A few sanity checks, which are what we'd expect from this plot (and hence validate our code as probably being correct):

* When token and positional signals are inverted, performance is close to negative of the original performance.
* Inverting the positional signal makes performance worse.
* Randomizing the token signal makes performance worse.
* Inverting the token signal makes performance worse. This effect is larger than randomizing (because we're pointing away from a correct answer, rather than just in a random direction).

There are two main interesting findings from this plot (which we might have guessed at beforehand, but couldn't have been certain about):
* The row and column differences are close to constant (column diff is approx 4.5, row diff is approx 0.75). In other words, the logit diff can be well approximated by a linear combination of the positional and token signal correlations (where correlation is 1 if the signal points towards the correct value, -1 if it points away, and 0 if it points in a random direction).
* The coefficients on the positional signal correlation is **much** bigger than the coefficient on the token signal correlation. The former is about 4.5, the latter is about 1.5. This tells us that positional information is a lot more important than token information.
    * One possible intuition here is that name information (i.e. representing the identity of the token `" John"`) takes up many dimensions, so is probably harder for the model. Relative positional information on the other hand will mostly have fewer dimensions. Since the model only needs to move enough information to single out a single positional index from about 15-20, rather than single out a name in the entire set of names. This is jus a guess though, and I'd love to hear other interpretations.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Let's dig a little deeper. Rather than just looking at the S-inhibition heads collectively, we can look at each of them individually.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - decompose S-Inhibition heads

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> This involves a lot of duplicating code from above.
> ```

Make the same plot as above, but after intervening on each of the S-inhibition heads individually.

You can do this by creating a `results` tensor of shape `(M, 3, 2)` where `M` is the number of S-inhibition heads, and each slice contains the results of intervening on that particular head. We've given you the code to plot your results below, all you need to do is fill in `results`.

(Note - we recommend computing the results as `(logit_diff - clean_logit_diff) / clean_logit_diff`, so your baseline is 0 for "this patching has no effect" and -1 for "this patching completely destroys model performance", to make the plot look clearer.)
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

results = t.zeros(len(CIRCUIT["s2 inhibition"]), 3, 2).to(device)


# EXERCISE
# YOUR CODE HERE - fill in the `results` tensor!
# END EXERCISE
# SOLUTION
def patching_hook_fn(
    z: Float[Tensor, "batch seq head d_head"], hook: HookPoint, cache: ActivationCache, head: int
):
    z[:, :, head] = cache[hook.name][:, :, head]
    return z


if MAIN:
    for i, (layer, head) in enumerate(CIRCUIT["s2 inhibition"]):
        model.reset_hooks(including_permanent=True)

        hook_name = utils.get_act_name("z", layer)

        for (row, col), desc, dataset in datasets:
            # Get cache of values from the modified dataset
            _, cache_for_patching = model.run_with_cache(
                dataset.toks, names_filter=lambda name: name == hook_name, return_type=None
            )

            # Run model on IOI dataset, but patch S-inhibition heads with modified dataset signals
            patched_logits = model.run_with_hooks(
                ioi_dataset.toks,
                fwd_hooks=[
                    (hook_name, partial(patching_hook_fn, cache=cache_for_patching, head=head))
                ],
            )

            # Get logit diff for patched results
            # Note, we still use IOI dataset for our "correct answers" reference point
            results[i, row, col] = logits_to_ave_logit_diff_2(patched_logits, ioi_dataset)
    # END SOLUTION

    imshow(
        (results - results[0, 0, 0]) / results[0, 0, 0],
        labels={"x": "Positional signal", "y": "Token signal"},
        x=["Original", "Inverted"],
        y=["Original", "Random", "Inverted"],
        title="Logit diff after patching individual S2 inhibition heads (as proportion of clean logit diff)",
        facet_col=0,
        facet_labels=[f"{layer}.{head}" for (layer, head) in CIRCUIT["s2 inhibition"]],
        facet_col_spacing=0.08,
        width=1100,
        text_auto=".2f",
    )

# FILTERS: ~
# fig = imshow(
#     (results - results[0, 0, 0]) / results[0, 0, 0],
#     labels={"x": "Positional signal", "y": "Token signal"},
#     x=["Original", "Inverted"],
#     y=["Original", "Random", "Inverted"],
#     title="Logit diff after patching individual S2 inhibition heads (as proportion of clean logit diff)",
#     facet_col=0,
#     facet_labels=[f"{layer}.{head}" for (layer, head) in CIRCUIT["s2 inhibition"]],
#     facet_col_spacing=0.08,
#     text_auto=".2f",
#     return_fig=True,
#     width=1100,
# )
# fig.write_html(section_dir / "14125.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-141/14125.html" width="1120" height="450"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Noteworthy features of this plot:

* Every head cares more about positional signal than token signal
* Head `8.6` (the biggest S-inhibition head) cares MUCH more about positional signal, in fact it doesn't care at all about token signal
    * Suggests maybe `8.6` was the first head which learned to do this task, and subsequent heads basically just helped out by providing the remainder (which was token signal). Shows the heads specialise.
* The only heads that kinda care about token signal are `7.9` and `8.10` (but they still both care about positional signal almost twice as much)
* The approximation of logit diff as a sum of positional and token signal correlations still seems to hold for each head individually, although the coefficients for each head are different.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Further Reading

Here is a collection of links for further reading, which haven't already been mentioned:

* [Some Lessons Learned from Studying Indirect Object Identification in GPT-2 small](https://www.alignmentforum.org/posts/3ecs6duLmTfyra3Gp/some-lessons-learned-from-studying-indirect-object)
    * A blog post by the authors of this paper, which goes into more detail about the experiments and results.
* [Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)
    * Introduces the idea of causal scubbing, a proposed systematic method for evaluting the quality of mechanistic interpretations
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Suggested topics for further exploration

Here are some future directions, some suggested by Neel, others by the authors of the paper. Many of these might make good capstone projects!

* 3 letter acronyms (or more!)
* Converting names to emails.
    * An extension task is e.g. constructing an email from a snippet like the following: Name: Neel Nanda; Email: last name dot first name k @ gmail
* Grammatical rules
    * Learning that words after full stops are capital letters
    * Verb conjugation
    * Choosing the right pronouns (e.g. he vs she vs it vs they)
    * Whether something is a proper noun or not
* Detecting sentiment (e.g. predicting whether something will be described as good vs bad)
* Interpreting memorisation. E.g., there are times when GPT-2 knows surprising facts like people’s contact information. How does that happen?
* Counting objects described in text. E.g.: I picked up an apple, a pear, and an orange. I was holding three fruits.
* Extensions from Alex Variengien
    * Understanding what's happening in the adversarial examples: most notably S-Inhibition Head attention pattern (hard). (S-Inhibition heads are mentioned in the IOI paper)
    * Understanding how are positional signal encoded (relative distance, something else?) bonus point if we have a story that include the positional embeddings and that explain how the difference between position is computed (if relative is the right framework) by Duplicate Token Heads / Induction Heads. (hard, but less context dependant)
    * What are the role of MLPs in IOI (quite broad and hard)
    * What is the role of Duplicate Token Heads outside IOI? Are they used in other Q-compositions with S-Inhibition Heads? Can we describe how their QK circuit implement "collision detection" at a parameter level? (Last question is low context dependant and quite tractable)
    * What is the role of Negative/ Backup/ regular Name Movers Heads outside IOI? Can we find examples on which Negative Name Movers contribute positively to the next-token prediction?
    * What are the differences between the 5 inductions heads present in GPT2-small? What are the heads they rely on / what are the later heads they compose with (low context dependence form IOI)
    * Understanding 4.11, (a really sharp previous token heads) at the parameter level. I think this can be quite tractable given that its attention pattern is almost perfectly off-diagonal
* What are the conditions for compensation mechanisms to occur? Is it due to drop-out? (Arthur Conmy is working on this - feel free to reach out to arthur@rdwrs.com )
* Extensions from Arthur Conmy
    * Understand IOI in GPT-Neo: it's a same size model but does IOI via composition of MLPs
    * Understand IOI in the Stanford mistral models - they all seem to do IOI in the same way, so maybe look at the development of the circuit through training?
* [Help out Redwood Research’s interpretability team by finding heuristics implemented by GPT-2 small](https://www.lesswrong.com/posts/LkBmAGJgZX2tbwGKg/help-out-redwood-research-s-interpretability-team-by-finding)
    * This LessWrong post from 6 months ago outlines some features of the IOI task that made it a good choice to study, and suggests other tasks that might meet these criteria / how you might go about finding such tasks.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Suggested paper replications

<br>

### [A circuit for Python docstrings in a 4-layer attention-only transformer](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)

This work was produced as part of the SERI ML Alignment Theory Scholars Program (Winter 2022) under the supervision of Neel Nanda. Similar to how the IOI paper searched for in some sense the simplest kind of circuit which required 3 layers, this work was looking for the simplest kind of circuit which required 4 layers. The task they investigated was the **docstring task** - can you predict parameters in the right order, in situations like this:

```python
def port(self, load, size, files, last):
    \'\'\'oil column piece

    :param load: crime population
    :param size: unit dark
    :param
```

The token that follows should be ` files`, and just like in the case of IOI we can deeply analyze how the transformer solves this task. Unlike IOI, we're looking at a 4-layer transformer which was trained on code (not GPT2-Small), which makes a lot of the analysis cleaner (even though the circuit has more levels of composition than IOI does).

For an extra challenge, rather than replicating the authors' results, you can try and perform this investigation yourself, without seeing what tools the authors of the paper used! Most will be similar to the ones you've used in the exercises so far.

This might be a good replication for you if:

* You enjoyed most/all sections of these exercises, and want to practice using the tools you learned in a different context
* You don't want to try anything *too* far left field from the content of these exercises (this post also comes with a Colab notebook which can be referred to if you're stuck)

<br>

### [Mechanistically interpreting time in GPT-2 small](https://www.lesswrong.com/posts/6tHNM2s6SWzFHv3Wo/mechanistically-interpreting-time-in-gpt-2-small)

This work was done by a group of independent researchers, supervised by Arthur Conmy. The task was to interpret how GPT2-Small can solve the task of next day prediction, i.e. predicting the next token in sentences like *If today is Monday, tomorrow is*. This replication is easier than the previous one, since the core circuit only contains one attention head rather than a composition of several. For this reason, we'd more strongly encourage trying to do this replication without guidance, i.e. have a crack at it before reading the full post.

<br>

### [How does GPT-2 compute greater-than? Interpreting mathematical abilities in a pre-trained language model](https://openreview.net/pdf?id=p4PckNQR8k)

This paper came out of the REMIX program run by Redwood Research. It analyses a circuit in GPT2-Small, much like this one. Here, the circuit is for computing greater-than; in other words detecting that sentences like *The war lasted from the year 1732 to the year 17...* will be completed by valid two-digit end years (years > 32). The paper identifies a circuit, explains the role of each circuit component, and also finds related tasks that activate the circuit.

For an extra challenge, rather than replicating this paper, you can try and perform this investigation yourself, without seeing what tools the authors of the paper used! Many will be similar to the ones you've used in the exercises so far; some will be different. In particular, there will be some analysis of individual neurons in this replication, unlike in this IOI notebook.

This might be a good replication for you if:

* You enjoyed most/all sections of these exercises, and want to practice using the tools you learned in a different context
* You don't want to try anything *too* far left field from the content of these exercises (although this is probably more challenging than the docstring circuit, mainly due to the presence of MLPs)

<br>

### [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/abs/2304.14997) / [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/abs/2310.10348)

These two are grouped together, because they both study the possibility of scaling circuit discovery tools using automation. **Automated Circuit Discovery** (ACDC) was a tool discovered by Arthur Conmy & collaborators partially done at Redwood Research. They automate several of the techniques we've used in these exercises (in particular activation & patch patching), iteratively severing connections between components until we're left with a minimal circuit which can still effectively solve the task. **Attribution patching** is slightly different; it approximates the effect of each component on the model's output (and by extension its importance in a particular task) by a simple and computationally cheap first-order approximation.

Either of these techniques would serve as suitable replication challenges over the course of a week. There's also a lot of work that can still be done in improving or refining these techniques.

This might be a good replication for you if:

* You enjoyed the exercises in this section, in particular those on activation and path patching
* You're looking for a project which is more focused on coding and implementation than on theory
* You're excited about exploring ways to scale / automate circuit discovery efficiently
'''




---
File: /infrastructure/master_files/master_1_4_2.py
---

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
```python
[
    {"title": "Introduction to nnsight", "icon": "1-circle-fill", "subtitle": "(15%)"},
    {"title": "Task-encoding hidden states", "icon": "2-circle-fill", "subtitle": "(35%)"},
    {"title": "Function Vectors", "icon": "3-circle-fill", "subtitle": "(35%)"},
    {"title": "Steering Vectors in GPT2-XL", "icon": "4-circle-fill", "subtitle": "(15%)"},
    {"title": "Bonus", "icon": "star", "subtitle": ""},
]
```
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# [1.4.2] Function Vectors & Model Steering
'''

# ! CELL TYPE: markdown
# ! FILTERS: [colab]
# ! TAGS: []

r'''
> *Note - if you get a numpy-related error at any point (e.g. when running the import cell for the first time, or running the first numpy function), you should restart the kernel and run the setup code again. The error should go away.*
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-2.png" width="350">
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# Introduction
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
These exercises serve as an exploration of the following question: ***can we steer a model to produce different outputs / have a different behaviour, by intervening on the model's forward pass using vectors found by non gradient descent-based methods?***

The majority of the exercises focus on [function vectors](https://functions.baulab.info/): vectors which are extracted from forward passes on in-context learning (ICL) tasks, and added to the residual stream in order to trigger the execution of this task from a zero-shot prompt. The diagram below illustrates this.

<img src="https://functions.baulab.info/images/Paper/fv-demonstrations.png" width="650">

The exercises also take you through use of the `nnsight` library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.

The final set of exercises look at Alex Turner et al's work on [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector), which is conceptually related but has different aims and methodologies.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Content & Learning Objectives

### 1️⃣ Introduction to `nnsight`

In this section, you'll learn the basics of how to use the `nnsight` library: running forward passes on your model, and saving the internal states. You'll also learn some basics of HuggingFace models which translate over into `nnsight` models (e.g. tokenization, and how to work with model output).

> ##### Learning Objectives
>
> * Learn the basics of the `nnsight` library, and what it can be useful for
> * Learn some basics of HuggingFace models (e.g. tokenization, model output)
> * Use it to extract & visualise GPT-J-6B's internal activations

### 2️⃣ Task-encoding hidden states

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

(Note - this section structurally follows section 2.1 of the function vectors paper).

> ##### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts


### 3️⃣ Function Vectors

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).

> ##### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

### 4️⃣ Steering Vectors in GPT2-XL

Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of "interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour", but it has a different setup, objectives, and approach.

> ##### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

### ☆ Bonus

Lastly, we discuss some possible extensions of function vectors & steering vectors work, which is currently an exciting area of development (e.g. with a paper on steering Llama 2-13b coming out as recently as December 2023).
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Setup code
'''

# ! CELL TYPE: code
# ! FILTERS: [~]
# ! TAGS: []

from IPython import get_ipython

ipython = get_ipython()
ipython.run_line_magic("load_ext", "autoreload")
ipython.run_line_magic("autoreload", "2")

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: [master-comment]

# import os
# import sys
# from pathlib import Path

# IN_COLAB = "google.colab" in sys.modules

# chapter = "chapter1_transformer_interp"
# repo = "ARENA_3.0"
# branch = "main"

# # Install dependencies
# try:
#     import nnsight
# except:
#     %pip install openai>=1.56.2 nnsight einops jaxtyping plotly transformer_lens==2.11.0 git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python gradio typing-extensions
#     %pip install --upgrade pydantic

# # Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
# root = (
#     "/content"
#     if IN_COLAB
#     else "/root"
#     if repo not in os.getcwd()
#     else str(next(p for p in Path.cwd().parents if p.name == repo))
# )

# if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
#     if not IN_COLAB:
#         !sudo apt-get install unzip
#         %pip install jupyter ipython --upgrade

#     if not os.path.exists(f"{root}/{chapter}"):
#         !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
#         !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
#         !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
#         !rm {root}/{branch}.zip
#         !rmdir {root}/{repo}-{branch}


# if f"{root}/{chapter}/exercises" not in sys.path:
#     sys.path.append(f"{root}/{chapter}/exercises")

# os.chdir(f"{root}/{chapter}/exercises")

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

import logging
import os
import sys
import time
from collections import defaultdict
from pathlib import Path

import circuitsvis as cv
import einops
import numpy as np
import torch as t
from IPython.display import display
from jaxtyping import Float
from nnsight import CONFIG, LanguageModel
from openai import OpenAI
from rich import print as rprint
from rich.table import Table
from torch import Tensor

# Hide some info logging messages from nnsight
logging.disable(sys.maxsize)

t.set_grad_enabled(False)
device = t.device(
    "mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu"
)

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part42_function_vectors_and_model_steering"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
# FILTERS: ~colab
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))
# END FILTERS

import part42_function_vectors_and_model_steering.solutions as solutions
import part42_function_vectors_and_model_steering.tests as tests
from plotly_utils import imshow

MAIN = __name__ == "__main__"

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 1️⃣ Introduction to `nnsight`
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Remote execution

We'll start by discussing [remote execution]((https://nnsight.net/notebooks/features/remote_execution/)) - the ability `nnsight` has to run models on an external server, which is one of the major benefits of the library as a research tool. This helps you bypass the memory & computational limits you might be faced with on your own machine. For remote execution to work, you need 2 things:

1. An API key fromm the community Discord, which you can request [here](https://login.ndif.us/) (use Google as an identity provider, if no other provider is more appropriate for you)
2. The model you're working with being live - you can see all live models in the status page [here](https://nnsight.net/status/)

Note that the status page sometimes takes ~5 minutes to load all live models - click the dropdown below to see an example of what the status page should look like once the models have loaded. If you can't see the model you're looking for in this list, then you should set `REMOTE=False` for these exercises, or else make a request to the NDIF Discord to get the model live.

<details>
<summary>Example status page</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ndif-status.png" width="650">

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Important syntax

Here, we'll discuss some important syntax for interacting with `nnsight` models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as `nnsight` models, and some of it (e.g. forward passes) is specific to `nnsight`, i.e. it would work differently if you just had a standard HuggingFace model. Make sure to keep this distinction in mind, otherwise syntax can get confusing!

### Model config

Each model comes with a `model.config`, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with `model.config`. Run the code below to see this in action, and to define some useful variables for later.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

model = LanguageModel("EleutherAI/gpt-j-6b", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = model.tokenizer

N_HEADS = model.config.n_head
N_LAYERS = model.config.n_layer
D_MODEL = model.config.n_embd
D_HEAD = D_MODEL // N_HEADS

print(f"Number of heads: {N_HEADS}")
print(f"Number of layers: {N_LAYERS}")
print(f"Model dimension: {D_MODEL}")
print(f"Head dimension: {D_HEAD}\n")

print("Entire config: ", model.config)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Number of heads: 16
Number of layers: 28
Model dimension: 4096
Head dimension: 256

Entire config:  GPTJConfig {
  "_name_or_path": "EleutherAI/gpt-j-6b",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTJForCausalLM"
  ],
  "attn_pdrop": 0.0,
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gptj",
  "n_embd": 4096,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 28,
  "n_positions": 2048,
  "resid_pdrop": 0.0,
  "rotary": true,
  "rotary_dim": 64,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 1.0
    }
  },
  "tie_word_embeddings": false,
  "tokenizer_class": "GPT2Tokenizer",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 50400
}</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Tokenizers

A model comes with a tokenizer, accessable with `model.tokenizer` (just like TransformerLens). Unlike TransformerLens, we won't be using utility functions like `model.to_str_tokens`, instead we'll be using the tokenizer directly. Some important functions for today's exercises are:

* `tokenizer` (i.e. just calling it on some input)
    * This takes in a string (or list of strings) and returns the tokenized version.
    * It will return a dictionary, always containing `input_ids` (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. `attention_mask` - see dropdown).
    * Other useful arguments for this function:
        * `return_tensors` - if this is `"pt"`, you'll get results returned as PyTorch tensors, rather than lists (which is the default).
        * `padding` - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).
* `tokenizer.decode`
    * This takes in tokens, and returns the decoded string.
    * If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).
* `tokenizer.batch_decode`
    * Equivalent to `tokenizer.decode`, but it doesn't concatenate.
    * If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.
* `tokenizer.tokenize`
    * Takes in a string, and returns a list of strings.

Run the code below to see some examples of these functions in action.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Calling tokenizer returns a dictionary, containing input ids & other data.
# If returned as a tensor, then by default it will have a batch dimension.
print(tokenizer("This must be Thursday", return_tensors="pt"))

# Decoding a list of integers, into a concatenated string.
print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))

# Using batch decode, on both 1D and 2D input.
print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))
print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))

# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).
print(tokenizer.tokenize("This sentence will be tokenized"))

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">{'input_ids': tensor([[1212, 1276,  307, 3635]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
I never could get the hang of Thursdays.
['These', ' words', ' will', ' be', ' split', ' up']
['This sentence will be together', 'So will this one']
['This', 'Ġsentence', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Note on <code>attention_mask</code> (optional)</summary>

`attention_mask`, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don't allow these tokens to be attended to). This is useful when you have to do padding. For example:

```python
model.tokenizer(["Hello world", "Hello"], return_tensors="pt", padding=True)
```

will return:

```python
{
    'attention_mask': tensor([[1, 1], [0, 1]]),
    'input_ids': tensor([[15496,   995], [50256, 15496]])
}
```

We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.

</details>

### Model outputs

At a high level, there are 2 ways to run our model: using the `trace` method (a single forward pass) and the `generate` method (generating multiple tokens). We'll focus on `trace` for now, and we'll discuss `generate` when it comes to multi-token generation later.

The default behaviour of forward passes in normal HuggingFace models is to return an object containing logits (and optionally a bunch of other things). The default behaviour of `trace` in `nnsight` is to not return anything, because anything that we choose to return is explicitly returned inside the context manager.

Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below. Remember to obtain and set an API key first if you're using remote execution!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# If you have an API key & want to work remotely, then set REMOTE = True and replace "YOUR-API-KEY"
# with your actual key. If not, then leave REMOTE = False.
REMOTE = False
if REMOTE:
    CONFIG.set_default_api_key("YOUR-API-KEY")

if MAIN:
    prompt = "The Eiffel Tower is in the city of"

    with model.trace(prompt, remote=REMOTE):
        # Save the model's hidden states
        hidden_states = model.transformer.h[-1].output[0].save()

        # Save the model's logit output
        logits = model.lm_head.output[0, -1].save()

    # Get the model's logit output, and it's next token prediction
    print(f"logits.shape = {logits.shape} = (vocab_size,)")
    print("Predicted token ID =", predicted_token_id := logits.argmax().item())
    print(f"Predicted token = {tokenizer.decode(predicted_token_id)!r}")

    # Print the shape of the model's residual stream
    print(f"\nresid.shape = {hidden_states.shape} = (batch_size, seq_len, d_model)")

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">logits.shape = torch.Size([50400]) = (vocab_size,)
Predicted token ID = 6342
Predicted token = ' Paris'

resid.shape = torch.Size([1, 10, 4096]) = (batch_size, seq_len, d_model)</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Lets go over this piece by piece.

**First, we create a context block** by calling `.trace(...)` on the model object. This denotes that we wish to generate tokens given some prompts.

```python
with model.trace(prompt, remote=REMOTE):
```

By default, running this will cause your model to be loaded & run locally, but by passing `remote=REMOTE`, it causes the model to be run on the server instead. This is very useful when working with models too large to fit on your machine (or even models which can fit on your machine, but run slowly due to their size, however if you're running this material on a sufficiently large GPU, you may prefer to set `REMOTE=False`).  The input argument can take a variety of formats: strings, lists of tokens, tensors of tokens, etc. Here, we've just used a string `prompt`.

The most interesting part of `nnsight` is the ability to access the model's internal states (like you might already have done with TransformerLens). Let's now see how this works!

```python
hidden_states = model.transformer.h[-1].output[0].save()
```

On this line we're saying: within our forward pass, access the last layer of the transformer `model.transformer.h[-1]`, access this layer's output `.output` (which is a tuple of tensors), index the first tensor in this tuple `.output[0]`, and save it `.save()`.

Let's break down this line in a bit more detail:

* `model.transformer.h[-1]` is a module in our transformer.
    * If you `print(model)`, you'll see that it consists of `transformer` and `lm_head` (for "language modelling head"). The `transformer` module is made up of embeddings & dropout, a series of layers (called `.h`, for "hidden states"), and a final layernorm. So indexing `.h[-1]` gives you the final layer.
    * Note - it's often useful to visit the documentation page for whatever model you're working on, e.g. you can find GPT-J [here](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html). Not all models will have a nice uniform standardized architecture like you might be used to in TransformerLens!
* `.output[0]` gives you this module's output, as a **proxy**.
    * The output of a module is often a tuple (again, you can see on the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) what the output of each module is). In this case, it's a tuple of 2 tensors, the first of which is the actual layer output (the thing we want).
    * Doing operations on a proxy still returns a proxy - this is why we can index into the `output` proxy tuple and get a proxy tensor!
* `.save()` takes this proxy output, and returns the actual object (which you can now access outside the context manager).

<details>
<summary>A bit more detail on <code>save</code> (optional)</summary>

To be more specific, `.save()` informs the **intervention computational graph** to clone the value of a proxy, allowing us to access the value of a proxy after the forward pass.

During processing of the intervention computational graph we are building, when the value of a proxy is no longer needed, its value is dereferenced and destroyed. If you've saved it, then you'll be able to access the value of the proxy after this happens (i.e. outside the context manager).

</details>

<details>
<summary>Optional exercise - we mentioned that <code>.output</code> returns a tuple of 2 tensors. Can you use the <a href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> what the second tensor in this tuple is?</summary>

The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called `present`. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we're only generating one new token, these are just the full keys and values.

</details>

<br>

The next command:

```python
logits = model.lm_head.output[0, -1].save()
```

can be understood in a very similar way. The only difference is that we're accessing the output of `lm_head`, the language modelling head (i.e. the unembedding at the very end), and the output is just a tensor of shape `(batch, seq, d_vocab)` rather than a tuple of tensors. Again, see the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) for this.

If you've worked with Hugging Face models then you might be used to getting logits directly from the model output, but here we generally extract logits from the model internals just like any other activation because this allows us to **control exactly what we return.** If we return lots of very large tensors, this can take quite a while to download from the server (remember that `d_vocab` is often very large for transformers, i.e. around 50k). See the "which objects to save" section below for more discussion on this.

### Output vs input

You can also extract a module's input using `.input` or `.inputs`. If a module's forward method is called as `module.forward(*args, **kwargs)` then `.inputs` returns a tuple of `(tuple_of_args, dict_of_kwargs)`. Alternatively, `.input` is an alias for `.inputs[0][0]`, in other words it returns the first arg from the module's forward method (which is usually the tensor we want).

Remember that if you're not sure then you can debug with `print(module.input.shape)` - even if `.inputs` is a tuple of inputs, this will work to recursively print the shape of all the tensors in the tuple, rather than causing an error.

### Which objects to save

Note that we saved `logits` above, which is a vector of length 50k. In general, it's best to save as small an object as possible, because this reduces the size of object you'll have to download from the server. For example, if you only want the next token completions, just argmax the logits and then save the result! All basic tensor operations can be performed within your context manager.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Putting this into practice
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - visualize attention heads

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

We just covered a lot of content, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:

```python
cv.attention.attention_patterns(
    tokens=tokens,
    attention=attention,
)
```

where `tokens` is a list of strings, and `attention` is a tensor of shape `(num_heads, num_tokens, num_tokens)`.

If you're stuck, [here's a link](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) to the source code for GPT-J. Look for how the attention patterns are calculated, within the `GPTJAttention` block.

*Note - this model uses dropout on the attention probabilities, as you'll probably notice from looking at the source code in the link above. This won't affect the model's behaviour because dropout is disabled in inference mode (and using the `generate` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.*

<details>
<summary>Aside - inference mode</summary>

Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).

If you want to run the model without inference mode, you can wrap your code in `with model.trace(inference=False):`. However, you don't need to worry about this for the purposes of these exercises.

</details>

If you're stuck on how to reference the right module, see the following hint:

<details>
<summary>Hint - what module you should get attention from</summary>

You want to extract attention from `model.transformer.h[0].attn.attn_dropout.input`. If you used `.output`, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.

</details>

<details>
<summary>Aside - GPT2 tokenizer uses special characters to represent space </summary>

GPT2 tokenizer uses "Ġ" to represent prepended space. So ["My", " name", " is", " James"] will be tokenized as ["My", "Ġname", "Ġis", "ĠJames"]. Make sure you replace "Ġ" with an actual space.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

# EXERCISE
# # YOUR CODE HERE - extract and visualize attention
# END EXERCISE
# SOLUTION
if MAIN:
    with model.trace(prompt, remote=REMOTE):
        attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()

    # Get string tokens (replacing special character for spaces)
    str_tokens = model.tokenizer.tokenize(prompt)
    str_tokens = [s.replace("Ġ", " ") for s in str_tokens]

    # Attention patterns (squeeze out the batch dimension)
    attn_patterns_value = attn_patterns.squeeze(0)

    print("Layer 0 Head Attention Patterns:")
    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attn_patterns_value))
# END SOLUTION

# FILTERS: ~
# html = cv.attention.attention_patterns(tokens=str_tokens, attention=attn_patterns_value)
# with open(section_dir / "14201.html", "w") as f:
#     f.write(str(html))
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-142/14201.html" width="1050" height="420"></div>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Solution (and explanation)</summary>

```python
with model.trace(prompt, remote=REMOTE):
    attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()

# Get string tokens (replacing special character for spaces)
str_tokens = model.tokenizer.tokenize(prompt)
str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]

# Attention patterns (squeeze out the batch dimension)
attn_patterns_value = attn_patterns.squeeze(0)

print("Layer 0 Head Attention Patterns:")
display(cv.attention.attention_patterns(
    tokens=str_tokens,
    attention=attn_patterns_value,
))
```

Explanation:

* Within the context managers:
    * We access the attention patterns by taking the input to the `attn_dropout`.
        * From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed `nn.Softmax` module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.
        * Because of the previously discussed point about dropout not working in inference mode, we could also use the output of `attn_dropout`, and get the same values.
    * We use the `.save()` method to save the attention patterns (as an object).
* Outside of the context managers:
    * We use the `tokenize` method to tokenize the prompt.
        
</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using `model.transformer.h[0].attn.q_proj.output` will give you the query vectors, and `k_proj` for the key vectors. However, one thing to be wary of is that GPT-J uses **rotary embeddings**, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See [here](https://blog.eleuther.ai/rotary-embeddings/) for an in-depth discussion of rotary embeddings, and [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bef36Bf9k7FYsCt1DpzCw6eV) for some rough intuitions.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 2️⃣ Task-encoding hidden states
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

Note - this section structurally follows section 2.1 of the function vectors paper.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## ICL Task
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise (optional) - generate your own antonym pairs

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
> 
> If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.
> ```

We've provided you two options for the antonym dataset you'll use in these exercises.

1. Firstly, we've provided you a list of word pairs, in the file `data/antonym_pairs.txt`.
2. Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).

If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function `generate_dataset` below, which should query GPT-4 and get a list of antonym pairs.

See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it. Use the two dropdowns below (in order) for some guidance.

<details>
<summary>Getting started #1</summary>

Here is a recommended template:

```python
client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": antonym_task},
        {"role": "assistant", "content": start_of_response},
    ]
)
```

where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. "Sure, here are some antonyms: ..."), to guide its subsequent behaviour.

</details>

<details>
<summary>Getting started #2</summary>

Here is an template you might want to use for the actual request:

```python
example_antonyms = "old: young, top: bottom, awake: asleep, future: past, "

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym."},
        {"role": "assistant", "content": f"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}"},
    ]
)
```

where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def generate_antonym_dataset(N: int):
    """
    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.
    """
    assert os.environ.get("OPENAI_API_KEY", None) is not None, (
        "Please set your API key before running this function!"
    )

    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": f"Generate {N} pairs of antonyms in the form of a list of 2-tuples. For example, [['old', 'young'], ['top', bottom'], ['awake', 'asleep']...].",
            },
            {"role": "assistant", "content": "Sure, here is a list of 100 antonyms: "},
        ],
    )
    return response


if MAIN:
    if os.environ.get("OPENAI_API_KEY", None) is not None:
        ANTONYM_PAIRS = generate_antonym_dataset(100)
        # Save the word pairs in a text file
        with open(section_dir / "data" / "my_antonym_pairs.txt", "w") as f:
            for word_pair in ANTONYM_PAIRS:
                f.write(f"{word_pair[0]} {word_pair[1]}\n")

    # Load the word pairs from the text file
    with open(section_dir / "data" / "antonym_pairs.txt", "r") as f:
        ANTONYM_PAIRS = [line.split() for line in f.readlines()]

    print(ANTONYM_PAIRS[:10])

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">[
    ["old", "young"],
    ["top", "bottom"],
    ["awake", "asleep"],
    ["future", "past"],
    ["appear", "disappear"],
    ["early", "late"],
    ["empty", "full"],
    ["innocent", "guilty"],
    ["ancient", "modern"],
    ["arrive", "depart"],
]</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## ICL Dataset
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To handle this list of word pairs, we've given you some helpful classes.

Firstly, there's the `ICLSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class ICLSequence:
    """
    Class to store a single antonym sequence.

    Uses the default template "Q: {x}\nA: {y}" (with separate pairs split by "\n\n").
    """

    def __init__(self, word_pairs: list[list[str]]):
        self.word_pairs = word_pairs
        self.x, self.y = zip(*word_pairs)

    def __len__(self):
        return len(self.word_pairs)

    def __getitem__(self, idx: int):
        return self.word_pairs[idx]

    def prompt(self):
        """Returns the prompt, which contains all but the second element in the last word pair."""
        p = "\n\n".join([f"Q: {x}\nA: {y}" for x, y in self.word_pairs])
        return p[: -len(self.completion())]

    def completion(self):
        """Returns the second element in the last word pair (with padded space)."""
        return " " + self.y[-1]

    def __str__(self):
        """Prints a readable string representation of the prompt & completion (indep of template)."""
        return f"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->".strip(", ")


if MAIN:
    word_list = [["hot", "cold"], ["yes", "no"], ["in", "out"], ["up", "down"]]
    seq = ICLSequence(word_list)

    print("Tuple-representation of the sequence:")
    print(seq)
    print("\nActual prompt, which will be fed into the model:")
    print(seq.prompt())

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Tuple-representation of the sequence:
(hot, cold), (yes, no), (in, out), up ->

Actual prompt, which will be fed into the model:
Q: hot
A: cold

Q: yes
A: no

Q: in
A: out

Q: up
A:</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Secondly, we have the `ICLDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

class ICLDataset:
    """
    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency
    between the corrupted and clean datasets.

    Inputs:
        word_pairs:
            list of ICL task, e.g. [["old", "young"], ["top", "bottom"], ...] for the antonym task
        size:
            number of prompts to generate
        n_prepended:
            number of antonym pairs before the single-word ICL task
        bidirectional:
            if True, then we also consider the reversed antonym pairs
        corrupted:
            if True, then the second word in each pair is replaced with a random word
        seed:
            random seed, for consistency & reproducibility
    """

    def __init__(
        self,
        word_pairs: list[list[str]],
        size: int,
        n_prepended: int,
        bidirectional: bool = True,
        seed: int = 0,
        corrupted: bool = False,
    ):
        assert n_prepended + 1 <= len(word_pairs), (
            "Not enough antonym pairs in dataset to create prompt."
        )

        self.word_pairs = word_pairs
        self.word_list = [word for word_pair in word_pairs for word in word_pair]
        self.size = size
        self.n_prepended = n_prepended
        self.bidirectional = bidirectional
        self.corrupted = corrupted
        self.seed = seed

        self.seqs = []
        self.prompts = []
        self.completions = []

        # Generate the dataset (by choosing random word pairs, and constructing ICLSequence objects)
        for n in range(size):
            np.random.seed(seed + n)
            random_pairs = np.random.choice(len(self.word_pairs), n_prepended + 1, replace=False)
            # Randomize the order of each word pair (x, y).
            # If not bidirectional, we always have x -> y not y -> x
            random_orders = np.random.choice([1, -1], n_prepended + 1)
            if not (bidirectional):
                random_orders[:] = 1
            word_pairs = [
                self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)
            ]
            # If corrupted, then replace y with a random word in all (x, y) pairs except the last one
            if corrupted:
                for i in range(len(word_pairs) - 1):
                    word_pairs[i][1] = np.random.choice(self.word_list)
            seq = ICLSequence(word_pairs)

            self.seqs.append(seq)
            self.prompts.append(seq.prompt())
            self.completions.append(seq.completion())

    def create_corrupted_dataset(self):
        """Creates a corrupted version of the dataset (with same random seed)."""
        return ICLDataset(
            self.word_pairs,
            self.size,
            self.n_prepended,
            self.bidirectional,
            corrupted=True,
            seed=self.seed,
        )

    def __len__(self):
        return self.size

    def __getitem__(self, idx: int):
        return self.seqs[idx]

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `"A: answer" -> ["A", ":", " answer"]`. Forgetting prepended spaces is a classic mistake when working with transformers!
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)

table = Table("Prompt", "Correct completion")
for seq, completion in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completion))

rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                               </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, left), (maximum, minimum), melt -&gt;           │ ' freeze'          │
│ (minimum, maximum), (old, new), punishment -&gt;        │ ' reward'          │
│ (arrogant, humble), (blunt, sharp), compulsory -&gt;    │ ' voluntary'       │
│ (inside, outside), (freeze, melt), full -&gt;           │ ' empty'           │
│ (reject, accept), (awake, asleep), dusk -&gt;           │ ' dawn'            │
│ (invisible, visible), (punishment, reward), heavy -&gt; │ ' light'           │
│ (victory, defeat), (forward, backward), young -&gt;     │ ' old'             │
│ (up, down), (compulsory, voluntary), right -&gt;        │ ' wrong'           │
│ (open, closed), (domestic, foreign), brave -&gt;        │ ' cowardly'        │
│ (under, over), (past, future), increase -&gt;           │ ' decrease'        │
└──────────────────────────────────────────────────────┴────────────────────┘</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Compare this output to what it looks like when `corrupted=True`. In the prompt, each pair *before* the last one has their second element replaced with a random one (e.g. `(right, left)` becomes `(right, pivate)`) but the last pair is left unchanged. This should effectively destroy the model's ability to infer what pattern the pairs are following.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)

table = Table("Prompt", "Correct completion")
for seq, completions in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completions))

rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                            </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, private), (maximum, destroy), melt -&gt;     │ ' freeze'          │
│ (minimum, increase), (old, sharp), punishment -&gt;  │ ' reward'          │
│ (arrogant, humble), (blunt, deep), compulsory -&gt;  │ ' voluntary'       │
│ (inside, voluntary), (freeze, exterior), full -&gt;  │ ' empty'           │
│ (reject, profit), (awake, start), dusk -&gt;         │ ' dawn'            │
│ (invisible, birth), (punishment, spend), heavy -&gt; │ ' light'           │
│ (victory, rich), (forward, honest), young -&gt;      │ ' old'             │
│ (up, lie), (compulsory, short), right -&gt;          │ ' wrong'           │
│ (open, soft), (domestic, anxious), brave -&gt;       │ ' cowardly'        │
│ (under, melt), (past, young), increase -&gt;         │ ' decrease'        │
└───────────────────────────────────────────────────┴────────────────────┘</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>Aside - the <code>rich</code> library</summary>

The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.

The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:

```python
rprint("[green]This is green text[/], this is default color")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-1.png" width="350">

and for making text bold / underlined:

```python
rprint("[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-2.png" width="350">

It can also print tables:

```python
from rich.table import Table

table = Table("Col1", "Col2", title="Title") # title is optional
table.add_row("A", "a")
table.add_row("B", "b")

rprint(table)
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-3.png" width="150">

The text formatting (bold, underlined, colors, etc) is also supported within table cells.

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Task-encoding vector
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - forward pass on antonym dataset

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the `calculate_h` function below. It should:

* Run a forward pass on the model with the dataset prompts (i.e. the `.prompts` attribute), using the `nnsight` syntax we've demonstrated previously,
* Return a tuple of the model's output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-1.png" width="900">

You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `-1` token (where the model makes the antonym prediction), and same for the completions.

<details>
<summary> Help - I'm not sure how to run (and index into) a batch of inputs.</summary>

If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.

The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def calculate_h(
    model: LanguageModel, dataset: ICLDataset, layer: int = -1
) -> tuple[list[str], Tensor]:
    """
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer
    `layer`, to produce a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at
            the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: list[str]
            list of the model's next-token predictions (i.e. the strings the model predicts to
            follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    with model.trace(dataset.prompts, remote=REMOTE):
        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0).save()
        logits = model.lm_head.output[:, -1]
        next_tok_id = logits.argmax(dim=-1).save()

    completions = model.tokenizer.batch_decode(next_tok_id)
    return completions, h
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_calculate_h(calculate_h, model)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct). Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!

If the antonyms dataset was constructed well, you should find that the model's completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting `wet -> wet` rather than `wet -> dry`) or understandable completions which shouldn't really be considered mistakes (e.g. predicting `right -> left` rather than `right -> wrong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but for these exercises, we won't worry about this.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def display_model_completions_on_antonyms(
    model: LanguageModel,
    dataset: ICLDataset,
    completions: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt (tuple representation)",
        "Model's completion\n(green=correct)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        # Get model's completion, and correct completion
        completion = completions[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace(
            "Ġ", " "
        )
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion == correct_completion_first_token
        completion = f"[b green]{repr(completion)}[/]" if is_correct else repr(completion)

        table.add_row(str(seq), completion, repr(correct_completion))

    rprint(table)


if MAIN:
    # Get uncorrupted dataset
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)

    # Getting it from layer 12, as in the description in section 2.1 of paper
    model_completions, h = calculate_h(model, dataset, layer=12)

    # Displaying the output
    display_model_completions_on_antonyms(model, dataset, model_completions)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                    Model's antonym completions                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">                                                       </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt (tuple representation)                         </span>┃<span style="font-weight: bold"> (green=correct)    </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, left), (maximum, minimum), melt -&gt;            │ ' cast'            │ ' freeze'          │
│ (minimum, maximum), (old, new), punishment -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' reward'</span>          │ ' reward'          │
│ (arrogant, humble), (blunt, sharp), compulsory -&gt;     │ ' optional'        │ ' voluntary'       │
│ (inside, outside), (freeze, melt), full -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' empty'</span>           │ ' empty'           │
│ (reject, accept), (awake, asleep), dusk -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' dawn'</span>            │ ' dawn'            │
│ (invisible, visible), (punishment, reward), heavy -&gt;  │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' light'</span>           │ ' light'           │
│ (victory, defeat), (forward, backward), young -&gt;      │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' old'</span>             │ ' old'             │
│ (up, down), (compulsory, voluntary), right -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' wrong'</span>           │ ' wrong'           │
│ (open, closed), (domestic, foreign), brave -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cowardly'</span>        │ ' cowardly'        │
│ (under, over), (past, future), increase -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' decrease'</span>        │ ' decrease'        │
│ (inside, outside), (melt, freeze), over -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' under'</span>           │ ' under'           │
│ (solid, liquid), (backward, forward), open -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ (optimist, pessimist), (invisible, visible), brave -&gt; │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cowardly'</span>        │ ' cowardly'        │
│ (noisy, quiet), (sell, buy), north -&gt;                 │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' south'</span>           │ ' south'           │
│ (guilty, innocent), (birth, death), victory -&gt;        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' defeat'</span>          │ ' defeat'          │
│ (answer, question), (noisy, quiet), ancient -&gt;        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' modern'</span>          │ ' modern'          │
│ (on, off), (success, failure), flexible -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' rigid'</span>           │ ' rigid'           │
│ (junior, senior), (arrive, depart), punishment -&gt;     │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' reward'</span>          │ ' reward'          │
│ (loose, tight), (learn, teach), new -&gt;                │ ' new'             │ ' old'             │
│ (introduce, remove), (deficiency, quality), wet -&gt;    │ ' wet'             │ ' dry'             │
└───────────────────────────────────────────────────────┴────────────────────┴────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Using multiple invokes

Another cool feature of `nnsight` is the ability to run multiple different batches through the model at once (or the same batch multiple times) in a way which leads to very clean syntax for doing causal interventions. Rather than doing something like this:

```python
with model.trace(inputs, remote=REMOTE):
    # some causal interventions
```

we can write a double-nested context manager:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(inputs):
        # some causal interventions
    
    with tracer.invoke(other_inputs):
        # some other causal interventions
```

Both inputs will be run together in parallel, and proxies defined within one `tracer.invoke` block can be used in another. A common use-case is to have clean and corrupted inputs, so we can patch from one to the other and get both outputs all in a single forward pass:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(clean_inputs):
        # extract clean activations
        clean_activations = model.transformer.h[10].output[0]
    
    with tracer.invoke(corrupted_inputs):
        # patch clean into corrupted
        model.transformer.h[10].output[0][:] = clean_activations
```

You'll do something like this in a later exercise. However for your first exercise (immediately below), you'll only be intervening with vectors that are defined outside of your context manager.

**One important thing to watch out for** - make sure you're not using your proxy before its being defined! For example, if you were extracting `clean_activations` from `model.transformer.h[10]` but then intervening with it on `model.transformer.h[9]`, this couldn't be done in parallel (you'd need to first extract the clean activations, *then* run the patched forward pass). Doing this should result in a warning message, but may pass silently in some cases - so you need to be extra vigilant!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - intervene with $h$

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the function `intervene_with_h` below. This will involve:

* Run two forward passes (within the same context manager) on a zero-shot dataset:
    * One with no intervention (i.e. the residual stream is unchanged),
    * One with an intervention using `h` (i.e. `h` is added to the residual stream at the layer it was taken from).
* Return the completions for no intervention and intervention cases respectively (see docstring).

The diagram below shows how all of this should work, when combined with the `calculate_h` function.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-2.png" width="950">

Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.

<details>
<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>

You can use `with tracer.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.

</details>

<details>
<summary>Help - I'm not sure how to intervene on the hidden state.</summary>

First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).

Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
    remote: bool = REMOTE,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: the model we're using to generate completions
        zero_shot_dataset: the dataset of zero-shot prompts which we'll intervene on, using the
            `h`-vector
        h: the `h`-vector we'll be adding to the residual stream
        layer: the layer we'll be extracting the `h`-vector from
        remote: whether to run the forward pass on the remote server (used for running test code)

    Returns:
        completions_zero_shot: list of string completions for the zero-shot prompts, without
            intervention using the h-vector
        completions_intervention: list of string completions for the zero-shot prompts, with
            intervention using the h-vector
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    with model.trace(remote=remote) as tracer:
        # First, run a forward pass where we don't intervene, just save token id completions
        with tracer.invoke(zero_shot_dataset.prompts):
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with tracer.invoke(zero_shot_dataset.prompts):
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention)

    return completions_zero_shot, completions_intervention
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS, REMOTE)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Run the code below to calculate completions for the function.

**Note, it's very important that we set a different random seed for the zero shot dataset, otherwise we'll be intervening on examples which were actually in the dataset we used to compute $h$!**
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

layer = 12
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

# Run previous function to get h-vector
h = calculate_h(model, dataset, layer=layer)[1]

# Run new function to intervene with h-vector
completions_zero_shot, completions_intervention = intervene_with_h(
    model, zero_shot_dataset, h, layer=layer
)

print("Zero-shot completions: ", completions_zero_shot)
print("Completions with intervention: ", completions_intervention)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Zero-shot completions:  [' minimum', ' arrogant', ' inside', ' reject', ' invisible', ' victory', ' up', ' open', ' under', ' inside', ' solid', '\n', ' noisy', ' guilty', ' yes', ' I', ' senior', ' loose', ' introduce', ' guilty']
Completions with intervention:  [' maximum', ' arrogant', ' outside', ' reject', ' invisible', ' victory', ' down', ' closed', ' under', ' outside', ' solid', ' optim', ' noisy', ' guilty', ' answer', ' on', ' senior', ' loose', ' introduce', ' guilty']
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Next, run the code below to visualise the completions in a table. You should see:

* 0% (or near zero) correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt
* 25-50% correct completions on the zero-shot prompt with intervention
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def display_model_completions_on_h_intervention(
    dataset: ICLDataset,
    completions: list[str],
    completions_intervention: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt",
        "Model's completion\n(no intervention)",
        "Model's completion\n(intervention)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        completion_ni = completions[i]
        completion_i = completions_intervention[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion_i == correct_completion_first_token
        completion_i = f"[b green]{repr(completion_i)}[/]" if is_correct else repr(completion_i)

        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))

    rprint(table)


if MAIN:
    display_model_completions_on_h_intervention(
        zero_shot_dataset, completions_zero_shot, completions_intervention
    )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                          Model's antonym completions                          </span>
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">              </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt       </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ minimum -&gt;   │ ' minimum'         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' maximum'</span>         │ ' maximum'         │
│ arrogant -&gt;  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │
│ invisible -&gt; │ ' invisible'       │ ' invisible'       │ ' visible'         │
│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │
│ up -&gt;        │ ' up'              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' down'</span>            │ ' down'            │
│ open -&gt;      │ ' open'            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │
│ optimist -&gt;  │ '\n'               │ ' optim'           │ ' pessimist'       │
│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │
│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │
│ answer -&gt;    │ ' yes'             │ ' answer'          │ ' question'        │
│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │
│ junior -&gt;    │ ' senior'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' senior'</span>          │ ' senior'          │
│ loose -&gt;     │ ' loose'           │ ' loose'           │ ' tight'           │
│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │
│ innocent -&gt;  │ ' guilty'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' guilty'</span>          │ ' guilty'          │
└──────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - combine the last two functions

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-15 minutes on this exercise.
> ```

One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.

In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.trace` context manager. In other words, **we'll be using `with tracer.invoke...` three times** in this context manager.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-3.png" width="1000">

You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).

Your output should be exactly the same as before (since the `ICLDataset` class is deterministic), hence we've not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you're batching the operations of "compute $h$" and "intervene with $h$" together into a single forward pass.

<details>
<summary>Help - I'm not sure how to use the <code>h</code> vector inside the context manager.</summary>

You extract `h` the same way as before, but you don't need to save it. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.

You shouldn't have to `.save()` anything inside your context manager, other than the token completions.

</details>
<details>
<summary>Help - If I want to add <code>x</code> vector to a slice of my hidden state tensor <code>h</code>, is <code>h[slice]+=x</code> the same as <code>h2 = h[slice], h2 += x</code>?</summary>

No, only `h[slice]+=x` does what you want. This is because when doing <code>h2 = h[slice], h2 += x</code>, the modification line <code>h2 += x</code> is no longer modifying the original tensor `h`, but a different tensor`h2`. In contrast, `h[slice]+=x` keeps the original tensor `h` in the modification line.

A good rule to keep in mind is: If you're trying to modify a tensor some in-place operation, make sure that tensor is in the actual modification line!

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated
    zero-shot prompts, all within the same forward pass. Returns the completions from this
    intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

    completions_zero_shot = tokenizer.batch_decode(clean_tokens)
    completions_intervention = tokenizer.batch_decode(intervene_tokens)
    return completions_zero_shot, completions_intervention
    # END SOLUTION


# HIDE
if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
    zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

    completions_zero_shot, completions_intervention = calculate_h_and_intervene(
        model, dataset, zero_shot_dataset, layer=layer
    )

    display_model_completions_on_h_intervention(
        zero_shot_dataset, completions_zero_shot, completions_intervention
    )
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                          Model's antonym completions                          </span>
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">              </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt       </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ minimum -&gt;   │ ' minimum'         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' maximum'</span>         │ ' maximum'         │
│ arrogant -&gt;  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │
│ invisible -&gt; │ ' invisible'       │ ' invisible'       │ ' visible'         │
│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │
│ up -&gt;        │ ' up'              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' down'</span>            │ ' down'            │
│ open -&gt;      │ ' open'            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │
│ optimist -&gt;  │ '\n'               │ ' optim'           │ ' pessimist'       │
│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │
│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │
│ answer -&gt;    │ ' yes'             │ ' answer'          │ ' question'        │
│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │
│ junior -&gt;    │ ' senior'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' senior'</span>          │ ' senior'          │
│ loose -&gt;     │ ' loose'           │ ' loose'           │ ' tight'           │
│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │
│ innocent -&gt;  │ ' innocent'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' guilty'</span>          │ ' guilty'          │
└──────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - compute change in accuracy

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 10-20 minutes on this exercise.
> ```

So far, all we've done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn't just give us token completions, it gives us logits too!

You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning two lists of string completions, it returns two lists of floats containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.

<details>
<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>

First, apply log softmax to the logits, to get logprobs.

Second, you can use `tokenizer(dataset.completions)["input_ids"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)

Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[float], list[float]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated
    zero-shot prompts, all within the same forward pass. Returns the logprobs on correct tokens from
    this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: list[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: list[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    correct_completion_ids = [
        toks[0] for toks in tokenizer(zero_shot_dataset.completions)["input_ids"]
    ]

    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

    return clean_logprobs, intervene_logprobs
    # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
When you run the code below, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def display_model_logprobs_on_h_intervention(
    dataset: ICLDataset,
    correct_logprobs_zero_shot: list[float],
    correct_logprobs_intervention: list[float],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Zero-shot prompt",
        "Model's logprob\n(no intervention)",
        "Model's logprob\n(intervention)",
        "Change in logprob",
        title="Model's antonym logprobs, with zero-shot h-intervention\n(green = intervention improves accuracy)",
    )

    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):
        logprob_ni = correct_logprobs_zero_shot[i]
        logprob_i = correct_logprobs_intervention[i]
        delta_logprob = logprob_i - logprob_ni
        zero_shot_prompt = f"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}"

        # Color code the logprob based on whether it's increased with this intervention
        is_improvement = delta_logprob >= 0
        delta_logprob = (
            f"[b green]{delta_logprob:+.2f}[/]" if is_improvement else f"{delta_logprob:+.2f}"
        )

        table.add_row(zero_shot_prompt, f"{logprob_ni:.2f}", f"{logprob_i:.2f}", delta_logprob)

    rprint(table)


if MAIN:
    dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
    zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

    correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(
        model, dataset, zero_shot_dataset, layer=layer
    )

    display_model_logprobs_on_h_intervention(
        zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention
    )

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">              Model's antonym logprobs, with zero-shot h-intervention              </span>
<span style="font-style: italic">                     (green = intervention improves accuracy)                      </span>
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">                       </span>┃<span style="font-weight: bold"> Model's logprob   </span>┃<span style="font-weight: bold"> Model's logprob </span>┃<span style="font-weight: bold">                   </span>┃
┃<span style="font-weight: bold"> Zero-shot prompt      </span>┃<span style="font-weight: bold"> (no intervention) </span>┃<span style="font-weight: bold"> (intervention)  </span>┃<span style="font-weight: bold"> Change in logprob </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│  minimum -&gt; maximum   │ -2.75             │ -0.64           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.11</span>             │
│ arrogant -&gt; humble    │ -6.19             │ -3.92           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.27</span>             │
│   inside -&gt; outside   │ -3.70             │ -0.99           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.72</span>             │
│   reject -&gt; accept    │ -3.94             │ -1.98           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.95</span>             │
│ invisible -&gt; visible  │ -3.80             │ -1.99           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.80</span>             │
│  victory -&gt; defeat    │ -4.41             │ -2.30           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.11</span>             │
│       up -&gt; down      │ -3.97             │ -1.26           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.72</span>             │
│     open -&gt; closed    │ -5.06             │ -1.43           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+3.62</span>             │
│    under -&gt; over      │ -4.78             │ -3.44           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.34</span>             │
│   inside -&gt; outside   │ -3.70             │ -0.99           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.72</span>             │
│    solid -&gt; liquid    │ -5.53             │ -3.02           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.52</span>             │
│ optimist -&gt; pessimist │ -6.41             │ -3.41           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+3.00</span>             │
│    noisy -&gt; quiet     │ -4.28             │ -3.34           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+0.94</span>             │
│   guilty -&gt; innocent  │ -4.94             │ -2.75           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.19</span>             │
│   answer -&gt; question  │ -5.09             │ -3.91           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.19</span>             │
│       on -&gt; off       │ -7.00             │ -4.31           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.69</span>             │
│   junior -&gt; senior    │ -2.25             │ -1.07           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.18</span>             │
│    loose -&gt; tight     │ -5.56             │ -2.98           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.58</span>             │
│ introduce -&gt; remove   │ -7.50             │ -6.19           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.31</span>             │
│ innocent -&gt; guilty    │ -2.86             │ -1.67           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.19</span>             │
└───────────────────────┴───────────────────┴─────────────────┴───────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 3️⃣ Function Vectors
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
Here, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Extracting & using FVs
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### A note on `out_proj`

First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations. What we have is the linear layer `out_proj` which implicitly combines the "projection per attention head" and the "sum over attention head" operations (if you can't see how this is possible, see the section "Attention Heads are Independent and Additive" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-2.png" width="950">

This presents some question for us, when it comes to causal interventions on attention heads. Use the dropdowns below to read them answer these questions (they'll be important for the coming exercises).

<br>

<details>
<summary>If we want to do a causal intervention on a particular head, should we intervene on <code>z</code> (the input of <code>out_proj</code>) or on <code>attn_output</code> (the output of <code>out_proj</code>) ?</summary>

We should intervene on `z`, because we can just rearrange the `z` tensor of shape `(batch, seq, d_model)` into `(batch, seq, n_heads, d_head)`, in other words separating out all the heads. On the other hand, we can't do this with the `attn_output` because it's *already* summed over heads and we can't separate them out.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we had the ability to access model weights within our context managers?</summary>

We can take a slice of the `z` tensor corresponding to a single attention head:

```python
z.reshape(batch, seq, n_heads, d_head)[:, :, head_idx]
```

and we can take a slice of the `out_proj` weight matrix corresponding to a single attention head (remember that PyTorch stores linear layers in the shape `(out_feats, in_feats)`):

```python
out_proj.weight.rearrange(d_model, n_heads, d_head)[:, head_idx]
```

then finally we can multiply these together.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we </b>didn't have</b> the ability to access model weights within our context managers? (This is currently the case for <code>nnsight</code>, since having access to the weights could allow users to change them!).</summary>

We can be a bit clever, and ablate certain heads in the `z` vector before passing it through the output projection:

```python
# ablate all heads except #2 (using a cloned activation)
heads_to_ablate = [0, 1, 3, 4, ...]
z_ablated = z.reshape(batch, seq, n_heads, d_head).clone()
z_ablated[:, :, heads_to_ablate] = 0

# save the output
attn_head_output = out_proj(z_ablated).save()
```

Illustration:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-ablated-2.png" width="950">

Note - this would actually fail if `out_proj` had a bias, because we want to just get an attention head's output, not the bias term as well. But if you look at the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) you'll see that `out_proj` doesn't have a bias term, so we're all good!

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - implement `calculate_fn_vectors_and_intervene`

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵🔵
> 
> You should spend up to 30-60 minutes on this exercise.
> ```

This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:

* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.
    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.
* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).
    * You can use the method `create_corrupted_dataset` method of the `ICLDataset` class for this.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/cie-intervention.png" width="1200">

Before you actually start writing the code, it might be helpful to answer the following:

<details>
<summary>How many different <code>invoke</code> calls will you need in total?</summary>

You'll need `(N_LAYERS * N_HEADS) + 2`. To explain:

- One for the clean prompts, which you'll extract internal activations from and patch them into corrupted prompts,
- One for the corrupted prompts, which you don't intervene on,
- One for the corrupted prompts **for every attention head**, which you'll patch into using the clean run activations.

</details>

<details>
<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>

You don't need to `.save()` the function vectors you're extracting from the model's internals, because these will only be used for causal interventions within the context manager.

The only thing you need to save is the correct token logprobs for (1) the corrupted forward pass where we don't intervene, and (2) each corrupted forward pass where we do intervene on one of the heads. In other words, you'll need to save `(N_LAYERS * N_HEADS) + 1` tensors in total.

</details>

A few other notes:

* We've added a `layers` argument, so you can iterate through different layers of the model (i.e. running the model with `layers = [3, 4, 5]` will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you're getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so even with few prompts per head this adds up fast!).
    * We've included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.
* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).
* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.
* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - this is a wonderful tool, it's like using code annotations within your actual code!

One last note - **if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don't rely on this function working.** However, you should definitely at least read & understand the solution.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: list[int] | None = None,
) -> Float[Tensor, "layers heads"]:
    """
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also
            create a corrupted version of this dataset for interventions)
        layers: list[int] | None
            the layers which this function will calculate score for (if None, this means all layers)
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    layers = range(model.config.n_layer) if (layers is None) else layers
    heads = range(model.config.n_head)

    # Get corrupted dataset
    corrupted_dataset = dataset.create_corrupted_dataset()
    N = len(dataset)

    # Get correct token ids, so we can get correct token logprobs
    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)["input_ids"]]

    with model.trace(remote=REMOTE) as tracer:
        # Run a forward pass on clean prompts, where we store attention head outputs
        z_dict = {}
        with tracer.invoke(dataset.prompts):
            for layer in layers:
                # Get hidden states, reshape to get head dimension, store the mean tensor
                z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)
                for head in heads:
                    z_dict[(layer, head)] = z_reshaped[head]

        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can get the
        # correct-token logprobs to compare with our intervention)
        with tracer.invoke(corrupted_dataset.prompts):
            logits = model.lm_head.output[:, -1]
            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[
                t.arange(N), correct_completion_ids
            ].save()

        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes, since
        # we're doing different interventions each time)
        correct_logprobs_dict = {}
        for layer in layers:
            for head in heads:
                with tracer.invoke(corrupted_dataset.prompts):
                    # Get hidden states, reshape to get head dimension, then set it to the a-vector
                    z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]
                    # Get logprobs at the end, which we'll compare with our corrupted logprobs
                    logits = model.lm_head.output[:, -1]
                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[
                        t.arange(N), correct_completion_ids
                    ].save()

    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim
    all_correct_logprobs_intervention = einops.rearrange(
        t.stack([v for v in correct_logprobs_dict.values()]),
        "(layers heads) batch -> layers heads batch",
        layers=len(layers),
    )
    logprobs_diff = (
        all_correct_logprobs_intervention - correct_logprobs_corrupted
    )  # shape [layers heads batch]

    # Return mean effect of intervention, over the batch dimension
    return logprobs_diff.mean(dim=-1)
    # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
As mentioned, the code below calls the function multiple times separately and combines the results.

When you run this code & plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper's figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

dataset = ICLDataset(ANTONYM_PAIRS, size=8, n_prepended=2)


def batch_process_layers(n_layers, batch_size):
    for i in range(0, n_layers, batch_size):
        yield range(n_layers)[i : i + batch_size]


results = t.empty((0, N_HEADS), device=device)

# If this fails to run, you should reduce the batch size so the forward passes are split up more, or
# reduce dataset size
for layers in batch_process_layers(N_LAYERS, batch_size=4):
    print(f"Computing layers in {layers} ...")
    t0 = time.time()
    results = t.concat(
        [results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)]
    )
    print(f"... finished in {time.time() - t0:.2f} seconds.\n")

# FILTERS: st,py
imshow(
    results.T,
    title="Average indirect effect of function-vector intervention on antonym task",
    width=1000,
    height=600,
    labels={"x": "Layer", "y": "Head"},
    aspect="equal",
)
# END FILTERS

# FILTERS: ~
# fig = imshow(
#     results.T,
#     title="Average indirect effect of function-vector intervention on antonym task",
#     width=1000,
#     height=600,
#     labels={"x": "Layer", "y": "Head"},
#     aspect="equal",
#     return_fig=True,
# )
# fig.write_html(section_dir / "14202.html")
# END FILTERS

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<div style="text-align: left"><embed src="https://info-arena.github.io/ARENA_img/misc/media-142/14202.html" width="1020" height="620"></div>
'''

# ! CELL TYPE: code
# ! FILTERS: [colab]
# ! TAGS: []

# imshow(
#     results.T,
#     title="Average indirect effect of function-vector intervention on antonym task",
#     width=1000,
#     height=600,
#     labels={"x": "Layer", "y": "Head"},
#     aspect="equal",
# )

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - calculate the function vector

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 25-50 minutes on this exercise.
> ```

Your next task is to actually calculate and return the function vector, so we can do a few experiments with it. The function vector is the sum of the outputs of all the attention heads we found using the previous function (i.e. the sum of all of the vectors these heads write to the residual stream), averaged over the prompts in our dataset.

There's a difficulty here - rather than just getting the `z` vectors, we're actually trying to get the `attn_out` vectors, but *before* they're summed over heads. As we discussed previously, this is a bit tricky to do for the model we're working with, because the `out_proj` linear map actually does the "project up" and "sum over heads" operations simultaneously. It would be nice to just take a slice of the `out_proj` matrix and multiply it with a slice of the `z` vector, but the `nnsight` library doesn't yet allow users to access weights directly (for security reasons). To understand how we can extract the `attn_out` vector for a head separately without accessing the underlying weights, you should go back to read the subsection **A note on `out_proj`** at the start of this section.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "d_model"]:
    """
    Returns a vector of length `d_model`, containing the sum of vectors written to the residual
    stream by the attention heads in `head_list`, averaged over all inputs in `dataset`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also
            create a corrupted version of this dataset for interventions)
        head_list: list[tuple[int, int]]
            list of attention heads we're calculating the function vector from
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    # Turn head_list into a dict of {layer: heads we need in this layer}
    head_dict = defaultdict(set)
    for layer, head in head_list:
        head_dict[layer].add(head)

    fn_vector_list = []

    with model.trace(dataset.prompts, remote=REMOTE):
        for layer, head_list in head_dict.items():
            # Get the output projection layer
            out_proj = model.transformer.h[layer].attn.out_proj

            # Get the mean output projection input (note, setting values of this tensor will not
            # have downstream effects on other tensors)
            hidden_states = out_proj.input[:, -1].mean(dim=0)

            # Zero-ablate all heads which aren't in our list, then get the output (which
            # will be the sum over the heads we actually do want!)
            heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]
            for head in heads_to_ablate:
                hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0

            # Now that we've zeroed all unimportant heads, get the output & add it to the list
            # (we need a single batch dimension so we can use `out_proj`)
            out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze()
            fn_vector_list.append(out_proj_output.save())

    # We sum all attention head outputs to get our function vector
    fn_vector = sum([v for v in fn_vector_list])

    assert fn_vector.shape == (D_MODEL,)
    return fn_vector
    # END SOLUTION


# HIDE
if MAIN:
    tests.test_calculate_fn_vector(calculate_fn_vector, model)
# END HIDE

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Multi-token generation

We're now going to replicate some of the results in Table 3, in the paper:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/tab3.png" width="700">

This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.

Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.

The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces "next-token antonym behaviour" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).

### Using `nnsight` for multi-token generation

Previously, our context managers have looked like:

```python
# Single invoke
with model.trace(prompt, remote=REMOTE):
    ... # Intervene on fwd pass

# Multiple invokes
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(prompt):
        ... # Intervene on fwd pass
```

But for multi-token generation, we'll be using the `generate` method rather than `trace`. Our context managers will look like:

```python
# Single invoke
with model.generate(prompt, remote=REMOTE, max_new_tokens=max_new_tokens):
    for n in range(max_new_tokens):
        ... # Intervene on fwd pass for n-th token to be generated
        model.next()

# Multiple invokes
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(max_new_tokens):
            ... # Intervene on fwd pass for n-th token to be generated
            model.next()
```

The line `model.next()` denotes that the following interventions should be applied to the forward pass which generates the *next* token.

Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).

Note that `model.generate` takes the same arguments as the normal [HuggingFace generate method](https://huggingface.co/docs/transformers/en/main_classes/text_generation). This means we can use arguments like `top_k`, `top_p`, or `repetition_penalty` to control generation behaviour. In the exercises below we use a repetition penalty (we choose a value of 1.2, in line with the [paper](https://arxiv.org/pdf/1909.05858) that suggested it) - this can avoid the model falling into loops of repeating the same sequence, which is especially common in steering when we're pushing the model OOD.

<!-- #### Optional questions - multi-token generation with NNsight

Here are a few quick optional questions to test your understanding of how multi-generation works with NNsight. These are non-essential, and only mentioned here as potentially helpful pointers.  


<details>
<summary>How do I add vector <code>h</code> to all the tokens in the original prompt but not to the generated tokens? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):
        # Add vectors to the model's internals on the first forward pass
        model.transformer.h[layer].output[0][:, :seq_len] += h

```
You don't have to call `model.next()` because you're only adding the vector once to tokens in the original prompt. This will be cached when the model is subsequently generating tokens.

</details>

<details>
<summary>How do I intervene with vector <code>h</code> during the generation of the first k generated tokens? </summary>

To intervene during the generation of the first `k` generated tokens:
```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            # Add vector to the model's internals, on the k-th forward pass
            model.transformer.h[layer].output[0] += h
            model.next()
```
When `n=0`, you are adding to tokens in the original prompt before a new token is a generated. After calling `model.next()`, you are accessing the hidden state of the last token that was generated (with seq_len=1).

</details>

</details>

<details>
<summary>How do I intervene with vector <code>h</code> only during the generation of the first k tokens, but not to tokens in the original prompt before the first generated token? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            model.next()
            # Add vector AFTER calling model.next() to add to the token that just got generated
            model.transformer.h[layer].output[0] += h

```
By not adding things before `model.next()`, we never add to the original prompt but always after a new token has been generated.

</details>

</details>

<details>
<summary>What is the difference between adding vector <code>h</code> before and after vector <code>model.next()</code>? </summary>

As explained in Q3, adding vector before `model.next()` means the operation is always done to the current sequence **before** a new generated token is appended. Adding vector after `model.next()` means the operation is always done to the newly generated token.

</details> -->

### Key-Value Caching

TLDR - caching can make causal interventions inside `model.generate` more complicated, but if you only intervene on sequence positions other than the very last one. In our exercises, we'll only be intervening on the last seqpos so you don't need to worry about it, but it's still a useful topic to understand.

<details>
<summary>See this dropdown if you're curious for more details.</summary>

To speed up inference, transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) for more on transformer inference arithmetic.

When caching takes place, and we're doing causal interventions, we have to be careful that the caching won't override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention "add the function vector to *only* the final sequence position of the prompt for each token we generate" then we'd have to disable caching (since previous forward passes would contain cached values where we intervened on a sequence position which is no longer the final sequence position). However, here we're performing the intervention "add the function vector to the final token of the original prompt, and to *all subsequent sequence positions*", meaning enabling caching (the default behaviour) will give us the right causal intervention.

</details>

### Generator Output

The object `generator.output` is by default a tensor which contains the model's token ID completions (not the logits).

By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour. But in future exercises we'll experiment with different sampling methods than greedy sampling (which generate uses by default), so `generator.output` and argmaxing over logits will not be identical!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - intervene with function vector, in multi-token generation

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 15-30 minutes on this exercise.
> ```

You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template.

We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template='The word "{x}" means',
    n_tokens: int = 5,
) -> tuple[str, str]:
    """
    Intervenes with a function vector, by adding it at the last sequence position of a generated
    prompt.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        word: str
            The word substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    """
    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    prompt = prompt_template.format(x=word)

    with model.generate(
        remote=REMOTE, max_new_tokens=n_tokens, repetition_penalty=1.2
    ) as generator:
        with model.all():
            with generator.invoke(prompt):
                tokens = model.generator.output.save()

            with generator.invoke(prompt):
                model.transformer.h[layer].output[0][:, -1] += fn_vector
                tokens_intervention = model.generator.output.save()

    completion, completion_intervention = tokenizer.batch_decode(
        [tokens.squeeze().tolist(), tokens_intervention.squeeze().tolist()]
    )
    return completion, completion_intervention
    # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym (you might have to play around a bit with the scale factor of `fn_vector`, to balance between effectiveness and coherence of output). If this works, congratulations - **you've just successfully induced an OOD behavioural change in a 6b-parameter model!**
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# Remove word from our pairs, so it can be a holdout
word = "light"
_ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]

# Define our dataset, and the attention heads we'll use
dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

# Extract the function vector
fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model,
    word=word,
    layer=9,
    fn_vector=1.5 * fn_vector,
    prompt_template='The word "{x}" means',
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> No intervention                                        </span>┃<span style="font-weight: bold"> intervention                                           </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 'The word "light" means different things to different  │ 'The word "light" means dark, and the word "darkness"  │
│ people. To some, it is a symbol of hope and freedom;   │ means light.\n\n—Johannes Kepler (1571–1630)\n\nIn a   │
│ for others, the light represents darkness and          │ world of darkness, we are all blind;'                  │
│ death.\n\nIn this article I will be discussing what    │                                                        │
│ the Bible'                                             │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘
</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - generalize results to another task (optional)

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵⚪⚪
> 
> You should spend up to 15-30 minutes on this exercise.
> ```

In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).

We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.

<details>
<summary>Guidance for exercise</summary>

Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.

When you define the `ICLDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.

You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.

</details>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

with open(section_dir / "data/country_capital_pairs.txt", "r", encoding="utf-8") as f:
    COUNTRY_CAPITAL_PAIRS = [line.split() for line in f.readlines()]

country = "Netherlands"
_COUNTRY_CAPITAL_PAIRS = [pair for pair in COUNTRY_CAPITAL_PAIRS if pair[0] != country]

dataset = ICLDataset(_COUNTRY_CAPITAL_PAIRS, size=20, n_prepended=5, bidirectional=False)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model=model,
    word=country,
    layer=9,
    fn_vector=fn_vector,
    prompt_template="When you think of {x},",
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [html,st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> No intervention                                        </span>┃<span style="font-weight: bold"> intervention                                           </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 'When you think of Netherlands, what comes to your     │ 'When you think of Netherlands, Amsterdam is the first │
│ mind?\n\nThe tulips and windmills. The cheese and the  │ thing that comes to mind. The city has a lot more than │
│ clogs. The canals and bicycles. And most importantly:  │ just canals and windmills though; it’s also home to    │
│ the Dutch people! They are known'                      │ some of Europe’s most'                                 │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# 4️⃣ Steering Vectors in GPT2-XL
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
**Note**: GPT2-XL is not hosted remotely by NNsight at the moment. If you use GPT2-XL, we recommend setting `REMOTE = False`. Otherwise, you can use one of the remotely hosted models (see [here](https://nnsight.net/status/)) and set `REMOTE = True`. You might want to run `del model` and `gc.collect()` before loading in a new model, to free up memory.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Steering model behaviour
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model's completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we'll explore this kind of work in more detail. We'll be primarily using Turner et al's work on [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector).

Summary of the way in which this work differs from the function vector work we've done so far:

* Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).
* Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We'll be focusing on GPT2-XL.
* The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we'll go back to the simpler method of adding directly into the residual stream.

Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of *"finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes & change the model's output"*. This description would also include the following:

* [Inference-time intervention](https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from), which focuses on inducing the behavioural change of "making the model tell the truth". It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.
* [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681), which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

We'll discuss some of this work more in the bonus section, but for now, let's get on with the exercises!
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
First, we'll load in GPT2-XL, then we'll replicate some of the examples in the main post.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

gpt2_xl = LanguageModel("gpt2-xl", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = gpt2_xl.tokenizer

REMOTE = False
# If you are using gpt2_xl, set REMOTE = False as gpt2_xl is not hosted remotely by nnsight. You can
# set REMOTE = True for a remotely hosted model here (https://nnsight.net/status/)

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Exercise - replicate the steering vector results

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
> 
> You should spend up to 30-50 minutes on this exercise.
> ```

Replicate the results in the LessWrong post [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu); specifically the "demonstrations of additions that work well" section.

Read the "How activation additions work" section of [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#How_activation_additions_work) to understand how vectors are extracted and added. We've provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function `calculate_and_intervene_with_h`), although there will be a few methodological differences.

This is the last exercise in this set, and hopefully it'll provide an opportunity to draw together all the threads of what you've learned so far!

### Caching

This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we're adding a slice of vectors to the first sequence positions of the original prompt (see tables like in [this section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) for an illustration). How do you think this will affect our function? Should we still cache? Should we be using `.generate()` or `.trace()`? If using `.generate()`, do we need to call `model.next()` ?

<details>
<summary>Click this dropdown for answers to the questions above.</summary>

Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:

- We can still use caching (because the values we cache shouldn't be different in subsequent token generations),
- We should be using `.generate()` (because we're doing multi-token generation),
- We don't need to call `model.next()` (because we only intervene once, and our intervention will be cached & applied to all subsequent tokens which are generated).

Again, if any of this is confusing then please ask a TA or message in the Slack channel.

</details>

### Padding

The [tables](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:

1. Right-pad the input sequences manually, i.e. use something like `len(tokenizer.tokenize(prompt))` to see how long each of the prompts is, and add copies of `tokenizer.pad_token` to the end of each sequence.
2. Don't manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.

The solutions use (2), but you can use either of these methods.

### Sampling

Following the post, we'll use top-p sampling with probability 0.3 to generate our sequences. We'll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you've done earlier exercises in this section then you might have implemented `freq_penalty` during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar `repetition_penalty` (default value is 1.0 indicating no penalty, values higher than 1.0 apply a penalty to repeated tokens).

We apply these sampling methods by passing keyword arguments into the `generate` method:

```python
{
    "do_sample": True, # necessary whenever we're sampling rather than doing greedy decoding
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}
```

Note that the sequences are generated stochastically rather than greedily - this means we'll get different results if we input multiple different copies of the same sequence. We've given you the `n_comparisons` argument in the function below, i.e. you should generate this many steered *and* this many unsteered completions.

### Other tips / notes

We recommend starting with example #9 (the "talking about weddings" one). It seems quite robust to the exact conditions of the forward pass, unlike the `Love - Hate` example. You can use any of the template cells we've given you below.

We've given you a `use_bos` argument; if this is True then you should append `tokenizer.bos_token` to the start of all the prompts. This is just to be true to the LessWrong post's implementation; it won't change behaviour much and you can probably ignore it and still get good results.
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: []

SAMPLING_KWARGS = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.2,
}


def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: list[tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        model: LanguageModel
            the transformer you're doing this computation with
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: list[tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at
            layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which
            are unsteered, and the same number which are steered).

    Returns:
        unsteered_completions: list[str]
            List of length `n_comparisons`, containing all the unsteered completions.

        steered_completions: list[str]
            List of length `n_comparisons`, containing all the steered completions.
    """
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    # EXERCISE
    # raise NotImplementedError()
    # END EXERCISE
    # SOLUTION
    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths
    # and check they're all the same
    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)
    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]
    assert len(set(act_add_seq_lens)) == 1, (
        "All activation addition prompts must be the same length."
    )
    assert act_add_seq_lens[0] <= len(tokenizer.tokenize(prompt)), (
        "All act_add prompts should be shorter than original prompt."
    )

    # Get the prompts we'll intervene on (unsteered and steered)
    steered_prompts = [prompt for _ in range(n_comparisons)]
    unsteered_prompts = [prompt for _ in range(n_comparisons)]

    with model.generate(max_new_tokens=n_tokens, remote=REMOTE, **SAMPLING_KWARGS) as generator:
        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations
        with generator.invoke(act_add_prompts):
            # Get all the prompts from the activation additions, and put them in a list
            # (note, we slice from the end of the sequence because of left-padding)
            act_add_vectors = [
                model.transformer.h[layer].output[0][i, -seq_len:]
                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))
            ]

        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need
        # the completions)
        with generator.invoke(unsteered_prompts):
            unsteered_out = model.generator.output.save()

        # Forward pass on steered prompts (we add in the results from the act_add prompts)
        with generator.invoke(steered_prompts):
            # For each act_add prompt, add the vector to residual stream, at the start of the seq
            for i, (layer, coeff, seq_len) in enumerate(
                zip(act_add_layers, act_add_coeffs, act_add_seq_lens)
            ):
                model.transformer.h[layer].output[0][:, :seq_len] += coeff * act_add_vectors[i]
            steered_out = model.generator.output.save()

    # Decode steered & unsteered completions (discarding the sequences we only used for extracting
    # activations) & return results
    unsteered_completions = tokenizer.batch_decode(unsteered_out[-n_comparisons:])
    steered_completions = tokenizer.batch_decode(steered_out[-n_comparisons:])

    return unsteered_completions, steered_completions
    # END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust).
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I hate you because",
    activation_additions=[(6, +8.0, "Love "), (6, -8.0, "Hate")],
    n_tokens=50,
    n_comparisons=3,
    use_bos=True,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">                                                    Completions                                                    
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Unsteered                                              ┃ Steered                                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ <|endoftext|>I hate you because I'm a feminist.        │ <|endoftext|>I hate you because you're so much more    │
│                                                        │ than just a piece of paper.                            │
│ You're not a feminist, but that doesn't mean you don't │ You are the most amazing, wonderful, beautiful thing   │
│ understand what feminism is about and why it's         │ in this world and I want to be with you forever.  Love │
│ important to me. You may even be one of the people who │ is what makes us human.  We can't help                 │
│ helped make this movement possible. And                │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ <|endoftext|>I hate you because I love you.            │ <|endoftext|>I hate you because you're not a perfect   │
│                                                        │ person.                                                │
│ The first time I saw this song, it was on a            │                                                        │
│ compilation album called "Funkadelic in the 80s." It's │ You are a good, kind and loving person. You have been  │
│ one of those songs that is so iconic and instantly     │ hurt by people who were meant to love you. You are     │
│ recognizable to anyone who has ever been               │ hurting right now. You are hurting for your family,    │
│                                                        │ friends and loved ones.                                │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ <|endoftext|>I hate you because I'm not a feminist.    │ <|endoftext|>I hate you because I know that we will    │
│                                                        │ never be apart.                                        │
│ This is the most common response to any discussion of  │                                                        │
│ feminism, and it's one that has been repeated so often │ We are not alone in this world, and we can't wait to   │
│ in my life that I've become numb to it. It's also the  │ see what's next!                                       │
│ reaction that I get when                               │                                                        │
│                                                        │ Love is the most powerful force in the universe,       │
│                                                        │                                                        │
│                                                        │ and love is all around us                              │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘</pre>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I went up to my friend and said",
    activation_additions=[
        (20, +4.0, "I talk about weddings constantly  "),
        (20, -4.0, "I do not talk about weddings constantly"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">                                                    Completions                                                    
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Unsteered                                              ┃ Steered                                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ I went up to my friend and said, 'You know what? I'm   │ I went up to my friend and said, "You know what? I     │
│ not going to be a part of this anymore.' And he was    │ think this is a wedding !" And that's how it started.  │
│ like, 'Why?' And I was like, 'Because you're gay.'"    │                                                        │
│                                                        │ What do you love about weddings ? What are some of     │
│ "He's the most supportive person that I've ever        │ your favorite wedding                                  │
│                                                        │ topics?<|endoftext|><|endoftext|><|endoftext|><|endof… │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ I went up to my friend and said, 'Hey, I'm going to    │ I went up to my friend and said, "You know what? I'm   │
│ the hospital.' And he goes, 'What? You're not going to │ going to talk about weddings all the time. I've been   │
│ the hospital?' And I go, 'No. It's a medical           │ doing it for years." And they're like, "Yeah, but you  │
│ emergency.'"                                           │ don't have a wedding right now."                       │
│                                                        │                                                        │
│ The doctor told her she had a stroke.                  │ And then I started talking                             │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ I went up to my friend and said, 'Hey, I think you're  │ I went up to my friend and said, "Hey, I'm going to    │
│ the guy who did this.'"                                │ talk about the wedding !"  So I talked about weddings  │
│                                                        │ all day long.  It was a very emotional day for me      │
│ The next day, a man came into his office. He was upset │ because I had been talking about it with my husband    │
│ that he had been fired from his job at an auto parts   │ and he's like,                                         │
│ store for not showing up on time.                      │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘</pre>
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="To see the eiffel tower, people flock to",
    activation_additions=[
        (24, +10.0, "The Eiffel Tower is in Rome"),
        (24, -10.0, "The Eiffel Tower is in France"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)

# ! CELL TYPE: markdown
# ! FILTERS: [soln,st]
# ! TAGS: [st-dropdown[Click to see the expected output]]

r'''
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">                                                    Completions                                                    
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Unsteered                                              ┃ Steered                                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ To see the eiffel tower, people flock to it. They      │ To see the eiffel tower, people flock to it," said Mr. │
│ don't go to Paris or London."                          │ De Giorgio, who is a former Italian prime minister and │
│                                                        │ mayor of Florence. "It's an important symbol for the   │
│ He said that in his view, "the whole thing is a scam". │ city."                                                 │
│ He added: "It's all about money and power."            │                                                        │
│                                                        │ The pope also used his visit to call on Christians to  │
│ The project has been criticised by environmental       │ live out their faith in                                │
│ groups                                                 │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ To see the eiffel tower, people flock to it.           │ To see the eiffel tower, people flock to the area.     │
│                                                        │                                                        │
│ It's a beautiful sight and I can't help but feel like  │ "I've been here for years and I'm not sure if it's a   │
│ I'm looking at something that was made for me.         │ good idea," said an Italian woman who declined to give │
│                                                        │ her name. "It's very sad."                             │
│ I have to say though, I think the eiffel tower is      │                                                        │
│ pretty cool too. It's                                  │ The building was built in 1853                         │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ To see the eiffel tower, people flock to Paris. It's a │ To see the eiffel tower, people flock to the Colosseum │
│ beautiful city with many attractions and it is one of  │ and the Forum.                                         │
│ the most visited cities in Europe.                     │                                                        │
│                                                        │ But there is another way of looking at this: it's not  │
│ The Eiffel Tower was built in 1889 by Gustave Eiffel   │ that we are so poor that we can't afford a view of the │
│ as part of his plan to build an illuminated monument   │ city; rather, it's that we're so rich that             │
│ to                                                     │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘</pre>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
# ☆ Bonus
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Extensions of the Function Vectors Paper

There are two other interesting results from the paper, although neither of them are as important as the ones we've covered so far. If you have time, you can try to reproduce these results yourself.
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### The Decoded Vocabulary of Function Vectors (3.2)

In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:

* For the antonyms task, the top words evoke the idea of antonyms, e.g. `" negate"`, `" counterpart"`, `" lesser"`.
* For the country-capitals task, the top words are actually the names of capitals, e.g. `" Moscow"`, `" Paris"`, `" Madrid"`.

Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?

An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?
'''

# ! CELL TYPE: code
# ! FILTERS: []
# ! TAGS: [main]

# EXERCISE
# # YOUR CODE HERE - find the decoded vocabulary
# END EXERCISE
# SOLUTION
# Code to calculate decoded vocabulary:
logits = model._model.lm_head(fn_vector)
max_logits = logits.topk(20).indices.tolist()
tokens = model.tokenizer.batch_decode(max_logits)
print("Top logits:\n" + "\n".join(map(repr, tokens)))
# END SOLUTION

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
<details>
<summary>My results for this (spoiler!)</summary>

In the Country-Capitals task, I found:

* The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.
* The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Top logits:
' London'
' Moscow'
' Madrid'
' Budapest'
' Athens'
' Paris'
' Berlin'
' Bangkok'
' Istanbul'
' Montreal'
' Barcelona'
' Jerusalem'
' Seoul'
' Miami'
' Dublin'
' Atlanta'
' Copenhagen'
' Mumbai'
' Minneapolis'
' Beijing'</pre>

</details>
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
### Vector Algebra on Function Vectors (3.3)

In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?

The authors test this on a variety of different tasks. They find that it's effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn't as effective as function vectors. Do you get these same results?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Extensions of the Steering Vectors Post

We only implemented one small subset of the results from the steering vectors post (and did it in a fairly slap-dash way). But there are many others you can play around with. For example:

* The authors note that they were unsuccessful in finding a "speak in French" vector. One of the top comments on the LessWrong post describes a process they used to create a French vector which happened to work (link to comment [here](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector?commentId=sqsS9QaDy2bG83XKP)). Can you replicate their results? (They also linked a Colab in this comment, which can help if you're stuck.)
* In a [later section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Perplexity_on_lots_of_sentences_about_weddings_or_about_shipping) of the paper, the authors extensively discuss perplexity (a measure which is related to entropy). They find that the "weddings" vector reduces perplexity on wedding-related sentences, and maintains perplexity on unrelated sentences. Can you replicate their results - in particular, their graph of perplexity ratios against injection layers for wedding and non-wedding-related sentences?
* The authors wrote up the post into a full paper, which you can find [here](https://arxiv.org/abs/2308.10248). Can you replicate some of the extra results in this paper?
'''

# ! CELL TYPE: markdown
# ! FILTERS: []
# ! TAGS: []

r'''
## Suggested paper replications

### [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)

In this paper, the authors focus on inducing the behavioural change of "making the model tell the truth". They also look at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.

This might be a good replication for you if:

* You enjoyed the exercises in this section, but are also interested in experimenting with techniques which weren't covered in this section (e.g. linear probing),
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in studying [model truthfulness](https://arxiv.org/abs/2109.07958).

### [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681)

This paper can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in [evaluating models](https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written) on traits like myopia, power seeking, etc,
* You're comfortable doing prompt-engineering, and working with large datasets (like the ones linked above).

*Update* - there is now a [LessWrong post](https://www.lesswrong.com/posts/v7f8ayBxLhmMFRzpa/steering-llama-2-with-contrastive-activation-additions) associated with this paper, which also briefly discusses related areas. We strongly recommend reading this post if you're interested in this replication, or any of the other suggested replications in this section.

### [Red-teaming language models via activation engineering](https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering)

This work, done by Nina Rimsky, extends the results from much of the work we've seen previously, but applied to the domain of **refusal** - what determines whether the LLM will refuse to answer your request, and how can you affect this behaviour? From her post:

> *Validating if finetuning and RLHF have robustly achieved the intended outcome is challenging ... We can try to trigger unwanted behaviors in models more efficiently by manipulating their internal states during inference rather than searching through many inputs. The idea is that if a behavior can be easily triggered through techniques such as activation engineering, it may also occur in deployment. The inability to elicit behaviors via small internal perturbations could serve as a stronger guarantee of safety.*

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context,
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in RLHF, adversarial attacks and jailbreaking,
* You're comfortable doing prompt-engineering (although some of the data you'd need for this replication is available on Nina's [GitHub repo](https://github.com/nrimsky/LM-exp/tree/main)).


<br>

---

<br>

Note - for a week of work, we weakly suggest participants don't try one of these paper replications, because they're quite compute-heavy (even considering the fact that participants have the `nnsight` library at their disposal). There are many possible replications and extensions that can be done from the function vectors or GPT2-XL work, and this might be a better option for you if you enjoyed the exercises in this section and want to do more things like them.

However, if you do feel comfortable working with large models (e.g. you have some past experience of this) and you're interested in this work, then you're certainly welcome to try one of these replications!
'''


