Directory Structure:

└── ./
    ├── demos
    │   └── conftest.py
    ├── docs
    │   ├── source
    │   │   ├── _static
    │   │   │   └── model_properties_table_interactive.html
    │   │   ├── content
    │   │   │   ├── news
    │   │   │   │   └── release-2.0.md
    │   │   │   ├── citation.md
    │   │   │   ├── contributing.md
    │   │   │   ├── gallery.md
    │   │   │   ├── getting_started_mech_interp.md
    │   │   │   ├── getting_started.md
    │   │   │   ├── special_cases.md
    │   │   │   └── tutorials.md
    │   │   └── conf.py
    │   └── make_docs.py
    ├── easy_transformer
    │   └── __init__.py
    ├── tests
    │   ├── acceptance
    │   │   ├── test_activation_cache.py
    │   │   ├── test_evals.py
    │   │   ├── test_hook_tokens.py
    │   │   ├── test_hooked_encoder_decoder.py
    │   │   ├── test_hooked_encoder.py
    │   │   ├── test_hooked_transformer.py
    │   │   ├── test_multi_gpu.py
    │   │   └── test_tokenizer_special_tokens.py
    │   ├── integration
    │   │   ├── test_attention_mask.py
    │   │   ├── test_cache_hook_names.py
    │   │   ├── test_cache_pos_slice.py
    │   │   ├── test_create_hooked_encoder.py
    │   │   ├── test_cross_entropy_loss.py
    │   │   ├── test_d_vocab.py
    │   │   ├── test_grouped_query_attention.py
    │   │   ├── test_head_detector.py
    │   │   ├── test_hooks.py
    │   │   ├── test_kv_cache.py
    │   │   ├── test_left_padding.py
    │   │   ├── test_loading_from_pretrained.py
    │   │   ├── test_match_huggingface.py
    │   │   ├── test_only_tokenizer.py
    │   │   ├── test_prepend_bos.py
    │   │   ├── test_start_at_layer.py
    │   │   ├── test_stop_at_layer.py
    │   │   ├── test_tokenization_methods.py
    │   │   └── test_utils_tokens.py
    │   ├── manual_checks
    │   │   ├── manual_checks_type_annotations.py
    │   │   └── manual_checks_typing.py
    │   └── unit
    │       ├── components
    │       │   ├── mlps
    │       │   │   ├── test_can_be_used_as_mlp.py
    │       │   │   ├── test_gated_mlp.py
    │       │   │   ├── test_mlp.py
    │       │   │   └── test_moe.py
    │       │   ├── test_abstract_attention.py
    │       │   └── test_attention.py
    │       ├── factored_matrix
    │       │   ├── test_constructor.py
    │       │   ├── test_get_item.py
    │       │   ├── test_multiply_by_factored_matrix.py
    │       │   ├── test_multiply_by_matrix.py
    │       │   ├── test_multiply_by_scalar.py
    │       │   ├── test_multiply_by_vector.py
    │       │   └── test_properties.py
    │       ├── factories
    │       │   ├── test_activation_function_factory.py
    │       │   └── test_mlp_factory.py
    │       ├── pretrained_weight_conversions
    │       │   └── test_neo.py
    │       ├── utilities
    │       │   └── test_devices.py
    │       ├── test_hook_points.py
    │       ├── test_hooked_root_module.py
    │       ├── test_hooked_transformer_config.py
    │       ├── test_loading_from_pretrained_utilities.py
    │       ├── test_make_docs.py
    │       ├── test_next_sentence_prediction.py
    │       ├── test_split_qkv.py
    │       ├── test_svd_interpreter.py
    │       ├── test_use_attn_result.py
    │       └── test_utils.py
    ├── transformer_lens
    │   ├── components
    │   │   ├── mlps
    │   │   │   ├── can_be_used_as_mlp.py
    │   │   │   ├── gated_mlp_4bit.py
    │   │   │   ├── gated_mlp.py
    │   │   │   ├── mlp.py
    │   │   │   └── moe.py
    │   │   ├── __init__.py
    │   │   ├── abstract_attention.py
    │   │   ├── attention.py
    │   │   ├── bert_block.py
    │   │   ├── bert_embed.py
    │   │   ├── bert_mlm_head.py
    │   │   ├── bert_nsp_head.py
    │   │   ├── bert_pooler.py
    │   │   ├── embed.py
    │   │   ├── grouped_query_attention.py
    │   │   ├── layer_norm_pre.py
    │   │   ├── layer_norm.py
    │   │   ├── pos_embed.py
    │   │   ├── rms_norm_pre.py
    │   │   ├── rms_norm.py
    │   │   ├── t5_attention.py
    │   │   ├── t5_block.py
    │   │   ├── token_typed_embed.py
    │   │   ├── transformer_block.py
    │   │   └── unembed.py
    │   ├── factories
    │   │   ├── activation_function_factory.py
    │   │   └── mlp_factory.py
    │   ├── pretrained
    │   │   ├── weight_conversions
    │   │   │   ├── __init__.py
    │   │   │   ├── bert.py
    │   │   │   ├── bloom.py
    │   │   │   ├── coder.py
    │   │   │   ├── gemma.py
    │   │   │   ├── gpt2.py
    │   │   │   ├── gptj.py
    │   │   │   ├── llama.py
    │   │   │   ├── mingpt.py
    │   │   │   ├── mistral.py
    │   │   │   ├── mixtral.py
    │   │   │   ├── nanogpt.py
    │   │   │   ├── neel_solu_old.py
    │   │   │   ├── neo.py
    │   │   │   ├── neox.py
    │   │   │   ├── opt.py
    │   │   │   ├── phi.py
    │   │   │   ├── phi3.py
    │   │   │   ├── qwen.py
    │   │   │   ├── qwen2.py
    │   │   │   ├── qwen3.py
    │   │   │   └── t5.py
    │   │   └── __init__.py
    │   ├── utilities
    │   │   ├── __init__.py
    │   │   ├── activation_functions.py
    │   │   ├── addmm.py
    │   │   ├── attention.py
    │   │   └── devices.py
    │   ├── __init__.py
    │   ├── ActivationCache.py
    │   ├── BertNextSentencePrediction.py
    │   ├── evals.py
    │   ├── FactoredMatrix.py
    │   ├── head_detector.py
    │   ├── hook_points.py
    │   ├── HookedEncoder.py
    │   ├── HookedEncoderDecoder.py
    │   ├── HookedTransformer.py
    │   ├── HookedTransformerConfig.py
    │   ├── loading_from_pretrained.py
    │   ├── past_key_value_caching.py
    │   ├── patching.py
    │   ├── SVDInterpreter.py
    │   ├── train.py
    │   └── utils.py
    ├── further_comments.md
    └── README.md



---
File: /demos/conftest.py
---

def pytest_collectstart(collector):
    """Ignore several mimetypes when comparing notebooks."""
    if collector.fspath and collector.fspath.ext == ".ipynb":
        collector.skip_compare += (
            "text/html",
            "application/javascript",
            "application/vnd.plotly.v1+json",  # Plotly
        )



---
File: /docs/source/_static/model_properties_table_interactive.html
---

<!DOCTYPE html>
<html>

<head>
	<title>TransformerLens models</title>
	<script src="https://cdn.jsdelivr.net/npm/ag-grid-community/dist/ag-grid-community.min.js"></script>
	<style>
		header {
			background-color: #f4f4f4;
			padding: 10px;
			text-align: left;
		}

		body,
		html {
			margin: 0;
			padding: 0;
			width: 100%;
			/* Ensure the body takes up full viewport width */
		}

		.ag-theme-alpine {
			height: 80%;
			width: 100%;
			/* This should already keep the grid within the page width */
			box-sizing: border-box;
			/* Ensure padding and borders are included in the width */
		}

		.ag-paging-panel {
			/* Aligns children (the pagination buttons) to the start of the container, i.e., left side */
			justify-content: flex-start;
			align-items: center;
		}
	</style>
</head>

<body>
	<header>
		Model table for <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a>. Source code: <a
			href="https://github.com/mivanit/transformerlens-model-table">github.com/mivanit/transformerlens-model-table</a>.
		Hover a cell to view full text, left click to copy to clipboard, right click to open contents in new tab.
	</header>

	<div id="modelTable" class="ag-theme-alpine"></div>

	<script>
		const CLICK_TO_COPY_EMOJI = String.fromCodePoint(0x1F446) + String.fromCodePoint(0x1F4CB);
		// read a jsonl file
		async function fetchJsonlData(url) {
			const response = await fetch(url);
			const text = await response.text();
			return text.trim().split('\n').map(line => JSON.parse(line));
		}

		async function fetchVersion() {
			try {
				const response = await fetch('model_table.version');
				// load the contents of the file as json
				return response.json().then(json => json.version);
			} catch (error) {
				console.error('Could not fetch version: ', error);
				return 'unknown';
			}
		}

		// fancy cell rendering -- hover/copy/open the data, make it emojis if its too long
		function longCellRenderer(params) {
			// Create the div element
			var div = document.createElement('div');
			div.title = params.value; // Set the title text to the full text
			div.style.cursor = 'pointer'; // Ensure the cursor style is set
			// If its too long, make it emojis
			if (params.value !== null) {
				if (params.value.length > 50) {
					div.textContent = CLICK_TO_COPY_EMOJI;
					div.style.cssText = 'font-size: 20px; display: flex; justify-content: center; align-items: center; background-color: #f4f4f4; border: 1px solid #d4d4d4; border-radius: 5px; height: 30px; width: 60px; cursor: pointer;';
				}
			}

			// Add click event listener to copy text to the clipboard
			div.onclick = function () {
				navigator.clipboard.writeText(params.value).then(function () {
					console.log('Successfully copied to clipboard');
				}).catch(function (err) {
					console.error('Could not copy text to clipboard: ', err);
				});
			};

			// On right click, open a new plain text tab whose contents are the cell's value
			div.oncontextmenu = function (event) {
				event.preventDefault(); // Prevent the default context menu from appearing
				const newWindowName = params.node.data['name.default_alias'] + ' : ' + params.colDef.headerName;
				const newWindow = window.open('', newWindowName);
				// Set the title of the page to the rows "name.default_alias" and the column's header
				newWindow.document.write('<title>' + newWindowName + '</title>');
				// Set the contents of the new window to the cell's value
				newWindow.document.write('<pre>' + params.value + '</pre>');
				// newWindow.document.close();
				return false; // Prevent the default context menu from appearing
			};

			// Return the div as the cell's DOM
			return div;
		}

		// huggingface model name cell renderer -- add a link to the model page
		function hf_link_cell_renderer(params) {
			// Create the div element
			var div = document.createElement('div');
			div.style.cursor = 'pointer'; // Ensure the cursor style is set
			// If its too long, make it emojis
			if (params.value !== null) {
				// make a link with the text of the value, pointing to https://huggingface.co/{x}
				// make it open in a new tab
				div.innerHTML = `<a href="https://huggingface.co/${params.value}" target="_blank" rel="noopener noreferrer">${params.value}</a>`;
				div.style.cssText = 'cursor: pointer;';
			}
			// Return the div as the cell's DOM
			return div;
		}

		function value_formatter(params) {
			if (params.value === null) {
				return '';
			}
			return typeof params.value === 'object' ? JSON.stringify(params.value) : params.value;
		}

		async function setupGrid() {
			const version = await fetchVersion();
			document.querySelector('header').innerHTML = `Interactive Model table for <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a> v${version}.  Hover a cell with [${CLICK_TO_COPY_EMOJI}] to view full text, left click to copy to clipboard, right click to open contents in new tab.`;

			// read the data
			const rowData = await fetchJsonlData('model_table/data.jsonl');
			const columnGroups = {};

			// create the column definitions
			Object.keys(rowData[0]).forEach(key => {
				// if key ends with "__", then ignore it (raw tensor shapes)
				if (key.endsWith('__')) {
					return;
				}
				// treat dot separated keys as column groups
				const keyParts = key.split('.');
				if (keyParts.length === 2) {
					// column in a group
					const groupName = keyParts[0];
					const fieldName = keyParts[1];
					// init an empty group if it doesn't exist
					if (!columnGroups[groupName]) {
						columnGroups[groupName] = {
							headerName: groupName,
							children: [],
						};
					}
					// add the column to the group
					const columnDef = {
						headerName: fieldName,
						field: key,
						// if it's an object, stringify it
						valueFormatter: value_formatter,
						// only show the first child if there are many (we modify this later in some special cases)
						columnGroupShow: columnGroups[groupName].children.length < 1 ? null : 'open',
						// hacky width calculation (doesn't work great)
						width: Math.min(Math.max(130, key.length * 5), 500),
						// numeric if it's a number
						type: typeof rowData[0][key] === 'number' ? 'numericColumn' : 'textColumn',
					};

					// special renderer for tensor shapes
					if (groupName === 'tensor_shapes') {
						columnDef.cellRenderer = longCellRenderer;
					}

					// special renderer for huggingface model name
					if (groupName === 'name' && fieldName === 'huggingface') {
						columnDef.cellRenderer = hf_link_cell_renderer;
					}

					columnGroups[groupName].children.push(columnDef);
				} else {
					// solo column
					const columnDef = {
						headerName: key,
						field: key,
						valueFormatter: value_formatter,
					};
					// special renderer for full cfg
					if (key === 'config') {
						columnDef.cellRenderer = longCellRenderer;
					}
					columnGroups[key] = columnDef;
				}
			});

			// special modifications
			columnGroups['model_type'].width = 130;
			columnGroups['config'].width = 100;
			columnGroups['tensor_shapes'].width = 200;
			// open these groups by default
			columnGroups['tensor_shapes'].openByDefault = true;
			columnGroups['cfg'].openByDefault = true;

			const columnDefs = Object.values(columnGroups);

			// create the grid
			const gridOptions = {
				columnDefs: columnDefs,
				rowData: rowData,
				rowSelection: 'multiple',
				// customize pagination
				pagination: true,
				paginationPageSize: 500,
				paginationPageSizeSelector: [10, 25, 50, 100, 500, 1000],
				enableCellTextSelection: true,
				enableBrowserTooltips: true,
				// default column options
				defaultColDef: {
					resizable: true,
					filter: true,
					// always show the floating filter
					floatingFilter: true,
					// disable filter hamburger menu (for space)
					menuTabs: [],
				},
				// we assume dots are groups, don't treat them as field notation
				suppressFieldDotNotation: true,
				// define column type to avoid warning
				columnTypes: {
					textColumn: {},
				},
				// this disables animations, but makes the horizontal scrolling not painfully slow
				domLayout: 'print',
			};

			// create the grid
			new agGrid.createGrid(document.getElementById('modelTable'), gridOptions);
		}

		setupGrid();
	</script>
</body>

</html>


---
File: /docs/source/content/news/release-2.0.md
---

# TransformerLens 2.0

I am very happy to announce TransformerLens now has a 2.0 release! If you have been using recent versions of TransformerLens, then the good news is that not much has changed at all. The primary motivation behind this jump is to transition the project to strictly following semantic versioning as described [here](https://semver.org/). At the last minute we did also remove the recently added HookedSAE, so if you had been using that, I would direct you to Joseph Bloom’s [SAELens](http://github.com/jbloomAus/SAELens). Bundled with this major version change are also a handful of internal modifications that only affect contributors.

## First, an introduction

My name is Bryce Meyer. I am a software engineer, with just a little under 15 years of professional experience with a wide range of expertise from embedded computing to API design. Within the last couple years I have gotten more and more involved in ML, and especially AI safety within the last nine months. At the end of March, I was chatting a bit with Joseph Bloom, and he asked me if I might be interested in taking on the role as primary maintainer of TransformerLens. I have been doing so on a part time basis since the beginning of April, and so far I am pretty happy with the progress that has been made.

In this first month, with the help of the many kind contributors to TransformerLens we have managed to address every single pull request, many of which had been awaiting reply for quite some time. In total around 30 pull requests have been merged with contributions from around 20 people. Within those PRs a number of features were added, including, but not limited to support for Mixtral, LLaMA 3, 4-bit Quantized LLaMA, and HookedSAETransformer, a brand new class to splice sparse autoencoders into HookedTransformer.

I have two primary immediate goals for my time as primary maintainer of this project. The first is to position TransformerLens in a way where it is approachable to as many people as possible, while also remaining powerful for people who are pushing the limits of the field. My second goal is to find ways to make the code base for this project easier to manage for the future, as the development and availability of LLMs continues to accelerate.

I feel that this project has a massive amount of momentum at the moment, and I am hoping to carry that over into the future with new features. I have a software engineering background, not research, and I know I need to talk a lot with users to ensure the library is meeting their needs. I have personally spoken with around a dozen people in the community on their experience with TransformerLens, and what they may want to see happen in the future. If you have not spoken with me, but you would like to, please [make an appointment](https://calendly.com/bryce-c7e/30min). I am curious to hear from anyone who is using this tool, from absolute beginner to complete experts.

## Adopting Semantic Versioning

In the last month, a lot has changed with TransformerLens. Not only have a lot of changes been made to the code, but ideas of how the project will be managed are also evolving. The biggest change in the management of the project is the previously mentioned adoption of Semantic Versioning. Previously, the project had not been officially managed under Semantic Versioning, and there were instances where compatibility was not maintained through the 1.x branch. Going forward, API changes will be managed strictly, and in a way that maintains compatibility through major versions. If you are starting a project today using TransformerLens 2.0, then you can be rest assured that you will be able to upgrade your code all the way through the 2.x branch without worrying about needing to make changes to your code. For full details of how Semantic Versioning will affect TransformerLens, please see the appendix.

## Deprecations

There are right now two deprecations in the code base. The parameter `move_model` in `ActivationCache.to`, and the function `cache_all` in `hook_points`. To keep things simple for the change to semantic versioning, they will be remaining. However, if you are using them, then make sure to adapt your code right away. They will be removed in 3.0. Along with that, anything new that is marked deprecated in 2.x will also be removed when the next major version comes around.

Whenever something new does become deprecated, it will also be prominently noted in the release notes to make sure these sorts of things do not slip by. In my previously mentioned scenario where a key was renamed, in the future a situation like this will be handled by changing the code to the new key, but then persisting the deprecated old key pointing at the new key. This will allow anyone relying on that key to continue to do so without interruption.

However, I would still encourage everyone to periodically check release notes to make sure they are keeping an eye out for when things become deprecated. I don’t imagine it will happen often, but it will happen. Keeping an eye on it will save a lot of trouble in the future when moving from one major release to the next.

## Roadmap

There are three primary timeframes that I am approaching the current development plans of TransformerLens.

### Immediate - within the next month

At the moment, TransformerLens is in a state where all pull requests are being addressed quickly, but the issue tracker is still full of items that have not been addressed. The first thing to be done now is to go through all issues, categorize them, and address anything that is easy to address. Once that is done, both issues and pull requests will be up to date, and should remain that way going forward.

### Mid-term - within the next 3 months

Please note that the below is a draft roadmap. We are very happy to change our prioritization if user feedback surfaces other issues.

#### Performance

One of the first new items to be improved in TransformerLens is general performance. This will be achieved in a couple ways. The first involves diagnosing various areas in the code where memory leaks may be occurring. It seems like there are a number of places where references are not properly being released, thus causing garbage collection to not run correctly. If we can identify these places, and find proper ways to run garbage collection, we should be able to improve overall performance a lot, especially when dealing with larger models.

The second performance task will be exploring ways to improve the ability to batch processes. I have already had code shared with myself that seems to work well at batching just about anything together in a very general way. There is also a separate volunteer working on going through said code and finding a good implementation to add to TransformerLens.

#### Streamlining Adding New Models

Improving the model submission process is another item to be addressed in the near future. It may be a good use of time to have some open discussions on this, but I do think this needs to be improved. In my discussions these last few weeks, I have found two primary problems involving models. The first is a general confusion among a good number of people on how they would go about adding a model to TransformerLens. The second problem is ensuring that the logits calculated within TransformerLens match HuggingFace. The goal is to solve both of these problems by systematizing the submission process (e.g. with good tutorials), and requiring that all new models submitted to the project have calculated logits to show that the models they are submitting match HuggingFace. This requirement at the moment would be quite a bit to ask for contributors. In order to alleviate that problem, we will build a little tool that will automatically calculate these logits, and spit out a table of said logits to be submitted with the PR. This would also give us the ability to snapshot said logits, store them in the repo, and periodically regenerate them to make sure cumulative changes to the code base have not affected these values.

### Long-term - within the next year

#### Model Testing

Finding a way to create more robust tests with models integrated is a pretty big item that has been discussed. This is already implemented for the smaller models in each family, but is hard for model families like LLaMA where even the smallest is 7B. A lot of thoughts have been thrown around on this topic, but our best guess for a reasonable solution is to create an untrained small version of the model on HuggingFace (eg randomly initialized weights) and to verify that we can load that in. The resulting tests would not be accurate in the sense that using the full model would be, but it would allow testing for consistency on a smaller sample size of the larger model, and thus allow for the ability to test code against those more bite sized models. If we can find a successful way to go about handling this, then this could turn into a resource available for a lot of other projects to allow people to write efficient integration tests.

#### Model Integration

Making it easier for TransformerLens to handle a large range of models is another long term project that is at the moment a very hard problem to solve. Doing so, however, will be key to ensuring that the library is more future-proof. There have been many ideas put out of how to solve this, and this seems to be a topic that a lot of people have very strong opinions on. Most likely, there will need to be a handful of roundtable discussions on this to find the best solution. 

One of the ideas is to have a generalized wrapper that can take in a model from HuggingFace. Another idea is to create a way to allow TransformerLens to have plugins, so addition of models can be handled outside of the main project, and people can publish compatibility with new models themselves without having to put them into the main project. Finally, there is an idea to keep the submission within TransformerLens as is, but to overhaul the way they are configured so that code can be shared more across models, and something like configuration composition can then be utilized to allow for common cases to be tested in isolation, and thus more rapidly allowing new settings to be accepted, and for people to attempt to configure models themselves without having to have the configuration itself in the repo. 

It is very likely that none of these current ideas will end up being the solution, but we do need to come up with a solution. In my discussions with the community, one of the most common pain points was not having model compatibility. Up to now, it has been managed relatively well, but we still end up taking some time to accept new models, and there are a lot of models that TransformerLens does not support. This problem is only going to grow exponentially as the amount of available models grows as the whole field explodes.

## Contributors

This next section is only relevant to contributors, so if anyone is reading this who is only using TransformerLens as a tool, then you can skip this section. 

### New Dev Branches

There have been two new branches setup. One with the last release of 1.x, and another that will act as the current active development branch. The vast majority of pull requests should be made to the new dev branch. The reason for doing this is due to a potential mismatch between docs and the last release. Previously, all pull requests were put into the main branch, which caused the docs to be generated This meant that there were quite a few instances where the docs referenced features and functions that had not yet been released to people installing via pip. 

From now on, dev will represent the most up to date version of the code with the vast majority of pull requests going to dev. The old main branch will only be updated when a release is ready to be sent out. Releases will be sent out when there are enough changes in dev to justify a release, and when bugs are found in the current release with PRs fixing those bugs going directly into main with an immediate release following.

### Integration Tests

The existing unit tests have been split out into two main groups. Those groups are now called integration tests, and, once again, unit tests. A lot of the existing unit tests were testing various pieces of the code base, and outside resources working together. Traditionally, unit tests should test everything in absolute isolation. That means that if the code is working with other parts of the code base, or outside resources, those pieces should be mocked out, and spied on to absolutely control all input and output of the functions, including side effects of said functions. The goal of this is to be able to be absolutely certain that the logic in the tested functions is being tested in absolute isolation, so that bugs can be entirely ruled out within that functions logic.

A lot of the tests that would be categorized as integration, are still incredibly useful, but they are useful in a different way. To make both of them more useful in general, it makes sense to separate them. Unit tests are the first place to look when fixing a bug in a code base, but if they are dealing with outside resources, you cannot be absolutely certain that the bug does not originate from the outside resource. If the unit tests all test code in isolation, then if all of your unit tests pass, but your integration tests do not, then you can immediately rule out a whole bunch of places where bugs may be possible, and start looking in different places for bugs. Being able to separate the two test styles is going to make everyones lives a lot easier when it comes to maintaining the project.

### Test Coverage
 
The CI has recently been updated to publish a test coverage report. The overall coverage of the project should be improved. If anyone would like to begin improving the coverage, that could be a great way to start getting involved. There are quite a few parts of the code base that have no coverage. Reviewing that report, and finding a place to write a good, meaningful test is a great way to get started, and it should be easier than ever to do so. I have a handful of unit tests that I would like to have written that will improve coverage substantially, so if anyone would like to volunteer to take on some of those, let me know and I will be happy to point you in the right direction. 

### Components Refactor

This is the biggest change to the actual code base in this shift to 2.0. The whole components.py file has been removed in favor of a full module with individual files for each class that was previously in that file. The file was approaching 3000 lines with 18 distinct classes within it. There was no order to it either, since a lot of components were interdependent on each other, and it thus ended up being ordered from least dependent to most dependent. Now, everything has its own file, and every class is really easy to find in case of needing to reference anything. The refactor has been done in a way where no code needed to be changed anywhere else in the project, or outside the project. If you have been doing something like `import MLP from transformer_lens.components`, that will work, and continue to work exactly the same.

## Conclusion

Thank you very much for taking the time to read through this. I am very excited to be able to take on maintaining this project, and I am hoping to be able to work on this full time for at least the next year. I am not a researcher, and I am approaching all of this from a software engineering standpoint. This does give me a bit of an outsider's perspective on this in comparison to a lot of the people that have put in so much work to make this tool worth using. 

I really think this tool is incredibly important, and it is enabling research that is going to have a huge impact on the world. My hope is that I can bring my expertise in software engineering to this project, so that the researchers using this tool can be more efficient, and so they can get to the more important tasks they need to complete. I will do my best to make that a reality.

## Appendix

### Semantic Versioning

One of the main goals of semantic versioning (semver) is to communicate to people using the software if a new version is going to be completely compatible with an older version, or if they may need to check a change log to see if a feature they are using has changed. In the case of something like TransformerLens, the full API itself is what we base our version changes on. That includes classes, functions, parameters to functions, and data returned from said functions, including all exposed object properties or keys. When anything is added, then the minor version number gets a bump. When the API remains the same, but a bug has been fixed, the patch number gets a bump. Finally, when anything is removed whatsoever, then that requires a major version change. 

With TransformerLens, the reason why it is necessary in this transition to bump to 2.0 is simply due to the fact that things from 1.0 have been removed along the 1.x branch, thus breaking the exposed API. In most cases, the breaking changes were simple things like exposed keys being renamed. I discovered a handful of these cases within the last month while bringing demos up from earlier versions of TransformerLens into more recent versions of 1.x. 

I do not know exactly what the full extent of these changes were, and doing an exploration of this is probably not a great use of time. Regardless, the point stands that if you have a project using TransformerLens 1.0, you cannot reliably upgrade to 1.17 without possibly needing to change your code due to these exposed changes.The easiest thing to do in this situation while adopting semver is to simply bump the major version, and start fresh.

Going forward, code that you write using TransformerLens will work, and will consume the same minimal API for all minor and patch releases in the 2.x branch.



---
File: /docs/source/content/citation.md
---


# Citation

Please cite this library as:

```BibTeX
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}
```



---
File: /docs/source/content/contributing.md
---

# Contributing

## Setup

### DevContainer

For a one-click setup of your development environment, this project includes a
[DevContainer](https://containers.dev/). It can be used locally with [VS
Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) or
with [GitHub Codespaces](https://github.com/features/codespaces).

### Manual Setup

This project uses [Poetry](https://python-poetry.org/docs/#installation) for package management.
Install as follows (this will also setup your virtual environment):

```bash
poetry config virtualenvs.in-project true
poetry install --with dev,docs,jupyter
```

## Testing

If adding a feature, please add unit tests for it. If you need a model, please use one of the ones
that are cached by GitHub Actions (so that it runs quickly on the CD). These are `gpt2`,
`attn-only-1l`, `attn-only-2l`, `attn-only-3l`, `attn-only-4l`, `tiny-stories-1M`. Note `gpt2` is
quite slow (as we only have CPU actions) so the smaller models like `attn-only-1l` and
`tiny-stories-1M` are preferred if possible.

### Running the tests

- Unit tests only via `make unit-test`
- Acceptance tests only via `make acceptance-test`
- Docstring tests only via `make docstring-test`
- Notebook tests only via `make notebook-test`
- Run all test suites mentioned `make test`

## Formatting

This project uses `pycln`, `isort` and `black` for formatting, pull requests are checked in github
actions.

- Format all files via `make format`
- Only check the formatting via `make check-format`

Note that `black` line length is set to 100 in `pyproject.toml` (instead of the default 88).

## Documentation

Please make sure to add thorough documentation for any features you add. You should do this directly
in the docstring, and this will then automatically generate the API docs when merged into `main`.
They will also be automatically checked with [pytest](https://docs.pytest.org/) (via
[doctest](https://docs.python.org/3/library/doctest.html)).

If you want to view your documentation changes, run `poetry run docs-hot-reload`. This will give you
hot-reloading docs (they change in real time as you edit docstrings).

### Docstring Style Guide

We follow the Google Python Docstring Style for writing docstrings, with some added features from
reStructuredText (reST).

#### Sections and Order

You should follow this order:

```python
"""Title In Title Case.

A description of what the function/class does, including as much detail as is necessary to fully understand it.

Warning:

Any warnings to the user (e.g. common pitfalls).

Examples:

Include any examples here. They will be checked with doctest.

  >>> print(1 + 2)
  3

Args:
    param_without_type_signature:
        Each description should be indented once more.
    param_2:
        Another example parameter.

Returns:
    Returns description without type signature.

Raises:
    Information about the error it may raise (if any).
"""
```

#### Supported Sphinx Properties

##### References to Other Functions/Classes

You can reference other parts of the codebase using
[cross-referencing](https://www.sphinx-doc.org/en/master/usage/domains/python.html#cross-referencing-python-objects)
(noting that you can omit the full path if it is in the same file).

```reStructuredText
:mod:transformer_lens # Function or module

:const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES`

:class:`transformer_lens.HookedTransformer`

:meth:`transformer_lens.HookedTransformer.from_pretrained`

:attr:`transformer_lens.HookedTransformer.cfg`
```

##### Maths

You can use LaTeX, but note that as you're placing this in python strings the backwards slash (`\`)
must be repeated (i.e. `\\`). You can write LaTeX inline, or in "display mode".

```reStructuredText
:math:`(a + b)^2 = a^2 + 2ab + b^2`
```

```reStructuredText
.. math::
   :nowrap:

   \\begin{eqnarray}
      y    & = & ax^2 + bx + c \\
      f(x) & = & x^2 + 2xy + y^2
   \\end{eqnarray}
```

#### Markup

- Italics - `*text*`
- Bold - `**text**`
- Code - ` ``code`` `
- List items - `*item`
- Numbered items - `1. Item`
- Quotes - indent one level
- External links = ``` `Link text <https://domain.invalid/>` ```



---
File: /docs/source/content/gallery.md
---

# Gallery

Research done involving TransformerLens:

- [Progress Measures for Grokking via Mechanistic
  Interpretability](https://arxiv.org/abs/2301.05217) (ICLR Spotlight, 2023) by Neel Nanda, Lawrence
  Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt
- [Finding Neurons in a Haystack: Case Studies with Sparse
  Probing](https://arxiv.org/abs/2305.01610) by Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
  Harvey, Dmitrii Troitskii, Dimitris Bertsimas
- [Towards Automated Circuit Discovery for Mechanistic
  Interpretability](https://arxiv.org/abs/2304.14997) by Arthur Conmy, Augustine N. Mavor-Parker,
  Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso
- [Actually, Othello-GPT Has A Linear Emergent World Representation](https://neelnanda.io/othello)
  by Neel Nanda
- [A circuit for Python docstrings in a 4-layer attention-only
  transformer](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)
  by Stefan Heimersheim and Jett Janiak
- [A Toy Model of Universality](https://arxiv.org/abs/2302.03025) (ICML, 2023) by Bilal Chughtai,
  Lawrence Chan, Neel Nanda
- [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language
  Models](https://openreview.net/forum?id=ZB6bK6MTYq) (2023, ICLR Workshop RTML) by Alex Foote, Neel
  Nanda, Esben Kran, Ioannis Konstas, Fazl Barez
- [Eliciting Latent Predictions from Transformers with the Tuned
  Lens](https://arxiv.org/abs/2303.08112) by Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
  Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt

User contributed examples of the library being used in action:

- [Induction Heads Phase Change
  Replication](https://colab.research.google.com/github/ckkissane/induction-heads-transformer-lens/blob/main/Induction_Heads_Phase_Change.ipynb):
  A partial replication of [In-Context Learning and Induction
  Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
  from Connor Kissane
- [Decision Transformer
  Interpretability](https://github.com/jbloomAus/DecisionTransformerInterpretability): A set of
  scripts for training decision transformers which uses transformer lens to view intermediate
  activations, perform attribution and ablations. A write up of the initial work can be found
  [here](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability).


---
File: /docs/source/content/getting_started_mech_interp.md
---

# Getting Started in Mechanistic Interpretability

Mechanistic interpretability is a very young and small field, and there are a _lot_ of open
problems. This means there's both a lot of low-hanging fruit, and that the bar for entry is low - if
you would like to help, please try working on one! The standard answer to "why has no one done this
yet" is just that there aren't enough people! Key resources:

- [A Guide to Getting Started in Mechanistic Interpretability](https://neelnanda.io/getting-started)
- [ARENA Mechanistic Interpretability Tutorials](https://arena-ch1-transformers.streamlit.app/) from
  Callum McDougall. A comprehensive practical introduction to mech interp, written in
  TransformerLens - full of snippets to copy and they come with exercises and solutions! Notable
  tutorials:
  - [Coding GPT-2 from
    scratch](https://arena-ch1-transformers.streamlit.app/[1.1]_Transformer_from_Scratch), with
    accompanying video tutorial from me ([1](https://neelnanda.io/transformer-tutorial)
    [2](https://neelnanda.io/transformer-tutorial-2)) - a good introduction to transformers
  - [Introduction to Mech Interp and
    TransformerLens](https://arena-ch1-transformers.streamlit.app/[1.2]_Intro_to_Mech_Interp): An
    introduction to TransformerLens and mech interp via studying induction heads. Covers the
    foundational concepts of the library
  - [Indirect Object
    Identification](https://arena-ch1-transformers.streamlit.app/[1.3]_Indirect_Object_Identification):
    a replication of interpretability in the wild, that covers standard techniques in mech interp
    such as [direct logit
    attribution](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ),
    [activation patching and path
    patching](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)
- [Mech Interp Paper Reading List](https://neelnanda.io/paper-list)
- [200 Concrete Open Problems in Mechanistic
  Interpretability](https://neelnanda.io/concrete-open-problems)
- [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): To look
  up all the jargon and unfamiliar terms you're going to come across!
- [Neel Nanda's Youtube channel](https://www.youtube.com/channel/UCBMJ0D-omcRay8dh4QT0doQ): A range
  of mech interp video content, including [paper
  walkthroughs](https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&index=1),
  and [walkthroughs of doing
  research](https://www.youtube.com/watch?v=yo4QvDn-vsU&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T)


---
File: /docs/source/content/getting_started.md
---

# Getting Started

**Start with the [main demo](https://neelnanda.io/transformer-lens-demo) to learn how the library works, and the basic features**.

To see what using it for exploratory analysis in practice looks like, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!

Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **Check out my [list of concrete open problems](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit) to figure out where to start.**. It begins with advice on skilling up, and key resources to check out. 

If you're new to transformers, check out my [what is a transformer tutorial](https://neelnanda.io/transformer-tutorial) and [tutorial on coding GPT-2 from scratch](https://neelnanda.io/transformer-tutorial-2) (with [an accompanying template](https://neelnanda.io/transformer-template) to write one yourself!

## Advice for Reading the Code

One significant design decision made was to have a single transformer implementation that could support a range of subtly different GPT-style models. This has the upside of interpretability code just working for arbitrary models when you change the model name in `HookedTransformer.from_pretrained`! But it has the significant downside that the code implementing the model (in `HookedTransformer.py` and `components.py`) can be difficult to read. I recommend starting with my [Clean Transformer Demo](https://neelnanda.io/transformer-solution), which is a clean, minimal implementation of GPT-2 with the same internal architecture and activation names as HookedTransformer, but is significantly clearer and better documented.

## Installation

`pip install git+https://github.com/TransformerLensOrg/TransformerLens`

Import the library with `import transformer_lens`

(Note: This library used to be known as EasyTransformer, and some breaking changes have been made since the rename. If you need to use the old version with some legacy code, run `pip install git+https://github.com/TransformerLensOrg/TransformerLens@v1`.)

## Huggingface Gated Access

Some of the models available in TransformerLens require gated access to be used. Luckily TransformerLens provides a way to access those models via the configuration of an environmental variable. Simply configure your access token found [here](https://huggingface.co/settings/tokens) as `HF_TOKEN` in your environment.

You will need to make sure you accept the agreements for any gated models, but once you do, the models will work with TransformerLens without issue. If you attempt to ues one of these models before you have accepted any related agreements, the console output will be very helpful and point you to the URL where you need to accept an agreement. As of 23/4/24, the current list of gated models supported by TransformerLens is as follows.

* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1
* https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
* https://huggingface.co/mistralai/Mistral-7B-v0.1



---
File: /docs/source/content/special_cases.md
---

# Special Cases

## Mixture of Experts error rates
Due to the Top-K gating performed in the hidden layer of Mixture of Experts models, small errors can be amplified 
greatly in cases where a different expert is selected, which leads to a higher than normal variance in the error rate
of the final logits. In testing done on Mixtral running in half precision, the standard deviation of the absolute error 
rate of the logits compared to those from the default model was found to be around 2e-3.

There are two main ways to mitigate this:
1. Disable preprocessing options by using `HookedTransformer.from_pretrained_no_processing` instead of `HookedTransformer.from_pretrained`
2. Increase the precision of the data type used in the model



---
File: /docs/source/content/tutorials.md
---

# Tutorials

- **Start with the [main demo](https://neelnanda.io/transformer-lens-demo) to learn how the library works, and the basic features**.

## Where To Start

- To see what using it for exploratory analysis in practice looks like, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!

- [What is a Transformer tutorial](https://neelnanda.io/transformer-tutorial)

## Demos

- [**Activation Patching in TransformerLens**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Activation_Patching_in_TL_Demo.ipynb) - Accompanies the [Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory Analysis Demo.ipynb). This demo explains how to use [Activation Patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx) in TransformerLens, a mechanistic interpretability technique that uses causal intervention to identify which activations in a model matter for producing an output.

- [**Attribution Patching**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb) - [Attribution Patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) is an incomplete project that uses gradients to take a linear approximation to activation patching. It's a good approximation when patching in small activations like the outputs of individual attention heads, and bad when patching in large activations like a residual stream.

- [**Exploratory Analysis**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb) - Probably the best place to start, after the Main Demo. Demonstrates how to use TransformerLens to perform exploratory analysis - focuses less on rigor and more on getting a grasp of what's going on quickly. Uses a lot of useful interpretability techniques like logit attribution and activation patching. Steal liberally from this!

- [**Grokking**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb) - "Grokking" is a phenomenon where a model can learn to memorise the training data (minimising training loss) but then, if trained for a lot longer, can learn to generalise, leading to a sharp decrease in test loss as well. This demo shows training a model on the task of modular addition, verifying that it groks, and doing analysis. The demo is light on explanation, so you'll probably want to pair it with [Neel's video series](https://www.youtube.com/watch?v=ob4vuiqG2Go) on the paper it's based on.

- [**Head Detector**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb) - Shows how to use TransformerLens to automatically detect several common types of attention head, as well as create your own custom detection algorithms to find your own!

- [**Interactive Neuroscope**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Interactive_Neuroscope.ipynb) - Very hacky demo, but this is a feature, not a bug. Shows how to quickly create useful web-based visualisations of data, even if you're not a professional front-end developer. This demo creates an interactive Neuroscope - a visualization of a neuron's activations on text that will dynamically update as you edit the text.

- [**LLaMA**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb) - Converts Meta's LLaMA model (7B parameter version for now until multi-GPU support is added) to TransformerLens.

- [**Main Demo**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) - The main demo. This is where to start if you're new to TransformerLens. Shows a lot of great features for getting started, including available models, how to access model activations, and generally useful features you should know about.

- [**No Position Experiment**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb) - The accompanying notebook to Neel's [real-time research video](https://www.youtube.com/watch?v=yo4QvDn-vsU). Trains a model with no positional embeddings to predict the previous token, and makes a start at analysing what's going on there!

- [**Othello-GPT**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Othello_GPT.ipynb) - This is a demo notebook porting the weights of the Othello-GPT Model from the excellent [Emergent World Representations](https://arxiv.org/pdf/2210.13382.pdf) paper to TransformerLens. Neel's [sequence on investigating this](https://www.lesswrong.com/s/nhGNHyJHbrofpPbRG) is also well worth reading if you're interested in this topic!

- [**SVD Interpreter Demo**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/SVD_Interpreter_demo.ipynb) - Based on the [Conjecture post](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight#Directly_editing_SVD_representations) about how the singular value decompositions of transformer matrices are surprisingly interpretable, this demo shows how to use TransformerLens to reproduce this and investigate further.

- [**Tracr to TransformerLens**](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Tracr_to_Transformer_Lens_Demo.ipynb) - [Tracr](https://github.com/deepmind/tracr) is a cool new DeepMind tool that compiles a written program in [RASP](https://arxiv.org/abs/2106.06981) to transformer weights.This is a (hacky!) script to convert Tracr weights from the JAX form to a TransformerLens HookedTransformer in PyTorch.



---
File: /docs/source/conf.py
---

"""Sphinx configuration.

https://www.sphinx-doc.org/en/master/usage/configuration.html
"""
# pylint: disable=invalid-name
from pathlib import Path
from typing import Any, Optional

from sphinx.ext import apidoc

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = "TransformerLens"
copyright = "2023, Neel Nanda"
author = "Neel Nanda"
release = "0.0.0"

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "myst_parser",
    "sphinx.ext.githubpages",
    "nbsphinx",
]

source_suffix = {
    ".rst": "restructuredtext",
    ".md": "markdown",
}

templates_path = ["_templates"]


# -- Napoleon Extension Configuration -----------------------------------------

napoleon_include_init_with_doc = True
napoleon_use_admonition_for_notes = True
napoleon_custom_sections = [
    "Motivation:",
    "Warning:",
    "Getting Started:",
]

# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = "furo"
html_title = "TransformerLens Documentation"
html_static_path = ["_static"]
html_logo = "_static/transformer_lens_logo.png"
html_favicon = "favicon.ico"

# Fix to get Plotly Working
nbsphinx_prolog = r"""
.. raw:: html

    <script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js"></script>
    <script>
    require=requirejs;
    require.config({
        paths: {
            plotly: 'https://cdn.plot.ly/plotly-latest.min.js'
        }
    });
    </script>
"""

# -- Sphinx-Apidoc Configuration ---------------------------------------------

# Functions to ignore as they're not interesting to the end user
functions_to_ignore = [
    # functions from load_from_pretrained.py
    "convert_hf_model_config",
    "convert_bert_weights",
    "convert_gpt2_weights",
    "convert_gptj_weights",
    "convert_llama_weights",
    "convert_mingpt_weights",
    "convert_nanogpt_weights",
    "convert_neel_solu_old_weights",
    "convert_neo_weights",
    "convert_neox_weights",
    "convert_neel_model_config",
    "convert_opt_weights",
    "convert_gemma_weights",
    "fill_missing_keys",
    "get_basic_config",
    "get_official_model_name",
    "get_pretrained_state_dict",
    "make_model_alias_map",
    # functions from make_docs.py
    "get_config",
    "get_property",
    # functions from patching.py
    "make_df_from_ranges",
    # functions from utils.py
    "check_structure",
    "clear_huggingface_cache",
    "select_compatible_kwargs",
]

# Default AutoDoc Options
# https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options
autodoc_default_options = {
    "exclude-members": ", ".join(functions_to_ignore),
    "special-members": "__getitem__, __len__, __iter__",
}


def run_apidoc(_app: Optional[Any] = None):
    """Run Sphinx-Apidoc.

    Allows us to automatically generate API documentation from docstrings, every time we build the
    docs.
    """

    # Path to the package codebase
    package_path = Path(__file__).resolve().parents[2] / "transformer_lens"

    # Template directory
    template_dir = Path(__file__).resolve().parent / "apidoc_templates"

    # Output path for the generated reStructuredText files
    generated_path = Path(__file__).resolve().parent / "generated"
    output_path = generated_path / "code"
    generated_path.mkdir(parents=True, exist_ok=True)
    output_path.mkdir(parents=True, exist_ok=True)

    # Arguments for sphinx-apidoc
    args = [
        "--force",  # Overwrite existing files
        "--separate",  # Put documentation for each module on its own page.
        "--templatedir=" + str(template_dir),  # Use custom templates
        "-o",
        str(output_path),
        str(package_path),
    ]

    # Call sphinx-apidoc
    apidoc.main(args)


# -- Sphinx Notebook Demo Config ---------------------------------------------

nbsphinx_execute = "always"  # Always re-run so Plotly charts are created correctly.

# -- Sphinx Setup Overrides --------------------------------------------------


def setup(app):
    """Sphinx setup overrides."""
    # Connect functions to run when watch detects a file change
    app.connect("builder-inited", run_apidoc)
    # app.connect("builder-inited", copy_demos) # Don't run as too slow



---
File: /docs/make_docs.py
---

"""Build the API Documentation."""

import base64
import hashlib
import json
import multiprocessing
import os
import shutil
import subprocess
import warnings
from copy import deepcopy
from functools import lru_cache, partial
from pathlib import Path
from typing import Any, Callable, Literal, Optional, Sequence, Union

import pandas as pd  # type: ignore[import-untyped]
import torch
import tqdm  # type: ignore[import-untyped]
import yaml  # type: ignore[import-untyped]
from muutils.dictmagic import TensorDictFormats, condense_tensor_dict
from muutils.misc import shorten_numerical_to_str
from transformers import AutoTokenizer  # type: ignore[import-untyped]
from transformers import PreTrainedTokenizer

import transformer_lens  # type: ignore[import-untyped]
from transformer_lens import (
    ActivationCache,
    HookedTransformer,
    HookedTransformerConfig,
    loading,
)
from transformer_lens.loading_from_pretrained import (  # type: ignore[import-untyped]
    NON_HF_HOSTED_MODEL_NAMES,
    get_pretrained_model_config,
)

DEVICE: torch.device = torch.device("meta")

# disable the symlink warning
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

try:
    HF_TOKEN = os.environ.get("HF_TOKEN", "")
    if not HF_TOKEN.startswith("hf_"):
        raise ValueError("Invalid Hugging Face token")
except Exception as e:
    warnings.warn(
        f"Failed to get Hugging Face token -- info about certain models will be limited\n{e}"
    )

# Docs Directories
CURRENT_DIR: Path = Path(__file__).parent
SOURCE_PATH: Path = CURRENT_DIR / "../docs/source"
BUILD_PATH: Path = CURRENT_DIR / "../docs/build"
PACKAGE_DIR: Path = CURRENT_DIR.parent
DEMOS_DIR: Path = CURRENT_DIR.parent / "demos"
GENERATED_DIR: Path = CURRENT_DIR.parent / "docs/source/generated"
STATIC_DIR: Path = CURRENT_DIR.parent / "docs/source/_static"


@lru_cache(maxsize=None)
def get_config(model_name: str):
    """Retrieve the configuration of a pretrained model.

    Args:
        model_name (str): Name of the pretrained model.

    Returns:
        dict: Configuration of the pretrained model.
    """
    return loading.get_pretrained_model_config(model_name)


# manually defined known model types
KNOWN_MODEL_TYPES: Sequence[str] = (
    "gpt2",
    "distillgpt2",
    "opt",
    "gpt-neo",
    "gpt-j",
    "gpt-neox",
    "stanford-gpt2",
    "pythia",
    "solu",
    "gelu",
    "attn-only",
    "llama",
    "Llama-2",
    "bert",
    "tiny-stories",
    "stablelm",
    "bloom",
    "qwen",
    "mistral",
    "CodeLlama",
    "phi",
    "gemma",
    "yi",
    "t5",
    "mixtral",
    "Qwen2",
)

MODEL_ALIASES_MAP: dict[str, str] = transformer_lens.loading.make_model_alias_map()

# these will be copied as table columns
CONFIG_ATTRS_COPY: Sequence[str] = (
    "n_params",
    "n_layers",
    "n_heads",
    "d_model",
    "d_vocab",
    "act_fn",
    "positional_embedding_type",
    "parallel_attn_mlp",
    "original_architecture",
    "normalization_type",
)

# modify certain values when saving config
CONFIG_VALUES_PROCESS: dict[str, Callable] = {
    "initializer_range": float,
    "dtype": str,
    "device": str,
}

COLUMNS_ABRIDGED: Sequence[str] = (
    "name.default_alias",
    "name.huggingface",
    "n_params.as_str",
    "n_params.as_int",
    "cfg.n_params",
    "cfg.n_layers",
    "cfg.n_heads",
    "cfg.d_model",
    "cfg.d_vocab",
    "cfg.act_fn",
    "cfg.positional_embedding_type",
    "cfg.parallel_attn_mlp",
    "cfg.original_architecture",
    "cfg.normalization_type",
    "tokenizer.name",
    "tokenizer.class",
    "tokenizer.vocab_size",
    "tokenizer.vocab_hash",
)


def get_tensor_shapes(
    model: HookedTransformer,
    tensor_dims_fmt: TensorDictFormats = "yaml",
    except_if_forward_fails: bool = False,
) -> dict:
    """get the tensor shapes from a model"""
    model_info: dict = dict()
    # state dict
    model_info["tensor_shapes.state_dict"] = condense_tensor_dict(
        model.state_dict(), fmt=tensor_dims_fmt
    )
    model_info["tensor_shapes.state_dict.raw__"] = condense_tensor_dict(
        model.state_dict(), fmt="dict"
    )

    try:
        # input shape for activations -- "847"~="bat", subtract 7 for the context window to make it unique
        input_shape: tuple[int, int] = (847, model.cfg.n_ctx - 7)
        # why? to replace the batch and seq_len dims with "batch" and "seq_len" in the yaml
        dims_names_map: dict[int, str] = {
            input_shape[0]: "batch",
            input_shape[1]: "seq_len",
        }
        # run with cache to activation cache
        with torch.no_grad():
            cache: ActivationCache
            _, cache = model.run_with_cache(
                torch.empty(input_shape, dtype=torch.long, device=DEVICE)
            )
        # condense using muutils and store
        model_info["tensor_shapes.activation_cache"] = condense_tensor_dict(
            cache.cache_dict,
            fmt=tensor_dims_fmt,
            dims_names_map=dims_names_map,
        )
        model_info["tensor_shapes.activation_cache.raw__"] = condense_tensor_dict(
            cache.cache_dict,
            fmt="dict",
            dims_names_map=dims_names_map,
        )
    except Exception as e:
        msg: str = f"Failed to get activation cache for '{model.cfg.model_name}':\n{e}"
        if except_if_forward_fails:
            raise ValueError(msg) from e
        else:
            warnings.warn(msg)

    return model_info


def tokenizer_vocab_hash(tokenizer: PreTrainedTokenizer) -> str:
    # sort
    vocab: dict[str, int]
    try:
        vocab = tokenizer.vocab
    except Exception:
        vocab = tokenizer.get_vocab()

    vocab_hashable: list[tuple[str, int]] = list(
        sorted(
            vocab.items(),
            key=lambda x: x[1],
        )
    )
    # hash it
    hash_obj = hashlib.sha1(bytes(str(vocab_hashable), "UTF-8"))
    # convert to base64
    return base64.b64encode(
        hash_obj.digest(),
        altchars=b"-_",  # - and _ as altchars
    ).decode("UTF-8")


def get_tokenizer_info(model: HookedTransformer) -> dict:
    tokenizer: PreTrainedTokenizer = model.tokenizer
    model_info: dict = dict()
    # basic info
    model_info["tokenizer.name"] = tokenizer.name_or_path
    model_info["tokenizer.vocab_size"] = int(tokenizer.vocab_size)
    model_info["tokenizer.max_len"] = int(tokenizer.model_max_length)
    model_info["tokenizer.class"] = tokenizer.__class__.__name__

    # vocab hash
    model_info["tokenizer.vocab_hash"] = tokenizer_vocab_hash(tokenizer)
    return model_info


def get_model_info(
    model_name: str,
    include_cfg: bool = True,
    include_tensor_dims: bool = True,
    include_tokenizer_info: bool = True,
    tensor_dims_fmt: TensorDictFormats = "yaml",
    allow_warn: bool = True,
) -> tuple[str, dict]:
    """get information about the model from the default alias model name

    # Parameters:
     - `model_name : str`
        the default alias model name
     - `include_cfg : bool`
        whether to include the model config as a yaml string
       (defaults to `True`)
     - `include_tensor_dims : bool`
        whether to include the model tensor shapes
       (defaults to `True`)
     - `include_tokenizer_info : bool`
        whether to include the tokenizer info
        (defaults to `True`)
     - `tensor_dims_fmt : TensorDictFormats`
        the format of the tensor shapes. one of "yaml", "json", "dict"
       (defaults to `"yaml"`)
    """

    # assumes the input is a default alias
    if model_name not in transformer_lens.loading.DEFAULT_MODEL_ALIASES:
        raise ValueError(f"Model name '{model_name}' not found in default aliases")

    # get the names and model types
    official_name: Optional[str] = MODEL_ALIASES_MAP.get(model_name, None)
    model_info: dict = {
        "name.default_alias": model_name,
        "name.huggingface": official_name,
        "name.aliases": ", ".join(
            list(transformer_lens.loading.MODEL_ALIASES.get(official_name, []))  # type: ignore[arg-type]
        ),
        "model_type": None,
    }

    # Split the model name into parts
    parts: list[str] = model_name.split("-")

    # identify model type by known types
    for known_type in KNOWN_MODEL_TYPES:
        if known_type in model_name:
            model_info["model_type"] = known_type
            break

    # search for model size in name
    param_count_from_name: Optional[str] = None
    for part in parts:
        if part[-1].lower() in ["m", "b", "k"] and part[:-1].replace(".", "", 1).isdigit():
            param_count_from_name = part
            break

    # update model info from config
    model_cfg: HookedTransformerConfig = get_pretrained_model_config(model_name)
    model_info.update(
        {
            "name.from_cfg": model_cfg.model_name,
            "n_params.as_str": shorten_numerical_to_str(model_cfg.n_params),  # type: ignore[arg-type]
            "n_params.as_int": model_cfg.n_params,
            "n_params.from_name": param_count_from_name,
            **{f"cfg.{attr}": getattr(model_cfg, attr) for attr in CONFIG_ATTRS_COPY},
        }
    )

    # put the whole config as yaml (for readability)
    if include_cfg:
        # modify certain values to make them pretty-printable
        model_cfg_dict: dict = {
            key: (val if key not in CONFIG_VALUES_PROCESS else CONFIG_VALUES_PROCESS[key](val))
            for key, val in model_cfg.to_dict().items()
        }

        # raw config
        model_info["config.raw__"] = model_cfg_dict
        # dump to yaml
        model_info["config"] = yaml.dump(
            model_cfg_dict,
            default_flow_style=False,
            sort_keys=False,
            width=1000,
        )

    # get tensor shapes
    if include_tensor_dims or include_tokenizer_info:
        # set default device to meta, so that we don't actually allocate tensors
        # this can't be done at the root level because it would break other tests when we import this file
        # and it has to be done inside this function due to usage of multiprocessing
        with torch.device(DEVICE):
            got_model: bool = False
            try:
                # copy the config, so we can modify it
                model_cfg_copy: HookedTransformerConfig = deepcopy(model_cfg)
                # set device to "meta" -- don't actually initialize the model with real tensors
                model_cfg_copy.device = str(DEVICE)
                if not include_tokenizer_info:
                    # don't need to download the tokenizer
                    model_cfg_copy.tokenizer_name = None
                # init the fake model
                model: HookedTransformer = HookedTransformer(model_cfg_copy, move_to_device=True)
                # HACK: use https://huggingface.co/huggyllama to get tokenizers for original llama models
                if model.cfg.tokenizer_name in NON_HF_HOSTED_MODEL_NAMES:
                    model.set_tokenizer(
                        AutoTokenizer.from_pretrained(
                            f"huggyllama/{model.cfg.tokenizer_name.removesuffix('-hf')}",
                            add_bos_token=True,
                            token=HF_TOKEN,
                            legacy=False,
                        )
                    )
                got_model = True
            except Exception as e:
                msg: str = f"Failed to init model '{model_name}', can't get tensor shapes or tokenizer info"
                if allow_warn:
                    warnings.warn(f"{msg}:\n{e}")
                else:
                    raise ValueError(msg) from e

            if got_model:
                if include_tokenizer_info:
                    try:
                        tokenizer_info: dict = get_tokenizer_info(model)
                        model_info.update(tokenizer_info)
                    except Exception as e:
                        msg = f"Failed to get tokenizer info for model '{model_name}'"
                        if allow_warn:
                            warnings.warn(f"{msg}:\n{e}")
                        else:
                            raise ValueError(msg) from e

                if include_tensor_dims:
                    try:
                        tensor_shapes_info: dict = get_tensor_shapes(model, tensor_dims_fmt)
                        model_info.update(tensor_shapes_info)
                    except Exception as e:
                        msg = f"Failed to get tensor shapes for model '{model_name}'"
                        if allow_warn:
                            warnings.warn(f"{msg}:\n{e}")
                        else:
                            raise ValueError(msg) from e

    return model_name, model_info


def safe_try_get_model_info(
    model_name: str, kwargs: Optional[dict] = None
) -> tuple[str, Union[dict, Exception]]:
    """for parallel processing, to catch exceptions and return the exception instead of raising them"""
    if kwargs is None:
        kwargs = {}
    try:
        return get_model_info(model_name, **kwargs)
    except Exception as e:
        warnings.warn(f"Failed to get model info for '{model_name}': {e}")
        return model_name, e


def make_model_table(
    verbose: bool,
    allow_except: bool = False,
    parallelize: Union[bool, int] = True,
    model_names_pattern: Optional[str] = None,
    **kwargs,
) -> pd.DataFrame:
    """make table of all models. kwargs passed to `get_model_info()`"""
    model_names: list[str] = list(transformer_lens.loading.DEFAULT_MODEL_ALIASES)
    model_data: list[tuple[str, Union[dict, Exception]]] = list()

    # filter by regex pattern if provided
    if model_names_pattern:
        model_names = [
            model_name for model_name in model_names if model_names_pattern in model_name
        ]

    if parallelize:
        # parallel
        n_processes: int = parallelize if int(parallelize) > 1 else multiprocessing.cpu_count()
        if verbose:
            print(f"running in parallel with {n_processes=}")
        with multiprocessing.Pool(processes=n_processes) as pool:
            # Use imap for ordered results, wrapped with tqdm for progress bar
            imap_results: list[tuple[str, Union[dict, Exception]]] = list(
                tqdm.tqdm(
                    pool.imap(
                        partial(safe_try_get_model_info, **kwargs),
                        model_names,
                    ),
                    total=len(model_names),
                    desc="Loading model info",
                    disable=not verbose,
                )
            )

        model_data = imap_results

    else:
        # serial
        with tqdm.tqdm(
            transformer_lens.loading.DEFAULT_MODEL_ALIASES,
            desc="Loading model info",
            disable=not verbose,
        ) as pbar:
            for model_name in pbar:
                pbar.set_postfix_str(f"model: '{model_name}'")
                try:
                    model_data.append(get_model_info(model_name, **kwargs))
                except Exception as e:
                    if allow_except:
                        # warn and continue if we allow exceptions
                        warnings.warn(f"Failed to get model info for '{model_name}': {e}")
                        model_data.append((model_name, e))
                    else:
                        # raise exception right away if we don't allow exceptions
                        # note that this differs from the parallel version, which will only except at the end
                        raise ValueError(f"Failed to get model info for '{model_name}'") from e

    # figure out what to do with failed models
    failed_models: dict[str, Exception] = {
        model_name: result for model_name, result in model_data if isinstance(result, Exception)
    }
    msg: str = (
        f"Failed to get model info for {len(failed_models)}/{len(model_names)} models: {failed_models}\n"
        + "\n".join(f"\t'{model_name}': {expt}" for model_name, expt in failed_models.items())
    )
    if not allow_except:
        if failed_models:
            # raise exception if we don't allow exceptions
            raise ValueError(msg + "\n\n" + "=" * 80 + "\n\n" + "NO DATA WRITTEN")
    else:
        if failed_models:
            warnings.warn(msg + "\n\n" + "-" * 80 + "\n\n" + "WRITING PARTIAL DATA")

    # filter out failed models if we allow exceptions
    model_data_filtered: list[dict] = [
        result for _, result in model_data if not isinstance(result, Exception)
    ]
    return pd.DataFrame(model_data_filtered)


OutputFormat = Literal["jsonl", "csv", "md"]


def huggingface_name_to_url(df: pd.DataFrame) -> pd.DataFrame:
    """convert the huggingface model name to a url"""
    df_new: pd.DataFrame = df.copy()
    df_new["name.huggingface"] = df_new["name.huggingface"].map(
        lambda x: f"[{x}](https://huggingface.co/{x})" if x else x
    )
    return df_new


MD_TABLE_HEARDER: str = """---
title: Model Properties Table
hide-toc: true
---
# Model Properties Table

also see the [interactive model table](../_static/model_properties_table_interactive.html)
"""


def write_model_table(
    model_table: pd.DataFrame,
    path: Path,
    format: OutputFormat = "jsonl",
    include_TL_version: bool = True,
    md_hf_links: bool = True,
    md_header: str = MD_TABLE_HEARDER,
) -> None:
    """write the model table to disk in the specified format"""

    # make sure the directory exists
    path.parent.mkdir(parents=True, exist_ok=True)

    if include_TL_version:
        # get `transformer_lens` version
        tl_version: str = "unknown"
        try:
            from importlib.metadata import PackageNotFoundError, version

            tl_version = version("transformer_lens")
        except PackageNotFoundError as e:
            warnings.warn(f"Failed to get transformer_lens version: package not found\n{e}")
        except Exception as e:
            warnings.warn(f"Failed to get transformer_lens version: {e}")

        with open(path.with_suffix(".version"), "w") as f:
            json.dump({"version": tl_version}, f)

    if format == "jsonl":
        model_table.to_json(path.with_suffix(".jsonl"), orient="records", lines=True)
    elif format == "csv":
        model_table.to_csv(path.with_suffix(".csv"), index=False)
    elif format == "md":
        model_table_processed: pd.DataFrame = model_table
        # convert huggingface name to url
        if md_hf_links:
            model_table_processed = huggingface_name_to_url(model_table_processed)
        model_table_md_text: str = md_header + model_table_processed.to_markdown(index=False)
        with open(path.with_suffix(".md"), "w") as f:
            f.write(model_table_md_text)
    else:
        raise KeyError(f"Invalid format: {format}")


def abridge_model_table(
    model_table: pd.DataFrame,
    columns_keep: Sequence[str] = COLUMNS_ABRIDGED,
    null_to_empty: bool = True,
) -> pd.DataFrame:
    """keep only columns in COLUMNS_ABRIDGED

    primarily used to make the csv and md versions of the table readable

    also replaces `None` with empty string if `null_to_empty` is `True`
    """

    output: pd.DataFrame = model_table.copy()
    # filter columns
    output = output[list(columns_keep)]

    if null_to_empty:
        output = output.fillna("")

    return output


def get_model_table(
    model_table_path: Path,
    verbose: bool = True,
    force_reload: bool = True,
    do_write: bool = True,
    parallelize: Union[bool, int] = True,
    model_names_pattern: Optional[str] = None,
    **kwargs,
) -> pd.DataFrame:
    """get the model table either by generating or reading from jsonl file

    # Parameters:
     - `model_table_path : Path`
        the path to the model table file, and the base name for the csv and md files
     - `verbose : bool`
        whether to show progress bar
       (defaults to `True`)
     - `force_reload : bool`
        force creating the table from scratch, even if file exists
       (defaults to `True`)
     - `do_write : bool`
        whether to write the table to disk, if generating
       (defaults to `True`)
     - `model_names_pattern : Optional[str]`
        filter the model names by making them include this string. passed to `make_model_table()`. no filtering if `None`
        (defaults to `None`)
     - `**kwargs`
        passed to `make_model_table()`

    # Returns:
     - `pd.DataFrame`
        the model table. rows are models, columns are model attributes
    """

    # modify the name if a pattern is provided
    if model_names_pattern is not None:
        model_table_path = model_table_path.with_name(
            model_table_path.stem + f"-{model_names_pattern}"
        )

    model_table: pd.DataFrame
    if not model_table_path.exists() or force_reload:
        # generate it from scratch
        model_table = make_model_table(
            verbose=verbose,
            parallelize=parallelize,
            model_names_pattern=model_names_pattern,
            **kwargs,
        )
        if do_write:
            # full data as jsonl
            write_model_table(model_table, model_table_path, format="jsonl")
            # abridged data as csv, md
            abridged_table: pd.DataFrame = abridge_model_table(model_table)
            write_model_table(abridged_table, model_table_path, format="csv")
            write_model_table(abridged_table, model_table_path, format="md")
    else:
        # read the table from jsonl
        model_table = pd.read_json(model_table_path, orient="records", lines=True)

    return model_table


def copy_demos(_app: Optional[Any] = None):
    """Copy demo notebooks to the generated directory."""
    copy_to_dir = GENERATED_DIR / "demos"
    notebooks_to_copy = [
        "Exploratory_Analysis_Demo.ipynb",
        "Main_Demo.ipynb",
    ]

    if copy_to_dir.exists():
        shutil.rmtree(copy_to_dir)

    copy_to_dir.mkdir()
    for filename in notebooks_to_copy:
        shutil.copy(DEMOS_DIR / filename, copy_to_dir)


def build_docs():
    """Build the docs."""
    get_model_table(
        model_table_path=GENERATED_DIR / "model_properties_table.jsonl",
        force_reload=True,
    )
    copy_demos()

    # Generating docs
    subprocess.run(
        [
            "sphinx-build",
            SOURCE_PATH,
            BUILD_PATH,
            # "-n",  # Nitpicky mode (warn about all missing references)
            "-W",  # Turn warnings into errors
        ],
        check=True,
    )


def get_property(name, model_name):
    """Retrieve a specific property of a pretrained model.

    Args:
        name (str): Name of the property to retrieve.
        model_name (str): Name of the pretrained model.

    Returns:
        str: Value of the specified property.
    """
    cfg = get_config(model_name)

    if name == "act_fn":
        if cfg.attn_only:
            return "attn_only"
        if cfg.act_fn == "gelu_new":
            return "gelu"
        if cfg.act_fn == "gelu_fast":
            return "gelu"
        if cfg.act_fn == "solu_ln":
            return "solu"
        return cfg.act_fn
    if name == "n_params":
        n_params = cfg.n_params
        if n_params < 1e4:
            return f"{n_params/1e3:.1f}K"
        if n_params < 1e6:
            return f"{round(n_params/1e3)}K"
        if n_params < 1e7:
            return f"{n_params/1e6:.1f}M"
        if n_params < 1e9:
            return f"{round(n_params/1e6)}M"
        if n_params < 1e10:
            return f"{n_params/1e9:.1f}B"
        if n_params < 1e12:
            return f"{round(n_params/1e9)}B"
        raise ValueError(f"Passed in {n_params} above 1T?")
    return cfg.to_dict()[name]


def docs_hot_reload():
    """Hot reload the docs."""
    get_model_table(
        model_table_path=GENERATED_DIR / "model_properties_table.jsonl", force_reload=False
    )
    copy_demos()

    subprocess.run(
        [
            "sphinx-autobuild",
            "--watch",
            str(PACKAGE_DIR) + "," + str(DEMOS_DIR),
            "--open-browser",
            SOURCE_PATH,
            BUILD_PATH,
        ],
        check=True,
    )



---
File: /easy_transformer/__init__.py
---

import logging

logging.warning("DEPRECATED: Library has been renamed, import transformer_lens instead")
from transformer_lens import *



---
File: /tests/acceptance/test_activation_cache.py
---

import pytest
import torch
from fancy_einsum import einsum

from transformer_lens import HookedTransformer, utils
from transformer_lens.utils import Slice

# Create IOI prompts
ioi_prompt_formats = [
    "When John and Mary went to the shops,{} gave the bag to",
    "When Tom and James went to the park,{} gave the ball to",
    "When Dan and Sid went to the shops,{} gave an apple to",
    "After Martin and Amy went to the park,{} gave a drink to",
]
ioi_names = [
    (" Mary", " John"),
    (" Tom", " James"),
    (" Dan", " Sid"),
    (" Martin", " Amy"),
]


def get_ioi_tokens_and_answer_tokens(model):
    # List of prompts
    prompts = []
    # List of answers, in the format (correct, incorrect)
    answers = []
    # List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)
    answer_tokens = []
    for i in range(len(ioi_prompt_formats)):
        for j in range(2):
            answers.append((ioi_names[i][j], ioi_names[i][1 - j]))
            answer_tokens.append(
                (
                    model.to_single_token(answers[-1][0]),
                    model.to_single_token(answers[-1][1]),
                )
            )
            # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.
            prompts.append(ioi_prompt_formats[i].format(answers[-1][1]))
    answer_tokens = torch.tensor(answer_tokens)

    tokens = model.to_tokens(prompts, prepend_bos=True)

    return tokens, answer_tokens


def load_model(name):
    return HookedTransformer.from_pretrained(
        name,
        center_unembed=True,
        center_writing_weights=True,
        fold_ln=True,
        refactor_factored_attn_matrices=True,
    )


@torch.no_grad
def test_logit_attrs_matches_reference_code():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, answer_tokens = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get accumulated resid
    accumulated_residual = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1)

    # Get ref ave logit diffs (cribbed notebook code)
    answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)
    logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]
    scaled_residual_stack = cache.apply_ln_to_stack(accumulated_residual, layer=-1, pos_slice=-1)
    ref_ave_logit_diffs = einsum(
        "... batch d_model, batch d_model -> ...",
        scaled_residual_stack,
        logit_diff_directions,
    ) / len(tokens)

    # Get our ave logit diffs
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        pos_slice=-1,
        tokens=answer_tokens[:, 0],
        incorrect_tokens=answer_tokens[:, 1],
    )
    ave_logit_diffs = logit_diffs.mean(dim=-1)

    assert torch.isclose(ref_ave_logit_diffs, ave_logit_diffs, atol=1.1e-7).all()


@torch.no_grad
def test_logit_accumulated_resid_on_last_layer_variants():
    model = load_model("solu-2l")
    tokens, answer_tokens = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    accumulated_resid = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1)
    assert torch.equal(
        accumulated_resid,
        cache.accumulated_resid(layer=model.cfg.n_layers, incl_mid=True, pos_slice=-1),
    )

    assert torch.equal(
        accumulated_resid, cache.accumulated_resid(layer=None, incl_mid=True, pos_slice=-1)
    )


@torch.no_grad
def test_logit_accumulated_resid_without_mid():
    model = load_model("solu-2l")
    tokens, answer_tokens = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    accumulated_resid, labels = cache.accumulated_resid(
        layer=-1, incl_mid=False, pos_slice=-1, return_labels=True
    )
    assert len(labels) == accumulated_resid.size(0)
    assert all("mid" not in label for label in labels)


@torch.no_grad
def test_logit_attrs_works_for_all_input_shapes():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, answer_tokens = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get accumulated resid
    accumulated_residual = cache.accumulated_resid(
        layer=-1, incl_mid=True, pos_slice=-1, return_labels=False
    )

    # Get ref logit diffs (cribbed notebook code)
    answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)
    logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]
    scaled_residual_stack = cache.apply_ln_to_stack(accumulated_residual, layer=-1, pos_slice=-1)
    ref_logit_diffs = einsum(
        "... d_model, ... d_model -> ...", scaled_residual_stack, logit_diff_directions
    )

    # All tokens
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        pos_slice=-1,
        tokens=answer_tokens[:, 0],
        incorrect_tokens=answer_tokens[:, 1],
    )
    assert torch.isclose(ref_logit_diffs, logit_diffs).all()

    # Single token
    batch = -1
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        batch_slice=batch,
        pos_slice=Slice(-1),
        tokens=answer_tokens[batch, 0],
        incorrect_tokens=answer_tokens[batch, 1],
    )
    assert torch.isclose(ref_logit_diffs[:, batch], logit_diffs).all()

    # Single token (int)
    batch = -1
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        batch_slice=Slice(batch),
        pos_slice=-1,
        tokens=int(answer_tokens[batch, 0]),
        incorrect_tokens=int(answer_tokens[batch, 1]),
    )
    assert torch.isclose(ref_logit_diffs[:, batch], logit_diffs).all()

    # Single token (str)
    batch = -1
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        batch_slice=batch,
        pos_slice=-1,
        tokens=model.to_string(answer_tokens[batch, 0]),
        incorrect_tokens=model.to_string(answer_tokens[batch, 1]),
    )
    assert torch.isclose(ref_logit_diffs[:, batch], logit_diffs).all()

    # Single token and residual stack without batch dim
    batch = -1
    logit_diffs = cache.logit_attrs(
        accumulated_residual[:, batch, :],
        has_batch_dim=False,
        batch_slice=batch,
        pos_slice=-1,
        tokens=answer_tokens[batch, 0],
        incorrect_tokens=answer_tokens[batch, 1],
    )
    assert torch.isclose(ref_logit_diffs[:, batch], logit_diffs).all()

    # Array slice of tokens
    batch = [2, 5, 7]
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        batch_slice=batch,
        pos_slice=-1,
        tokens=answer_tokens[batch, 0],
        incorrect_tokens=answer_tokens[batch, 1],
    )
    assert torch.isclose(ref_logit_diffs[:, batch], logit_diffs).all()

    # Different shape for tokens and incorrect_tokens
    with pytest.raises(ValueError):
        cache.logit_attrs(
            accumulated_residual[:, batch, :],
            has_batch_dim=False,
            batch_slice=batch,
            pos_slice=-1,
            tokens=answer_tokens[batch, 0],
            incorrect_tokens=answer_tokens[batch, 0:1],
        )

    # No incorrect tokens
    ref_logit_diffs = einsum(
        "... d_model, ... d_model -> ...", scaled_residual_stack, answer_residual_directions[:, 0]
    )
    logit_diffs = cache.logit_attrs(
        accumulated_residual,
        pos_slice=-1,
        tokens=answer_tokens[:, 0],
        incorrect_tokens=None,
    )
    assert torch.isclose(ref_logit_diffs, logit_diffs).all()


@torch.no_grad
def test_accumulated_resid_with_apply_ln():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, _ = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get accumulated resid and apply ln seperately (cribbed notebook code)
    accumulated_residual = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1)
    ref_scaled_residual_stack = cache.apply_ln_to_stack(
        accumulated_residual, layer=-1, pos_slice=-1
    )

    # Get scaled_residual_stack using apply_ln parameter
    scaled_residual_stack = cache.accumulated_resid(
        layer=-1, incl_mid=True, pos_slice=-1, apply_ln=True
    )
    assert torch.isclose(ref_scaled_residual_stack, scaled_residual_stack, atol=1e-7).all()

    # Now do the same but using None as the layer and Slice(-1) as the pos_slice
    scaled_residual_stack, labels = cache.accumulated_resid(
        layer=None, incl_mid=True, pos_slice=Slice(-1), apply_ln=True, return_labels=True
    )
    assert torch.isclose(ref_scaled_residual_stack, scaled_residual_stack, atol=1e-7).all()

    expected_labels = []
    for l in range(model.cfg.n_layers + 1):
        if l == model.cfg.n_layers:
            expected_labels.append("final_post")
            continue
        expected_labels.append(f"{l}_pre")
        expected_labels.append(f"{l}_mid")

    assert labels == expected_labels


@torch.no_grad
def test_decompose_resid_with_apply_ln():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, _ = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get decomposed resid and apply ln seperately (cribbed notebook code)
    per_layer_residual = cache.decompose_resid(layer=-1, pos_slice=-1)
    ref_scaled_residual_stack = cache.apply_ln_to_stack(per_layer_residual, layer=-1, pos_slice=-1)

    # Get scaled_residual_stack using apply_ln parameter
    scaled_residual_stack = cache.decompose_resid(layer=None, pos_slice=Slice(-1), apply_ln=True)

    assert torch.isclose(ref_scaled_residual_stack, scaled_residual_stack, atol=1e-7).all()


@torch.no_grad
def test_decompose_resid_including_attention():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_attention_resids = torch.stack(
        [cache["attn_out", l][:, -1] for l in range(model.cfg.n_layers)]
    )
    residual_stack = cache.decompose_resid(
        layer=1, pos_slice=Slice(-1), mlp_input=True, apply_ln=False, incl_embeds=False, mode="attn"
    )

    assert torch.isclose(ref_attention_resids, residual_stack, atol=1e-7).all()


@torch.no_grad
def test_stack_head_results_with_apply_ln():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, _ = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get per head resid stack and apply ln seperately (cribbed notebook code)
    per_head_residual = cache.stack_head_results(layer=-1, pos_slice=-1)
    ref_scaled_residual_stack = cache.apply_ln_to_stack(
        per_head_residual, layer=None, pos_slice=Slice(-1)
    )

    # Get scaled_residual_stack using apply_ln parameter
    scaled_residual_stack = cache.stack_head_results(layer=-1, pos_slice=-1, apply_ln=True)

    assert torch.isclose(ref_scaled_residual_stack, scaled_residual_stack, atol=1e-7).all()


@torch.no_grad
def test_stack_head_results_including_remainder():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_resid_post = cache["resid_post", 0][None, :, -1]
    per_head_residual, labels = cache.stack_head_results(
        layer=1, pos_slice=-1, incl_remainder=True, return_labels=True
    )
    remainder = ref_resid_post - per_head_residual[:-1].sum(dim=0)
    assert torch.isclose(remainder, per_head_residual[-1]).all()
    assert labels[:-1] == [f"L0H{i}" for i in range(model.cfg.n_heads)]
    assert labels[-1] == "remainder"

    ref_resid_post = cache["resid_post", -1][None, :, -1]
    per_head_residual, labels = cache.stack_head_results(
        layer=0, pos_slice=-1, incl_remainder=True, return_labels=True
    )
    assert torch.isclose(ref_resid_post, per_head_residual, atol=1e-7).all()
    assert len(labels) == 1
    assert labels[-1] == "remainder"

    per_head_residual, labels = cache.stack_head_results(
        layer=0, pos_slice=-1, incl_remainder=False, return_labels=True
    )
    assert torch.isclose(per_head_residual, torch.zeros_like(per_head_residual)).all()
    assert len(labels) == 0


@torch.no_grad
def test_stack_neuron_results_with_apply_ln():
    # Load solu-2l
    model = load_model("solu-2l")

    tokens, _ = get_ioi_tokens_and_answer_tokens(model)

    # Run the model and cache all activations
    _, cache = model.run_with_cache(tokens)

    # Get neuron result stack and apply ln seperately
    neuron_result_stack = cache.stack_neuron_results(layer=-1, pos_slice=-1)
    ref_scaled_residual_stack = cache.apply_ln_to_stack(neuron_result_stack, layer=-1, pos_slice=-1)

    # Get scaled_residual_stack using apply_ln parameter
    scaled_residual_stack = cache.stack_neuron_results(layer=-1, pos_slice=Slice(-1), apply_ln=True)

    assert torch.isclose(ref_scaled_residual_stack, scaled_residual_stack, atol=1e-7).all()


@torch.no_grad
def test_stack_neuron_results_including_remainder():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_resid_post = cache["resid_post", 0][None, :, -1]
    neuron_result_stack, labels = cache.stack_neuron_results(
        layer=1, pos_slice=Slice(-1), incl_remainder=True, return_labels=True
    )
    remainder = ref_resid_post - neuron_result_stack[:-1].sum(dim=0)
    assert torch.isclose(remainder, neuron_result_stack[-1]).all()
    assert labels[:-1] == [f"L0N{i}" for i in range(model.cfg.d_mlp)]
    assert labels[-1] == "remainder"

    ref_resid_post = cache["resid_post", -1][None, :, -1]
    neuron_result_stack, labels = cache.stack_neuron_results(
        layer=0, pos_slice=-1, incl_remainder=True, return_labels=True
    )
    assert torch.isclose(ref_resid_post, neuron_result_stack, atol=1e-7).all()
    assert len(labels) == 1
    assert labels[-1] == "remainder"

    neuron_result_stack, labels = cache.stack_neuron_results(
        layer=0, pos_slice=-1, incl_remainder=False, return_labels=True
    )
    assert torch.isclose(neuron_result_stack, torch.zeros_like(neuron_result_stack)).all()
    assert len(labels) == 0


@torch.no_grad
def test_stack_neuron_results_using_neuron_slice():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    neuron_result_stack, labels = cache.stack_neuron_results(
        layer=1, pos_slice=Slice(-1), neuron_slice=Slice([0, 1, 2]), return_labels=True
    )
    assert labels == [f"L0N{i}" for i in range(3)]


@torch.no_grad
def test_remove_batch_dim():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens[:1])

    assert cache.has_batch_dim
    shapes_before_removal = {key: cache.cache_dict[key].shape for key in cache.cache_dict}

    # Removing batch dim changes the shape of the cached tensors
    cache.remove_batch_dim()
    assert not cache.has_batch_dim
    assert all(
        shapes_before_removal[key][1:] == cache.cache_dict[key].shape
        for key in shapes_before_removal
    )

    # Removing batch dim again does not change anything
    cache.remove_batch_dim()
    assert not cache.has_batch_dim
    assert all(
        shapes_before_removal[key][1:] == cache.cache_dict[key].shape
        for key in shapes_before_removal
    )


@torch.no_grad
def test_remove_batch_dim_fails_if_batch_gt_1():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert cache.has_batch_dim
    with pytest.raises(AssertionError):
        cache.remove_batch_dim()


@torch.no_grad
def test_retrieve_activations():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    key = ("scale", 1, "ln1")
    str_key = utils.get_act_name(*key)
    assert torch.equal(cache[key], cache[str_key])

    key = ("scale", -1, "ln1")
    str_key = f"scale{model.cfg.n_layers - 1}ln1"
    assert torch.equal(cache[key], cache[str_key])

    key = ("k", -1, None)
    str_key = f"blocks.{model.cfg.n_layers - 1}.attn.hook_k"
    assert torch.equal(cache[key], cache[str_key])

    key = "embed"
    str_key = utils.get_act_name(key)
    assert torch.equal(cache[key], cache[str_key])

    key = ("embed", None)
    str_key = utils.get_act_name(*key)
    assert torch.equal(cache[key], cache[str_key])


@torch.no_grad
def test_get_items():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert all(
        cache_key == cache_dict_key and torch.equal(cache_val, cache_dict_val)
        for (cache_key, cache_val), (cache_dict_key, cache_dict_val) in zip(
            cache.items(), cache.cache_dict.items()
        )
    )


@torch.no_grad
def test_get_values():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert all(
        torch.equal(cache_val, cache_dict_val)
        for cache_val, cache_dict_val in zip(cache.values(), cache.cache_dict.values())
    )


@torch.no_grad
def test_get_keys():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert all(
        cache_key == cache_dict_key
        for cache_key, cache_dict_key in zip(cache.keys(), cache.cache_dict.keys())
    )


@torch.no_grad
def test_apply_slice_to_batch_dim():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert cache.has_batch_dim
    batch_slice = Slice((2, 4))
    new_cache = cache.apply_slice_to_batch_dim(batch_slice)

    assert new_cache.has_batch_dim
    assert all(torch.equal(cache[key][2:4], new_cache[key]) for key in cache.cache_dict)

    batch_slice = 2
    new_cache = cache.apply_slice_to_batch_dim(batch_slice)

    assert not new_cache.has_batch_dim
    assert all(torch.equal(cache[key][2], new_cache[key]) for key in cache.cache_dict)


@torch.no_grad
def test_toggle_autodiff():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert not torch.is_grad_enabled()
    cache.toggle_autodiff(mode=True)
    assert torch.is_grad_enabled()


@torch.no_grad
def test_stack_activation():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    stack = cache.stack_activation("scale", -1, "ln1")
    assert all(
        torch.equal(cache[("scale", layer, "ln1")], stack[layer])
        for layer in range(model.cfg.n_layers)
    )

    stack = cache.stack_activation("scale", 1, "ln1")
    assert all(torch.equal(cache[("scale", layer, "ln1")], stack[layer]) for layer in range(1))


@torch.no_grad
def test_get_full_resid_decomposition():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_head_stack, ref_head_stack_labels = cache.stack_head_results(
        layer=model.cfg.n_layers, pos_slice=Slice(-1), apply_ln=True, return_labels=True
    )
    ref_mlp_stack, ref_mlp_stack_labels = cache.decompose_resid(
        layer=model.cfg.n_layers,
        mlp_input=False,
        pos_slice=Slice(-1),
        incl_embeds=False,
        mode="mlp",
        apply_ln=True,
        return_labels=True,
    )
    ref_embed = cache.apply_ln_to_stack(
        cache["embed"][None, :, -1], pos_slice=Slice(-1), mlp_input=False
    )
    ref_pos_embed = cache.apply_ln_to_stack(
        cache["pos_embed"][None, :, -1], pos_slice=Slice(-1), mlp_input=False
    )

    ref_bias = model.accumulated_bias(model.cfg.n_layers, mlp_input=False, include_mlp_biases=False)
    ref_bias = ref_bias.expand((1,) + ref_head_stack.shape[1:])
    ref_bias = cache.apply_ln_to_stack(ref_bias, pos_slice=Slice(-1), mlp_input=False)

    head_stack_len = ref_head_stack.size(0)
    mlp_stack_len = ref_mlp_stack.size(0)

    residual_stack, residual_stack_labels = cache.get_full_resid_decomposition(
        layer=-1, pos_slice=-1, apply_ln=True, expand_neurons=False, return_labels=True
    )
    assert torch.isclose(ref_head_stack, residual_stack[:head_stack_len], atol=1e-7).all()
    assert ref_head_stack_labels == residual_stack_labels[:head_stack_len]

    assert torch.isclose(
        ref_mlp_stack, residual_stack[head_stack_len : head_stack_len + mlp_stack_len], atol=1e-7
    ).all()
    assert (
        ref_mlp_stack_labels
        == residual_stack_labels[head_stack_len : head_stack_len + mlp_stack_len]
    )

    assert torch.isclose(
        ref_embed,
        residual_stack[head_stack_len + mlp_stack_len : head_stack_len + mlp_stack_len + 1],
        atol=1e-7,
    ).all()
    assert "embed" == residual_stack_labels[head_stack_len + mlp_stack_len]

    assert torch.isclose(
        ref_pos_embed,
        residual_stack[head_stack_len + mlp_stack_len + 1 : head_stack_len + mlp_stack_len + 2],
        atol=1e-7,
    ).all()
    assert "pos_embed" == residual_stack_labels[head_stack_len + mlp_stack_len + 1]

    assert torch.isclose(
        ref_bias,
        residual_stack[head_stack_len + mlp_stack_len + 2 : head_stack_len + mlp_stack_len + 3],
        atol=1e-7,
    ).all()
    assert "bias" == residual_stack_labels[head_stack_len + mlp_stack_len + 2]


@torch.no_grad
def test_get_full_resid_decomposition_with_neurons_expanded():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_head_stack, ref_head_stack_labels = cache.stack_head_results(
        layer=1, pos_slice=Slice(-1), apply_ln=True, return_labels=True
    )
    ref_neuron_stack, ref_neuron_labels = cache.stack_neuron_results(
        1, pos_slice=Slice(-1), return_labels=True
    )
    ref_neuron_stack = cache.apply_ln_to_stack(ref_neuron_stack, layer=1, pos_slice=Slice(-1))

    head_stack_len = ref_head_stack.size(0)
    neuron_stack_len = ref_neuron_stack.size(0)

    residual_stack, residual_stack_labels = cache.get_full_resid_decomposition(
        layer=1, pos_slice=Slice(-1), apply_ln=True, expand_neurons=True, return_labels=True
    )

    assert torch.isclose(
        ref_neuron_stack,
        residual_stack[head_stack_len : head_stack_len + neuron_stack_len],
        atol=1e-7,
    ).all()
    assert (
        ref_neuron_labels
        == residual_stack_labels[head_stack_len : head_stack_len + neuron_stack_len]
    )


@torch.no_grad
def test_get_full_resid_decomposition_without_applying_ln():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_head_stack = cache.stack_head_results(
        layer=1, pos_slice=Slice(-1), apply_ln=True, return_labels=False
    )
    ref_neuron_stack = cache.stack_neuron_results(1, pos_slice=Slice(-1), return_labels=False)

    head_stack_len = ref_head_stack.size(0)
    neuron_stack_len = ref_neuron_stack.size(0)

    residual_stack = cache.get_full_resid_decomposition(
        layer=1, pos_slice=Slice(-1), apply_ln=False, expand_neurons=True, return_labels=False
    )

    assert torch.isclose(
        ref_neuron_stack,
        residual_stack[head_stack_len : head_stack_len + neuron_stack_len],
        atol=1e-7,
    ).all()


@torch.no_grad
def test_get_full_resid_decomposition_attn_only_model():
    model = load_model("attn-only-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    ref_head_stack = cache.stack_head_results(
        layer=1, pos_slice=Slice(-1), apply_ln=False, return_labels=False
    )

    head_stack_len = ref_head_stack.size(0)

    residual_stack = cache.get_full_resid_decomposition(
        layer=1, pos_slice=Slice(-1), apply_ln=False, expand_neurons=False, return_labels=False
    )

    assert torch.isclose(ref_head_stack, residual_stack[:head_stack_len], atol=1e-7).all()


@torch.no_grad
def test_compute_test_head_results_does_not_compute_results_twice():
    model = load_model("attn-only-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    assert "blocks.0.attn.hook_result" not in cache.cache_dict
    cache.compute_head_results()
    assert "blocks.0.attn.hook_result" in cache.cache_dict

    # set infinity to the first element of the head results
    assert cache.cache_dict["blocks.0.attn.hook_result"][0, 0, 0, 0] != float("inf")
    cache.cache_dict["blocks.0.attn.hook_result"][0, 0, 0, 0] = float("inf")
    cache.compute_head_results()

    # assert the value has not changed
    assert cache.cache_dict["blocks.0.attn.hook_result"][0, 0, 0, 0] == float("inf")


@torch.no_grad
def test_get_neuron_results():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    layer = 1
    ref_neuron_acts = (
        cache[f"blocks.{layer}.mlp.hook_post"][:, -1, :2, None] * model.blocks[layer].mlp.W_out[:2]
    )

    neuron_acts = cache.get_neuron_results(
        layer,
        neuron_slice=[0, 1],
        pos_slice=-1,
    )

    assert torch.isclose(ref_neuron_acts, neuron_acts).all()


@torch.no_grad
def test_get_neuron_results_without_slice():
    model = load_model("solu-2l")
    tokens, _ = get_ioi_tokens_and_answer_tokens(model)
    _, cache = model.run_with_cache(tokens)

    layer = 1
    ref_neuron_acts = (
        cache[f"blocks.{layer}.mlp.hook_post"][..., None] * model.blocks[layer].mlp.W_out
    )

    neuron_acts = cache.get_neuron_results(
        layer,
        neuron_slice=None,
        pos_slice=None,
    )

    assert torch.isclose(ref_neuron_acts, neuron_acts).all()



---
File: /tests/acceptance/test_evals.py
---

import pytest

from transformer_lens.evals import IOIDataset, ioi_eval
from transformer_lens.HookedTransformer import HookedTransformer


@pytest.fixture(scope="module")
def model():
    return HookedTransformer.from_pretrained("gpt2-small", device="cpu")


def test_basic_ioi_eval(model):
    """
    Test IOI evaluation with default dataset and settings.
    """
    results = ioi_eval(model, num_samples=100)
    assert results["Accuracy"] >= 0.99


def test_symmetric_samples(model):
    """
    Test IOI evaluation with symmetric=True so prompts are in symmetric pairs.
    """
    ds = IOIDataset(tokenizer=model.tokenizer, num_samples=100, symmetric=True)
    results = ioi_eval(model, dataset=ds)
    assert results["Logit Difference"] > 2.0
    assert results["Accuracy"] > 0.9


def test_custom_dataset_ioi_eval(model):
    """
    Test IOI eval with custom dataset using different templates, names, and objects.
    """
    ds = IOIDataset(
        tokenizer=model.tokenizer,
        num_samples=100,
        templates=["[A] met with [B]. [B] gave the [OBJECT] to [A]"],
        names=["Alice", "Bob", "Charlie"],
        nouns={"OBJECT": ["ball", "book"]},
    )
    results = ioi_eval(model, dataset=ds)
    assert results["Logit Difference"] > 2.0
    assert results["Accuracy"] >= 0.99


def test_multitoken_names_ioi_eval(model):
    """
    Test the IOI evaluation with multi-token names in the dataset.
    """
    ds = IOIDataset(
        tokenizer=model.tokenizer,
        num_samples=100,
        names=["John Smith", "John Doe"],
    )
    results = ioi_eval(model, dataset=ds)
    assert results["Logit Difference"] > 2.0
    assert results["Accuracy"] >= 0.99


def test_inverted_template(model):
    """
    Test IOI eval with an unnatural template (BAAA).
    This should result in a negative logit difference and very low accuracy.
    """
    ds = IOIDataset(
        tokenizer=model.tokenizer,
        num_samples=100,
        templates=["[B] met with [A]. [A] said hello to [A]"],
    )
    results = ioi_eval(model, dataset=ds)
    assert results["Logit Difference"] < -2.0
    assert results["Accuracy"] <= 0.01



---
File: /tests/acceptance/test_hook_tokens.py
---

# %%

import functools

import torch as t
from jaxtyping import Int

from transformer_lens import HookedTransformer, HookedTransformerConfig
from transformer_lens.hook_points import HookPoint


def test_patch_tokens():
    # Define small transformer
    cfg = HookedTransformerConfig(
        n_layers=1,
        d_mlp=10,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        act_fn="relu",
        tokenizer_name="gpt2",
        use_hook_tokens=True,
    )
    model = HookedTransformer(cfg=cfg)

    # Define short prompt, and a token to replace the first token with (note this is index 1, because BOS)
    prompt = "Hello World!"
    modified_prompt = "Hi World!"
    new_first_token = model.to_single_token("Hi")

    # Define hook function to alter the first token
    def hook_fn(tokens: Int[t.Tensor, "batch seq"], hook: HookPoint, new_first_token: int):
        assert (
            tokens[0, 0].item() != new_first_token
        )  # Need new_first_token to be different from original
        tokens[0, 0] = new_first_token
        return tokens

    # Run with hooks
    out_from_hook = model.run_with_hooks(
        prompt,
        prepend_bos=False,
        fwd_hooks=[("hook_tokens", functools.partial(hook_fn, new_first_token=new_first_token))],
    )

    out_direct = model(modified_prompt, prepend_bos=False)

    t.testing.assert_close(out_from_hook, out_direct)



---
File: /tests/acceptance/test_hooked_encoder_decoder.py
---

import pytest
import torch
from jaxtyping import Float
from torch.testing import assert_close
from transformers import AutoTokenizer, T5ForConditionalGeneration

from transformer_lens import HookedEncoderDecoder

MODEL_NAME = "t5-small"


@pytest.fixture(scope="module")
def our_model():
    return HookedEncoderDecoder.from_pretrained(MODEL_NAME, device="cpu")


@pytest.fixture(scope="module")
def huggingface_model():
    return T5ForConditionalGeneration.from_pretrained(MODEL_NAME).eval()


@pytest.fixture(scope="module")
def tokenizer():
    return AutoTokenizer.from_pretrained(MODEL_NAME)


@pytest.fixture
def hello_world_tokens(tokenizer):
    return tokenizer("Hello, world!", return_tensors="pt")["input_ids"]


@pytest.fixture
def decoder_input_ids(tokenizer):
    return torch.LongTensor([[tokenizer.pad_token_id]])


def test_full_model(our_model, huggingface_model, tokenizer, decoder_input_ids):
    sequences = ["Hello, world!", "this is another sequence of tokens"]

    tokenized = tokenizer(sequences, return_tensors="pt", padding=True)
    decoder_ids = torch.stack([decoder_input_ids[0]] * len(sequences), dim=0)
    input_ids = tokenized["input_ids"]

    attention_mask = tokenized["attention_mask"]

    huggingface_model_out = huggingface_model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        decoder_input_ids=decoder_ids,
    ).logits
    our_model_out = our_model(
        input_ids,
        decoder_input=decoder_ids,
        one_zero_attention_mask=attention_mask,
    )
    assert_close(huggingface_model_out, our_model_out, rtol=1.3e-6, atol=4e-5)


def test_encoder(our_model, huggingface_model, hello_world_tokens):
    our_embeds = our_model.embed(hello_world_tokens)
    pos_bias = our_model.encoder[0].attn.compute_relative_attention_bias(
        hello_world_tokens.shape[1], hello_world_tokens.shape[1]
    )

    for our_layer in our_model.encoder:
        our_embeds = our_layer(resid_pre=our_embeds, position_bias=pos_bias)

    our_encoder_out = our_model.encoder_final_ln(our_embeds)

    huggingface_encoder_out = huggingface_model.encoder(hello_world_tokens).last_hidden_state

    assert_close(our_encoder_out, huggingface_encoder_out, rtol=1.3e-6, atol=4e-5)


def test_decoder(our_model, huggingface_model, hello_world_tokens, decoder_input_ids):
    encoder_hidden = huggingface_model.encoder(hello_world_tokens)[0]

    embeds = our_model.embed(decoder_input_ids)
    pos_bias = our_model.decoder[0].attn.compute_relative_attention_bias(
        decoder_input_ids.shape[1], decoder_input_ids.shape[1]
    )
    for layer in our_model.decoder:
        embeds = layer(embeds, encoder_hidden_states=encoder_hidden, position_bias=pos_bias)

    our_decoder_out = our_model.decoder_final_ln(embeds)
    hf_decoder_out = huggingface_model.decoder(
        decoder_input_ids, encoder_hidden_states=encoder_hidden
    )[0]

    assert_close(our_decoder_out, hf_decoder_out, rtol=1.3e-6, atol=4e-5)


def test_embed_one_sentence(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.encoder.embed_tokens
    our_embed = our_model.embed

    huggingface_embed_out = huggingface_embed(hello_world_tokens)[0]
    our_embed_out = our_embed(hello_world_tokens).squeeze(0)
    assert_close(huggingface_embed_out, our_embed_out)


def test_relative_attention_bias(our_model, huggingface_model, hello_world_tokens):
    # it is used only in self attention of first layer of encoder
    huggingface_embed = huggingface_model.encoder.embed_tokens
    huggingface_attn = huggingface_model.encoder.block[0].layer[0].SelfAttention
    our_attn = our_model.encoder[0].attn

    assert huggingface_attn.has_relative_attention_bias
    assert our_attn.has_relative_attention_bias
    assert (
        our_attn.relative_attention_num_buckets == huggingface_attn.relative_attention_num_buckets
    )
    assert (
        our_attn.relative_attention_max_distance == huggingface_attn.relative_attention_max_distance
    )
    assert_close(our_attn.rel_pos_bias.weight, huggingface_attn.relative_attention_bias.weight)

    input_len = hello_world_tokens.shape[1]
    our_bias = our_attn.compute_relative_attention_bias(input_len, input_len)
    hf_bias = huggingface_attn.compute_bias(input_len, input_len)
    assert_close(our_bias, hf_bias, rtol=1e-5, atol=1e-5)

    embed_out = huggingface_embed(hello_world_tokens)

    cache_position = torch.arange(input_len)
    huggingface_attn_out = huggingface_attn(embed_out, cache_position=cache_position)[0]
    our_attn_out = our_attn(embed_out, embed_out, embed_out, position_bias=our_bias)

    assert_close(our_attn_out, huggingface_attn_out, rtol=7.4e-4, atol=1e-5)


def test_relative_attention_layer(our_model, huggingface_model, hello_world_tokens):
    # it is used only in self attention of first layer of encoder
    hf_block = huggingface_model.encoder.block[0].layer[0]
    our_block = our_model.encoder[0]
    resid = huggingface_model.encoder.embed_tokens(hello_world_tokens)

    input_len = hello_world_tokens.shape[1]
    our_bias = our_block.attn.compute_relative_attention_bias(input_len, input_len)
    resid_norm = our_block.ln1(resid)
    our_out = resid + our_block.attn(resid_norm, resid_norm, resid_norm, position_bias=our_bias)

    cache_position = torch.arange(input_len)
    hf_out = hf_block(resid, cache_position=cache_position)[0]
    assert_close(our_out, hf_out, rtol=1.3e-6, atol=4e-5)


def test_attention(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.encoder.embed_tokens
    huggingface_attn = huggingface_model.encoder.block[1].layer[0].SelfAttention

    embed_out = huggingface_embed(hello_world_tokens)
    our_attn = our_model.encoder[1].attn

    our_attn_out = our_attn(embed_out, embed_out, embed_out)

    input_len = hello_world_tokens.shape[1]
    cache_position = torch.arange(input_len)
    huggingface_attn_out = huggingface_attn(embed_out, cache_position=cache_position)[0]

    assert_close(our_attn_out, huggingface_attn_out, rtol=5e-4, atol=1e-5)


def test_decoder_attention(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.decoder.embed_tokens
    huggingface_attn = huggingface_model.decoder.block[1].layer[0].SelfAttention

    embed_out = huggingface_embed(hello_world_tokens)
    our_attn = our_model.decoder[1].attn

    our_attn_out = our_attn(embed_out, embed_out, embed_out)

    input_len = hello_world_tokens.shape[1]
    cache_position = torch.arange(input_len)
    huggingface_attn_out = huggingface_attn(embed_out, cache_position=cache_position)[0]
    assert_close(our_attn_out, huggingface_attn_out, rtol=3e-4, atol=1e-5)


def test_attention_layer(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.encoder.embed_tokens
    huggingface_attn = huggingface_model.encoder.block[1].layer[0]

    embed_out = huggingface_embed(hello_world_tokens)
    our_attn = our_model.encoder[1].attn
    norm_embed = our_model.encoder[1].ln1(embed_out)
    our_attn_out = our_attn(norm_embed, norm_embed, norm_embed) + embed_out

    input_len = hello_world_tokens.shape[1]
    cache_position = torch.arange(input_len)
    huggingface_attn_out = huggingface_attn(embed_out, cache_position=cache_position)[0]
    assert_close(our_attn_out, huggingface_attn_out, rtol=2e-4, atol=1e-5)


def test_decoder_attention_layer(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.decoder.embed_tokens
    huggingface_attn = huggingface_model.decoder.block[1].layer[0]

    embed_out = huggingface_embed(hello_world_tokens)
    our_attn = our_model.decoder[1].attn
    norm_embed = our_model.decoder[1].ln1(embed_out)
    our_attn_out = our_attn(norm_embed, norm_embed, norm_embed) + embed_out

    input_len = hello_world_tokens.shape[1]
    cache_position = torch.arange(input_len)
    huggingface_attn_out = huggingface_attn(embed_out, cache_position=cache_position)[0]
    assert_close(our_attn_out, huggingface_attn_out, rtol=3e-4, atol=4e-5)


def test_cross_attention(our_model, huggingface_model, hello_world_tokens, decoder_input_ids):
    encoder_hidden = huggingface_model.encoder(hello_world_tokens).last_hidden_state
    decoder_hidden = huggingface_model.decoder.embed_tokens(decoder_input_ids)

    huggingface_cross_attn = huggingface_model.decoder.block[0].layer[1].EncDecAttention
    our_cross_attn = our_model.decoder[0].cross_attn

    our_cross_attn_out = our_cross_attn(decoder_hidden, encoder_hidden, encoder_hidden)
    huggingface_cross_attn_out = huggingface_cross_attn(
        decoder_hidden, key_value_states=encoder_hidden, cache_position=encoder_hidden
    )[0]
    assert_close(our_cross_attn_out, huggingface_cross_attn_out, rtol=2e-4, atol=1e-5)


def test_cross_attention_layer(our_model, huggingface_model, hello_world_tokens, decoder_input_ids):
    encoder_hidden = huggingface_model.encoder(hello_world_tokens).last_hidden_state
    decoder_hidden = huggingface_model.decoder.embed_tokens(decoder_input_ids)

    hf_layer = huggingface_model.decoder.block[0].layer[1]
    our_layer = our_model.decoder[0]
    # assert ln weights are the same
    assert_close(hf_layer.layer_norm.weight, our_layer.ln2.w)

    our_cross_attn_out = (
        our_layer.cross_attn(our_layer.ln2(decoder_hidden), encoder_hidden, encoder_hidden)
        + decoder_hidden
    )
    huggingface_cross_attn_out = hf_layer(
        decoder_hidden, key_value_states=encoder_hidden, cache_position=encoder_hidden
    )[0]
    assert_close(our_cross_attn_out, huggingface_cross_attn_out, rtol=2e-4, atol=1e-5)


def test_encoder_block(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.encoder.embed_tokens
    huggingface_block = huggingface_model.encoder.block[1]
    our_block = our_model.encoder[1]

    embed_out = huggingface_embed(hello_world_tokens)

    input_len = hello_world_tokens.shape[1]
    cache_position = torch.arange(input_len)
    hf_out = huggingface_block(embed_out, cache_position=cache_position)[0]
    our_out = our_block(embed_out)

    assert_close(our_out, hf_out, rtol=2e-4, atol=2e-5)


def test_decoder_block(our_model, huggingface_model, hello_world_tokens, decoder_input_ids):
    huggingface_embed = huggingface_model.decoder.embed_tokens
    huggingface_block = huggingface_model.decoder.block[1]
    our_block = our_model.decoder[1]

    encoder_hidden = huggingface_model.encoder(hello_world_tokens)[0]

    input_len = decoder_input_ids.shape[1]
    cache_position = torch.arange(input_len)
    decoder_hidden = huggingface_model.decoder.block[0](
        huggingface_embed(decoder_input_ids), cache_position=cache_position
    )[0]

    our_out = our_block(decoder_hidden, encoder_hidden_states=encoder_hidden)
    hf_out = huggingface_block(
        decoder_hidden, encoder_hidden_states=encoder_hidden, cache_position=encoder_hidden
    )[0]

    assert_close(hf_out, our_out, rtol=2e-4, atol=2e-5)


def test_layernorm(our_model, huggingface_model, hello_world_tokens):
    huggingface_embed = huggingface_model.encoder.embed_tokens
    huggingface_layernorm = huggingface_model.encoder.block[0].layer[0].layer_norm
    our_layernorm = our_model.encoder[0].ln1

    embed_out = huggingface_embed(hello_world_tokens)

    our_layernorm_out = our_layernorm(embed_out)
    huggingface_layernorm_out = huggingface_layernorm(embed_out)
    assert_close(our_layernorm_out, huggingface_layernorm_out)


def test_unembed(our_model, huggingface_model, hello_world_tokens):
    huggingface_model_hidden = huggingface_model.decoder(hello_world_tokens).last_hidden_state

    our_model_logits = our_model.unembed(huggingface_model_hidden)
    huggingface_model_logits = huggingface_model.lm_head(huggingface_model_hidden)

    assert_close(our_model_logits, huggingface_model_logits, rtol=1.3e-3, atol=1e-5)


def test_run_with_cache(our_model, hello_world_tokens, decoder_input_ids):
    logits, cache = our_model.run_with_cache(hello_world_tokens, decoder_input=decoder_input_ids)

    # check that an arbitrary subset of the keys exist and have the right shape
    seq_len = 5
    generated_len = 1
    assert "hook_embed" in cache
    assert cache["hook_embed"].shape == (1, seq_len, 512)
    assert "encoder.1.attn.hook_v" in cache
    assert cache["encoder.1.attn.hook_v"].shape == (1, seq_len, 8, 64)
    assert "encoder.3.attn.hook_attn_scores" in cache
    assert cache["encoder.3.attn.hook_attn_scores"].shape == (1, 8, seq_len, seq_len)
    assert "decoder.0.cross_attn.hook_k" in cache
    assert cache["decoder.0.cross_attn.hook_attn_scores"].shape == (
        1,
        8,
        generated_len,
        seq_len,
    )
    assert "decoder.3.hook_resid_post" in cache
    assert cache["decoder.3.hook_resid_post"].shape == (1, generated_len, 512)


def test_from_pretrained_revision():
    """
    Check that the from_pretrained parameter `revision` (= git version) works
    """

    _ = HookedEncoderDecoder.from_pretrained(MODEL_NAME, revision="main")

    try:
        _ = HookedEncoderDecoder.from_pretrained(MODEL_NAME, revision="inexistent_branch_name")
    except:
        pass
    else:
        raise AssertionError("Should have raised an error")


def test_predictions(our_model, huggingface_model, tokenizer, decoder_input_ids):
    input_ids = tokenizer("My name is Wolfgang and I live in Berlin", return_tensors="pt")[
        "input_ids"
    ]

    def get_predictions(logits: Float[torch.Tensor, "batch pos d_vocab"]):
        predicted_tokens = logits[0].argmax(dim=-1)
        return tokenizer.batch_decode(predicted_tokens)

    our_model_logits = our_model(input_ids, decoder_input=decoder_input_ids)
    our_prediction = get_predictions(our_model_logits)

    huggingface_model_logits = huggingface_model(
        input_ids, decoder_input_ids=decoder_input_ids
    ).logits
    huggingface_prediction = get_predictions(huggingface_model_logits)

    assert our_prediction == huggingface_prediction


def test_predictions_string_input(our_model, huggingface_model, tokenizer):
    prompt = "translate English to German: Hello, do you like bananas?"

    encodings = tokenizer(prompt, return_tensors="pt")
    tokens = encodings.input_ids
    batch_size, seq_len = tokens.shape
    decoder_input_ids = torch.full((batch_size, 1), tokenizer.pad_token_id)

    our_model_logits = our_model(prompt)

    huggingface_model_logits = huggingface_model(
        input_ids=tokens,
        attention_mask=encodings.attention_mask,
        decoder_input_ids=decoder_input_ids,
    ).logits

    assert_close(our_model_logits, huggingface_model_logits, rtol=1e-5, atol=1e-5)


def test_predictions_string_list_input(our_model, huggingface_model, tokenizer):
    prompt = [
        "translate English to German: Hello, do you like bananas?",
        "translate English to French: Hello, do you like bananas?",
        "translate English to Spanish: Hello, do you like bananas?",
    ]

    encodings = tokenizer(prompt, return_tensors="pt")
    tokens = encodings.input_ids
    batch_size, seq_len = tokens.shape
    decoder_input_ids = torch.full((batch_size, 1), tokenizer.pad_token_id)

    our_model_logits = our_model(prompt)

    huggingface_model_logits = huggingface_model(
        input_ids=tokens,
        attention_mask=encodings.attention_mask,
        decoder_input_ids=decoder_input_ids,
    ).logits

    assert_close(our_model_logits, huggingface_model_logits, rtol=1e-5, atol=1e-5)


def test_generate(our_model, huggingface_model, tokenizer):
    prompt = "translate English to German: Hello, do you like bananas?"

    encodings = tokenizer(prompt, return_tensors="pt")

    our_generation = our_model.generate(prompt, do_sample=False, max_new_tokens=20)
    huggingface_generated_tokens = huggingface_model.generate(
        input_ids=encodings.input_ids,
        attention_mask=encodings.attention_mask,
        do_sample=False,
    )[0]

    huggingface_generation = tokenizer.decode(
        huggingface_generated_tokens, skip_special_tokens=True
    )

    assert our_generation.lower() == huggingface_generation.lower()


@pytest.mark.skipif(not torch.cuda.is_available(), reason="Requires a CUDA device")
def test_cuda(hello_world_tokens, decoder_input_ids):
    model = HookedEncoderDecoder.from_pretrained(MODEL_NAME)
    model(hello_world_tokens, decoder_input=decoder_input_ids.cuda())



---
File: /tests/acceptance/test_hooked_encoder.py
---

from typing import List

import pytest
import torch
import torch.nn.functional as F
from jaxtyping import Float
from torch.testing import assert_close
from transformers import AutoTokenizer, BertForPreTraining

from transformer_lens import HookedEncoder

MODEL_NAME = "bert-base-cased"


@pytest.fixture(scope="module")
def our_bert():
    return HookedEncoder.from_pretrained(MODEL_NAME, device="cpu")


@pytest.fixture(scope="module")
def huggingface_bert():
    return BertForPreTraining.from_pretrained(MODEL_NAME)


@pytest.fixture(scope="module")
def tokenizer():
    return AutoTokenizer.from_pretrained(MODEL_NAME)


@pytest.fixture
def tokens(tokenizer):
    return tokenizer("The [MASK] sat on the mat", return_tensors="pt")["input_ids"]


def test_full_model(our_bert, huggingface_bert, tokenizer):
    sequences = [
        "Hello, my [MASK] is Bert.",
        "I went to the [MASK] to buy some groceries.",
    ]
    tokenized = tokenizer(sequences, return_tensors="pt", padding=True)
    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]

    huggingface_bert_logits = huggingface_bert(
        input_ids, attention_mask=attention_mask
    ).prediction_logits
    our_bert_logits = our_bert(input_ids, one_zero_attention_mask=attention_mask)
    assert_close(huggingface_bert_logits, our_bert_logits, rtol=1.3e-6, atol=4e-5)


def test_embed_one_prediction(our_bert, huggingface_bert, tokens):
    huggingface_embed = huggingface_bert.bert.embeddings
    our_embed = our_bert.embed

    huggingface_embed_out = huggingface_embed(tokens)[0]
    our_embed_out = our_embed(tokens).squeeze(0)
    assert_close(huggingface_embed_out, our_embed_out)


def test_embed_two_predictions(our_bert, huggingface_bert, tokenizer):
    encoding = tokenizer(
        "Hello, my [MASK] is Bert.",
        "I went to the [MASK] to buy some groceries.",
        return_tensors="pt",
    )
    input_ids = encoding["input_ids"]
    token_type_ids = encoding["token_type_ids"]

    huggingface_embed_out = huggingface_bert.bert.embeddings(
        input_ids, token_type_ids=token_type_ids
    )[0]
    our_embed_out = our_bert.embed(input_ids, token_type_ids=token_type_ids).squeeze(0)
    assert_close(huggingface_embed_out, our_embed_out)


def test_attention(our_bert, huggingface_bert, tokens):
    huggingface_embed = huggingface_bert.bert.embeddings
    huggingface_attn = huggingface_bert.bert.encoder.layer[0].attention

    embed_out = huggingface_embed(tokens)

    our_attn = our_bert.blocks[0].attn

    our_attn_out = our_attn(embed_out, embed_out, embed_out)
    huggingface_self_attn_out = huggingface_attn.self(embed_out)[0]
    huggingface_attn_out = huggingface_attn.output.dense(huggingface_self_attn_out)
    assert_close(our_attn_out, huggingface_attn_out)


def test_bert_block(our_bert, huggingface_bert, tokens):
    huggingface_embed = huggingface_bert.bert.embeddings
    huggingface_block = huggingface_bert.bert.encoder.layer[0]

    embed_out = huggingface_embed(tokens)

    our_block = our_bert.blocks[0]

    our_block_out = our_block(embed_out)
    huggingface_block_out = huggingface_block(embed_out)[0]
    assert_close(our_block_out, huggingface_block_out)


def test_bert_pooler(our_bert, huggingface_bert, tokens):
    huggingface_embed_out = huggingface_bert.bert.embeddings(tokens)
    huggingface_encoder_out = huggingface_bert.bert.encoder(huggingface_embed_out)
    cls_token_representation = huggingface_encoder_out[0]

    our_pooler_out = our_bert.pooler(cls_token_representation)
    huggingface_pooler_out = huggingface_bert.bert.pooler(cls_token_representation)
    assert_close(our_pooler_out, huggingface_pooler_out)


def test_nsp_head(our_bert, huggingface_bert, tokens):
    huggingface_bert_pooler_output = huggingface_bert.bert(tokens).pooler_output
    our_nsp_head_out = our_bert.nsp_head(huggingface_bert_pooler_output)
    huggingface_nsp_head_out = huggingface_bert.cls.seq_relationship(huggingface_bert_pooler_output)

    assert_close(our_nsp_head_out, huggingface_nsp_head_out)


def test_mlm_head(our_bert, huggingface_bert, tokens):
    huggingface_bert_core_outputs = huggingface_bert.bert(tokens).last_hidden_state

    our_mlm_head_out = our_bert.mlm_head(huggingface_bert_core_outputs)
    huggingface_predictions_out = huggingface_bert.cls.predictions.transform(
        huggingface_bert_core_outputs
    )

    print((our_mlm_head_out - huggingface_predictions_out).abs().max())
    assert_close(our_mlm_head_out, huggingface_predictions_out, rtol=1.3e-3, atol=1e-5)


def test_unembed(our_bert, huggingface_bert, tokens):
    huggingface_bert_core_outputs = huggingface_bert.bert(tokens).last_hidden_state

    our_mlm_head_out = our_bert.mlm_head(huggingface_bert_core_outputs)
    our_unembed_out = our_bert.unembed(our_mlm_head_out)
    huggingface_predictions_out = huggingface_bert.cls.predictions(huggingface_bert_core_outputs)

    assert_close(our_unembed_out, huggingface_predictions_out, rtol=1.3e-6, atol=4e-5)


def test_run_with_cache(our_bert, tokens):
    _, cache = our_bert.run_with_cache(tokens)

    # check that an arbitrary subset of the keys exist
    assert "embed.hook_embed" in cache
    assert "blocks.0.attn.hook_q" in cache
    assert "blocks.3.attn.hook_attn_scores" in cache
    assert "blocks.7.hook_resid_post" in cache
    assert "mlm_head.ln.hook_normalized" in cache


def test_from_pretrained_revision():
    """
    Check that the from_pretrained parameter `revision` (= git version) works
    """

    _ = HookedEncoder.from_pretrained(MODEL_NAME, revision="main")

    try:
        _ = HookedEncoder.from_pretrained(MODEL_NAME, revision="inexistent_branch_name")
    except:
        pass
    else:
        raise AssertionError("Should have raised an error")


@pytest.mark.skipif(
    torch.backends.mps.is_available() or not torch.cuda.is_available(),
    reason="bfloat16 unsupported by MPS: https://github.com/pytorch/pytorch/issues/78168 or no GPU",
)
@pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
def test_half_precision(dtype):
    """Check the 16 bits loading and inferences."""
    model = HookedEncoder.from_pretrained(MODEL_NAME, torch_dtype=dtype)
    assert model.W_K.dtype == dtype

    _ = model(model.tokenizer("Hello, world", return_tensors="pt")["input_ids"])


def _get_predictions(
    logits: Float[torch.Tensor, "batch pos d_vocab"], positions: List[int], tokenizer
):
    logits_at_position = logits.squeeze(0)[positions]
    predicted_tokens = F.softmax(logits_at_position, dim=-1).argmax(dim=-1)
    return tokenizer.batch_decode(predicted_tokens)


def test_predictions_mlm(our_bert, huggingface_bert, tokenizer):
    input_ids = tokenizer("The [MASK] sat on the mat", return_tensors="pt")["input_ids"]

    our_bert_logits = our_bert(input_ids)
    our_prediction = _get_predictions(our_bert_logits, [2], tokenizer)

    huggingface_bert_out = huggingface_bert(input_ids).prediction_logits
    huggingface_prediction = _get_predictions(huggingface_bert_out, [2], tokenizer)

    assert our_prediction == huggingface_prediction


def test_predictions_from_forward_function_mlm(our_bert, huggingface_bert, tokenizer):
    input_ids = tokenizer("The [MASK] sat on the mat", return_tensors="pt")["input_ids"]
    our_prediction = our_bert(input_ids, return_type="predictions")

    huggingface_bert_out = huggingface_bert(input_ids).prediction_logits
    huggingface_prediction = _get_predictions(huggingface_bert_out, [2], tokenizer)[
        0
    ]  # prediction is returned as a list

    assert our_prediction == huggingface_prediction


def test_input_list_of_strings_mlm(our_bert, huggingface_bert, tokenizer):
    prompts = ["The [MASK] sat on the mat", "She [MASK] to the store", "The dog [MASK] the ball"]
    encodings = tokenizer(prompts, return_tensors="pt", truncation=True, padding=True)
    our_bert_logits = our_bert(prompts)

    huggingface_bert_logits = huggingface_bert(**encodings).prediction_logits

    assert_close(our_bert_logits, huggingface_bert_logits, rtol=1.3e-6, atol=4e-5)


@pytest.mark.skipif(not torch.cuda.is_available(), reason="Requires a CUDA device")
def test_cuda(mlm_tokens):
    model = HookedEncoder.from_pretrained(MODEL_NAME)
    model(mlm_tokens)



---
File: /tests/acceptance/test_hooked_transformer.py
---

import gc
import os

import pandas as pd
import pytest
import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

from transformer_lens import HookedTransformer
from transformer_lens.components import LayerNormPre
from transformer_lens.HookedTransformer import DTYPE_FROM_STRING
from transformer_lens.loading_from_pretrained import (
    OFFICIAL_MODEL_NAMES,
    get_official_model_name,
)
from transformer_lens.utils import clear_huggingface_cache

TINY_STORIES_MODEL_NAMES = [
    name for name in OFFICIAL_MODEL_NAMES if name.startswith("roneneldan/TinyStories")
]

PYTHIA_MODEL_NAMES = [name for name in OFFICIAL_MODEL_NAMES if name.startswith("EleutherAI/pythia")]

# Small subsets for basic testing
TINY_STORIES_SMALL_MODELS = ["roneneldan/TinyStories-1M"]
PYTHIA_SMALL_MODELS = ["EleutherAI/pythia-70m"]

# Use full lists if HF_TOKEN is available, otherwise use small subsets
TINY_STORIES_TEST_MODELS = (
    TINY_STORIES_MODEL_NAMES if os.environ.get("HF_TOKEN", "") else TINY_STORIES_SMALL_MODELS
)
PYTHIA_TEST_MODELS = PYTHIA_MODEL_NAMES if os.environ.get("HF_TOKEN", "") else PYTHIA_SMALL_MODELS

# Small models for basic testing
PUBLIC_MODEL_NAMES = [
    "attn-only-demo",
    "gpt2-small",
    "opt-125m",
    "pythia-70m",
    "tiny-stories-33M",
    "microsoft/phi-1",
    "google/gemma-2b",
]

# Full set of models to test
FULL_MODEL_NAMES = [
    "attn-only-demo",
    "gpt2-small",
    "opt-125m",
    "gpt-neo-125M",
    "stanford-gpt2-small-a",
    "solu-4l-old",
    "solu-6l",
    "attn-only-3l",
    "pythia",
    "gelu-2l",
    "othello-gpt",
    "tiny-stories-33M",
    "bloom-560m",
    "santacoder",
    "microsoft/phi-1",
    "microsoft/phi-1_5",
    "microsoft/phi-2",
    "google/gemma-2b",
    "google/gemma-7b",
]

# Use full model list if HF_TOKEN is available, otherwise use public models only
model_names = FULL_MODEL_NAMES if os.environ.get("HF_TOKEN", "") else PUBLIC_MODEL_NAMES

text = "Hello world!"

""" 
# Code to regenerate loss store
store = {}
for name in model_names:
    model = HookedTransformer.from_pretrained(name, device='cuda')
    loss = model(text,return_type="loss")
    store[name] = loss.item()
print(store)
"""

# Loss values for minimal testing
SMALL_LOSS_STORE = {
    "gpt2-small": 5.331855773925781,
    "pythia-70m": 4.659344673156738,
}

# Full set of loss values
FULL_LOSS_STORE = {
    "attn-only-demo": 5.701841354370117,
    "gpt2-small": 5.331855773925781,
    "opt-125m": 6.159054279327393,
    "gpt-neo-125M": 4.900552272796631,
    "stanford-gpt2-small-a": 5.652035713195801,
    "solu-4l-old": 5.6021833419799805,
    "solu-6l": 5.7042999267578125,
    "attn-only-3l": 5.747507095336914,
    "pythia": 4.659344673156738,
    "gelu-2l": 6.501802444458008,
    "redwood_attn_2l": 10.530948638916016,
    "solu-1l": 5.256411552429199,
    "tiny-stories-33M": 12.203617095947266,
    "bloom-560m": 5.237126350402832,
}

# Use full store if HF_TOKEN is available, otherwise use small store
loss_store = FULL_LOSS_STORE if os.environ.get("HF_TOKEN", "") else SMALL_LOSS_STORE

no_processing = [
    ("solu-1l", 5.256411552429199),
    (
        "redwood_attn_2l",
        10.530948638916016,
    ),  # TODO can't be loaded with from_pretrained
]


@pytest.mark.parametrize("name,expected_loss", list(loss_store.items()))
def test_model(name, expected_loss):
    # Runs the model on short text and checks if the loss is as expected
    model = HookedTransformer.from_pretrained(name)
    loss = model(text, return_type="loss")
    assert (loss.item() - expected_loss) < 4e-5
    del model
    gc.collect()

    if "GITHUB_ACTIONS" in os.environ:
        clear_huggingface_cache()


def test_othello_gpt():
    # like test model but Othello GPT has a weird input format
    # so we need to test it separately

    model = HookedTransformer.from_pretrained("othello-gpt")
    sample_input = torch.tensor(
        [
            [
                # fmt: off
                20, 19, 18, 10, 2, 1, 27, 3, 41, 42, 34, 12, 4, 40, 11, 29, 43, 13, 48, 56, 33,
                39, 22, 44, 24, 5, 46, 6, 32, 36, 51, 58, 52, 60, 21, 53, 26, 31, 37, 9, 25, 38,
                23, 50, 45, 17, 47, 28, 35, 30, 54, 16, 59, 49, 57, 14, 15, 55, 7,
                # fmt: on
            ]
        ]
    )
    loss = model(sample_input, return_type="loss")
    expected_loss = 1.9079375267028809
    assert (loss.item() - expected_loss) < 4e-5


@pytest.mark.parametrize("name,expected_loss", no_processing)
def test_from_pretrained_no_processing(name, expected_loss):
    # Checks if manually overriding the boolean flags in from_pretrained
    # is equivalent to using from_pretrained_no_processing

    model_ref = HookedTransformer.from_pretrained_no_processing(name)
    model_ref_config = model_ref.cfg
    reff_loss = model_ref(text, return_type="loss")
    del model_ref
    model_override = HookedTransformer.from_pretrained(
        name,
        fold_ln=False,
        center_writing_weights=False,
        center_unembed=False,
        refactor_factored_attn_matrices=False,
    )
    assert model_ref_config == model_override.cfg

    if name != "redwood_attn_2l":  # TODO can't be loaded with from_pretrained
        # Do the converse check, i.e. check that overriding boolean flags in
        # from_pretrained_no_processing is equivalent to using from_pretrained
        model_ref = HookedTransformer.from_pretrained(name)
        model_ref_config = model_ref.cfg
        reff_loss = model_ref(text, return_type="loss")
        del model_ref
        model_override = HookedTransformer.from_pretrained_no_processing(
            name,
            fold_ln=True,
            center_writing_weights=True,
            center_unembed=True,
            refactor_factored_attn_matrices=False,
        )
        assert model_ref_config == model_override.cfg

    # also check losses
    print(reff_loss.item())
    assert (reff_loss.item() - expected_loss) < 4e-5


def test_process_weights_inplace():
    """Check that process_weights_ works"""
    model = HookedTransformer.from_pretrained_no_processing("gpt2-small")
    model.process_weights_()
    loss = model.forward(text, return_type="loss")
    assert (loss.item() - loss_store["gpt2-small"]) < 4e-5
    assert isinstance(model.ln_final, LayerNormPre)


def test_from_pretrained_revision():
    """
    Check that the from_pretrained parameter `revision` (= git version) works
    """

    _ = HookedTransformer.from_pretrained("gpt2", revision="main")

    try:
        _ = HookedTransformer.from_pretrained("gpt2", revision="inexistent_branch_name")
    except:
        pass
    else:
        raise AssertionError("Should have raised an error")


def test_bloom_similarity_with_hf_model_with_kv_cache_activated():
    tf_model = HookedTransformer.from_pretrained(
        "bigscience/bloom-560m", default_prepend_bos=False, device="cpu"
    )
    hf_model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m")
    hf_tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

    output_tf = tf_model.generate(
        text, do_sample=False, use_past_kv_cache=True, verbose=False, max_new_tokens=10
    )
    output_hf_tokens = hf_model.generate(
        hf_tokenizer(text, return_tensors="pt").input_ids,
        do_sample=False,
        max_new_tokens=10,
    )
    output_hf_str = hf_tokenizer.decode(output_hf_tokens[0], skip_special_tokens=True)

    assert output_tf == output_hf_str


def check_norm_folding(
    model_name,
    hf_model=None,
    tokenizer=None,
    prompt="Hello, world!",
    device=None,
    dtype=None,
):
    """
    Checks that loading a model with Layer/RMS Norm folding enabled does not (significantly) change its outputs.

    Returns the maximum difference between the logits produced by the same model with and without norm folding enabled.

    Also asserts that this difference is within some tolerance, although this is deliberately set to a high value
    in order to account for lower precision models.
    """

    # If a device/dtype is not specified, and hf_model is provided, use its device/dtype
    # Otherwise, default to cuda (if available)/float32
    if device is None:
        if hf_model:
            device = hf_model.device
        else:
            device = "cuda" if torch.cuda.is_available() else "cpu"
    if dtype is None:
        if hf_model:
            dtype = hf_model.dtype
        else:
            dtype = "float32"

    folded_model = HookedTransformer.from_pretrained(
        model_name=model_name,
        hf_model=hf_model,
        device=device,
        tokenizer=tokenizer,
        dtype=dtype,
        fold_ln=True,
        center_writing_weights=False,
        center_unembed=False,
    )
    tokens = folded_model.to_tokens(prompt)
    folded_logits = folded_model(tokens).detach()
    del folded_model
    torch.cuda.empty_cache()

    unfolded_model = HookedTransformer.from_pretrained(
        model_name=model_name,
        hf_model=hf_model,
        device=device,
        tokenizer=tokenizer,
        dtype=dtype,
        fold_ln=False,
        center_writing_weights=False,
        center_unembed=False,
    )
    unfolded_logits = unfolded_model(tokens).detach()
    del unfolded_model
    torch.cuda.empty_cache()

    assert torch.allclose(
        torch.softmax(folded_logits, dim=-1),
        torch.softmax(unfolded_logits, dim=-1),
        atol=1e-2,
    )

    return torch.max(
        torch.abs(torch.softmax(folded_logits, dim=-1) - torch.softmax(unfolded_logits, dim=-1))
    )


def calculate_error(logits1, logits2):
    t1 = torch.softmax(logits1, dim=-1).to("cpu")
    t2 = torch.softmax(logits2, dim=-1).to("cpu")
    err = torch.abs(t1 - t2)
    return {
        "max": torch.max(err).item(),
        "mean": torch.mean(err).item(),
        "median": torch.median(err).item(),
        "std": torch.std(err).item(),
    }


def benchmark_model_options(
    model_name: str,
    hf_model=None,
    tokenizer=None,
    device="cuda",
    n_devices=1,
    dtype=torch.float16,
    cache_in_cpu=True,
):
    options = {
        "fold_ln": False,
        "center_writing_weights": False,
        "center_unembed": False,
        "fold_value_biases": False,
    }

    prompts = [
        "Hello, world!",
        "This is a test.",
        "What is it about?",
        "I don't know.",
    ]

    model_name = get_official_model_name(model_name)

    if hf_model is None:
        hf_model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=dtype, device_map="auto"
        )
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model_name)

    tokens = tokenizer(prompts, return_tensors="pt", truncation=True, max_length=4).input_ids.to(
        device
    )

    # hf_model = hf_model.to(device)
    hf_logits = hf_model(tokens).logits.detach()
    hf_logits = hf_logits.to("cpu")

    if cache_in_cpu:
        hf_model = hf_model.to("cpu")
    else:
        del hf_model
        hf_model = None

    torch.cuda.empty_cache()
    gc.collect()

    results = {}

    # Check the error when all processing options are disabled
    tl_model = HookedTransformer.from_pretrained(
        model_name,
        hf_model=hf_model,
        tokenizer=tokenizer,
        device=device,
        n_devices=n_devices,
        dtype=dtype,
        **options,
    )
    tl_logits = tl_model(tokens).detach().to("cpu")
    results["no_options"] = calculate_error(hf_logits, tl_logits)
    del tl_model, tl_logits
    torch.cuda.empty_cache()

    # Check the error when each processing option is enabled individually
    for option in options:
        gc.collect()
        new_options = options.copy()
        new_options[option] = True
        tl_model = HookedTransformer.from_pretrained(
            model_name,
            hf_model=hf_model,
            tokenizer=tokenizer,
            device=device,
            n_devices=n_devices,
            dtype=dtype,
            **new_options,
        )
        tl_logits = tl_model(tokens).detach().to("cpu")
        results[option] = calculate_error(hf_logits, tl_logits)

        del tl_model, tl_logits
        torch.cuda.empty_cache()
        gc.collect()

    # Check the error when all processing options are enabled
    all_options = {k: True for k, v in options.items()}
    tl_model = HookedTransformer.from_pretrained(
        model_name,
        hf_model=hf_model,
        tokenizer=tokenizer,
        device=device,
        n_devices=n_devices,
        dtype=dtype,
        **all_options,
    )
    tl_logits = tl_model(tokens).detach().to("cpu")
    results["all_options"] = calculate_error(hf_logits, tl_logits)

    del tl_model, tl_logits

    del hf_model
    del tokens
    gc.collect()
    torch.cuda.empty_cache()

    return results


def benchmark_models(models, device="cuda", n_devices=1, cache_in_cpu=True):
    """
    Benchmark the error introduced by different options and data types for a list of models.
    :param models: A dict mapping model names to a list of dtypes to test
    """
    rows = []

    for model in models:
        dtypes = models[model]
        for dtype in dtypes:
            print(f"Testing {model} with dtype {dtype}")
            results = benchmark_model_options(
                model,
                device=device,
                n_devices=n_devices,
                dtype=DTYPE_FROM_STRING[dtype],
                cache_in_cpu=cache_in_cpu,
            )
            for option, result in results.items():
                rows.append({"model": model, "dtype": dtype, "options": option, **result})

    return pd.DataFrame(rows)


def check_similarity_with_hf_model(tl_model, hf_model, prompt="Hello, world!"):
    """
    Check that the TransformerLens model and the HuggingFace model
    give approximately the same results.

    The logits typically differ by a constant value, but check only the results
    after the softmax because this is what matters most.
    """
    tokens = tl_model.tokenizer.encode(prompt, return_tensors="pt")
    logits = tl_model(tokens, prepend_bos=False)
    hf_logits = hf_model(tokens).logits
    assert torch.allclose(
        torch.softmax(logits, dim=-1), torch.softmax(hf_logits, dim=-1), atol=1e-5
    )


def check_performance(tl_model, hf_model, margin):
    """
    Check that the TransformerLens model and the HuggingFace have
    approximately the same confidence in the expected answer.
    """
    prompt = " Unable"
    tokens = tl_model.tokenizer(prompt, return_tensors="pt")["input_ids"].to(
        "cuda" if torch.cuda.is_available() else "cpu"
    )

    expected_token = tl_model.tokenizer.encode(" to")[
        0
    ]  # Assume this is the expected token to predict

    tl_logits = tl_model(tokens, prepend_bos=False)[0, -1].float()
    hf_logits = hf_model(tokens).logits[0, -1].float()
    tl_prob = torch.softmax(tl_logits, dim=-1)[expected_token].item()
    hf_prob = torch.softmax(hf_logits, dim=-1)[expected_token].item()
    assert tl_prob + margin > hf_prob


def check_dtype(dtype, margin, no_processing=False):
    """Check the loading and inferences for different dtypes."""
    for model_path in ["gpt2", "roneneldan/TinyStories-33M", "EleutherAI/pythia-70m"]:
        if no_processing:
            # For low precision, the processing is not advised.
            model = HookedTransformer.from_pretrained_no_processing(model_path, torch_dtype=dtype)
        else:
            model = HookedTransformer.from_pretrained(model_path, torch_dtype=dtype)

        hf_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=dtype,
        ).to("cuda" if torch.cuda.is_available() else "cpu")

        for layer_name, layer in model.state_dict().items():
            assert layer.dtype in [dtype, torch.bool] or "IGNORE" in layer_name

        check_performance(model, hf_model, margin)

        # Check that generate doesn't throw an error
        _ = model.generate("Hello, World!")

        del model
        del hf_model
        gc.collect()


@pytest.mark.skipif(
    torch.backends.mps.is_available() or not torch.cuda.is_available(),
    reason="some operations unsupported by MPS: https://github.com/pytorch/pytorch/issues/77754 or no GPU",
)
@pytest.mark.parametrize("dtype", [torch.float64, torch.float32])
def test_dtype_float(dtype):
    check_dtype(dtype, margin=5e-4)


@pytest.mark.skipif(
    torch.backends.mps.is_available() or not torch.cuda.is_available(),
    reason="bfloat16 unsupported by MPS: https://github.com/pytorch/pytorch/issues/78168 or no GPU",
)
@pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
def test_half_precision(dtype):
    """Check the 16 bits loading and inferences.
    Note that bfloat16 is generally preferred to float16 for ML due to numerical instabilities,
    and some float16 operations require having a GPU.
    bfloat16 can be used without GPU, but surprisingly it doesn't give the same results in this case.
    """
    check_dtype(dtype, margin=0.05, no_processing=True)


@torch.no_grad()
def test_pos_embed_hook():
    """
    Checks that pos embed hooks:
    - do not permanently change the pos embed
    - can be used to alter the pos embed for a specific batch element
    """
    model = HookedTransformer.from_pretrained("gpt2-small")
    initial_W_pos = model.W_pos.detach().clone()

    def remove_pos_embed(z, hook):
        z[:] = 0.0
        return z

    _ = model.run_with_hooks("Hello, world", fwd_hooks=[("hook_pos_embed", remove_pos_embed)])

    # Check that pos embed has not been permanently changed
    assert (model.W_pos == initial_W_pos).all()

    def edit_pos_embed(z, hook):
        sequence_length = z.shape[1]
        z[1, :] = 0.0
        # Check that the second batch element is zeroed
        assert (z[1, :] == 0.0).all()
        # Check that the first batch element is unchanged
        assert (z[0, :] == initial_W_pos[:sequence_length]).all()
        return z

    _ = model.run_with_hooks(
        ["Hello, world", "Goodbye, world"],
        fwd_hooks=[("hook_pos_embed", edit_pos_embed)],
    )


def test_all_tinystories_models_exist():
    for model in TINY_STORIES_TEST_MODELS:
        try:
            AutoConfig.from_pretrained(model)
        except OSError:
            pytest.fail(
                f"Could not download model '{model}' from Huggingface."
                " Maybe the name was changed or the model has been removed."
            )


def test_all_pythia_models_exist():
    for model in PYTHIA_TEST_MODELS:
        try:
            AutoConfig.from_pretrained(model)
        except OSError:
            pytest.fail(
                f"Could not download model '{model}' from Huggingface."
                " Maybe the name was changed or the model has been removed."
            )


@pytest.mark.parametrize(
    "input_type,return_type",
    [
        ("str", "input"),
        ("str", "str"),
        ("str", "tokens"),
        ("str", "embeds"),
        ("tokens", "input"),
        ("tokens", "str"),
        ("tokens", "tokens"),
        ("tokens", "embeds"),
        ("embeds", "input"),
        ("embeds", "str"),
        ("embeds", "tokens"),
        ("embeds", "embeds"),
    ],
)
def test_different_inputs_for_generation(
    input_type, return_type, print_output=False, max_new_tokens=3
):
    from typing import List

    device = "cuda" if torch.cuda.is_available() else "cpu"
    hooked_llm = HookedTransformer.from_pretrained("gpt2", device=device)

    hooked_llm.eval()
    for text_input in [
        "What is the meaning of life?",
        ["AI will destroy world", "AI will save us"],
    ]:
        is_batched = False if isinstance(text_input, str) else True

        tokens_input = hooked_llm.to_tokens(text_input)
        embeddings_input = hooked_llm.embed(tokens_input)

        if input_type == "str":
            model_input = text_input
        elif input_type == "tokens":
            model_input = tokens_input
        elif input_type == "embeds":
            model_input = embeddings_input
        else:
            raise ValueError(f"Unknown input_type: {input_type}")

        output = hooked_llm.generate(
            input=model_input, max_new_tokens=max_new_tokens, return_type=return_type, verbose=False
        )

        if return_type == "str" or (return_type == "input" and input_type == "str"):
            if is_batched:
                assert isinstance(output, List), f"Expected list output but got {type(output)}"
                assert isinstance(
                    output[0], str
                ), f"Expected list of strings but got list of {type(output[0])}"
            else:
                assert isinstance(output, str), f"Expected string output but got {type(output)}"
        elif return_type == "tokens" or (return_type == "input" and input_type == "tokens"):
            assert isinstance(
                output, torch.Tensor
            ), f"Expected tensor output but got {type(output)}"
            assert output.ndim == 2, f"Expected 2D tensor but got {output.ndim}D"
        elif return_type == "embeds" or (return_type == "input" and input_type == "embeds"):
            assert isinstance(
                output, torch.Tensor
            ), f"Expected tensor output but got {type(output)}"
            assert output.ndim == 3, f"Expected 3D tensor but got {output.ndim}D"

        if print_output:
            print(f"Input type: {input_type}, return type: {return_type}, output:\n{output}")

    if print_output:
        print()



---
File: /tests/acceptance/test_multi_gpu.py
---

import time

import pytest
import torch

from transformer_lens.HookedTransformer import HookedTransformer
from transformer_lens.utilities.devices import get_best_available_device


@pytest.fixture
def gpt2_medium_on_1_device():
    model = HookedTransformer.from_pretrained("gpt2-medium", fold_ln=False, n_devices=1)
    return model


@pytest.fixture
def gpt2_medium_on_4_devices():
    model = HookedTransformer.from_pretrained("gpt2-medium", fold_ln=False, n_devices=4)
    return model


@pytest.mark.skipif(torch.cuda.device_count() < 4, reason="Requires at least 4 CUDA devices")
@pytest.mark.parametrize("n_devices", [1, 2, 3, 4])
def test_device_separation_and_cache(gpt2_medium_on_1_device, n_devices):
    model_1_device = gpt2_medium_on_1_device
    model_n_devices = HookedTransformer.from_pretrained(
        "gpt2-medium", fold_ln=False, n_devices=n_devices
    )

    model_description_text = """## Loading Models
    HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. 
    See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. 
    Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. 
    For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!"""

    # run model on single device
    start_time_1_device = time.time()
    loss_1_device = model_1_device(model_description_text, return_type="loss")
    elapsed_time_1_device = time.time() - start_time_1_device

    # get model on n_devices
    start_time_n_devices = time.time()
    loss_n_devices = model_n_devices(model_description_text, return_type="loss")
    elapsed_time_n_devices = time.time() - start_time_n_devices

    gpt2_text = (
        "Natural language processing tasks, such as question answering, machine translation, reading comprehension, "
        "and summarization, are typically approached with supervised learning on taskspecific datasets."
    )
    gpt2_tokens = model_1_device.to_tokens(gpt2_text)

    gpt2_logits_1_device, gpt2_cache_1_device = model_1_device.run_with_cache(
        gpt2_tokens, remove_batch_dim=True
    )
    gpt2_logits_n_devices, gpt2_cache_n_devices = model_n_devices.run_with_cache(
        gpt2_tokens, remove_batch_dim=True
    )

    # Make sure the tensors in cache remain on their respective devices
    for i in range(model_n_devices.cfg.n_layers):
        expected_device = get_best_available_device(model_n_devices.cfg)
        cache_device = gpt2_cache_n_devices[f"blocks.{i}.mlp.hook_post"].device
        assert cache_device == expected_device

    assert torch.allclose(gpt2_logits_1_device.to("cpu"), gpt2_logits_n_devices.to("cpu"))
    for key in gpt2_cache_1_device.keys():
        assert torch.allclose(
            gpt2_cache_1_device[key].to("cpu"), gpt2_cache_n_devices[key].to("cpu")
        )

    cuda_devices = set()
    n_params_on_device = {}
    for name, param in model_n_devices.named_parameters():
        if param.device.type == "cuda":
            cuda_devices.add(param.device.index)
        if param.device.index not in n_params_on_device:
            n_params_on_device[param.device.index] = 0
        n_params_on_device[param.device.index] += 1

    for device in cuda_devices:
        prop_device = n_params_on_device[device] / len(model_n_devices.state_dict())
        expected_prop_device = 1 / n_devices
        assert prop_device == pytest.approx(expected_prop_device, rel=0.20)

    print(
        f"Number of devices: {n_devices}, Model loss (1 device): {loss_1_device}, Model loss ({n_devices} devices): {loss_n_devices}, "
        f"Time taken (1 device): {elapsed_time_1_device:.4f} seconds, Time taken ({n_devices} devices): {elapsed_time_n_devices:.4f} seconds"
    )


@pytest.mark.skipif(torch.cuda.device_count() < 2, reason="Requires at least 2 CUDA devices")
def test_load_model_on_target_device():
    model = HookedTransformer.from_pretrained("gpt2-small", device="cuda:1")
    assert model.cfg.device == "cuda:1"

    for name, param in model.named_parameters():
        assert param.device == torch.device(
            "cuda:1"
        ), f"Parameter {name} is on {param.device} instead of cuda:1"

    output = model("Hello world")
    assert output.device == torch.device("cuda:1")


@pytest.mark.skipif(torch.cuda.device_count() < 2, reason="Requires at least 2 CUDA devices")
def test_cache_device():
    model = HookedTransformer.from_pretrained("gpt2-small", device="cuda:1")

    logits, cache = model.run_with_cache("Hello there")
    assert norm_device(cache["blocks.0.mlp.hook_post"].device) == norm_device(
        torch.device("cuda:1")
    )

    logits, cache = model.run_with_cache("Hello there", device="cpu")
    assert norm_device(cache["blocks.0.mlp.hook_post"].device) == norm_device(torch.device("cpu"))

    model.to("cuda")
    logits, cache = model.run_with_cache("Hello there")
    assert norm_device(cache["blocks.0.mlp.hook_post"].device) == norm_device(logits.device)


def norm_device(device):
    """
    Convenience function to normalize device strings for comparison.
    """
    device_str = str(device)
    if device_str.startswith("cuda") and ":" not in device_str:
        device_str += ":0"
    return device_str



---
File: /tests/acceptance/test_tokenizer_special_tokens.py
---

import os

from transformers import AutoTokenizer

import transformer_lens.loading_from_pretrained as loading
from transformer_lens import HookedTransformer, HookedTransformerConfig

# Small models for basic testing
PUBLIC_MODEL_TESTING_LIST = ["gpt2-small", "opt-125m", "pythia-70m"]

# Full set of models to test when HF_TOKEN is available
FULL_MODEL_TESTING_LIST = [
    "solu-1l",
    "gpt2-small",
    "gpt-neo-125M",
    "opt-125m",
    "opt-30b",
    "stanford-gpt2-small-a",
    "pythia-70m",
]

# Use full model list if HF_TOKEN is available, otherwise use public models only
MODEL_TESTING_LIST = (
    FULL_MODEL_TESTING_LIST if os.environ.get("HF_TOKEN", "") else PUBLIC_MODEL_TESTING_LIST
)


def test_d_vocab_from_tokenizer():
    cfg = HookedTransformerConfig(
        n_layers=1, d_mlp=10, d_model=10, d_head=5, n_heads=2, n_ctx=20, act_fn="relu"
    )
    test_string = "a fish."
    # Test tokenizers for different models
    for model_name in MODEL_TESTING_LIST:
        if model_name == "solu-1l":
            tokenizer_name = "NeelNanda/gpt-neox-tokenizer-digits"
        else:
            tokenizer_name = loading.get_official_model_name(model_name)

        model = HookedTransformer(cfg=cfg, tokenizer=AutoTokenizer.from_pretrained(tokenizer_name))

        tokens_with_bos = model.to_tokens(test_string)
        tokens_without_bos = model.to_tokens(test_string, prepend_bos=False)

        # Check that the lengths are different by one
        assert (
            tokens_with_bos.shape[-1] == tokens_without_bos.shape[-1] + 1
        ), "BOS Token not added when expected"
        # Check that we don't have BOS when we disable the flag
        assert (
            tokens_without_bos.squeeze()[0] != model.tokenizer.bos_token_id
        ), "BOS token is present when it shouldn't be"



---
File: /tests/integration/test_attention_mask.py
---

import torch

from transformer_lens import utils
from transformer_lens.HookedTransformer import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_attention_mask():
    # Verify the attention mask attends properly, including for low attention scores
    cfg = HookedTransformerConfig(
        d_head=1,
        d_model=12,
        d_vocab=2,
        n_ctx=5,
        n_layers=1,
        attn_only=True,
        attention_dir="causal",
    )
    model = HookedTransformer(cfg)
    input_length = 5
    input = torch.ones((1, input_length), dtype=torch.int64)
    layer = 0
    low_attn_score = 1e-6
    ones_input_matrix = torch.ones((input_length, input_length))
    masked = torch.triu(ones_input_matrix, diagonal=1).bool()

    def attn_scores_hook(attn_scores, hook):
        assert torch.all(
            attn_scores[:, :, masked] == float("-inf")
        ), "Attention scores excluded by the mask are not being set to -inf"

        # Set low attention scores that are attended to by the mask
        attn_scores[:, :, ~masked] = low_attn_score

        return attn_scores

    def attn_hook(attn, hook):
        assert torch.all(attn[:, :, masked] == 0), "Attention pattern attends outside the mask"

        return attn

    fwd_hooks = [
        (utils.get_act_name("attn_scores", layer), attn_scores_hook),
        (utils.get_act_name("attn", layer), attn_hook),
    ]

    model.run_with_hooks(input, fwd_hooks=fwd_hooks)


def test_masked_tokens():
    """Test that masking tokens works as expected."""
    MODEL = "solu-1l"
    prompts = [
        "Hello world!",
        "The quick brown fox jumps over the lazy dog.",
    ]
    model = HookedTransformer.from_pretrained(MODEL)
    tokens = model.to_tokens(prompts)

    # Part 1: If the mask is all ones, the output should be the same as if there was no mask.
    full_mask = torch.ones_like(tokens)
    no_mask_out = model(tokens)
    full_mask_out = model(tokens, attention_mask=full_mask)
    assert torch.allclose(no_mask_out, full_mask_out), "Full mask should be equivalent to no mask"

    # Part 2: If the mask has a column of zeros, the output should be the same as if that token
    # position was removed from the input.
    remove_tok_idx = 2
    edited_tokens = torch.cat([tokens[:, :remove_tok_idx], tokens[:, remove_tok_idx + 1 :]], dim=1)
    edited_mask = full_mask.clone()
    edited_mask[:, remove_tok_idx] = 0
    edited_no_mask_out = model(edited_tokens)
    edited_mask_out = model(tokens, attention_mask=edited_mask)
    edited_mask_out = torch.cat(
        [edited_mask_out[:, :remove_tok_idx], edited_mask_out[:, remove_tok_idx + 1 :]], dim=1
    )
    assert torch.allclose(
        edited_no_mask_out, edited_mask_out, atol=1e-4
    ), "Edited mask should be equivalent to no mask"



---
File: /tests/integration/test_cache_hook_names.py
---

from transformer_lens import HookedTransformer

MODEL = "solu-1l"

prompt = "Hello World!"
model = HookedTransformer.from_pretrained(MODEL)

act_names_in_cache = [
    "hook_embed",
    "hook_pos_embed",
    "blocks.0.hook_resid_pre",
    "blocks.0.ln1.hook_scale",
    "blocks.0.ln1.hook_normalized",
    "blocks.0.attn.hook_q",
    "blocks.0.attn.hook_k",
    "blocks.0.attn.hook_v",
    "blocks.0.attn.hook_attn_scores",
    "blocks.0.attn.hook_pattern",
    "blocks.0.attn.hook_z",
    "blocks.0.hook_attn_out",
    "blocks.0.hook_resid_mid",
    "blocks.0.ln2.hook_scale",
    "blocks.0.ln2.hook_normalized",
    "blocks.0.mlp.hook_pre",
    "blocks.0.mlp.hook_mid",
    "blocks.0.mlp.ln.hook_scale",
    "blocks.0.mlp.ln.hook_normalized",
    "blocks.0.mlp.hook_post",
    "blocks.0.hook_mlp_out",
    "blocks.0.hook_resid_post",
    "ln_final.hook_scale",
    "ln_final.hook_normalized",
]


def test_cache_hook_names():
    _, cache = model.run_with_cache(prompt)
    assert list(cache.keys()) == act_names_in_cache



---
File: /tests/integration/test_cache_pos_slice.py
---

# %%

import torch

from transformer_lens import HookedTransformer

MODEL = "tiny-stories-1M"

prompt = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
model = HookedTransformer.from_pretrained(MODEL)
# %%
d_model = model.cfg.d_model
d_head = model.cfg.d_head
n_heads = model.cfg.n_heads
n_layers = model.cfg.n_layers
# %%


def test_run_with_cache_pos_slice_keep_batch():
    _, cache_no_slice = model.run_with_cache(prompt, return_type=None)
    num_tokens = len(model.tokenizer.encode(prompt))

    for i in range(-1, num_tokens + 1):
        _, cache_with_slice = model.run_with_cache(prompt, return_type=None, pos_slice=i)

        assert cache_with_slice["embed"].shape == torch.Size([1, 1, d_model])
        assert cache_with_slice["q", 0].shape == torch.Size([1, 1, n_heads, d_head])

        assert torch.equal(cache_no_slice["embed"][0, i, :], cache_with_slice["embed"][0, 0, :])
        assert torch.equal(
            cache_no_slice["pos_embed"][0, i, :], cache_with_slice["pos_embed"][0, 0, :]
        )

        for layer in range(n_layers):
            assert torch.equal(
                cache_no_slice["resid_pre", layer][0, i, :],
                cache_with_slice["resid_pre", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["resid_post", layer][0, i, :],
                cache_with_slice["resid_post", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["resid_mid", layer][0, i, :],
                cache_with_slice["resid_mid", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["scale", layer, "ln1"][0, i, :],
                cache_with_slice["scale", layer, "ln1"][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["scale", layer, "ln2"][0, i, :],
                cache_with_slice["scale", layer, "ln2"][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["normalized", layer, "ln1"][0, i, :],
                cache_with_slice["normalized", layer, "ln1"][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["normalized", layer, "ln2"][0, i, :],
                cache_with_slice["normalized", layer, "ln2"][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "q",
                    layer,
                ][0, i, :, :],
                cache_with_slice[
                    "q",
                    layer,
                ][0, 0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "k",
                    layer,
                ][0, i, :, :],
                cache_with_slice[
                    "k",
                    layer,
                ][0, 0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "v",
                    layer,
                ][0, i, :, :],
                cache_with_slice[
                    "v",
                    layer,
                ][0, 0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "z",
                    layer,
                ][0, i, :, :],
                cache_with_slice[
                    "z",
                    layer,
                ][0, 0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "attn_scores",
                    layer,
                ][0, :, i, :],
                cache_with_slice[
                    "attn_scores",
                    layer,
                ][0, :, 0, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "pattern",
                    layer,
                ][0, :, i, :],
                cache_with_slice[
                    "pattern",
                    layer,
                ][0, :, 0, :],
            )
            assert torch.equal(
                cache_no_slice["attn_out", layer][0, i, :],
                cache_with_slice["attn_out", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["pre", layer][0, i, :],
                cache_with_slice["pre", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["post", layer][0, i, :],
                cache_with_slice["post", layer][0, 0, :],
            )
            assert torch.equal(
                cache_no_slice["mlp_out", layer][0, i, :],
                cache_with_slice["mlp_out", layer][0, 0, :],
            )


def test_run_with_cache_pos_slice_remove_batch():
    _, cache_no_slice = model.run_with_cache(prompt, remove_batch_dim=True, return_type=None)
    num_tokens = len(model.tokenizer.encode(prompt))

    for i in range(-1, num_tokens + 1):
        _, cache_with_slice = model.run_with_cache(prompt, remove_batch_dim=True, pos_slice=i)

        assert cache_with_slice["embed"].shape == torch.Size([1, d_model])
        assert cache_with_slice["q", 0].shape == torch.Size([1, n_heads, d_head])

        assert torch.equal(cache_no_slice["embed"][i, :], cache_with_slice["embed"][0, :])
        assert torch.equal(cache_no_slice["pos_embed"][i, :], cache_with_slice["pos_embed"][0, :])

        for layer in range(n_layers):
            assert torch.equal(
                cache_no_slice["resid_pre", layer][i, :],
                cache_with_slice["resid_pre", layer][0, :],
            )
            assert torch.equal(
                cache_no_slice["resid_post", layer][i, :],
                cache_with_slice["resid_post", layer][0, :],
            )
            assert torch.equal(
                cache_no_slice["resid_mid", layer][i, :],
                cache_with_slice["resid_mid", layer][0, :],
            )
            assert torch.equal(
                cache_no_slice["scale", layer, "ln1"][i, :],
                cache_with_slice["scale", layer, "ln1"][0, :],
            )
            assert torch.equal(
                cache_no_slice["scale", layer, "ln2"][i, :],
                cache_with_slice["scale", layer, "ln2"][0, :],
            )
            assert torch.equal(
                cache_no_slice["normalized", layer, "ln1"][i, :],
                cache_with_slice["normalized", layer, "ln1"][0, :],
            )
            assert torch.equal(
                cache_no_slice["normalized", layer, "ln2"][i, :],
                cache_with_slice["normalized", layer, "ln2"][0, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "q",
                    layer,
                ][i, :, :],
                cache_with_slice[
                    "q",
                    layer,
                ][0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "k",
                    layer,
                ][i, :, :],
                cache_with_slice[
                    "k",
                    layer,
                ][0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "v",
                    layer,
                ][i, :, :],
                cache_with_slice[
                    "v",
                    layer,
                ][0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "z",
                    layer,
                ][i, :, :],
                cache_with_slice[
                    "z",
                    layer,
                ][0, :, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "attn_scores",
                    layer,
                ][:, i, :],
                cache_with_slice[
                    "attn_scores",
                    layer,
                ][:, 0, :],
            )
            assert torch.equal(
                cache_no_slice[
                    "pattern",
                    layer,
                ][:, i, :],
                cache_with_slice[
                    "pattern",
                    layer,
                ][:, 0, :],
            )
            assert torch.equal(
                cache_no_slice["attn_out", layer][i, :],
                cache_with_slice["attn_out", layer][0, :],
            )
            assert torch.equal(
                cache_no_slice["pre", layer][i, :], cache_with_slice["pre", layer][0, :]
            )
            assert torch.equal(
                cache_no_slice["post", layer][i, :],
                cache_with_slice["post", layer][0, :],
            )
            assert torch.equal(
                cache_no_slice["mlp_out", layer][i, :],
                cache_with_slice["mlp_out", layer][0, :],
            )



---
File: /tests/integration/test_create_hooked_encoder.py
---

import pytest
from transformers import AutoTokenizer, BertTokenizerFast

from transformer_lens import HookedEncoder, HookedTransformerConfig


@pytest.fixture
def cfg():
    return HookedTransformerConfig(d_head=4, d_model=12, n_ctx=5, n_layers=3, act_fn="gelu")


def test_pass_tokenizer(cfg):
    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
    model = HookedEncoder(cfg, tokenizer=tokenizer)
    assert model.tokenizer == tokenizer


def test_load_tokenizer_from_config(cfg):
    cfg.tokenizer_name = "bert-base-cased"
    model = HookedEncoder(cfg)
    assert isinstance(model.tokenizer, BertTokenizerFast)


def test_load_without_tokenizer(cfg):
    cfg.d_vocab = 22
    model = HookedEncoder(cfg)
    assert model.tokenizer is None


def test_cannot_load_without_tokenizer_or_d_vocab(cfg):
    with pytest.raises(AssertionError) as e:
        HookedEncoder(cfg)
    assert "Must provide a tokenizer if d_vocab is not provided" in str(e.value)



---
File: /tests/integration/test_cross_entropy_loss.py
---

import torch

from transformer_lens.HookedTransformer import HookedTransformer


def test_cross_entropy_attention_mask():
    """Check that adding a bunch of masked tokens to the input does not change the loss."""
    MODEL = "solu-1l"
    model = HookedTransformer.from_pretrained(MODEL)

    # Step 1: Get the default loss on a prompt
    prompt = ["The quick brown fox jumps over the lazy dog."]
    default_tokens = model.to_tokens(prompt)
    default_attention_mask = torch.ones_like(default_tokens)
    default_loss = model(default_tokens, return_type="loss")
    ones_mask_loss = model(
        default_tokens, attention_mask=default_attention_mask, return_type="loss"
    )
    assert torch.allclose(default_loss, ones_mask_loss, atol=1e-6)

    # Step 2: Get the loss when we add some extra tokens to the input and set their attention mask
    # to zero
    extra_prompt = ["Lorem ipsum dolor sit amet, consectetur adipiscing elit."]
    extra_tokens = model.to_tokens(extra_prompt)
    extra_zeros_attention_mask = torch.zeros_like(extra_tokens)

    combined_tokens = torch.cat([default_tokens, extra_tokens], dim=1)
    combined_attention_mask = torch.cat([default_attention_mask, extra_zeros_attention_mask], dim=1)
    combined_masked_loss = model(
        combined_tokens, attention_mask=combined_attention_mask, return_type="loss"
    )
    assert torch.allclose(default_loss, combined_masked_loss)



---
File: /tests/integration/test_d_vocab.py
---

from transformers import AutoTokenizer

from transformer_lens import HookedTransformer, HookedTransformerConfig


def test_d_vocab_from_tokenizer():
    cfg = HookedTransformerConfig(
        n_layers=1, d_mlp=10, d_model=10, d_head=5, n_heads=2, n_ctx=20, act_fn="relu"
    )
    model = HookedTransformer(cfg=cfg, tokenizer=AutoTokenizer.from_pretrained("gpt2"))
    assert model.cfg.d_vocab == 50257
    assert model.cfg.d_vocab_out == 50257


def test_d_vocab_from_tokenizer_name():
    cfg = HookedTransformerConfig(
        n_layers=1,
        d_mlp=10,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        act_fn="relu",
        tokenizer_name="gpt2",
    )
    model = HookedTransformer(cfg=cfg)
    assert model.cfg.d_vocab == 50257
    assert model.cfg.d_vocab_out == 50257


def test_d_vocab_out_set():
    cfg = HookedTransformerConfig(
        n_layers=1,
        d_mlp=10,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        act_fn="relu",
        d_vocab=100,
        d_vocab_out=90,
    )
    model = HookedTransformer(cfg=cfg)
    assert model.cfg.d_vocab == 100
    assert model.cfg.d_vocab_out == 90


def test_d_vocab_out_set_d_vocab_infer():
    cfg = HookedTransformerConfig(
        n_layers=1,
        d_mlp=10,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        act_fn="relu",
        d_vocab_out=90,
        tokenizer_name="gpt2",
    )
    model = HookedTransformer(cfg=cfg)
    assert model.cfg.d_vocab == 50257
    assert model.cfg.d_vocab_out == 90



---
File: /tests/integration/test_grouped_query_attention.py
---

import einops
import torch

from transformer_lens import HookedTransformer
from transformer_lens.components import Attention, GroupedQueryAttention
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_grouped_query_attention_output_is_correct():
    """Verifies that grouped query attention (GPA) block behaves correctly - see https://arxiv.org/abs/2305.13245v2 for details on GPA.
    A GPA block with h query heads, n key-value heads, key parameters _K and value parameters _V should have the same output as a regular attention block
    with h heads, whose parameters K and V are _K and _V repeated h/n times respectively. This test uses torch.repeat_interleave, which is also used by
    the GPA block internally, to generate K and V from _K and _V"""
    d_model = 512
    d_head = 32
    n_heads = 16
    n_ctx = 128
    n_key_value_heads = 4
    n_layers = 1

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        act_fn="silu",
    )

    regular_attention = Attention(cfg)
    grouped_query_attention = GroupedQueryAttention(cfg)

    W_Q = torch.rand((n_heads, d_model, d_head))
    b_Q = torch.rand((n_heads, d_head))
    _W_K = torch.rand((n_key_value_heads, d_model, d_head))
    W_K = torch.repeat_interleave(_W_K, dim=0, repeats=n_heads // n_key_value_heads)
    _b_K = torch.rand((n_key_value_heads, d_head))
    b_K = torch.repeat_interleave(_b_K, dim=0, repeats=n_heads // n_key_value_heads)
    _W_V = torch.rand((n_key_value_heads, d_model, d_head))
    W_V = torch.repeat_interleave(_W_V, dim=0, repeats=n_heads // n_key_value_heads)
    _b_V = torch.rand((n_key_value_heads, d_head))
    b_V = torch.repeat_interleave(_b_V, dim=0, repeats=n_heads // n_key_value_heads)
    W_O = torch.rand((n_heads, d_head, d_model))
    b_O = torch.rand(d_model)

    regular_attention_state_dict = {
        "W_Q": W_Q,
        "b_Q": b_Q,
        "W_O": W_O,
        "b_O": b_O,
        "W_K": W_K,
        "b_K": b_K,
        "W_V": W_V,
        "b_V": b_V,
        "mask": regular_attention.state_dict()["mask"],
        "IGNORE": regular_attention.state_dict()["IGNORE"],
    }
    grouped_query_attention_state_dict = {
        "W_Q": W_Q,
        "b_Q": b_Q,
        "W_O": W_O,
        "b_O": b_O,
        "_W_K": _W_K,
        "_b_K": _b_K,
        "_W_V": _W_V,
        "_b_V": _b_V,
        "mask": grouped_query_attention.state_dict()["mask"],
        "IGNORE": grouped_query_attention.state_dict()["IGNORE"],
    }

    regular_attention.load_state_dict(regular_attention_state_dict)
    grouped_query_attention.load_state_dict(grouped_query_attention_state_dict)

    query_input = torch.rand((1, 5, d_model))
    key_input = torch.rand((1, 5, d_model))
    value_input = torch.rand((1, 5, d_model))

    regular_attn_output = regular_attention(query_input, key_input, value_input)
    grouped_query_attn_output = grouped_query_attention(query_input, key_input, value_input)

    assert torch.equal(regular_attn_output, grouped_query_attn_output)

    # Test GQA behaves correctly when use_split_qkv_input is True
    grouped_query_attention.cfg.use_split_qkv_input = True

    split_query_input = einops.repeat(query_input, "b n d -> b n h d", h=n_heads).clone()
    split_key_input = einops.repeat(key_input, "b n d -> b n h d", h=n_key_value_heads).clone()
    split_value_input = einops.repeat(value_input, "b n d -> b n h d", h=n_key_value_heads).clone()

    split_grouped_query_attn_output = grouped_query_attention(
        split_query_input, split_key_input, split_value_input
    )

    assert torch.allclose(regular_attn_output, split_grouped_query_attn_output, rtol=1e-6)


def test_ungroup_grouped_query_attention_flag_produces_same_result():
    d_model = 512
    d_head = 32
    n_heads = 16
    n_ctx = 128
    n_key_value_heads = 4
    n_layers = 1

    cfg_flag_off = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        act_fn="silu",
        ungroup_grouped_query_attention=False,
    )
    grouped_query_attention_flag_off = GroupedQueryAttention(cfg_flag_off)

    cfg_flag_on = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        act_fn="silu",
        ungroup_grouped_query_attention=True,
    )
    grouped_query_attention_flag_on = GroupedQueryAttention(cfg_flag_on)

    W_Q = torch.rand((n_heads, d_model, d_head))
    b_Q = torch.rand((n_heads, d_head))
    _W_K = torch.rand((n_key_value_heads, d_model, d_head))
    _b_K = torch.rand((n_key_value_heads, d_head))
    _W_V = torch.rand((n_key_value_heads, d_model, d_head))
    _b_V = torch.rand((n_key_value_heads, d_head))
    W_O = torch.rand((n_heads, d_head, d_model))
    b_O = torch.rand(d_model)

    grouped_query_attention_state_dict = {
        "W_Q": W_Q,
        "b_Q": b_Q,
        "W_O": W_O,
        "b_O": b_O,
        "_W_K": _W_K,
        "_b_K": _b_K,
        "_W_V": _W_V,
        "_b_V": _b_V,
        "mask": grouped_query_attention_flag_off.state_dict()["mask"],
        "IGNORE": grouped_query_attention_flag_off.state_dict()["IGNORE"],
    }

    grouped_query_attention_flag_off.load_state_dict(grouped_query_attention_state_dict)
    grouped_query_attention_flag_on.load_state_dict(grouped_query_attention_state_dict)

    query_input = torch.rand((1, 5, d_model))
    key_input = torch.rand((1, 5, d_model))
    value_input = torch.rand((1, 5, d_model))

    grouped_query_attn_flag_off_output = grouped_query_attention_flag_off(
        query_input, key_input, value_input
    )
    grouped_query_attn_flag_on_output = grouped_query_attention_flag_on(
        query_input, key_input, value_input
    )

    assert torch.equal(grouped_query_attn_flag_off_output, grouped_query_attn_flag_on_output)


def test_ungroup_grouped_query_attention_flag_changes_k_v_hooks_shape():
    d_model = 512
    d_head = 32
    n_heads = 16
    n_ctx = 128
    n_key_value_heads = 4
    n_layers = 1
    d_vocab = 10

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        act_fn="silu",
        d_vocab=d_vocab,
        use_split_qkv_input=True,
        ungroup_grouped_query_attention=False,
    )

    model = HookedTransformer(cfg)
    assert model.cfg.ungroup_grouped_query_attention is False

    x = torch.arange(1, 9).unsqueeze(0)
    flag_off_output, flag_off_cache = model.run_with_cache(
        x,
        names_filter=[
            "blocks.0.attn.hook_k",
            "blocks.0.attn.hook_v",
            "blocks.0.hook_k_input",
            "blocks.0.hook_v_input",
        ],
    )

    model.set_ungroup_grouped_query_attention(True)
    assert model.cfg.ungroup_grouped_query_attention is True

    flag_on_output, flag_on_cache = model.run_with_cache(
        x,
        names_filter=[
            "blocks.0.attn.hook_k",
            "blocks.0.attn.hook_v",
            "blocks.0.hook_k_input",
            "blocks.0.hook_v_input",
        ],
    )

    assert (
        flag_on_cache["blocks.0.attn.hook_k"].shape[2]
        == flag_off_cache["blocks.0.attn.hook_k"].shape[2] * n_key_value_heads
    )
    assert (
        flag_on_cache["blocks.0.attn.hook_v"].shape[2]
        == flag_off_cache["blocks.0.attn.hook_v"].shape[2] * n_key_value_heads
    )
    assert (
        flag_on_cache["blocks.0.hook_k_input"].shape[2]
        == flag_off_cache["blocks.0.hook_k_input"].shape[2] * n_key_value_heads
    )
    assert (
        flag_on_cache["blocks.0.hook_v_input"].shape[2]
        == flag_off_cache["blocks.0.hook_v_input"].shape[2] * n_key_value_heads
    )

    assert torch.equal(flag_off_output, flag_on_output)



---
File: /tests/integration/test_head_detector.py
---

import math

import pytest
import torch
from beartype.roar import BeartypeCallHintParamViolation

from transformer_lens import HookedTransformer
from transformer_lens.head_detector import (
    HEAD_NAMES,
    ErrorMeasure,
    detect_head,
    get_duplicate_token_head_detection_pattern,
    get_induction_head_detection_pattern,
    get_previous_token_head_detection_pattern,
)

MODEL = "solu-2l"
ATOL = 1e-4  # Absolute tolerance - how far does a float have to be before we consider it no longer equal?
# ATOL is set to 1e-4 because the tensors we check on are also to 4 decimal places.

model = HookedTransformer.from_pretrained(MODEL)
test_regular_sequence = " four token sequence"  # Four tokens including BOS
test_duplicated_sequence = " seven token sequence seven token sequence"
test_duplicated_sequence2 = " one two three one two three"
test_regular_sequence_padded = " 2 2 2 seven token seq"
zeros_detection_pattern = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)
test_duplicated_seq_len = model.to_tokens(test_duplicated_sequence).shape[-1]


# expected_regular_sequence_previous

expected_regular_sequence_previous_match_mul = torch.tensor(
    [
        [0.3567, 0.2326, 0.2587, 0.2669, 0.1437, 0.5924, 0.2706, 0.2647],
        [0.3250, 0.2487, 0.2685, 0.2771, 0.2737, 0.2520, 0.2523, 0.3359],
    ]
)

expected_regular_sequence_previous_match_abs = torch.tensor(
    [
        [-0.0370, -0.2850, -0.2330, -0.2160, -0.4630, 0.4350, -0.2090, -0.2210],
        [-0.1000, -0.2530, -0.2130, -0.1960, -0.2030, -0.2460, -0.2450, -0.0780],
    ]
)

# expected_duplicated_sequence_previous

expected_duplicated_sequence_previous_match_mul = torch.tensor(
    [
        [0.2978, 0.1737, 0.1593, 0.1686, 0.0905, 0.6462, 0.1885, 0.1767],
        [0.2923, 0.2045, 0.1845, 0.2083, 0.1797, 0.1529, 0.1564, 0.2445],
    ]
)

expected_duplicated_sequence_previous_match_abs = torch.tensor(
    [
        [-0.2620, -0.5100, -0.5390, -0.5200, -0.6760, 0.4350, -0.4800, -0.5040],
        [-0.2720, -0.4480, -0.4880, -0.4410, -0.4980, -0.5510, -0.5440, -0.3680],
    ]
)

# expected_duplicated_sequence_duplicate


expected_duplicated_sequence_duplicate_match_mul = torch.tensor(
    [
        [0.0904, 0.0944, 0.0010, 0.0155, 0.2024, 0.0071, 0.0164, 0.0715],
        [0.0381, 0.0038, 0.0309, 0.0184, 0.0322, 0.0103, 0.0066, 0.0446],
    ]
)

expected_duplicated_sequence_duplicate_match_abs = torch.tensor(
    [
        [-0.2480, -0.2400, -0.4270, -0.3980, -0.0240, -0.4140, -0.3960, -0.2860],
        [-0.3520, -0.4210, -0.3670, -0.3920, -0.3640, -0.4080, -0.4150, -0.3390],
    ]
)

# expected_duplicated_sequence_induction

expected_duplicated_sequence_induction_match_mul = torch.tensor(
    [
        [0.1242, 0.0539, 0.0109, 0.0178, 0.0005, 0.0560, 0.0312, 0.0521],
        [0.0659, 0.1994, 0.0430, 0.0289, 0.0470, 0.0119, 0.1726, 0.0665],
    ]
)

expected_duplicated_sequence_induction_match_abs = torch.tensor(
    [
        [-0.1800, -0.3210, -0.4070, -0.3930, -0.4280, -0.3170, -0.3660, -0.3240],
        [-0.2970, -0.0300, -0.3430, -0.3710, -0.3350, -0.4050, -0.0830, -0.2950],
    ]
)

# expected_previous_exclude_bos

expected_previous_exclude_bos_match_mul = torch.tensor(
    [
        [0.4312, 0.1414, 0.4195, 0.3316, 0.0016, 0.7672, 0.4385, 0.2628],
        [0.4030, 0.1467, 0.3050, 0.3247, 0.3062, 0.2421, 0.2043, 0.3593],
    ]
)

expected_previous_exclude_bos_match_abs = torch.tensor(
    [
        [0.4560, 0.3180, 0.4930, 0.4770, 0.2720, 0.7640, 0.4870, 0.4300],
        [0.3770, 0.4750, 0.4380, 0.4330, 0.4550, 0.4850, 0.4870, 0.4130],
    ]
)

# expected_previous_exclude_current_token

expected_previous_exclude_current_token_match_mul = torch.tensor(
    [
        [0.5441, 0.4149, 0.3545, 0.3771, 0.2738, 0.8821, 0.3797, 0.3835],
        [0.6770, 0.3445, 0.3959, 0.4228, 0.4032, 0.3449, 0.3434, 0.5770],
    ]
)

expected_previous_exclude_current_token_match_abs = torch.tensor(
    [
        [0.3080, 0.1550, 0.0380, 0.0760, 0.0130, 0.7630, 0.0780, 0.0890],
        [0.4200, 0.0250, 0.1090, 0.1490, 0.1190, 0.0230, 0.0200, 0.3400],
    ]
)

# expected_previous_exclude_bos_and_current_token

expected_previous_exclude_bos_and_current_token_match_mul = torch.tensor(
    [
        [0.6092, 0.5601, 0.8043, 0.8732, 0.1130, 0.9122, 0.6857, 0.4405],
        [0.7011, 0.7523, 0.5545, 0.6449, 0.7958, 0.7565, 0.7082, 0.7833],
    ]
)


expected_previous_exclude_bos_and_current_token_match_abs = torch.tensor(
    [
        [0.5500, 0.5080, 0.5130, 0.5190, 0.4980, 0.8420, 0.5240, 0.4890],
        [0.6470, 0.5030, 0.5100, 0.5280, 0.5260, 0.5050, 0.5030, 0.5810],
    ]
)


# Successes
class Test_detect_head_successful:
    class Test_mul:
        @pytest.mark.parametrize(
            ("head_name", "expected"),
            (
                ("previous_token_head", expected_regular_sequence_previous_match_mul),
                ("duplicate_token_head", zeros_detection_pattern),
                ("induction_head", zeros_detection_pattern),
            ),
        )
        def test_regular_sequence(self, head_name, expected):
            result = detect_head(
                model,
                test_regular_sequence,
                detection_pattern=head_name,
                error_measure="mul",
            )
            assert torch.allclose(result, expected, atol=ATOL)

        @pytest.mark.parametrize(
            ("head_name", "expected"),
            (
                (
                    "previous_token_head",
                    expected_duplicated_sequence_previous_match_mul,
                ),
                (
                    "duplicate_token_head",
                    expected_duplicated_sequence_duplicate_match_mul,
                ),
                ("induction_head", expected_duplicated_sequence_induction_match_mul),
            ),
        )
        def test_duplicated_sequence(self, head_name, expected):
            result = detect_head(
                model,
                test_duplicated_sequence,
                detection_pattern=head_name,
                error_measure="mul",
            )
            assert torch.allclose(result, expected, atol=ATOL)

    class Test_abs:
        @pytest.mark.parametrize(
            ("head_name", "expected"),
            (
                ("previous_token_head", expected_regular_sequence_previous_match_abs),
                ("duplicate_token_head", zeros_detection_pattern),
                ("induction_head", zeros_detection_pattern),
            ),
        )
        def test_regular_sequence(self, head_name, expected):
            result = detect_head(
                model,
                test_regular_sequence,
                detection_pattern=head_name,
                error_measure="abs",
            )
            assert torch.allclose(result, expected, atol=ATOL)

        @pytest.mark.parametrize(
            ("head_name", "expected"),
            (
                (
                    "previous_token_head",
                    expected_duplicated_sequence_previous_match_abs,
                ),
                (
                    "duplicate_token_head",
                    expected_duplicated_sequence_duplicate_match_abs,
                ),
                ("induction_head", expected_duplicated_sequence_induction_match_abs),
            ),
        )
        def test_duplicated_sequence(self, head_name, expected):
            result = detect_head(
                model,
                test_duplicated_sequence,
                detection_pattern=head_name,
                error_measure="abs",
            )
            assert torch.allclose(result, expected, atol=ATOL)


@pytest.mark.parametrize("head_name", HEAD_NAMES)
def test_batched_equal_lengths(head_name):
    result_regular_padded = detect_head(model, test_regular_sequence_padded, head_name)
    result_duplicated = detect_head(model, test_duplicated_sequence, head_name)
    result_duplicated2 = detect_head(model, test_duplicated_sequence2, head_name)
    result_batched = detect_head(
        model,
        [
            test_regular_sequence_padded,
            test_duplicated_sequence,
            test_duplicated_sequence2,
        ],
        head_name,
    )
    expected = (result_regular_padded + result_duplicated + result_duplicated2) / 3
    assert torch.allclose(result_batched, expected, atol=ATOL)


def test_batched_unequal_lengths():
    s1 = test_regular_sequence
    s2 = test_duplicated_sequence
    s3 = [s1, s2]
    r1 = detect_head(model, s1, "previous_token_head")
    r2 = detect_head(model, s2, "previous_token_head")
    r3 = detect_head(model, s3, "previous_token_head")

    assert torch.allclose(r3, (r1 + r2) / 2, atol=ATOL)


@pytest.mark.parametrize(
    "error_measure, expected",
    (
        ("mul", expected_previous_exclude_bos_match_mul),
        ("abs", expected_previous_exclude_bos_match_abs),
    ),
)
def test_detect_head_exclude_bos(error_measure: ErrorMeasure, expected: torch.Tensor):
    assert torch.allclose(
        detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            exclude_bos=True,
            error_measure=error_measure,
        ),
        expected,
        atol=ATOL,
    )


@pytest.mark.parametrize(
    "error_measure, expected",
    (
        ("mul", expected_previous_exclude_current_token_match_mul),
        ("abs", expected_previous_exclude_current_token_match_abs),
    ),
)
def test_detect_head_exclude_current_token(error_measure: ErrorMeasure, expected: torch.Tensor):
    assert torch.allclose(
        detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            exclude_current_token=True,
            error_measure=error_measure,
        ),
        expected,
        atol=ATOL,
    )


@pytest.mark.parametrize(
    "error_measure, expected",
    (
        ("mul", expected_previous_exclude_bos_and_current_token_match_mul),
        ("abs", expected_previous_exclude_bos_and_current_token_match_abs),
    ),
)
def test_detect_head_exclude_bos_and_current_token(
    error_measure: ErrorMeasure, expected: torch.Tensor
):
    assert torch.allclose(
        detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            exclude_bos=True,
            exclude_current_token=True,
            error_measure=error_measure,
        ),
        expected,
        atol=ATOL,
    )


@pytest.mark.parametrize(
    "error_measure, expected",
    (
        ("mul", expected_regular_sequence_previous_match_mul),
        ("abs", expected_regular_sequence_previous_match_abs),
    ),
)
def test_detect_head_with_cache(error_measure: ErrorMeasure, expected: torch.Tensor):
    _, cache = model.run_with_cache(test_regular_sequence, remove_batch_dim=True)
    assert torch.allclose(
        detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            cache=cache,
            error_measure=error_measure,
        ),
        expected,
        atol=ATOL,
    )


##########
# Errors #
##########


def test_detect_head_with_invalid_head_name():
    with pytest.raises(BeartypeCallHintParamViolation) as e:
        detect_head(model, test_regular_sequence, "test")


def test_detect_head_with_zero_sequence_length():
    with pytest.raises(AssertionError) as e:
        detect_head(model, "", "previous_token_head")
    assert (
        str(e.value)
        == "The sequence must be non-empty and must fit within the model's context window."
    )


def test_detect_head_with_sequence_length_outside_context_window():
    with pytest.raises(AssertionError) as e:
        detect_head(model, "a " * model.cfg.n_ctx, "previous_token_head")
    assert (
        str(e.value)
        == "The sequence must be non-empty and must fit within the model's context window."
    )


def test_detect_head_with_invalid_detection_pattern():
    with pytest.raises(AssertionError) as e:
        detect_head(model, test_duplicated_sequence, torch.ones(4, 4))
    assert "The detection pattern must be a lower triangular" in str(e.value)


class Test_detect_head_non_lower_triangular_detection_pattern:
    detection_pattern = torch.tril(torch.ones(test_duplicated_seq_len, test_duplicated_seq_len))

    def test_no_error(self):
        detect_head(
            model,
            test_duplicated_sequence,
            self.detection_pattern.to(device=model.cfg.device),
        )
        assert True  # ugly, need to make a separate context manager for not raising an error

    def test_raises_error(self):
        detection_pattern = self.detection_pattern.clone()
        detection_pattern[0, 1] = 1
        with pytest.raises(AssertionError) as e:
            detect_head(model, test_duplicated_sequence, detection_pattern)
        assert "The detection pattern must be a lower triangular" in str(e.value)


#################################
# Detecting with specific heads #
#################################


class Test_specific_heads:
    class Test_regular_sentence_previous_token_head:
        match_mul = detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            heads=[(0, 0)],
            error_measure="abs",
        )
        match_abs = detect_head(
            model,
            test_regular_sequence,
            "previous_token_head",
            heads=[(0, 0)],
            error_measure="abs",
        )

        def test_allclose_mul(self):
            assert torch.allclose(
                self.match_mul[0, 0],
                expected_regular_sequence_previous_match_abs[0, 0],
                atol=ATOL,
            )

        def test_allclose_abs(self):
            assert torch.allclose(
                self.match_abs[0, 0],
                expected_regular_sequence_previous_match_abs[0, 0],
                atol=ATOL,
            )

        def test_isclose_mul(self):
            assert math.isclose(
                torch.sum(self.match_abs),
                self.match_mul[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

        def test_isclose_abs(self):
            assert math.isclose(
                torch.sum(self.match_abs),
                self.match_abs[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

    class Test_duplicated_sentence_previous_token_head:
        match_mul = detect_head(
            model,
            test_duplicated_sequence,
            "previous_token_head",
            heads=[(0, 0)],
            error_measure="mul",
        )
        match_abs = detect_head(
            model,
            test_duplicated_sequence,
            "previous_token_head",
            heads=[(0, 0)],
            error_measure="abs",
        )

        def test_allclose_mul(self):
            assert torch.allclose(
                self.match_mul[0, 0],
                expected_duplicated_sequence_previous_match_mul[0, 0],
                atol=ATOL,
            )

        def test_allclose_abs(self):
            assert torch.allclose(
                self.match_abs[0, 0],
                expected_duplicated_sequence_previous_match_abs[0, 0],
                atol=ATOL,
            )

        def test_isclose_mul(self):
            assert math.isclose(
                torch.sum(self.match_mul),
                self.match_mul[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

        def test_isclose_abs(self):
            assert math.isclose(
                torch.sum(self.match_abs),
                self.match_abs[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

    class Test_duplicated_sentence_duplicate_token_head:
        match_mul = detect_head(
            model,
            test_duplicated_sequence,
            "duplicate_token_head",
            heads=[(0, 0)],
            error_measure="mul",
        )
        match_abs = detect_head(
            model,
            test_duplicated_sequence,
            "duplicate_token_head",
            heads=[(0, 0)],
            error_measure="abs",
        )

        def test_allclose_mul(self):
            assert torch.allclose(
                self.match_mul[0, 0],
                expected_duplicated_sequence_duplicate_match_mul[0, 0],
                atol=ATOL,
            )

        def test_allclose_abs(self):
            assert torch.allclose(
                self.match_abs[0, 0],
                expected_duplicated_sequence_duplicate_match_abs[0, 0],
                atol=ATOL,
            )

        def test_isclose_mul(self):
            assert math.isclose(
                torch.sum(self.match_mul),
                self.match_mul[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

        def test_isclose_abs(self):
            assert math.isclose(
                torch.sum(self.match_abs),
                self.match_abs[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

    class Test_duplicated_sentence_induction_head:
        match_mul = detect_head(
            model,
            test_duplicated_sequence,
            "induction_head",
            heads=[(0, 0)],
            error_measure="mul",
        )
        match_abs = detect_head(
            model,
            test_duplicated_sequence,
            "induction_head",
            heads=[(0, 0)],
            error_measure="abs",
        )

        def test_allclose_mul(self):
            assert torch.allclose(
                self.match_mul[0, 0],
                expected_duplicated_sequence_induction_match_mul[0, 0],
                atol=ATOL,
            )

        def test_allclose_abs(self):
            assert torch.allclose(
                self.match_abs[0, 0],
                expected_duplicated_sequence_induction_match_abs[0, 0],
                atol=ATOL,
            )

        def test_isclose_mul(self):
            assert math.isclose(
                torch.sum(self.match_mul),
                self.match_mul[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )

        def test_isclose_abs(self):
            assert math.isclose(
                torch.sum(self.match_abs),
                self.match_abs[0, 0].item() - (model.cfg.n_layers * model.cfg.n_heads - 1),
                abs_tol=ATOL,
            )


######################
# Detection patterns #
######################


class Test_previous_token_head:
    regular_detection_pattern = get_previous_token_head_detection_pattern(
        model.to_tokens(test_regular_sequence).cpu()
    )

    def test_regular_detection_pattern1(self):
        assert self.regular_detection_pattern.shape == (4, 4)

    def test_regular_detection_pattern2(self):
        assert (self.regular_detection_pattern[1:, :-1] == torch.eye(3)).all()

    def test_regular_detection_pattern3(self):
        assert torch.sum(self.regular_detection_pattern) == 3

    duplicate_detection_pattern = get_previous_token_head_detection_pattern(
        model.to_tokens(test_duplicated_sequence).cpu()
    )

    def test_duplicate_detection_pattern1(self):
        assert self.duplicate_detection_pattern.shape == (7, 7)

    def test_duplicate_detection_pattern2(self):
        assert (self.duplicate_detection_pattern[1:, :-1] == torch.eye(6)).all()

    def test_duplicate_detection_pattern3(self):
        assert torch.sum(self.duplicate_detection_pattern) == 6


class Test_duplicate_token_head:
    detection_pattern = get_duplicate_token_head_detection_pattern(
        model.to_tokens(test_duplicated_sequence).cpu()
    )

    def test1(self):
        assert (
            get_duplicate_token_head_detection_pattern(model.to_tokens(test_regular_sequence).cpu())
            == torch.zeros(4, 4)
        ).all()

    def test2(self):
        assert self.detection_pattern.shape == (7, 7)

    def test3(self):
        assert (self.detection_pattern[4:, 1:4] == torch.eye(3)).all()

    def test4(self):
        assert torch.sum(self.detection_pattern) == 3


class Test_induction_head_detection:
    detection_pattern = get_induction_head_detection_pattern(
        model.to_tokens(test_duplicated_sequence).cpu()
    )

    def test1(self):
        assert (
            get_duplicate_token_head_detection_pattern(model.to_tokens(test_regular_sequence).cpu())
            == torch.zeros(4, 4)
        ).all()

    def test2(self):
        assert self.detection_pattern.shape == (7, 7)

    def test3(self):
        assert (self.detection_pattern[4:, 2:5] == torch.eye(3)).all()

    def test4(self):
        assert torch.sum(self.detection_pattern) == 3



---
File: /tests/integration/test_hooks.py
---

from typing import Any

import pytest
import torch

from transformer_lens import HookedTransformer

MODEL = "solu-1l"

prompt = "Hello World!"
model = HookedTransformer.from_pretrained(MODEL)
embed = lambda name: name == "hook_embed"


class Counter:
    def __init__(self):
        self.count = 0

    def inc(self, *args, **kwargs):
        self.count += 1


def test_hook_attaches_normally():
    c = Counter()
    _ = model.run_with_hooks(prompt, fwd_hooks=[(embed, c.inc)])
    assert all([len(hp.fwd_hooks) == 0 for _, hp in model.hook_dict.items()])
    assert c.count == 1
    model.remove_all_hook_fns(including_permanent=True)


def test_perma_hook_attaches_normally():
    c = Counter()
    model.add_perma_hook(embed, c.inc)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    model.run_with_hooks(prompt, fwd_hooks=[])
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    assert c.count == 1
    model.remove_all_hook_fns(including_permanent=True)


def test_hook_context_manager():
    c = Counter()
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
        model.forward(prompt)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    assert c.count == 1
    model.remove_all_hook_fns(including_permanent=True)


def test_nested_hook_context_manager():
    c = Counter()
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
        model.forward(prompt)
        assert c.count == 1
        with model.hooks(fwd_hooks=[(embed, c.inc)]):
            assert len(model.hook_dict["hook_embed"].fwd_hooks) == 2
            model.forward(prompt)
            assert c.count == 3  # 2 from outer, 1 from inner
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    assert c.count == 3
    model.remove_all_hook_fns(including_permanent=True)


def test_context_manager_run_with_cache():
    c = Counter()
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
        model.run_with_cache(prompt)
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    assert c.count == 1
    model.remove_all_hook_fns(including_permanent=True)


def test_backward_hook_runs_successfully():
    c = Counter()

    def skip_grad(output_grad: torch.Tensor, hook: Any):
        c.inc()
        return (output_grad,)

    with model.hooks(bwd_hooks=[(embed, skip_grad)]):
        assert len(model.hook_dict["hook_embed"].bwd_hooks) == 1
        out = model(prompt)
        assert c.count == 0
        out.sum().backward()  # this should run the hook
        assert len(model.hook_dict["hook_embed"].bwd_hooks) == 1
    assert len(model.hook_dict["hook_embed"].bwd_hooks) == 0
    assert c.count == 1
    model.remove_all_hook_fns(including_permanent=True)


def test_hook_context_manager_with_permanent_hook():
    c = Counter()
    model.add_perma_hook(embed, c.inc)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 2
        model.forward(prompt)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
    assert c.count == 2  # 1 from permanent, 1 from context manager
    model.remove_all_hook_fns(including_permanent=True)


def test_nested_context_manager_with_failure():
    def fail_hook(z, hook):
        raise ValueError("fail")

    c = Counter()
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        with pytest.raises(ValueError):
            with model.hooks(fwd_hooks=[(embed, fail_hook)]):
                assert len(model.hook_dict["hook_embed"].fwd_hooks) == 2
                model.forward(prompt)
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
        assert c.count == 1
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    model.remove_all_hook_fns(including_permanent=True)


def test_reset_hooks_in_context_manager():
    c = Counter()
    with model.hooks(fwd_hooks=[(embed, c.inc)]):
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1
        model.reset_hooks()
        assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0
    model.remove_all_hook_fns(including_permanent=True)


def test_remove_hook():
    c = Counter()
    model.add_perma_hook(embed, c.inc)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1  # 1 after adding
    model.remove_all_hook_fns()
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 1  # permanent not removed without flag
    model.remove_all_hook_fns(including_permanent=True)
    assert len(model.hook_dict["hook_embed"].fwd_hooks) == 0  # removed now
    model.run_with_hooks(prompt, fwd_hooks=[])
    assert c.count == 0
    model.remove_all_hook_fns(including_permanent=True)


def test_conditional_hooks():
    """Test that it's only possible to add certain hooks when certain conditions are met"""

    def identity_hook(z, hook):
        return z

    for hook_name, set_use_hook_function in [
        ("blocks.0.attn.hook_result", model.set_use_attn_result),
        ("blocks.0.hook_q_input", model.set_use_split_qkv_input),
        ("blocks.0.hook_mlp_in", model.set_use_hook_mlp_in),
        ("blocks.0.hook_attn_in", model.set_use_attn_in),
    ]:
        model.reset_hooks()
        set_use_hook_function(False)
        # Ensure that we get an error when we inappropriately add a hook
        with pytest.raises(AssertionError):
            model.add_hook(hook_name, identity_hook)

        # Ensure we DON'T get an error when we add a hook properly
        set_use_hook_function(True)
        model.add_hook(hook_name, identity_hook)

        # Reset the flag
        set_use_hook_function(False)

    # Check that hooks cache things with the right shape

    # The correct shapes of cached values for the hooks with three dimensions and with four dimensions
    # (1, 4, ... because the batch size is 1 and the sequence length is 4)
    correct_shapes = {
        3: (1, 4, model.cfg.d_model),
        4: (1, 4, model.cfg.n_heads, model.cfg.d_model),
    }

    for hook_name, set_use_hook_function, number_of_dimensions in [
        ("blocks.0.hook_q_input", model.set_use_split_qkv_input, 4),
        ("blocks.0.hook_attn_in", model.set_use_attn_in, 4),
        ("blocks.0.hook_mlp_in", model.set_use_hook_mlp_in, 3),
    ]:
        model.reset_hooks()
        set_use_hook_function(True)

        cache = model.run_with_cache(
            prompt,
            names_filter=lambda x: x == hook_name,
        )[1]

        assert list(cache.keys()) == [hook_name]
        assert cache[hook_name].shape == correct_shapes[number_of_dimensions]

        # Reset the flag
        set_use_hook_function(False)


@pytest.mark.parametrize(
    "zero_attach_pos,prepend",
    [(zero_attach_pos, prepend) for zero_attach_pos in range(2) for prepend in [True, False]],
)
def test_prepending_hooks(zero_attach_pos, prepend):
    """Add two hooks to a model: one that sets last layer activations to all 0s
    One that sets them to random noise.

    If the last activations are 0, then the logits will just be the model's logit bias.
    This is not true if the last activations are random noise.

    This test tests the prepending functionality by ensuring this property holds!"""

    def set_to_zero(z, hook):
        z[:] = 0.0
        return z

    def set_to_randn(z, hook):
        z = torch.randn_like(z) * 0.1
        return z

    model.reset_hooks()

    for hook_idx in range(2):
        model.add_hook(
            "blocks.0.hook_resid_post",
            set_to_zero if hook_idx == zero_attach_pos else set_to_randn,
            prepend=prepend,
        )
    logits = model(torch.arange(5)[None, :])

    logits_are_unembed_bias = (zero_attach_pos == 1) != prepend
    # the logits should be equal to the unembed bias
    # exactly when the zero hook is attached last XOR it is prepended

    assert torch.allclose(logits, model.unembed.b_U[None, :]) == logits_are_unembed_bias


def test_use_attn_in_with_gqa_raises_error():
    # Create model that uses GroupedQueryAttention
    model = HookedTransformer.from_pretrained("Qwen/Qwen2-0.5B")
    with pytest.raises(AssertionError):
        model.set_use_attn_in(True)



---
File: /tests/integration/test_kv_cache.py
---

import pytest
import torch as t

from transformer_lens import HookedTransformer
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache


# Pythia models seem to have some kind of numerical stability issue.
# See: https://github.com/TransformerLensOrg/TransformerLens/issues/385
@pytest.fixture(scope="session", params=[("gpt2-small", 1e-4), ("pythia-14m", 1e-2)])
def model_and_atol(request):
    return request.param


@pytest.fixture(scope="session")
def pretrained(model_and_atol):
    name, atol = model_and_atol
    model = HookedTransformer.from_pretrained(name, default_padding_side="left")
    return model, atol


def test_single_new_token(pretrained):
    model, atol = pretrained
    pre_prompt = "I went to Staten Island,"
    pre_prompt_tokens = model.to_tokens(pre_prompt)
    pre_prompt_tokens_len = pre_prompt_tokens.shape[-1]
    single_token_post_prompt = " Sharon"
    single_new_token = model.to_tokens(single_token_post_prompt, prepend_bos=False)
    full_prompt_tokens = t.cat([pre_prompt_tokens, single_new_token], dim=-1)
    no_cache_logits = model(full_prompt_tokens)
    assert full_prompt_tokens.shape[-1] == pre_prompt_tokens_len + 1

    past_kv_cache = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, pre_prompt_tokens.shape[0]
    )
    model(
        pre_prompt_tokens,
        past_kv_cache=past_kv_cache,
    )
    with_cache_logits = model(
        single_new_token,
        past_kv_cache=past_kv_cache,
    )
    assert t.allclose(no_cache_logits[:, -1], with_cache_logits[:, -1], atol=atol)
    assert t.allclose(no_cache_logits[:, -1:], with_cache_logits, atol=atol)


def test_multiple_new_tokens(pretrained):
    model, atol = pretrained
    pre_prompt = "I went to Staten Island,"
    pre_prompt_tokens = model.to_tokens(pre_prompt)
    pre_prompt_tokens_len = pre_prompt_tokens.shape[-1]
    post_prompt = " to buy myself a mandolin"
    new_tokens = model.to_tokens(post_prompt, prepend_bos=False)
    new_tokens_len = new_tokens.shape[-1]
    full_prompt_tokens = t.cat([pre_prompt_tokens, new_tokens], dim=-1)
    assert full_prompt_tokens.shape[-1] == pre_prompt_tokens_len + new_tokens_len
    no_cache_logits = model(full_prompt_tokens)

    past_kv_cache = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, pre_prompt_tokens.shape[0]
    )
    model(
        pre_prompt_tokens,
        past_kv_cache=past_kv_cache,
    )
    with_cache_logits = model(
        new_tokens,
        past_kv_cache=past_kv_cache,
    )
    assert t.allclose(no_cache_logits[:, -1], with_cache_logits[:, -1], atol=atol)
    assert t.allclose(no_cache_logits[:, -new_tokens_len:], with_cache_logits, atol=atol)


@pytest.mark.parametrize("pre_padding", ["left", "right", None])
@pytest.mark.parametrize("post_padding", ["left", "right", None])
def test_multi_token_batch(pretrained, pre_padding, post_padding):
    model, atol = pretrained
    padded_batch_pre_prompts = [
        "It's always locked",
        "I'd rather be burned in Canada",
    ]
    unpadded_batch_pre_prompts = [
        "It's always locked",
        "It's always blocked",
    ]
    padded_batch_post_prompts = [
        " by the magistrate",
        " than to freeze here in the South",
    ]
    unpadded_batch_post_prompts = [
        " by the magistrate",
        " by the candidate",
    ]

    first_post_prompt_tokens = model.to_tokens(padded_batch_post_prompts[0], prepend_bos=False)
    first_full_prompt_tokens = t.cat(
        [model.to_tokens(padded_batch_pre_prompts[0]), first_post_prompt_tokens], dim=-1
    )
    first_post_prompt_len = first_post_prompt_tokens.shape[-1]
    first_prompt_no_cache_logits = model(first_full_prompt_tokens)
    first_post_prompt_no_cache_logits = first_prompt_no_cache_logits[0, -first_post_prompt_len:]

    if pre_padding is None:
        batch_pre_prompt_tokens = model.to_tokens(unpadded_batch_pre_prompts)
    else:
        assert pre_padding == "left" or pre_padding == "right"
        batch_pre_prompt_tokens = model.to_tokens(
            padded_batch_pre_prompts, padding_side=pre_padding
        )

    if post_padding is None:
        batch_post_prompt_tokens = model.to_tokens(unpadded_batch_post_prompts, prepend_bos=False)
    else:
        assert post_padding == "left" or post_padding == "right"
        batch_post_prompt_tokens = model.to_tokens(
            padded_batch_post_prompts,
            prepend_bos=False,
            padding_side=post_padding,
        )

    past_kv_cache = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, batch_pre_prompt_tokens.shape[0]
    )
    model(batch_pre_prompt_tokens, past_kv_cache=past_kv_cache, padding_side=pre_padding)
    past_kv_cache.freeze()
    with_cache_logits = model(
        batch_post_prompt_tokens,
        past_kv_cache=past_kv_cache,
        padding_side=post_padding,
        prepend_bos=False,
    )
    if post_padding == "left" or post_padding is None:
        first_post_prompt_with_cache_logits = with_cache_logits[0, -first_post_prompt_len:]
    else:
        assert post_padding == "right"
        first_post_prompt_with_cache_logits = with_cache_logits[0, :first_post_prompt_len]

    no_cache_probs = t.softmax(first_post_prompt_no_cache_logits, dim=-1)
    with_cache_probs = t.softmax(first_post_prompt_with_cache_logits, dim=-1)
    assert t.allclose(no_cache_probs, with_cache_probs, atol=atol)


def test_freeze_cache(pretrained):
    model, atol = pretrained
    pre_prompt = "I went to Staten Island,"
    pre_prompt_tokens = model.to_tokens(pre_prompt)
    post_prompt_1 = " I'm headed to the church to play bingo."
    new_tokens_1 = model.to_tokens(post_prompt_1, prepend_bos=False)
    past_kv_cache_1 = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, pre_prompt_tokens.shape[0]
    )

    post_prompt_2 = " shine your light on me, Miss Liberty"
    new_tokens_2 = model.to_tokens(post_prompt_2, prepend_bos=False)
    past_kv_cache_2 = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, pre_prompt_tokens.shape[0]
    )

    model(
        pre_prompt_tokens,
        past_kv_cache=past_kv_cache_1,
    )
    past_kv_cache_1.freeze()
    with_cache_logits_1 = model(
        new_tokens_1,
        past_kv_cache=past_kv_cache_1,
    )

    model(
        pre_prompt_tokens,
        past_kv_cache=past_kv_cache_2,
    )
    past_kv_cache_2.freeze()
    model(
        new_tokens_2,
        past_kv_cache=past_kv_cache_2,
    )

    # Caches frozen at the same point should be identical
    assert len(past_kv_cache_1.entries) == len(past_kv_cache_2.entries)
    for entry_1, entry_2 in zip(past_kv_cache_1.entries, past_kv_cache_2.entries):
        assert entry_1.past_keys.shape == entry_2.past_keys.shape
        assert entry_1.past_values.shape == entry_2.past_values.shape
        assert t.allclose(entry_1.past_keys, entry_2.past_keys, atol=1e-3)
        assert t.allclose(entry_1.past_values, entry_2.past_values, atol=1e-3)

    # Rerunning the same prompt with a different cache that was frozen at the same
    # point should give the same results
    with_cache_2_logits_1 = model(
        new_tokens_1,
        past_kv_cache=past_kv_cache_2,
    )
    assert t.allclose(with_cache_logits_1, with_cache_2_logits_1, atol=atol)

    # Test unfreeze
    past_kv_cache_2.unfreeze()
    with_cache_2_logits_1 = model(
        new_tokens_1,
        past_kv_cache=past_kv_cache_2,
    )
    for entry_1, entry_2 in zip(past_kv_cache_1.entries, past_kv_cache_2.entries):
        assert entry_1.past_keys.shape[1] < entry_2.past_keys.shape[1]
        assert entry_1.past_values.shape[1] < entry_2.past_values.shape[1]

    # Rerunning the same prompt with a different cache should give different
    # results
    assert t.allclose(with_cache_logits_1, with_cache_2_logits_1, atol=atol)
    with_cache_2_logits_1 = model(
        new_tokens_1,
        past_kv_cache=past_kv_cache_2,
    )
    assert not t.allclose(with_cache_logits_1, with_cache_2_logits_1, atol=atol)


def test_kv_cache_with_custom_attention_mask(pretrained):
    model, atol = pretrained
    prompt_pre = "An apple"
    prompt_post = " a day keeps junk the"
    prompt_whole = "An apple a day keeps the"
    tokens_pre = model.to_tokens(prompt_pre)
    tokens_post = model.to_tokens(prompt_post, prepend_bos=False)
    tokens_whole = model.to_tokens(prompt_whole)
    correct_logits = model(tokens_whole)

    past_kv_cache = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, tokens_pre.shape[0]
    )
    model(tokens_pre, past_kv_cache=past_kv_cache)
    exp_logits = model(
        tokens_post,
        attention_mask=t.tensor([[1, 1, 1, 0, 1]], device=model.cfg.device),
        past_kv_cache=past_kv_cache,
    )
    assert t.allclose(correct_logits[:, -1], exp_logits[:, -1], atol=atol)


def test_kv_cache_and_start_at_layer(pretrained):
    model, atol = pretrained
    pre_prompt = "I went to Staten Island,"
    pre_prompt_tokens = model.to_tokens(pre_prompt)
    pre_prompt_tokens_len = pre_prompt_tokens.shape[-1]
    single_token_post_prompt = " Sharon"
    single_new_token = model.to_tokens(single_token_post_prompt, prepend_bos=False)
    full_prompt_tokens = t.cat([pre_prompt_tokens, single_new_token], dim=-1)
    no_cache_logits = model(full_prompt_tokens)
    assert full_prompt_tokens.shape[-1] == pre_prompt_tokens_len + 1

    past_kv_cache = HookedTransformerKeyValueCache.init_cache(
        model.cfg, model.cfg.device, pre_prompt_tokens.shape[0]
    )
    model(
        pre_prompt_tokens,
        past_kv_cache=past_kv_cache,
    )
    past_kv_cache.freeze()
    _, toks, shortformer_pos_embed, attn_mask = model.input_to_embed(
        single_new_token, past_kv_cache=past_kv_cache
    )
    _, cache = model.run_with_cache(single_new_token, stop_at_layer=4, past_kv_cache=past_kv_cache)
    resid_3 = cache["blocks.3.hook_resid_pre"]
    with_cache_logits = model(
        resid_3,
        start_at_layer=3,
        tokens=toks,
        shortformer_pos_embed=shortformer_pos_embed,
        attention_mask=attn_mask,
        past_kv_cache=past_kv_cache,
    )
    assert t.allclose(no_cache_logits[:, -1], with_cache_logits[:, -1], atol=atol)
    assert t.allclose(no_cache_logits[:, -1:], with_cache_logits, atol=atol)



---
File: /tests/integration/test_left_padding.py
---

import pytest
import torch

from transformer_lens import HookedTransformer, utils
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache


class TestLeftPadding:
    prompts = [
        "Hello world!",
        "How are you today?",
        "I'm fine, thank you.",
        "I am happy.",
    ]

    # helpers
    def check_outputs_identity(
        self,
        i,
        single_outputs,
        left_outputs,
        right_outputs,
        left_token_start,
        left_token_end,
        right_token_start,
        right_token_end,
    ):
        atol = 1e-4

        assert torch.allclose(
            left_outputs[i, left_token_start:left_token_end, :],
            right_outputs[i, right_token_start:right_token_end, :],
            atol=atol,
        )

        assert torch.allclose(
            left_outputs[i, left_token_start:left_token_end, :],
            single_outputs[0],
            atol=atol,
        )

        assert torch.allclose(
            right_outputs[i, right_token_start:right_token_end, :],
            single_outputs[0],
            atol=atol,
        )

    # fixtures
    @pytest.fixture(scope="class", params=["gpt2-small", "facebook/opt-125m"])
    def model_name(self, request):
        return request.param

    @pytest.fixture(scope="class")
    def model(self, model_name):
        model = HookedTransformer.from_pretrained(model_name)
        return model

    # tests
    @pytest.mark.parametrize("padding_side", ["left", "right"])
    @pytest.mark.parametrize("prepend_bos", [True, False])
    def test_pos_embed(self, model, padding_side, prepend_bos):
        # setup
        model.tokenizer.padding_side = padding_side

        prompts = self.prompts
        tokens = model.to_tokens(prompts, prepend_bos=prepend_bos)
        str_tokens = model.to_str_tokens(prompts, prepend_bos=prepend_bos)

        attention_mask = utils.get_attention_mask(
            model.tokenizer, tokens, prepend_bos
        )  # [batch pos]

        output_pos_embed = model.pos_embed(
            tokens, 0, attention_mask=attention_mask
        )  # [batch pos d_model]

        # check if the output pos_embeds have the correct shape
        assert output_pos_embed.shape == (
            tokens.shape[0],
            tokens.shape[1],
            model.pos_embed.W_pos.shape[1],
        )

        # check if the target pos_embeds are the same as the output pos_embeds
        target_position_ids = torch.tensor(
            sum([list(range(len(t))) for t in str_tokens], []), device=tokens.device
        )
        target_output_pos_embed = model.pos_embed.W_pos[target_position_ids, :]

        attended_output_pos_embed = output_pos_embed[attention_mask.bool()]

        assert torch.allclose(attended_output_pos_embed, target_output_pos_embed, atol=1e-4)

        # padded positions should have zero pos_embed
        assert output_pos_embed[~attention_mask.bool()].sum() == 0

    @pytest.mark.parametrize("padding_side", ["left", "right"])
    @pytest.mark.parametrize("prepend_bos", [True, False])
    def test_pos_embed_with_cache(self, model, padding_side, prepend_bos):
        # setup
        model.tokenizer.padding_side = padding_side

        prompts = self.prompts
        tokens = model.to_tokens(prompts, prepend_bos=prepend_bos)
        tokens_2 = model.to_tokens(prompts, prepend_bos=False)

        past_kv_cache = HookedTransformerKeyValueCache.init_cache(
            model.cfg, model.cfg.device, tokens.shape[0]
        )

        str_tokens = model.to_str_tokens(prompts, prepend_bos=prepend_bos)
        str_tokens_2 = model.to_str_tokens(prompts, prepend_bos=False)

        attention_mask = utils.get_attention_mask(
            model.tokenizer, tokens, prepend_bos
        )  # [batch pos]
        past_kv_cache.append_attention_mask(attention_mask)
        attention_mask_2 = utils.get_attention_mask(model.tokenizer, tokens_2, False)  # [batch pos]
        cached_attention_mask = past_kv_cache.append_attention_mask(attention_mask_2)

        output_pos_embed = model.pos_embed(
            tokens_2, tokens.shape[1], attention_mask=cached_attention_mask
        )  # [batch pos d_model]

        # check if the target pos_embeds are the same as the output pos_embeds
        target_position_ids = torch.tensor(
            sum(
                [
                    list(range(len(t1), len(t1) + len(t2)))
                    for t1, t2 in zip(str_tokens, str_tokens_2)
                ],
                [],
            ),
            device=tokens.device,
        )
        target_output_pos_embed = model.pos_embed.W_pos[target_position_ids, :]

        attended_output_pos_embed = output_pos_embed[attention_mask_2.bool()]

        assert torch.allclose(attended_output_pos_embed, target_output_pos_embed, atol=1e-4)

        # padded positions should have zero pos_embed
        assert output_pos_embed[~attention_mask_2.bool()].sum() == 0

    def test_left_padding_by_comparing_outputs(self, model):
        prompts = self.prompts

        num_str_tokens_list = [len(t) for t in model.to_str_tokens(prompts)]

        # left padding output
        model.tokenizer.padding_side = "left"
        left_logits, left_cache = model.run_with_cache(prompts)
        left_last_logits = left_logits[:, -1, :]
        left_first_token_positions = left_logits.shape[1] - torch.tensor(
            num_str_tokens_list, device=left_logits.device
        )
        left_first_logits = left_logits[
            torch.arange(len(prompts)), left_first_token_positions, :
        ].squeeze(1)

        # right padding output
        model.tokenizer.padding_side = "right"
        right_logits, right_cache = model.run_with_cache(prompts)
        right_last_token_positions = (
            torch.tensor(num_str_tokens_list, device=right_logits.device) - 1
        )
        right_last_logits = right_logits[
            torch.arange(len(prompts)), right_last_token_positions, :
        ].squeeze(1)
        right_first_logits = right_logits[:, 0, :]

        # check if the left and right padding outputs are the same for the first and last tokens
        assert torch.allclose(left_last_logits, right_last_logits, atol=1e-4)
        assert torch.allclose(left_first_logits, right_first_logits, atol=1e-4)

        # check if the left and right padding outputs are the same for all tokens
        # and if the batched padded outputs are the same as the single prompt outputs
        right_token_start = 0
        left_token_end = left_logits.shape[1]
        for i, (prompt, left_token_start, right_token_end) in enumerate(
            zip(
                prompts,
                left_first_token_positions.tolist(),
                (right_last_token_positions + 1).tolist(),
            )
        ):
            single_logits, single_cache = model.run_with_cache(prompt)

            assert (
                right_token_end - right_token_start
                == left_token_end - left_token_start
                == single_logits.shape[1]
            )

            self.check_outputs_identity(
                i,
                single_logits,
                left_logits,
                right_logits,
                left_token_start,
                left_token_end,
                right_token_start,
                right_token_end,
            )

            # check cache
            for name in ["k6a", "pre2", "embed", "k6", "scale4ln1", "pre5"]:
                self.check_outputs_identity(
                    i,
                    single_cache[name],
                    left_cache[name],
                    right_cache[name],
                    left_token_start,
                    left_token_end,
                    right_token_start,
                    right_token_end,
                )



---
File: /tests/integration/test_loading_from_pretrained.py
---

"""
Tests that verify than an arbitrary component (e.g. Embed) can be initialized using dict and object versions of HookedTransformerConfig and HookedEncoderConfig.
"""

from transformer_lens import loading_from_pretrained as loading


def test_get_basic_config():
    cfg = loading.get_basic_config("gpt2-small")
    assert cfg.d_model
    assert cfg.layer_norm_eps
    assert cfg.d_vocab
    assert cfg.init_range
    assert cfg.n_ctx
    assert cfg.d_head
    assert cfg.d_mlp
    assert cfg.n_heads
    assert cfg.n_layers



---
File: /tests/integration/test_match_huggingface.py
---

import pytest
import torch
from transformers import AutoModelForCausalLM

from transformer_lens import HookedTransformer


class TestMatchHuggingFace:
    # fixtures
    @pytest.fixture(scope="class", params=["gpt2"])
    def model_name(self, request):
        return request.param

    # tests
    def test_compare_huggingface_mlp_match_local_implementation(self, model_name):
        tl_model = HookedTransformer.from_pretrained_no_processing(model_name, device="cpu")
        hf_model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu")
        tensor_shape = (3, 5, tl_model.cfg.d_model)
        test_tensor = torch.randn(tensor_shape)

        for layer_n in range(len(tl_model.blocks)):
            tl_out = tl_model.blocks[layer_n].mlp(test_tensor)
            hf_out = hf_model.transformer.h[layer_n].mlp(test_tensor)

            assert torch.allclose(tl_out, hf_out, atol=1e-4)

    def test_compare_huggingface_attention_match_local_implementation(self, model_name):
        tl_model = HookedTransformer.from_pretrained_no_processing(model_name, device="cpu")
        hf_model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu")
        batch, pos, d_model = 3, 5, tl_model.cfg.d_model
        input = torch.randn(batch, pos, d_model)

        for layer_n in range(len(tl_model.blocks)):
            tl_out = tl_model.blocks[layer_n].attn(
                query_input=input,
                key_input=input,
                value_input=input,
                past_kv_cache_entry=None,
                attention_mask=None,
            )
            hf_out = hf_model.transformer.h[layer_n].attn(
                hidden_states=input, output_attentions=True
            )[0]

            assert torch.allclose(tl_out, hf_out, atol=1e-4)



---
File: /tests/integration/test_only_tokenizer.py
---

import logging
from typing import Dict

import pytest
from transformers import AutoTokenizer

import transformer_lens.loading_from_pretrained as loading
from transformer_lens import HookedTransformer, HookedTransformerConfig


class TokenizerOnlyHookedTransformer(HookedTransformer):
    def __init__(
        self,
        cfg,
        tokenizer=None,
        move_to_device=True,
        default_padding_side="right",
    ):
        if isinstance(cfg, Dict):
            cfg = HookedTransformerConfig(**cfg)
        elif isinstance(cfg, str):
            raise ValueError(
                "Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a "
                "pretrained model, use HookedTransformer.from_pretrained() instead."
            )
        self.cfg = cfg

        if tokenizer is not None:
            self.set_tokenizer(tokenizer, default_padding_side=default_padding_side)
        elif self.cfg.tokenizer_name is not None:
            # If we have a tokenizer name, we can load it from HuggingFace
            self.set_tokenizer(
                AutoTokenizer.from_pretrained(self.cfg.tokenizer_name, add_bos_token=True),
                default_padding_side=default_padding_side,
            )
        else:
            # If no tokenizer name is provided, we assume we're training on an algorithmic task and will pass in tokens
            # directly. In this case, we don't need a tokenizer.
            assert self.cfg.d_vocab != -1, "Must provide a tokenizer if d_vocab is not provided"
            self.tokenizer = None
            if default_padding_side != "right":
                logging.warning(
                    "default_padding_side is explictly given but ignored because tokenizer is not set."
                )

    @classmethod
    def from_pretrained(
        cls,
        model_name: str,
        fold_ln=True,
        center_writing_weights=True,
        center_unembed=True,
        refactor_factored_attn_matrices=False,
        checkpoint_index=None,
        checkpoint_value=None,
        hf_model=None,
        device=None,
        n_devices=1,
        tokenizer=None,
        move_to_device=True,
        fold_value_biases=True,
        default_prepend_bos=True,
        default_padding_side="right",
        **from_pretrained_kwargs,
    ) -> "TokenizerOnlyHookedTransformer":
        # Get the model name used in HuggingFace, rather than the alias.
        official_model_name = loading.get_official_model_name(model_name)

        # Load the config into an HookedTransformerConfig object. If loading from a
        # checkpoint, the config object will contain the information about the
        # checkpoint
        cfg = loading.get_pretrained_model_config(
            official_model_name,
            checkpoint_index=checkpoint_index,
            checkpoint_value=checkpoint_value,
            fold_ln=fold_ln,
            device=device,
            n_devices=n_devices,
            default_prepend_bos=default_prepend_bos,
            **from_pretrained_kwargs,
        )

        # Create the HookedTransformer object
        model = cls(
            cfg,
            tokenizer,
            move_to_device=False,
            default_padding_side=default_padding_side,
        )

        return model


class TestTokenizer:
    prompt = "Hello world!"
    prompts = ["Italy is in Europe.", "Seoul is the capital of South Korea."]

    # helper functions
    def get_num_tokens_in_prompt(self, model, prompt, intended_prepend_bos):
        tokenizer = AutoTokenizer.from_pretrained(model.tokenizer.name_or_path, add_bos_token=False)
        tokens = tokenizer(
            prompt,
        )["input_ids"]

        return len(tokens) + int(intended_prepend_bos)

    def check_first_token(self, model, str_tokens, tokens, intended_prepend_bos):
        if intended_prepend_bos:
            assert str_tokens[0] == model.tokenizer.bos_token
            assert tokens[0][0] == model.tokenizer.bos_token_id
        else:
            assert str_tokens[0] != model.tokenizer.bos_token
            assert tokens[0][0] != model.tokenizer.bos_token_id

    def check_tokens_length(self, model, str_tokens, tokens, intended_prepend_bos):
        expected_num_tokens = self.get_num_tokens_in_prompt(
            model, self.prompt, intended_prepend_bos
        )

        assert len(str_tokens) == tokens.shape[1] == expected_num_tokens

    def check_prompt(self, model, intended_prepend_bos, overriding_prepend_bos=None):
        str_tokens = model.to_str_tokens(self.prompt, prepend_bos=overriding_prepend_bos)
        tokens = model.to_tokens(self.prompt, prepend_bos=overriding_prepend_bos)

        self.check_first_token(model, str_tokens, tokens, intended_prepend_bos)
        self.check_tokens_length(model, str_tokens, tokens, intended_prepend_bos)

    def check_prompts(
        self,
        model,
        intended_prepend_bos,
        intended_padding_side,
        overriding_prepend_bos=None,
        overriding_padding_side=None,
    ):
        tokens = model.to_tokens(
            self.prompts,
            prepend_bos=overriding_prepend_bos,
            padding_side=overriding_padding_side,
        )
        if intended_padding_side == "left":
            if model.tokenizer.pad_token_id != model.tokenizer.bos_token_id:
                assert (tokens[:, 0] == model.tokenizer.pad_token_id).sum() == 1, tokens
                assert (tokens[:, 0] == model.tokenizer.bos_token_id).sum() == (
                    1 if intended_prepend_bos else 0
                ), tokens
            else:
                assert (tokens[:, 0] == model.tokenizer.pad_token_id).sum() == (
                    tokens.shape[0] if intended_prepend_bos else 1
                ), tokens
        else:
            assert (tokens[:, -1] == model.tokenizer.pad_token_id).sum() == 1, tokens
            if intended_prepend_bos:
                assert (tokens[:, 0] == model.tokenizer.bos_token_id).all(), tokens

        if model.tokenizer.pad_token_id != model.tokenizer.bos_token_id:
            if intended_prepend_bos:
                assert (tokens == model.tokenizer.bos_token_id).sum() == tokens.shape[0], tokens
            else:
                assert (tokens == model.tokenizer.bos_token_id).sum() == 0, tokens

    # fixtures
    @pytest.fixture(
        scope="class",
        params=[
            "gpt2-small",
            "facebook/opt-125m",
            "EleutherAI/pythia-14m",
            "EleutherAI/gpt-j-6b",
        ],
    )
    def model_name(self, request):
        return request.param

    @pytest.fixture(scope="class")
    def model(self, model_name):
        model = TokenizerOnlyHookedTransformer.from_pretrained(model_name)
        return model

    # tests
    def test_defaults(self, model_name):
        intended_prepend_bos, intended_padding_side = True, "right"
        model = TokenizerOnlyHookedTransformer.from_pretrained(model_name)

        assert (
            model.cfg.default_prepend_bos == intended_prepend_bos
        ), "Default prepend_bos should be True"
        assert (
            model.tokenizer.padding_side == intended_padding_side
        ), "Default padding_side should be right"

        self.check_prompt(model, intended_prepend_bos)
        self.check_prompts(model, intended_prepend_bos, intended_padding_side)

    def test_given_defaults(self, model_name):
        intended_prepend_bos, intended_padding_side = False, "left"
        model = TokenizerOnlyHookedTransformer.from_pretrained(
            model_name, default_prepend_bos=intended_prepend_bos
        )
        if model.tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.padding_side = intended_padding_side
            model.tokenizer = tokenizer
        else:
            model.tokenizer.padding_side = intended_padding_side

        self.check_prompt(model, intended_prepend_bos)
        self.check_prompts(model, intended_prepend_bos, intended_padding_side)

    @pytest.mark.parametrize("intended_prepend_bos", [True, False])
    @pytest.mark.parametrize("intended_padding_side", ["left", "right"])
    def test_changing_defaults(self, model, intended_prepend_bos, intended_padding_side):
        model.tokenizer.padding_side = intended_padding_side
        model.cfg.default_prepend_bos = intended_prepend_bos

        self.check_prompt(model, intended_prepend_bos)
        self.check_prompts(model, intended_prepend_bos, intended_padding_side)

    @pytest.mark.parametrize("intended_prepend_bos", [True, False])
    @pytest.mark.parametrize("intended_padding_side", ["left", "right"])
    def test_overriding_defaults(self, model, intended_prepend_bos, intended_padding_side):
        self.check_prompt(model, intended_prepend_bos, intended_prepend_bos)
        self.check_prompts(
            model,
            intended_prepend_bos,
            intended_padding_side,
            intended_prepend_bos,
            intended_padding_side,
        )

    def test_same_tokenization(self, model):
        prompt = self.prompt
        prompts = [
            "Italy is in Europe.",
            "Pyeongchang Olympics was held in 2018",
            "2023-09-09",
            "287594812673495",
        ]

        model.tokenizer.padding_side = "right"

        for input in [prompt, prompts]:
            tokens_with_bos = model.to_tokens(input, prepend_bos=True)
            tokens_without_bos = model.to_tokens(input, prepend_bos=False)
            assert tokens_with_bos[..., 1:].equal(tokens_without_bos)

            str_tokens_with_bos = model.to_str_tokens(input, prepend_bos=True)
            str_tokens_without_bos = model.to_str_tokens(input, prepend_bos=False)

            if isinstance(str_tokens_with_bos[0], list):
                for i in range(len(str_tokens_with_bos)):
                    assert str_tokens_with_bos[i][1:] == str_tokens_without_bos[i]
            else:
                assert str_tokens_with_bos[1:] == str_tokens_without_bos



---
File: /tests/integration/test_prepend_bos.py
---

import pytest
from transformers import AutoTokenizer

from transformer_lens import HookedTransformer


class TestPrependBos:
    prompt = "Hello world!"

    # helper functions
    def get_num_tokens_in_prompt(self, model, prompt, intended_prepend_bos):
        tokenizer = AutoTokenizer.from_pretrained(model.tokenizer.name_or_path, add_bos_token=False)
        tokens = tokenizer(
            prompt,
        )["input_ids"]

        return len(tokens) + int(intended_prepend_bos)

    def check_first_token(self, model, str_tokens, tokens, intended_prepend_bos):
        if intended_prepend_bos:
            assert str_tokens[0] == model.tokenizer.bos_token
            assert tokens[0][0] == model.tokenizer.bos_token_id
        else:
            assert str_tokens[0] != model.tokenizer.bos_token
            assert tokens[0][0] != model.tokenizer.bos_token_id

    def check_tokens_length(self, model, logits, str_tokens, tokens, intended_prepend_bos):
        expected_num_tokens = self.get_num_tokens_in_prompt(
            model, self.prompt, intended_prepend_bos
        )
        assert logits.shape[1] == len(str_tokens) == tokens.shape[1] == expected_num_tokens

    # fixtures
    @pytest.fixture(scope="class", params=["gpt2", "facebook/opt-125m"])
    def model_name(self, request):
        return request.param

    @pytest.fixture(scope="class")
    def model(self, model_name):
        return HookedTransformer.from_pretrained(model_name)

    # tests
    def test_default_prepend_bos(self, model_name):
        intended_prepend_bos = True

        model = HookedTransformer.from_pretrained(model_name)
        assert (
            model.cfg.default_prepend_bos == intended_prepend_bos
        ), "Default prepend_bos should be True"

        logits = model(self.prompt)  # [batch pos d_vocab]
        str_tokens = model.to_str_tokens(self.prompt)
        tokens = model.to_tokens(self.prompt)  # [batch pos]

        self.check_first_token(model, str_tokens, tokens, intended_prepend_bos)
        self.check_tokens_length(model, logits, str_tokens, tokens, intended_prepend_bos)

        bos_position = model.get_token_position(model.tokenizer.bos_token_id, self.prompt)
        assert bos_position == 0

    def test_default_prepend_bos_to_false(self, model_name):
        intended_prepend_bos = False

        model = HookedTransformer.from_pretrained(
            model_name, default_prepend_bos=intended_prepend_bos
        )

        logits = model(self.prompt)  # [batch pos d_vocab]
        str_tokens = model.to_str_tokens(self.prompt)
        tokens = model.to_tokens(self.prompt)

        self.check_first_token(model, str_tokens, tokens, intended_prepend_bos)
        self.check_tokens_length(model, logits, str_tokens, tokens, intended_prepend_bos)

    @pytest.mark.parametrize("intended_prepend_bos", [True, False])
    def test_override_prepend_bos(self, model, intended_prepend_bos):
        for default_prepend_bos in [True, False]:
            model.cfg.default_prepend_bos = default_prepend_bos

            logits = model(self.prompt, prepend_bos=intended_prepend_bos)  # [batch pos d_vocab]
            str_tokens = model.to_str_tokens(self.prompt, prepend_bos=intended_prepend_bos)
            tokens = model.to_tokens(self.prompt, prepend_bos=intended_prepend_bos)

            self.check_first_token(model, str_tokens, tokens, intended_prepend_bos)
            self.check_tokens_length(model, logits, str_tokens, tokens, intended_prepend_bos)

    def test_prepend_bos_with_get_token_position(self, model_name):
        model = HookedTransformer.from_pretrained(model_name)

        bos_position = model.get_token_position(model.tokenizer.bos_token_id, self.prompt)
        assert bos_position == 0

        with pytest.raises(AssertionError):
            bos_position = model.get_token_position(
                model.tokenizer.bos_token_id, self.prompt, prepend_bos=False
            )

        model.cfg.default_prepend_bos = False
        with pytest.raises(AssertionError):
            bos_position = model.get_token_position(model.tokenizer.bos_token_id, self.prompt)

        bos_position = model.get_token_position(
            model.tokenizer.bos_token_id, self.prompt, prepend_bos=True
        )
        assert bos_position == 0

    def test_same_tokenization(self, model):
        prompt = self.prompt
        prompts = [
            "Italy is in Europe.",
            "Pyeongchang Olympics was held in 2018",
            "2023-09-09",
            "287594812673495",
        ]

        model.tokenizer.padding_side = "right"

        for input in [prompt, prompts]:
            tokens_with_bos = model.to_tokens(input, prepend_bos=True)
            tokens_without_bos = model.to_tokens(input, prepend_bos=False)
            assert tokens_with_bos[..., 1:].equal(tokens_without_bos)

            str_tokens_with_bos = model.to_str_tokens(input, prepend_bos=True)
            str_tokens_without_bos = model.to_str_tokens(input, prepend_bos=False)

            if isinstance(str_tokens_with_bos[0], list):
                for i in range(len(str_tokens_with_bos)):
                    assert str_tokens_with_bos[i][1:] == str_tokens_without_bos[i]
            else:
                assert str_tokens_with_bos[1:] == str_tokens_without_bos



---
File: /tests/integration/test_start_at_layer.py
---

"""
Tests for the start_at_layer parameter in HookedTransformer
"""

from typing import Any, Dict

import pytest
import torch

from transformer_lens import HookedTransformer, HookedTransformerConfig


@pytest.fixture
def setup_data() -> Dict[str, Any]:
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))
    rand_embed, _, _, _ = model.input_to_embed(rand_input)
    return {"model": model, "rand_input": rand_input, "rand_embed": rand_embed}


def test_start_at_layer_1(setup_data: Dict[str, Any]):
    model, rand_embed = setup_data["model"], setup_data["rand_embed"]
    output, cache = model.run_with_cache(rand_embed, start_at_layer=1)

    assert output is not None
    assert "hook_embed" not in cache.keys()
    assert "hook_pos_embed" not in cache.keys()
    assert "blocks.0.hook_resid_pre" not in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()


def test_run_with_hooks(setup_data: Dict[str, Any]):
    model, rand_embed = setup_data["model"], setup_data["rand_embed"]

    counting_list = []

    def count_hook(activation, hook):
        counting_list.append(len(counting_list))
        return None

    output = model.run_with_hooks(
        rand_embed,
        start_at_layer=1,
        fwd_hooks=[
            ("hook_embed", count_hook),
            ("blocks.0.attn.hook_k", count_hook),
            ("blocks.1.mlp.hook_pre", count_hook),
            ("blocks.2.attn.hook_k", count_hook),
            ("blocks.2.mlp.hook_pre", count_hook)
            # ("blocks.2.mlp.hook_mid", count_hook),
        ],
    )

    assert output is not None
    assert len(counting_list) == 3


def test_manual_hooks(setup_data: Dict[str, Any]):
    model, rand_embed = setup_data["model"], setup_data["rand_embed"]
    counting_list = []

    def count_hook(activation, hook):
        counting_list.append(len(counting_list))
        return None

    model.hook_embed.add_hook(count_hook)
    model.blocks[0].hook_mlp_out.add_hook(count_hook)
    model.blocks[1].hook_resid_mid.add_hook(count_hook)
    model.blocks[2].attn.hook_z.add_hook(count_hook)

    output = model(rand_embed, start_at_layer=-2)
    assert output is not None
    assert len(counting_list) == 2


def test_start_at_final(setup_data: Dict[str, Any]):
    model, rand_embed = setup_data["model"], setup_data["rand_embed"]
    output, cache = model.run_with_cache(rand_embed, start_at_layer=-1)

    assert output is not None
    assert "hook_embed" not in cache.keys()
    assert "hook_pos_embed" not in cache.keys()
    assert "blocks.0.hook_resid_pre" not in cache.keys()
    assert "blocks.0.hook_resid_post" not in cache.keys()
    assert "blocks.1.hook_resid_pre" not in cache.keys()
    assert "blocks.1.hook_resid_post" not in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "blocks.3.hook_resid_pre" not in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()


def test_no_start_logit_output(setup_data: Dict[str, Any]):
    model, rand_input = setup_data["model"], setup_data["rand_input"]
    output, cache = model.run_with_cache(rand_input, start_at_layer=None)

    assert output is not None
    assert isinstance(output, torch.Tensor)
    assert output.shape == (2, 10, 50)

    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()


def test_no_start_none_output(setup_data: Dict[str, Any]):
    model, rand_input = setup_data["model"], setup_data["rand_input"]
    output, cache = model.run_with_cache(rand_input, start_at_layer=None, return_type=None)

    assert output is None
    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()


def test_start_and_stop(setup_data: Dict[str, Any]):
    model, rand_embed = setup_data["model"], setup_data["rand_embed"]
    output, cache = model.run_with_cache(rand_embed, start_at_layer=1, stop_at_layer=2)

    assert torch.allclose(output, cache["blocks.1.hook_resid_post"])
    assert "hook_embed" not in cache.keys()
    assert "hook_pos_embed" not in cache.keys()
    assert "blocks.0.hook_resid_pre" not in cache.keys()
    assert "blocks.0.hook_resid_post" not in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" not in cache.keys()
    assert "blocks.2.hook_resid_post" not in cache.keys()
    assert "blocks.3.hook_resid_pre" not in cache.keys()
    assert "ln_final.hook_normalized" not in cache.keys()


def test_start_at_layer_kwargs():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50257,
        act_fn="relu",
        positional_embedding_type="shortformer",
        tokenizer_name="gpt2",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    assert model.tokenizer is not None
    model.tokenizer.padding_side = "left"
    input = "As soon as this ferry boat docks I'm headed to the church to play bingo."

    (
        rand_embed,
        tokens,
        shortformer_pos_embed,
        attention_mask,
    ) = model.input_to_embed(input)
    assert tokens is not None and shortformer_pos_embed is not None and attention_mask is not None

    start_at_layer_output = model(
        rand_embed,
        tokens=tokens,
        shortformer_pos_embed=shortformer_pos_embed,
        attention_mask=attention_mask,
        start_at_layer=0,
        return_type="loss",
    )
    normal_output = model(input, return_type="loss")

    assert start_at_layer_output is not None
    assert normal_output is not None
    assert torch.allclose(start_at_layer_output, normal_output)



---
File: /tests/integration/test_stop_at_layer.py
---

"""
Tests for the stop_at_layer parameter in HookedTransformer
"""

import torch

from transformer_lens import HookedTransformer, HookedTransformerConfig


def test_stop_at_embed():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    _, normal_cache = model.run_with_cache(rand_input)
    output, cache = model.run_with_cache(rand_input, stop_at_layer=0)

    assert torch.allclose(output, normal_cache["blocks.0.hook_resid_pre"])
    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" not in cache.keys()
    assert "ln_final.hook_normalized" not in cache.keys()


def test_run_with_hooks():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    counting_list = []

    def count_hook(activation, hook):
        counting_list.append(len(counting_list))
        return None

    output = model.run_with_hooks(
        rand_input,
        stop_at_layer=1,
        fwd_hooks=[
            ("hook_embed", count_hook),
            ("blocks.0.attn.hook_k", count_hook),
            ("blocks.1.mlp.hook_pre", count_hook),
        ],
    )

    _, normal_cache = model.run_with_cache(rand_input, stop_at_layer=1)
    assert torch.allclose(output, normal_cache["blocks.0.hook_resid_post"])
    assert len(counting_list) == 2


def test_manual_hooks():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))
    _, normal_cache = model.run_with_cache(rand_input)

    counting_list = []

    def count_hook(activation, hook):
        counting_list.append(len(counting_list))
        return None

    model.hook_embed.add_hook(count_hook)
    model.blocks[0].hook_mlp_out.add_hook(count_hook)
    model.blocks[1].hook_resid_mid.add_hook(count_hook)
    model.blocks[2].attn.hook_z.add_hook(count_hook)

    output = model(rand_input, stop_at_layer=-1)
    assert torch.allclose(output, normal_cache["blocks.1.hook_resid_post"])
    assert len(counting_list) == 3


def test_stop_at_layer_1():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    _, normal_cache = model.run_with_cache(rand_input)
    output, cache = model.run_with_cache(rand_input, stop_at_layer=1)

    assert torch.allclose(output, normal_cache["blocks.0.hook_resid_post"])
    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" not in cache.keys()
    assert "ln_final.hook_normalized" not in cache.keys()
    for key in cache.keys():
        if key.startswith("blocks.0"):
            continue
        elif key in ["hook_embed", "hook_pos_embed"]:
            continue
        else:
            assert False, f"Unexpected key {key} in cache."


def test_stop_at_final():
    cfg = HookedTransformerConfig(
        n_layers=4,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    _, normal_cache = model.run_with_cache(rand_input)
    output, cache = model.run_with_cache(rand_input, stop_at_layer=-1)

    assert torch.allclose(output, normal_cache["blocks.2.hook_resid_post"])
    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "blocks.3.hook_resid_pre" not in cache.keys()
    assert "ln_final.hook_normalized" not in cache.keys()


def test_no_stop_logit_output():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    normal_output = model(rand_input)
    output, cache = model.run_with_cache(rand_input, stop_at_layer=None)

    assert torch.allclose(output, normal_output)
    assert isinstance(output, torch.Tensor)
    assert output.shape == (2, 10, 50)

    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()


def test_no_stop_no_output():
    cfg = HookedTransformerConfig(
        n_layers=3,
        d_mlp=8,
        d_model=10,
        d_head=5,
        n_heads=2,
        n_ctx=20,
        d_vocab=50,
        act_fn="relu",
    )
    model = HookedTransformer(
        cfg=cfg,
    )
    rand_input = torch.randint(0, 20, (2, 10))

    output, cache = model.run_with_cache(rand_input, stop_at_layer=None, return_type=None)

    assert output is None
    assert "hook_embed" in cache.keys()
    assert "hook_pos_embed" in cache.keys()
    assert "blocks.0.hook_resid_pre" in cache.keys()
    assert "blocks.0.hook_resid_post" in cache.keys()
    assert "blocks.1.hook_resid_pre" in cache.keys()
    assert "blocks.1.hook_resid_post" in cache.keys()
    assert "blocks.2.hook_resid_pre" in cache.keys()
    assert "blocks.2.hook_resid_post" in cache.keys()
    assert "ln_final.hook_normalized" in cache.keys()



---
File: /tests/integration/test_tokenization_methods.py
---

import pytest
import torch
from torch import Size, equal, tensor
from transformers import AutoTokenizer

from transformer_lens import HookedTransformer, HookedTransformerConfig

model = HookedTransformer.from_pretrained("solu-1l")


def test_set_tokenizer_during_initialization():
    assert (
        model.tokenizer is not None
        and model.tokenizer.name_or_path == "NeelNanda/gpt-neox-tokenizer-digits"
    ), "initialized with expected tokenizer"
    assert model.cfg.d_vocab == 48262, "expected d_vocab"


def test_set_tokenizer_lazy():
    cfg = HookedTransformerConfig(
        n_layers=1, d_model=10, n_ctx=1024, d_head=1, act_fn="relu", d_vocab=50256
    )
    model2 = HookedTransformer(cfg)
    original_tokenizer = model2.tokenizer
    assert original_tokenizer is None, "initialize without tokenizer"
    model2.set_tokenizer(AutoTokenizer.from_pretrained("gpt2"))
    tokenizer = model2.tokenizer
    assert tokenizer is not None and tokenizer.name_or_path == "gpt2", "set tokenizer"
    assert (
        model2.to_single_token(" SolidGoldMagikarp") == 43453
    ), "Glitch token didn't tokenize properly"
    assert (
        model2.to_string([43453]) == " SolidGoldMagikarp"
    ), "Glitch token didn't detokenize properly"


def test_to_tokens_default():
    s = "Hello, world!"
    tokens = model.to_tokens(s)
    assert equal(
        tokens, tensor([[1, 11765, 14, 1499, 3]]).to(model.cfg.device)
    ), "creates a tensor of tokens with BOS"


def test_to_tokens_without_bos():
    s = "Hello, world!"
    tokens = model.to_tokens(s, prepend_bos=False)
    assert equal(
        tokens, tensor([[11765, 14, 1499, 3]]).to(model.cfg.device)
    ), "creates a tensor without BOS"


@pytest.mark.skipif(
    torch.cuda.is_available() or torch.backends.mps.is_available(),
    reason="Test not relevant when running on GPU",
)
def test_to_tokens_device():
    s = "Hello, world!"
    tokens1 = model.to_tokens(s, move_to_device=False)
    tokens2 = model.to_tokens(s, move_to_device=True)
    assert equal(tokens1, tokens2), "move to device has no effect when running tests on CPU"


def test_to_tokens_truncate():
    assert model.cfg.n_ctx == 1024, "verify assumed context length"
    s = "@ " * 1025
    tokens1 = model.to_tokens(s)
    tokens2 = model.to_tokens(s, truncate=False)
    assert len(tokens1[0]) == 1024, "truncated by default"
    assert len(tokens2[0]) == 1027, "not truncated"


def test_to_string_from_to_tokens_without_bos():
    s = "Hello, world!"
    tokens = model.to_tokens(s, prepend_bos=False)
    s2 = model.to_string(tokens[0])
    assert s == s2, "same string when converted back to string"


def test_to_string_multiple():
    s_list = model.to_string(tensor([[1, 11765], [43453, 28666]]))
    assert s_list == [
        "<|BOS|>Hello",
        "Charlie Planet",
    ], "can handle list of lists"


def test_to_str_tokens_default():
    s_list = model.to_str_tokens(" SolidGoldMagikarp")
    assert s_list == [
        "<|BOS|>",
        " Solid",
        "Gold",
        "Mag",
        "ik",
        "arp",
    ], "not a glitch token"


def test_to_str_tokens_without_bos():
    s_list = model.to_str_tokens(" SolidGoldMagikarp", prepend_bos=False)
    assert s_list == [
        " Solid",
        "Gold",
        "Mag",
        "ik",
        "arp",
    ], "without BOS"


def test_to_single_token():
    token = model.to_single_token("biomolecules")
    assert token == 31847, "single token"


def test_to_single_str_tokent():
    s = model.to_single_str_token(31847)
    assert s == "biomolecules"


def test_get_token_position_not_found():
    single = "biomolecules"
    input = "There were some biomolecules"
    with pytest.raises(AssertionError) as exc_info:
        model.get_token_position(single, input)
    assert str(exc_info.value) == f"The token does not occur in the prompt", "assertion error"


def test_get_token_position_str():
    single = " some"
    input = "There were some biomolecules"
    pos = model.get_token_position(single, input)
    assert pos == 3, "first position"


def test_get_token_position_str_without_bos():
    single = " some"
    input = "There were some biomolecules"
    pos = model.get_token_position(single, input, prepend_bos=False)
    assert pos == 2, "without BOS"


def test_get_token_position_int_pos():
    single = 2
    input = tensor([2.0, 3, 4])
    pos1 = model.get_token_position(single, input)
    pos2 = model.get_token_position(single, input, prepend_bos=False)
    assert pos1 == 0, "first position"
    assert pos2 == 0, "no effect from BOS when using tensor as input"


def test_get_token_position_int_pos_last():
    single = 2
    input = tensor([2.0, 3, 4, 2, 5])
    pos1 = model.get_token_position(single, input, mode="last")
    assert pos1 == 3, "last position"


def test_get_token_position_int_1_pos():
    single = 2
    input = tensor([[2.0, 3, 4]])
    pos = model.get_token_position(single, input)
    assert pos == 0, "first position"


def test_tokens_to_residual_directions():
    res_dir = model.tokens_to_residual_directions(model.to_tokens(""))
    assert res_dir.shape == Size([512]), ""



---
File: /tests/integration/test_utils_tokens.py
---

from unittest import mock

import numpy as np
import pytest
import torch

import transformer_lens.utils as utils
from transformer_lens import HookedTransformer

MODEL = "solu-1l"

model = HookedTransformer.from_pretrained(MODEL)


@pytest.fixture
def nested_list_1():
    return [1]


@pytest.fixture
def nested_list_1x1():
    return [[6]]


@pytest.fixture
def nested_list_1x3():
    return [[1, 2, 3]]


def test_to_str_tokens(nested_list_1, nested_list_1x1, nested_list_1x3):
    tensor_1_to_str_tokens = model.to_str_tokens(torch.tensor(nested_list_1))
    assert isinstance(tensor_1_to_str_tokens, list)
    assert len(tensor_1_to_str_tokens) == 1
    assert isinstance(tensor_1_to_str_tokens[0], str)

    tensor_1x1_to_str_tokens = model.to_str_tokens(torch.tensor(nested_list_1x1))
    assert isinstance(tensor_1x1_to_str_tokens, list)
    assert len(tensor_1x1_to_str_tokens) == 1
    assert isinstance(tensor_1x1_to_str_tokens[0], str)

    ndarray_1_to_str_tokens = model.to_str_tokens(np.array(nested_list_1))
    assert isinstance(ndarray_1_to_str_tokens, list)
    assert len(ndarray_1_to_str_tokens) == 1
    assert isinstance(ndarray_1_to_str_tokens[0], str)

    ndarray_1x1_to_str_tokens = model.to_str_tokens(np.array(nested_list_1x1))
    assert isinstance(ndarray_1x1_to_str_tokens, list)
    assert len(ndarray_1x1_to_str_tokens) == 1
    assert isinstance(ndarray_1x1_to_str_tokens[0], str)

    single_int_to_single_str_token = model.to_single_str_token(3)
    assert isinstance(single_int_to_single_str_token, str)

    squeezable_tensor_to_str_tokens = model.to_str_tokens(torch.tensor(nested_list_1x3))
    assert isinstance(squeezable_tensor_to_str_tokens, list)
    assert len(squeezable_tensor_to_str_tokens) == 3
    assert isinstance(squeezable_tensor_to_str_tokens[0], str)
    assert isinstance(squeezable_tensor_to_str_tokens[1], str)
    assert isinstance(squeezable_tensor_to_str_tokens[2], str)

    squeezable_ndarray_to_str_tokens = model.to_str_tokens(np.array(nested_list_1x3))
    assert isinstance(squeezable_ndarray_to_str_tokens, list)
    assert len(squeezable_ndarray_to_str_tokens) == 3
    assert isinstance(squeezable_ndarray_to_str_tokens[0], str)
    assert isinstance(squeezable_ndarray_to_str_tokens[1], str)
    assert isinstance(squeezable_ndarray_to_str_tokens[2], str)


@pytest.mark.parametrize(
    "prepend_space_to_answer, tokenized_prompt, tokenized_answer",
    [
        (
            True,
            [
                "<|BOS|>",
                "The",
                " circumference",
                " is",
                " the",
                " perimeter",
                " of",
                " the",
                " circ",
            ],
            [" le", "."],
        ),
        (
            False,
            [
                "<|BOS|>",
                "The",
                " circumference",
                " is",
                " the",
                " perimeter",
                " of",
                " the",
                " circ",
            ],
            ["le", "."],
        ),
    ],
)
@mock.patch("builtins.print")
def test_test_prompt(
    mocked_print,
    prepend_space_to_answer,
    tokenized_prompt,
    tokenized_answer,
):
    """
    Tests that utils.test_prompt produces the correct tokenization. In particular, when prepend_space_to_answer = False, the last token of the prompt
    and the first answer token should not be turned into one token (e.g. 'circ' and 'le' don't become 'circle'). See https://github.com/TransformerLensOrg/TransformerLens/issues/271
    for a more detailed explanation.
    """
    utils.test_prompt(
        "The circumference is the perimeter of the circ",
        "le.",
        model,
        prepend_space_to_answer=prepend_space_to_answer,
    )

    printed_tokenized_prompt = mock.call("Tokenized prompt:", tokenized_prompt)
    printed_tokenized_answer = mock.call("Tokenized answer:", tokenized_answer)

    assert mocked_print.mock_calls[0] == printed_tokenized_prompt
    assert mocked_print.mock_calls[1] == printed_tokenized_answer



---
File: /tests/manual_checks/manual_checks_type_annotations.py
---

import torch
from jaxtyping import Float

from transformer_lens import HookedTransformer

MODEL = "gpt2"
model = HookedTransformer.from_pretrained(MODEL)

prompt = "Hello World!"
tokens = model.to_tokens(prompt, prepend_bos=False)
logits_tokens = model(tokens)
logits_text: Float[torch.Tensor, "1 n_tokens d_vocab"] = model(prompt, prepend_bos=False)

# n.b. that i used this file to see if my type annotations were working- they were! i occasionally
# changed one of the sizes and saw that the type checker caught it.



---
File: /tests/manual_checks/manual_checks_typing.py
---

# Adapted from [HookedTransformer_Demo.ipynb]. Useful for testing that all the typing mechanisms work
# out.

# %%

import torch as t
from jaxtyping import Float

from transformer_lens import HookedTransformer, utils

DEVICE = utils.get_device()
MODEL = "gpt2"

# %%
model = HookedTransformer.from_pretrained(MODEL)
model.to(DEVICE)

# %%

prompt = "Hello World!"
tokens = model.to_tokens(prompt, prepend_bos=False)
logits_tokens = model(tokens)
logits_text: Float[t.Tensor, "1 n_tokens d_vocab"] = model(prompt, prepend_bos=False)

# %%

logits_text.shape
# %%



---
File: /tests/unit/components/mlps/test_can_be_used_as_mlp.py
---

from typing import Any, Dict

import pytest
import torch

from transformer_lens.components import LayerNorm, LayerNormPre
from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.hook_points import HookPoint
from transformer_lens.utils import solu


@pytest.fixture
def cfg() -> Dict[str, Any]:
    return {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "solu_ln",
        "normalization_type": "LN",
        "load_in_4bit": False,
    }


def test_initialization(cfg: Dict[str, Any]):
    CanBeUsedAsMLP(cfg)


def test_initialization_fails_without_d_mlp(cfg: Dict[str, Any]):
    cfg["d_mlp"] = None
    pytest.raises(ValueError)
    CanBeUsedAsMLP(cfg)


def test_select_activation_function_selects_function():
    cfg = {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "silu",
        "normalization_type": "LN",
        "load_in_4bit": False,
    }

    model = CanBeUsedAsMLP(cfg)
    model.select_activation_function()
    assert model.act_fn is not None


def test_select_activation_function_with_layer_norm():
    cfg = {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "solu_ln",
        "normalization_type": "LN",
        "load_in_4bit": False,
    }

    model = CanBeUsedAsMLP(cfg)
    model.select_activation_function()
    assert model.act_fn == solu
    assert isinstance(model.hook_mid, HookPoint)
    assert isinstance(model.ln, LayerNorm)


def test_select_activation_function_with_layer_norm_pre():
    cfg = {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "solu_ln",
        "normalization_type": "LNPre",
        "load_in_4bit": False,
    }

    model = CanBeUsedAsMLP(cfg)
    model.select_activation_function()
    assert model.act_fn == solu
    assert isinstance(model.hook_mid, HookPoint)
    assert isinstance(model.ln, LayerNormPre)



---
File: /tests/unit/components/mlps/test_gated_mlp.py
---

from typing import Any, Dict

import pytest
import torch
import torch.nn as nn

from transformer_lens.components import GatedMLP, LayerNorm
from transformer_lens.utils import solu


@pytest.fixture
def cfg() -> Dict[str, Any]:
    return {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "solu_ln",
        "normalization_type": "LN",
        "load_in_4bit": False,
    }


def test_initialization(cfg: Dict[str, Any]):
    model = GatedMLP(cfg)
    assert isinstance(model.W_in, nn.Parameter)
    assert isinstance(model.W_gate, nn.Parameter)
    assert isinstance(model.W_out, nn.Parameter)
    assert isinstance(model.b_in, nn.Parameter)
    assert isinstance(model.b_out, nn.Parameter)
    assert model.act_fn == solu
    assert isinstance(model.ln, LayerNorm)


def test_forward(cfg: Dict[str, Any]):
    model = GatedMLP(cfg)
    x = torch.randn(2, 10, cfg["d_model"])
    output = model(x)
    assert output.shape == (2, 10, cfg["d_model"])



---
File: /tests/unit/components/mlps/test_mlp.py
---

from typing import Any, Dict

import pytest
import torch

from transformer_lens.components import LayerNorm
from transformer_lens.components.mlps.mlp import MLP
from transformer_lens.hook_points import HookPoint


@pytest.fixture
def cfg() -> Dict[str, Any]:
    return {
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_model": 128,
        "d_mlp": 256,
        "dtype": torch.float32,
        "act_fn": "solu_ln",
        "normalization_type": "LN",
        "load_in_4bit": False,
    }


def test_initialization(cfg: Dict[str, Any]):
    MLP(cfg)


def test_forward_without_layer_norm(cfg: Dict[str, Any]):
    cfg["act_fn"] = "solu"

    model = MLP(cfg)

    input = torch.full((1, 1, 128), 0.085)

    result = model(input)

    assert result.shape == (1, 1, 128)


def test_forward_with_layer_norm(cfg: Dict[str, Any]):
    model = MLP(cfg)
    assert isinstance(model.hook_mid, HookPoint)
    assert isinstance(model.ln, LayerNorm)

    input = torch.full((1, 1, 128), 0.85)
    result = model(input)
    assert result.shape == (1, 1, 128)



---
File: /tests/unit/components/mlps/test_moe.py
---

import torch

from transformer_lens.components import MoE


def test_forward():
    cfg = {
        "d_model": 32,
        "d_mlp": 14336,
        "d_head": 4,
        "num_experts": 32,
        "n_layers": 16,
        "n_ctx": 2048,
        "experts_per_token": 4,
        "gated_mlp": True,
        "act_fn": "silu",
    }
    moe = MoE(cfg)

    x = torch.rand((1, 4, 32))
    moe(x)



---
File: /tests/unit/components/test_abstract_attention.py
---

import torch

from transformer_lens.components import AbstractAttention


def test_create_alibi_slope():
    n_ctx = 100

    # Expected result computed non-vectorized way
    expected = torch.zeros((n_ctx, n_ctx))
    for row in range(n_ctx):
        for col in range(n_ctx):
            expected[row, col] = float(min(col - row, 0))

    # Check against the method's vectorized version
    result = AbstractAttention.create_alibi_slope(n_ctx)
    assert torch.allclose(expected, result)


def test_create_alibi_bias():
    n_heads = 2
    n_ctx = 4

    result = AbstractAttention.create_alibi_bias(n_heads, n_ctx, torch.device("cpu"))

    for matrix in result:
        n_row, n_col = matrix.size()
        slope = -matrix[1, 0]
        # Check if upper triangle is all zeros
        assert torch.equal(torch.triu(matrix), torch.zeros_like(matrix))

        ref_lower_triangle = torch.zeros_like(matrix)
        for i in range(1, n_row):
            for j in range(i):
                ref_lower_triangle[i, j] = -slope * (i - j)

        # Check if the lower triangle is decreasing by a constant slope (towards the bottom left corner).
        assert torch.equal(
            torch.tril(matrix, diagonal=-1), torch.tril(ref_lower_triangle, diagonal=-1)
        )



---
File: /tests/unit/components/test_attention.py
---

import einops
import pytest
import torch
import torch.nn as nn
from transformers.utils import is_bitsandbytes_available

from transformer_lens.components import Attention
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.attention import complex_attn_linear

if is_bitsandbytes_available():
    from bitsandbytes.nn.modules import Params4bit


def test_attention_hooked_transformer_config():
    cfg = HookedTransformerConfig(
        n_layers=12,
        d_model=512,
        n_ctx=1024,
        d_head=64,
        n_heads=8,
        load_in_4bit=False,
        dtype=torch.float32,
        act_fn="relu",
    )
    attn = Attention(cfg)
    assert attn.cfg == cfg
    assert attn.cfg.n_layers == 12
    assert attn.cfg.d_model == 512
    assert attn.cfg.n_ctx == 1024
    assert attn.cfg.d_head == 64
    assert attn.cfg.n_heads == 8
    assert attn.cfg.load_in_4bit == False
    assert attn.cfg.dtype == torch.float32
    assert attn.cfg.act_fn == "relu"

    assert isinstance(attn.W_K, nn.Parameter)
    assert isinstance(attn.W_V, nn.Parameter)
    assert attn.W_K.shape == (cfg.n_heads, cfg.d_model, cfg.d_head)
    assert attn.W_V.shape == (cfg.n_heads, cfg.d_model, cfg.d_head)

    assert attn.b_K.shape == (cfg.n_heads, cfg.d_head)
    assert attn.b_V.shape == (cfg.n_heads, cfg.d_head)
    assert torch.all(attn.b_K == 0)
    assert torch.all(attn.b_V == 0)


@pytest.mark.skipif(not is_bitsandbytes_available(), reason="bitsandbytes is not available")
def test_attention_load_in_4bit():
    cfg = HookedTransformerConfig(
        n_layers=12,
        d_model=512,
        n_ctx=1024,
        d_head=64,
        n_heads=8,
        load_in_4bit=True,
        dtype=torch.float32,
        act_fn="relu",
    )
    attn = Attention(cfg)
    assert attn.cfg == cfg
    assert attn.cfg.n_layers == 12
    assert attn.cfg.d_model == 512
    assert attn.cfg.n_ctx == 1024
    assert attn.cfg.d_head == 64
    assert attn.cfg.n_heads == 8
    assert attn.cfg.load_in_4bit == False
    assert attn.cfg.dtype == torch.float32
    assert attn.cfg.act_fn == "relu"

    assert isinstance(attn.W_K, Params4bit)
    assert isinstance(attn.W_V, Params4bit)
    nq = int((cfg.d_model * cfg.d_model) / 2)
    assert attn.W_K.data.shape == (nq, 1)
    assert attn.W_V.data.shape == (nq, 1)

    assert attn.b_K.shape == (cfg.n_heads, cfg.d_head)
    assert attn.b_V.shape == (cfg.n_heads, cfg.d_head)
    assert torch.all(attn.b_K == 0)
    assert torch.all(attn.b_V == 0)


def test_attention_config_dict():
    cfg = {
        "n_layers": 12,
        "d_model": 512,
        "n_ctx": 1024,
        "d_head": 64,
        "n_heads": 8,
        "load_in_4bit": False,
        "dtype": torch.float32,
        "act_fn": "relu",
    }
    attn = Attention(cfg)
    assert attn.cfg.n_layers == 12
    assert attn.cfg.d_model == 512
    assert attn.cfg.n_ctx == 1024
    assert attn.cfg.d_head == 64
    assert attn.cfg.n_heads == 8
    assert attn.cfg.load_in_4bit == False
    assert attn.cfg.dtype == torch.float32
    assert attn.cfg.act_fn == "relu"


def test_remove_einsum_from_complex_attn_linear():
    batch = 64
    pos = 128
    head_index = 8
    d_model = 512
    d_head = 64
    input = torch.randn(batch, pos, head_index, d_model)
    w = torch.randn(head_index, d_model, d_head)
    b = torch.randn(head_index, d_head)
    result_new = complex_attn_linear(input, w, b)

    # Check if new implementation without einsum produces correct shape
    assert result_new.shape == (batch, pos, head_index, d_head)

    # Old implementation used einsum
    result_old = (
        einops.einsum(
            input,
            w,
            "batch pos head_index d_model, head_index d_model d_head -> batch pos head_index d_head",
        )
        + b
    )

    # Check if the results are the same
    assert torch.allclose(result_new, result_old, atol=1e-4)



---
File: /tests/unit/factored_matrix/test_constructor.py
---

import pytest
import torch

from transformer_lens import FactoredMatrix


def test_factored_matrix():
    A = torch.randn(5, 3)
    B = torch.randn(3, 7)
    f = FactoredMatrix(A, B)

    assert torch.equal(f.A, A)
    assert torch.equal(f.B, B)

    assert (f.ldim, f.mdim, f.rdim) == (5, 3, 7)
    assert not f.has_leading_dims
    assert f.shape == (5, 7)


def test_factored_matrix_b_leading_dims():
    A = torch.ones((5, 3))
    B = torch.ones((2, 4, 3, 7))
    f = FactoredMatrix(A, B)

    assert f.A.shape == (2, 4, 5, 3)
    assert torch.equal(f.B, B)

    assert (f.ldim, f.mdim, f.rdim) == (5, 3, 7)
    assert f.has_leading_dims
    assert f.shape == (2, 4, 5, 7)


def test_factored_matrix_a_b_leading_dims():
    A = torch.ones((4, 5, 3))
    B = torch.ones((2, 4, 3, 7))
    f = FactoredMatrix(A, B)

    assert f.A.shape == (2, 4, 5, 3)
    assert torch.equal(f.B, B)

    assert (f.ldim, f.mdim, f.rdim) == (5, 3, 7)
    assert f.has_leading_dims
    assert f.shape == (2, 4, 5, 7)


def test_factored_matrix_broadcast_mismatch():
    A = torch.ones((9, 5, 3))
    B = torch.ones((2, 4, 3, 7))

    with pytest.raises(RuntimeError) as e:
        FactoredMatrix(A, B)

    assert "Shape mismatch" in str(e.value)


@pytest.mark.skip(
    """
    AssertionError will not be reached due to jaxtyping argument consistency
    checks, which are enabled at test time but not run time.

    See https://github.com/TransformerLensOrg/TransformerLens/issues/190
    """
)
def test_factored_matrix_inner_mismatch():
    A = torch.ones((2, 3, 4))
    B = torch.ones((2, 3, 5))
    with pytest.raises(AssertionError) as e:
        FactoredMatrix(A, B)

    assert "inner dimension" in str(e.value)



---
File: /tests/unit/factored_matrix/test_get_item.py
---

import pytest
import torch
from torch.testing import assert_close

from transformer_lens import FactoredMatrix


@pytest.fixture
def sample_factored_matrix():
    A = torch.rand(2, 2, 2, 2, 2)
    B = torch.rand(2, 2, 2, 2, 2)
    return FactoredMatrix(A, B)


def test_getitem_int(sample_factored_matrix):
    result = sample_factored_matrix[0]
    assert_close(result.A, sample_factored_matrix.A[0])
    assert_close(result.B, sample_factored_matrix.B[0])


def test_getitem_tuple(sample_factored_matrix):
    result = sample_factored_matrix[(0, 1)]
    assert_close(result.A, sample_factored_matrix.A[0, 1])
    assert_close(result.B, sample_factored_matrix.B[0, 1])


def test_getitem_slice(sample_factored_matrix):
    result = sample_factored_matrix[:, 1]
    assert_close(result.A, sample_factored_matrix.A[:, 1])
    assert_close(result.B, sample_factored_matrix.B[:, 1])


def test_getitem_error(sample_factored_matrix):
    with pytest.raises(IndexError):
        _ = sample_factored_matrix[(0, 1, 2)]


def test_getitem_multiple_slices(sample_factored_matrix):
    result = sample_factored_matrix[:, :, 1]
    assert_close(result.A, sample_factored_matrix.A[:, :, 1])
    assert_close(result.B, sample_factored_matrix.B[:, :, 1])


def test_index_dimension_get_line(sample_factored_matrix):
    result = sample_factored_matrix[0, 0, 0, 1]
    assert_close(result.AB.squeeze(), sample_factored_matrix.AB[0, 0, 0, 1])


def test_index_dimension_get_element(sample_factored_matrix):
    result = sample_factored_matrix[0, 0, 0, 0, 1]
    assert_close(result.AB.squeeze(), sample_factored_matrix.AB[0, 0, 0, 0, 1])


def test_index_dimension_too_big(sample_factored_matrix):
    with pytest.raises(Exception):
        _ = sample_factored_matrix[1, 1, 1, 1, 1, 1]


def test_getitem_sequences(sample_factored_matrix):
    A_idx = [0, 1]
    B_idx = [0]
    result = sample_factored_matrix[:, :, :, A_idx, B_idx]
    assert_close(result.A, sample_factored_matrix.A[:, :, :, A_idx, :])
    assert_close(result.B, sample_factored_matrix.B[:, :, :, :, B_idx])


def test_getitem_sequences_and_ints(sample_factored_matrix):
    A_idx = [0, 1]
    B_idx = 0
    result = sample_factored_matrix[:, :, :, A_idx, B_idx]
    assert_close(result.A, sample_factored_matrix.A[:, :, :, A_idx, :])
    # we squeeze result.B, because indexing by ints is designed not to delete dimensions
    assert_close(result.B.squeeze(-1), sample_factored_matrix.B[:, :, :, :, B_idx])


def test_getitem_tensors(sample_factored_matrix):
    A_idx = torch.tensor([0, 1])
    B_idx = torch.tensor([0])
    result = sample_factored_matrix[:, :, :, A_idx, B_idx]
    assert_close(result.A, sample_factored_matrix.A[:, :, :, A_idx, :])
    assert_close(result.B, sample_factored_matrix.B[:, :, :, :, B_idx])



---
File: /tests/unit/factored_matrix/test_multiply_by_factored_matrix.py
---

import pytest
from einops import repeat
from torch import randn
from torch.testing import assert_close

from transformer_lens import FactoredMatrix


@pytest.mark.parametrize(
    ("a", "b"),
    [
        pytest.param(
            (randn(3, 3), randn(3, 3)),
            (randn(3, 3), randn(3, 3)),
            id="both have ldim == mdim == rdim",
        ),
        pytest.param(
            (randn(1, 1), randn(1, 1)),
            (randn(1, 1), randn(1, 1)),
            id="both have ldim == mdim == rdim == 1",
        ),
        pytest.param(
            (randn(1, 2), randn(2, 1)),
            (randn(1, 6), randn(6, 1)),
            id="both have ldim == rdim == 1 != mdim",
        ),
        pytest.param(
            (randn(1, 1), randn(1, 1)),
            (randn(1, 2), randn(2, 5)),
            id="A has ldim == mdim == rdim == 1",
        ),
        pytest.param(
            (randn(5, 2), randn(2, 1)),
            (randn(1, 1), randn(1, 1)),
            id="B has ldim == mdim == rdim == 1",
        ),
        pytest.param(
            (randn(1, 2), randn(2, 3)),
            (randn(3, 2), randn(2, 4)),
            id="A has ldim == 1",
        ),
        pytest.param(
            (randn(3, 2), randn(2, 1)),
            (randn(1, 3), randn(3, 2)),
            id="B has ldim == 1",
        ),
        pytest.param(
            (randn(1, 2), randn(2, 5)),
            (randn(5, 3), randn(3, 1)),
            id="A has ldim == 1 and B has rdim == 1",
        ),
        pytest.param(
            (randn(3, 4), randn(4, 5)),
            (randn(5, 6), randn(6, 7)),
            id="both have rdim > mdim",
        ),
        pytest.param(
            (randn(8, 6), randn(6, 5)),
            (randn(5, 7), randn(7, 3)),
            id="both have rdim < mdim",
        ),
        pytest.param(
            (randn(3, 4), randn(4, 4)),
            (randn(4, 5), randn(5, 5)),
            id="both have rdim == mdim",
        ),
        pytest.param(
            (randn(7, 6), randn(6, 3)),
            (randn(3, 7), randn(7, 8)),
            id="A has rdim < mdim and B has rdim > mdim",
        ),
    ],
)
class TestMultiplyByFactoredMatrix:
    @staticmethod
    def _test_multiply(a_left, a_right, b_left, b_right) -> FactoredMatrix:
        factored_a = FactoredMatrix(a_left, a_right)
        factored_b = FactoredMatrix(b_left, b_right)

        product = factored_a @ factored_b
        expected_product = (a_left @ a_right) @ (b_left @ b_right)
        assert_close(product.AB, expected_product)

        assert product.ldim == factored_a.ldim
        assert product.mdim == min(
            factored_a.mdim, factored_a.rdim, factored_b.ldim, factored_b.mdim
        )
        assert product.rdim == factored_b.rdim

        return product

    def test_multiply_two_factored_matrices(self, a, b):
        self._test_multiply(*a, *b)

    def test_multiply_when_A_has_leading_dim(self, a, b):
        a_left, a_right = a

        a_left_with_leading = repeat(a_left, "x y -> b x y", b=2)
        a_right_with_leading = repeat(a_right, "x y -> b x y", b=2)

        product = self._test_multiply(a_left_with_leading, a_right_with_leading, *b)

        assert product.A.shape[:1] == (2,)
        assert product.B.shape[:1] == (2,)

    def test_multiply_when_B_has_leading_dim(self, a, b):
        b_left, b_right = b

        b_left_with_leading = repeat(b_left, "x y -> b x y", b=2)
        b_right_with_leading = repeat(b_right, "x y -> b x y", b=2)

        product = self._test_multiply(*a, b_left_with_leading, b_right_with_leading)

        assert product.A.shape[:1] == (2,)
        assert product.B.shape[:1] == (2,)

    def test_multiply_when_both_have_leading_dim(self, a, b):
        a_left, a_right = a
        b_left, b_right = b

        a_left_with_leading = repeat(a_left, "x y -> b1 b2 x y", b1=2, b2=9)
        a_right_with_leading = repeat(a_right, "x y -> b1 b2 x y", b1=2, b2=9)
        b_left_with_leading = repeat(b_left, "x y -> b x y", b=9)
        b_right_with_leading = repeat(b_right, "x y -> b x y", b=9)

        product = self._test_multiply(
            a_left_with_leading,
            a_right_with_leading,
            b_left_with_leading,
            b_right_with_leading,
        )

        assert product.A.shape[:2] == (2, 9)
        assert product.B.shape[:2] == (2, 9)


@pytest.mark.parametrize(
    ("a", "b", "error"),
    [
        pytest.param(
            FactoredMatrix(randn(2, 3, 4), randn(2, 4, 6)),
            FactoredMatrix(randn(4, 6, 7), randn(4, 7, 2)),
            RuntimeError,
            id="Leading dim mismatch where each has one leading dim",
        ),
        pytest.param(
            FactoredMatrix(randn(2, 9, 3, 4), randn(2, 9, 4, 6)),
            FactoredMatrix(randn(2, 6, 7), randn(2, 7, 2)),
            RuntimeError,
            id="Leading dim mismatch where A has two leading dims and B has one",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            FactoredMatrix(randn(5, 6), randn(6, 7)),
            AssertionError,
            id="Inner dimension mismatch",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            FactoredMatrix(randn(2, 5, 6), randn(2, 6, 7)),
            AssertionError,
            id="Inner dimension mismatch with batch",
        ),
    ],
)
def test_dimension_mismatch(a, b, error):
    with pytest.raises(error):
        _ = a @ b



---
File: /tests/unit/factored_matrix/test_multiply_by_matrix.py
---

from abc import ABC, abstractmethod

import pytest
from einops import repeat
from torch import randn
from torch.testing import assert_close

from transformer_lens import FactoredMatrix


class BaseMultiplyByMatrixTest(ABC):
    """
    Base class for tests of multiplication between FactoredMatrix and a regular Matrix.

    Includes for tests where each operand has or doesn't have leading dimensions.
    """

    @staticmethod
    @abstractmethod
    def _test_multiply(a, b, matrix):
        pass

    def test_left_multiply_by_matrix(self, a, b, matrix):
        self._test_multiply(a, b, matrix)

    def test_left_multiply_when_factored_matrix_has_leading_dim(self, a, b, matrix):
        a_with_leading = repeat(a, "x y -> b x y", b=2)
        b_with_leading = repeat(b, "x y -> b x y", b=2)

        product = self._test_multiply(a_with_leading, b_with_leading, matrix)

        assert product.A.shape[:-2] == (2,)
        assert product.B.shape[:-2] == (2,)

    def test_left_multiply_when_matrix_has_leading_dims(self, a, b, matrix):
        matrix_with_leading = repeat(matrix, "x y -> b1 b2 x y", b1=2, b2=12)

        product = self._test_multiply(a, b, matrix_with_leading)

        assert product.A.shape[:-2] == (2, 12)
        assert product.B.shape[:-2] == (2, 12)

    def test_left_multiply_when_both_have_leading_dim(self, a, b, matrix):
        a_with_leading = repeat(a, "x y -> b x y", b=2)
        b_with_leading = repeat(b, "x y -> b x y", b=2)
        matrix_with_leading = repeat(matrix, "x y -> b x y", b=2)

        product = self._test_multiply(a_with_leading, b_with_leading, matrix_with_leading)

        assert product.A.shape[:-2] == (2,)
        assert product.B.shape[:-2] == (2,)


@pytest.mark.parametrize(
    ("a", "b", "matrix"),
    [
        pytest.param(randn(2, 3), randn(3, 4), randn(4, 5), id="rdim > mdim"),
        pytest.param(randn(2, 6), randn(6, 4), randn(4, 5), id="rdim < mdim"),
        pytest.param(randn(2, 4), randn(4, 4), randn(4, 5), id="rdim == mdim"),
    ],
)
class TestLeftMultiplyByMatrix(BaseMultiplyByMatrixTest):
    @staticmethod
    def _test_multiply(a, b, matrix):
        factored_matrix = FactoredMatrix(a, b)

        product = factored_matrix @ matrix
        expected_product = (a @ b) @ matrix
        assert_close(product.AB, expected_product)

        assert product.ldim == factored_matrix.ldim
        assert product.mdim == min(factored_matrix.mdim, matrix.shape[-2])
        assert product.rdim == matrix.shape[-1]

        return product


@pytest.mark.parametrize(
    ("a", "b", "matrix"),
    [
        pytest.param(randn(6, 3), randn(3, 4), randn(4, 6), id="ldim > mdim"),
        pytest.param(randn(2, 6), randn(6, 4), randn(4, 2), id="ldim < mdim"),
        pytest.param(randn(2, 2), randn(2, 4), randn(4, 2), id="ldim == mdim"),
    ],
)
class TestRightMultiplyByMatrix(BaseMultiplyByMatrixTest):
    @staticmethod
    def _test_multiply(a, b, matrix):
        factored_matrix = FactoredMatrix(a, b)

        product = matrix @ factored_matrix
        expected_product = matrix @ (a @ b)
        assert_close(product.AB, expected_product)

        assert product.ldim == matrix.shape[-2]
        assert product.mdim == min(factored_matrix.mdim, matrix.shape[-1])
        assert product.rdim == factored_matrix.rdim

        return product


@pytest.mark.parametrize(
    ("factored_matrix", "matrix", "error"),
    [
        pytest.param(
            FactoredMatrix(randn(2, 3, 4), randn(2, 4, 6)),
            randn(4, 6, 7),
            RuntimeError,
            id="Leading dim mismatch where each has one leading dim",
        ),
        pytest.param(
            FactoredMatrix(randn(2, 9, 3, 4), randn(2, 9, 4, 6)),
            randn(2, 6, 7),
            RuntimeError,
            id="Leading dim mismatch where FactoredMatrix has two leading dims and regular matrix has one",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            randn(5, 6),
            AssertionError,
            id="Inner dimension mismatch",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            randn(2, 5, 6),
            AssertionError,
            id="Inner dimension mismatch with batch",
        ),
    ],
)
def test_dimension_mismatch_left_multiply(factored_matrix, matrix, error):
    with pytest.raises(error):
        _ = factored_matrix @ matrix


@pytest.mark.parametrize(
    ("factored_matrix", "matrix", "error"),
    [
        pytest.param(
            FactoredMatrix(randn(2, 3, 4), randn(2, 4, 6)),
            randn(4, 6, 3),
            RuntimeError,
            id="Leading dim mismatch where each has one leading dim",
        ),
        pytest.param(
            FactoredMatrix(randn(2, 9, 3, 4), randn(2, 9, 4, 6)),
            randn(2, 6, 3),
            RuntimeError,
            id="Leading dim mismatch where FactoredMatrix has two leading dims and regular matrix has one",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            randn(5, 6),
            AssertionError,
            id="Inner dimension mismatch",
        ),
        pytest.param(
            FactoredMatrix(randn(3, 4), randn(4, 6)),
            randn(2, 5, 6),
            AssertionError,
            id="Inner dimension mismatch with batch",
        ),
    ],
)
def test_dimension_mismatch_right_multiply(factored_matrix, matrix, error):
    with pytest.raises(error):
        _ = matrix @ factored_matrix



---
File: /tests/unit/factored_matrix/test_multiply_by_scalar.py
---

import random

import pytest
import torch
from torch.testing import assert_close

from transformer_lens import FactoredMatrix


# This test function is parametrized with different types of scalars, including non-scalar tensors and arrays, to check that the correct errors are raised.
# Considers cases with and without leading dimensions as well as left and right multiplication.
@pytest.mark.parametrize(
    "scalar, error_expected",
    [
        # Test cases with different types of scalar values.
        (torch.rand(1), None),  # 1-element Tensor. No error expected.
        (random.random(), None),  # float. No error expected.
        (random.randint(-100, 100), None),  # int. No error expected.
        # Test cases with non-scalar values that are expected to raise errors.
        (
            torch.rand(2, 2),
            AssertionError,
        ),  # Non-scalar Tensor. AssertionError expected.
        (torch.rand(2), AssertionError),  # Non-scalar Tensor. AssertionError expected.
    ],
)
@pytest.mark.parametrize("leading_dim", [False, True])
@pytest.mark.parametrize("multiply_from_left", [False, True])
def test_multiply(scalar, leading_dim, multiply_from_left, error_expected):
    # Prepare a FactoredMatrix, with or without leading dimensions
    if leading_dim:
        a = torch.rand(6, 2, 3)
        b = torch.rand(6, 3, 4)
    else:
        a = torch.rand(2, 3)
        b = torch.rand(3, 4)

    fm = FactoredMatrix(a, b)

    if error_expected:
        # If an error is expected, check that the correct exception is raised.
        with pytest.raises(error_expected):
            if multiply_from_left:
                _ = fm * scalar
            else:
                _ = scalar * fm
    else:
        # If no error is expected, check that the multiplication results in the correct value.
        # Use FactoredMatrix.AB to calculate the product of the two factor matrices before comparing with the expected value.
        if multiply_from_left:
            assert_close((fm * scalar).AB, (a @ b) * scalar)
        else:
            assert_close((scalar * fm).AB, scalar * (a @ b))
        # This next test is implementation dependant and can be broken and removed at any time!
        # It checks that the multiplication is performed on the A factor matrix.
        if multiply_from_left:
            assert_close((fm * scalar).A, a * scalar)
        else:
            assert_close((scalar * fm).A, scalar * a)



---
File: /tests/unit/factored_matrix/test_multiply_by_vector.py
---

import torch
from torch.testing import assert_close

from transformer_lens import FactoredMatrix


def test_left_matmul_by_vector_left():
    a = torch.rand(2, 3)
    b = torch.rand(3, 4)

    fm = FactoredMatrix(a, b)
    vector = torch.rand(4)

    assert_close(fm @ vector, (a @ b) @ vector)


def test_left_matmul_by_vector_leading_dim():
    a = torch.rand(6, 2, 3)
    b = torch.rand(6, 3, 4)

    fm = FactoredMatrix(a, b)
    vector = torch.rand(4)

    assert_close(fm @ vector, (a @ b) @ vector)


def test_right_matmul_by_vector():
    a = torch.rand(2, 3)
    b = torch.rand(3, 4)

    fm = FactoredMatrix(a, b)
    vector = torch.rand(2)

    assert_close(vector @ fm, vector @ (a @ b))


def test_right_matmul_by_vector_leading_dim():
    a = torch.rand(6, 2, 3)
    b = torch.rand(6, 3, 4)

    fm = FactoredMatrix(a, b)
    vector = torch.rand(2)

    assert_close(vector @ fm, vector @ (a @ b))



---
File: /tests/unit/factored_matrix/test_properties.py
---

import pytest
import torch
from torch import randn

from transformer_lens import FactoredMatrix, utils


@pytest.fixture(scope="module")
def random_matrices():
    return [
        (randn(3, 2), randn(2, 3)),
        (randn(4, 5), randn(5, 4)),
        (randn(6, 7), randn(7, 6)),
        (randn(6, 7), randn(7, 3)),
    ]


@pytest.fixture(scope="module")
def factored_matrices(random_matrices):
    return [FactoredMatrix(a, b) for a, b in random_matrices]


@pytest.fixture(scope="module")
def random_matrices_leading_ones():
    return [
        (randn(1, 8, 9), randn(1, 9, 8)),
    ]


@pytest.fixture(scope="module")
def factored_matrices_leading_ones(random_matrices_leading_ones):
    return [FactoredMatrix(a, b) for a, b in random_matrices_leading_ones]


class TestFactoredMatrixProperties:
    def test_AB_property(self, factored_matrices, random_matrices):
        for i, factored_matrix in enumerate(factored_matrices):
            a, b = random_matrices[i]
            expected_AB = a @ b
            assert torch.allclose(factored_matrix.AB, expected_AB, atol=1e-5)

    def test_BA_property(self, factored_matrices, random_matrices):
        for i, factored_matrix in enumerate(factored_matrices):
            a, b = random_matrices[i]
            if a.shape[-2] == b.shape[-1]:
                expected_BA = b @ a
                assert torch.allclose(factored_matrix.BA, expected_BA, atol=1e-5)
            else:
                with pytest.raises(AssertionError):
                    _ = factored_matrix.BA

    def test_transpose_property(self, factored_matrices):
        for factored_matrix in factored_matrices:
            transposed_factored_matrix = factored_matrix.T
            assert torch.allclose(
                transposed_factored_matrix.A,
                factored_matrix.B.transpose(-2, -1),
                atol=1e-5,
            )
            assert torch.allclose(
                transposed_factored_matrix.B,
                factored_matrix.A.transpose(-2, -1),
                atol=1e-5,
            )

    def test_svd_property(self, factored_matrices):
        for factored_matrix in factored_matrices:
            U, S, Vh = factored_matrix.svd()
            assert torch.allclose(factored_matrix.AB, U @ torch.diag_embed(S) @ Vh.T, atol=1e-5)
            # test that U and Vh are unitary
            assert torch.allclose(U.T @ U, torch.eye(U.shape[-1]), atol=1e-5)
            assert torch.allclose(Vh.T @ Vh, torch.eye(Vh.shape[-1]), atol=1e-5)

    def test_svd_property_leading_ones(self, factored_matrices_leading_ones):
        for factored_matrix in factored_matrices_leading_ones:
            U, S, Vh = factored_matrix.svd()
            assert torch.allclose(factored_matrix.AB, U @ torch.diag_embed(S) @ Vh.mT, atol=1e-5)
            # test that U and Vh are unitary
            assert torch.allclose(U.mT @ U, torch.eye(U.shape[-1]), atol=1e-5)
            assert torch.allclose(Vh.mT @ Vh, torch.eye(Vh.shape[-1]), atol=1e-5)

    @pytest.mark.skip(
        """
        Jaxtyping throws a TypeError when this test is run.
        TypeError: type of the return value must be jaxtyping.Float[Tensor, '*leading_dims mdim']; got torch.Tensor instead

        I'm not sure why. The error is not very informative. When debugging the shape was equal to mdim, and *leading_dims should
        match zero or more leading dims according to the [docs](https://github.com/google/jaxtyping/blob/main/API.md).

        Sort of related to https://github.com/TransformerLensOrg/TransformerLens/issues/190 because jaxtyping
        is only enabled at test time and not runtime.
        """
    )
    def test_eigenvalues_property(self, factored_matrices):
        for factored_matrix in factored_matrices:
            if factored_matrix.ldim == factored_matrix.rdim:
                eigenvalues = factored_matrix.eigenvalues
                expected_eigenvalues = torch.linalg.eig(
                    factored_matrix.B @ factored_matrix.A
                ).eigenvalues
                assert torch.allclose(
                    torch.abs(eigenvalues), torch.abs(expected_eigenvalues), atol=1e-5
                )
            else:
                with pytest.raises(AssertionError):
                    _ = factored_matrix.eigenvalues

    def test_ndim_property(self, factored_matrices, random_matrices):
        for i, factored_matrix in enumerate(factored_matrices):
            a, b = random_matrices[i]
            expected_ndim = max(a.ndim, b.ndim)
            assert factored_matrix.ndim == expected_ndim

    def test_pair_property(self, factored_matrices, random_matrices):
        for i, factored_matrix in enumerate(factored_matrices):
            a, b = random_matrices[i]
            assert torch.allclose(factored_matrix.pair[0], a, atol=1e-5)
            assert torch.allclose(factored_matrix.pair[1], b, atol=1e-5)

    def test_norm_property(self, factored_matrices):
        for factored_matrix in factored_matrices:
            assert torch.allclose(factored_matrix.norm(), factored_matrix.AB.norm(), atol=1e-5)

    def test_get_corner(self, factored_matrices):
        for factored_matrix in factored_matrices:
            k = 3
            result = factored_matrix.get_corner(k)
            expected = utils.get_corner(
                factored_matrix.A[..., :k, :] @ factored_matrix.B[..., :, :k], k
            )
            assert torch.allclose(result, expected)

    def test_ndim(self, factored_matrices):
        for factored_matrix in factored_matrices:
            assert factored_matrix.ndim == len(factored_matrix.shape)

    def test_collapse_l(self, factored_matrices):
        for factored_matrix in factored_matrices:
            result = factored_matrix.collapse_l()
            expected = factored_matrix.S[..., :, None] * utils.transpose(factored_matrix.Vh)
            assert torch.allclose(result, expected)

    def test_collapse_r(self, factored_matrices):
        for factored_matrix in factored_matrices:
            result = factored_matrix.collapse_r()
            expected = factored_matrix.U * factored_matrix.S[..., None, :]
            assert torch.allclose(result, expected)

    def test_unsqueeze(self, factored_matrices_leading_ones):
        for factored_matrix in factored_matrices_leading_ones:
            k = 0
            unsqueezed_A = factored_matrix.A.unsqueeze(k)
            unsqueezed_B = factored_matrix.B.unsqueeze(k)
            inner_dim_A = unsqueezed_A.size(-1)
            inner_dim_B = unsqueezed_B.size(-2)

            if inner_dim_A == inner_dim_B:
                result = FactoredMatrix(unsqueezed_A, unsqueezed_B)
                assert isinstance(result, FactoredMatrix)
                assert torch.allclose(result.A, unsqueezed_A)
                assert torch.allclose(result.B, unsqueezed_B)



---
File: /tests/unit/factories/test_activation_function_factory.py
---

import pytest
import torch

from transformer_lens.factories.activation_function_factory import (
    ActivationFunctionFactory,
)
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.activation_functions import SUPPORTED_ACTIVATIONS


@pytest.mark.parametrize("act_function", SUPPORTED_ACTIVATIONS.keys())
def test_pick_activation_function_runs(act_function):
    config = HookedTransformerConfig.unwrap(
        {"n_layers": 12, "n_ctx": 1024, "d_head": 64, "d_model": 128, "act_fn": act_function}
    )
    function = ActivationFunctionFactory.pick_activation_function(config)
    assert function is not None
    dummy_data = torch.zeros((1, 4, 32))
    result = function(dummy_data)
    assert isinstance(result, torch.Tensor)



---
File: /tests/unit/factories/test_mlp_factory.py
---

import pytest
from transformers.utils import is_bitsandbytes_available

from transformer_lens.components.mlps.gated_mlp import GatedMLP
from transformer_lens.components.mlps.gated_mlp_4bit import GatedMLP4Bit
from transformer_lens.components.mlps.mlp import MLP
from transformer_lens.factories.mlp_factory import MLPFactory
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_create_mlp_basic():
    config = HookedTransformerConfig.unwrap(
        {
            "n_layers": 12,
            "n_ctx": 1024,
            "d_head": 64,
            "d_model": 128,
            "act_fn": "solu",
        }
    )
    mlp = MLPFactory.create_mlp(config)
    assert isinstance(mlp, MLP)


def test_create_mlp_gated():
    config = HookedTransformerConfig.unwrap(
        {
            "n_layers": 12,
            "n_ctx": 1024,
            "d_head": 64,
            "d_model": 128,
            "act_fn": "solu",
            "gated_mlp": True,
        }
    )
    mlp = MLPFactory.create_mlp(config)
    assert isinstance(mlp, GatedMLP)


@pytest.mark.skipif(
    not is_bitsandbytes_available(),
    reason="4 bit not available on current architecture",
)
def test_create_mlp_gated_4bit():
    config = HookedTransformerConfig.unwrap(
        {
            "n_layers": 12,
            "n_ctx": 1024,
            "d_head": 64,
            "d_model": 128,
            "act_fn": "solu",
            "gated_mlp": True,
            "load_in_4bit": True,
        }
    )
    mlp = MLPFactory.create_mlp(config)
    assert isinstance(mlp, GatedMLP4Bit)


def test_create_moe():
    if is_bitsandbytes_available():
        config = HookedTransformerConfig.unwrap(
            {
                "n_layers": 12,
                "n_ctx": 1024,
                "d_head": 64,
                "d_model": 128,
                "act_fn": "solu",
                "gated_mlp": True,
                "num_experts": 32,
            }
        )
        mlp = MLPFactory.create_mlp(config)
        assert isinstance(mlp, GatedMLP4Bit)



---
File: /tests/unit/pretrained_weight_conversions/test_neo.py
---

from unittest import mock

import torch

from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.pretrained.weight_conversions.neo import convert_neo_weights


def get_default_config():
    return HookedTransformerConfig(
        d_model=128, d_head=8, n_heads=16, n_ctx=128, n_layers=1, d_vocab=50257, attn_only=True
    )


def test_convert_neo_weights_exposed():
    cfg = get_default_config()

    class MockNeo:
        def __init__(self):
            self.transformer = HookedTransformer(cfg)
            self.transformer.wte = torch.nn.Embedding(cfg.d_vocab, cfg.d_model)
            self.transformer.wpe = torch.nn.Embedding(cfg.n_ctx, cfg.d_model)
            self.transformer.final_norm = torch.nn.LayerNorm(cfg.d_model)
            self.transformer.h = [mock.Mock() for _ in range(cfg.n_layers)]
            self.lm_head = torch.nn.Linear(cfg.d_model, cfg.d_vocab)

            for layer in self.transformer.h:
                layer.ln_1 = torch.nn.LayerNorm(cfg.d_model)
                layer.ln_2 = torch.nn.LayerNorm(cfg.d_model)
                layer.attn = mock.Mock()
                layer.attn.attention = mock.Mock()
                layer.attn.attention.q_proj = torch.nn.Linear(cfg.d_model, cfg.d_model)
                layer.attn.attention.k_proj = torch.nn.Linear(cfg.d_model, cfg.d_model)
                layer.attn.attention.v_proj = torch.nn.Linear(cfg.d_model, cfg.d_model)
                layer.attn.attention.out_proj = torch.nn.Linear(cfg.d_model, cfg.d_model)
                layer.mlp = mock.Mock()
                layer.mlp.c_fc = torch.nn.Linear(cfg.d_model, cfg.d_model)
                layer.mlp.c_proj = torch.nn.Linear(cfg.d_model, cfg.d_model)

            self.transformer.ln_f = torch.nn.LayerNorm(cfg.d_model)

    neo = MockNeo()

    try:
        convert_neo_weights(neo, cfg)
        function_works = True
    except Exception as e:
        function_works = False
        print(f"The convert_neo_weights function raised an error: {e}")

    assert function_works



---
File: /tests/unit/utilities/test_devices.py
---

from unittest.mock import Mock

import torch

from transformer_lens.utilities.devices import (
    calculate_available_device_cuda_memory,
    determine_available_memory_for_available_devices,
    sort_devices_based_on_available_memory,
)


def mock_available_devices(memory_stats: list[tuple[int, int]]):
    torch.cuda.device_count = Mock(return_value=len(memory_stats))

    def device_props_return(*args, **kwargs):
        total_memory = memory_stats[args[0]][0]
        device_props = Mock()
        device_props.total_memory = total_memory
        return device_props

    def memory_allocated_return(*args, **kwargs):
        return memory_stats[args[0]][1]

    torch.cuda.get_device_properties = Mock(side_effect=device_props_return)
    torch.cuda.memory_allocated = Mock(side_effect=memory_allocated_return)


def test_calculate_available_device_cuda_memory():
    mock_available_devices([(80, 40)])

    result = calculate_available_device_cuda_memory(0)
    assert result == 40


def test_determine_available_memory_for_available_devices():
    mock_available_devices(
        [
            (80, 60),
            (80, 15),
            (80, 40),
        ]
    )

    result = determine_available_memory_for_available_devices(3)

    assert result == [
        (0, 20),
        (1, 65),
        (2, 40),
    ]


def test_sort_devices_based_on_available_memory():
    devices = [
        (0, 20),
        (1, 65),
        (2, 40),
    ]

    result = sort_devices_based_on_available_memory(devices)

    assert result == [
        (1, 65),
        (2, 40),
        (0, 20),
    ]



---
File: /tests/unit/test_hook_points.py
---

from unittest import mock

from transformer_lens.hook_points import HookPoint


def setup_hook_point_and_hook():
    hook_point = HookPoint()

    def hook(activation, hook):
        return activation

    return hook_point, hook


@mock.patch("torch.utils.hooks.RemovableHandle", autospec=True)
def test_add_hook_forward(mock_handle):
    mock_handle.return_value.id = 0
    hook_point, hook = setup_hook_point_and_hook()
    hook_point.add_hook(hook, dir="fwd")
    assert len(hook_point.fwd_hooks) == 1


@mock.patch("torch.utils.hooks.RemovableHandle", autospec=True)
def test_add_hook_backward(mock_handle):
    mock_handle.return_value.id = 0
    hook_point, hook = setup_hook_point_and_hook()
    hook_point.add_hook(hook, dir="bwd")
    assert len(hook_point.bwd_hooks) == 1


@mock.patch("torch.utils.hooks.RemovableHandle", autospec=True)
def test_add_hook_permanent(mock_handle):
    mock_handle.return_value.id = 0
    hook_point, hook = setup_hook_point_and_hook()
    hook_point.add_hook(hook, dir="fwd", is_permanent=True)
    assert hook_point.fwd_hooks[0].is_permanent


@mock.patch("torch.utils.hooks.RemovableHandle", autospec=True)
def test_add_hook_with_level(mock_handle):
    mock_handle.return_value.id = 0
    hook_point, hook = setup_hook_point_and_hook()
    hook_point.add_hook(hook, dir="fwd", level=5)
    assert hook_point.fwd_hooks[0].context_level == 5


@mock.patch("torch.utils.hooks.RemovableHandle")
def test_add_hook_prepend(mock_handle):
    mock_handle.id = 0
    mock_handle.next_id = 1

    hook_point, _ = setup_hook_point_and_hook()

    def hook1(activation, hook):
        return activation

    def hook2(activation, hook):
        return activation

    hook_point.add_hook(hook1, dir="fwd")
    hook_point.add_hook(hook2, dir="fwd", prepend=True)

    assert len(hook_point.fwd_hooks) == 2
    assert hook_point.fwd_hooks[0].hook.id == 2
    assert hook_point.fwd_hooks[1].hook.id == 1



---
File: /tests/unit/test_hooked_root_module.py
---

from unittest.mock import Mock

from transformer_lens.hook_points import HookedRootModule

MODEL_NAME = "solu-2l"


def test_enable_hook_with_name():
    model = HookedRootModule()
    model.mod_dict = {"linear": Mock()}
    model.context_level = 5

    hook = lambda x: False
    dir = "fwd"

    model._enable_hook_with_name("linear", hook=hook, dir=dir)

    model.mod_dict["linear"].add_hook.assert_called_with(hook, dir="fwd", level=5)


def test_enable_hooks_for_points():
    model = HookedRootModule()
    model.mod_dict = {}
    model.context_level = 5

    hook_points = {
        "linear": Mock(),
        "attn": Mock(),
    }

    enabled = lambda x: x == "attn"

    hook = lambda x: False
    dir = "bwd"

    print(hook_points.items())
    model._enable_hooks_for_points(
        hook_points=hook_points.items(), enabled=enabled, hook=hook, dir=dir
    )

    hook_points["attn"].add_hook.assert_called_with(hook, dir="bwd", level=5)
    hook_points["linear"].add_hook.assert_not_called()


def test_enable_hook_with_string_param():
    model = HookedRootModule()
    model.mod_dict = {"linear": Mock()}
    model.context_level = 5

    hook = lambda x: False
    dir = "fwd"

    model._enable_hook("linear", hook=hook, dir=dir)

    model.mod_dict["linear"].add_hook.assert_called_with(hook, dir="fwd", level=5)


def test_enable_hook_with_callable_param():
    model = HookedRootModule()
    model.mod_dict = {"linear": Mock()}
    model.hook_dict = {
        "linear": Mock(),
        "attn": Mock(),
    }
    model.context_level = 5

    enabled = lambda x: x == "attn"

    hook = lambda x: False
    dir = "fwd"

    model._enable_hook(enabled, hook=hook, dir=dir)

    model.mod_dict["linear"].add_hook.assert_not_called()
    model.hook_dict["attn"].add_hook.assert_called_with(hook, dir="fwd", level=5)
    model.hook_dict["linear"].add_hook.assert_not_called()



---
File: /tests/unit/test_hooked_transformer_config.py
---

"""
Tests that config passed around TransformerLens can be unwrapped into an actual configuration object
"""

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_hooked_transformer_config_object():
    hooked_transformer_config = HookedTransformerConfig(
        n_layers=2, d_vocab=100, d_model=6, n_ctx=5, d_head=2, attn_only=True
    )
    result = HookedTransformerConfig.unwrap(hooked_transformer_config)
    # Assert that the same object was returned
    assert result is hooked_transformer_config


def test_hooked_transformer_config_dict():
    hooked_transformer_config_dict = {
        "n_layers": 2,
        "d_vocab": 100,
        "d_model": 6,
        "n_ctx": 5,
        "d_head": 2,
        "attn_only": True,
    }
    result = HookedTransformerConfig.unwrap(hooked_transformer_config_dict)
    # Assert that the new returned value has been transformed into a config object
    assert isinstance(result, HookedTransformerConfig)


def test_is_layer_norm_activation_passes():
    hooked_transformer_config_dict = {
        "n_layers": 2,
        "d_vocab": 100,
        "d_model": 6,
        "n_ctx": 5,
        "d_head": 2,
        "attn_only": True,
        "act_fn": "solu_ln",
    }
    config = HookedTransformerConfig.unwrap(hooked_transformer_config_dict)
    assert config.is_layer_norm_activation()


def test_is_layer_norm_activation_fails():
    hooked_transformer_config_dict = {
        "n_layers": 2,
        "d_vocab": 100,
        "d_model": 6,
        "n_ctx": 5,
        "d_head": 2,
        "attn_only": True,
        "act_fn": "relu",
    }
    config = HookedTransformerConfig.unwrap(hooked_transformer_config_dict)
    assert not config.is_layer_norm_activation()



---
File: /tests/unit/test_loading_from_pretrained_utilities.py
---

from unittest import mock

from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.loading_from_pretrained import fill_missing_keys


def get_default_config():
    return HookedTransformerConfig(
        d_model=128, d_head=8, n_heads=16, n_ctx=128, n_layers=1, d_vocab=50257, attn_only=True
    )


# Successes


@mock.patch("logging.warning")
def test_fill_missing_keys(mock_warning: mock.MagicMock):
    cfg = get_default_config()
    model = HookedTransformer(cfg)
    default_state_dict = model.state_dict()

    incomplete_state_dict = {k: v for k, v in default_state_dict.items() if "W_" not in k}

    filled_state_dict = fill_missing_keys(model, incomplete_state_dict)

    assert set(filled_state_dict.keys()) == set(default_state_dict.keys())

    # Check that warnings were issued for missing weight matrices
    for key in default_state_dict:
        if "W_" in key and key not in incomplete_state_dict:
            mock_warning.assert_any_call(
                f"Missing key for a weight matrix in pretrained, filled in with an empty tensor: {key}"
            )


def test_fill_missing_keys_with_hf_model_keys():
    cfg = get_default_config()
    model = HookedTransformer(cfg)
    default_state_dict = model.state_dict()

    incomplete_state_dict = {k: v for k, v in default_state_dict.items() if "hf_model" not in k}

    filled_state_dict = fill_missing_keys(model, incomplete_state_dict)

    expected_keys = set(default_state_dict.keys()) - {
        k for k in default_state_dict.keys() if "hf_model" in k
    }
    assert set(filled_state_dict.keys()) == expected_keys


def test_fill_missing_keys_no_missing_keys():
    cfg = get_default_config()
    model = HookedTransformer(cfg)
    default_state_dict = model.state_dict()

    filled_state_dict = fill_missing_keys(model, default_state_dict)

    assert filled_state_dict == default_state_dict



---
File: /tests/unit/test_make_docs.py
---

"""Make Docs Tests."""

import pytest

from docs.make_docs import get_config, get_model_info, get_property
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_get_config():
    """Test get config with attn-only-1l model."""
    config: HookedTransformerConfig = get_config("attn-only-1l")
    assert config.attn_only is True


def test_get_property():
    """Test get property with attn-only-1l model."""
    act_fn = get_property("act_fn", "attn-only-1l")
    assert act_fn == "attn_only"

    n_params = get_property("n_params", "attn-only-1l")
    assert n_params == "1.0M"

    n_layers = get_property("n_layers", "attn-only-1l")
    assert n_layers == 1

    d_model = get_property("d_model", "attn-only-1l")
    assert d_model == 512

    n_heads = get_property("n_heads", "attn-only-1l")
    assert n_heads == 8

    n_ctx = get_property("n_ctx", "attn-only-1l")
    assert n_ctx == 1024

    d_vocab = get_property("d_vocab", "attn-only-1l")
    assert d_vocab == 48262

    d_head = get_property("d_head", "attn-only-1l")
    assert d_head == 64

    d_mlp = get_property("d_mlp", "attn-only-1l")
    assert d_mlp == 2048

    n_key_value_heads = get_property("n_key_value_heads", "attn-only-1l")
    assert n_key_value_heads is None

    # Test an unknown property
    with pytest.raises(KeyError):
        get_property("unknown_property", "attn-only-1l")


def test_get_model_info():
    get_model_info("attn-only-1l")



---
File: /tests/unit/test_next_sentence_prediction.py
---

from unittest.mock import Mock

import pytest
import torch
from transformers import AutoTokenizer, BertForNextSentencePrediction

from transformer_lens import BertNextSentencePrediction, HookedEncoder


@pytest.fixture
def mock_hooked_encoder():
    mock_encoder = Mock(spec=HookedEncoder)

    mock_encoder.cfg = Mock()
    mock_encoder.cfg.device = "cpu"
    mock_encoder.cfg.n_ctx = 512

    mock_encoder.tokenizer = Mock()

    mock_encodings = {
        "input_ids": torch.tensor([[101, 2034, 102, 2035, 102]]),
        "token_type_ids": torch.tensor([[0, 0, 0, 1, 1]]),
        "attention_mask": torch.tensor([[1, 1, 1, 1, 1]]),
    }
    mock_encoder.tokenizer.return_value = mock_encodings

    # Mock encoder output
    mock_encoder.encoder_output.return_value = (torch.randn(1, 7, 768), {})  # resid  # cache

    # Mock pooler and NSP head
    mock_encoder.pooler = Mock()
    mock_encoder.pooler.return_value = torch.randn(1, 768)
    mock_encoder.nsp_head = Mock()
    mock_encoder.nsp_head.return_value = torch.tensor([[0.6, 0.4]])

    # Mock run_with_cache
    mock_encoder.run_with_cache = Mock()

    return mock_encoder


@pytest.fixture
def bert_nsp(mock_hooked_encoder):
    return BertNextSentencePrediction(mock_hooked_encoder)


@pytest.fixture
def huggingface_bert():
    return BertForNextSentencePrediction.from_pretrained("bert-base-cased")


@pytest.fixture
def encodings():
    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
    sentence_a = "She went to the grocery store."
    sentence_b = "She bought an apple."
    return tokenizer(sentence_a, sentence_b, return_tensors="pt")


def test_init(mock_hooked_encoder):
    """Test initialization of BertNextSentencePrediction"""
    bert_nsp = BertNextSentencePrediction(mock_hooked_encoder)
    assert bert_nsp.model == mock_hooked_encoder


def test_call_chain(bert_nsp, mock_hooked_encoder):
    """Test that encoder_output, pooler and nsp_head are called with the correct parameters"""
    input_tensor = torch.tensor([[1, 2, 3]])
    token_type_ids = torch.tensor([[0, 0, 1]])
    attention_mask = torch.tensor([[1, 1, 1]])

    # Set up specific mock returns
    mock_resid = torch.randn(1, 3, 768)
    mock_hooked_encoder.encoder_output.return_value = mock_resid

    mock_pooled = torch.randn(1, 768)
    mock_hooked_encoder.pooler.return_value = mock_pooled

    mock_nsp_output = torch.tensor([[0.7, 0.3]])
    mock_hooked_encoder.nsp_head.return_value = mock_nsp_output

    # Call forward
    output = bert_nsp.forward(
        input_tensor, token_type_ids=token_type_ids, one_zero_attention_mask=attention_mask
    )

    # Verify the entire chain of calls
    mock_hooked_encoder.encoder_output.assert_called_once_with(
        input_tensor, token_type_ids, attention_mask
    )
    mock_hooked_encoder.pooler.assert_called_once_with(mock_resid)
    mock_hooked_encoder.nsp_head.assert_called_once_with(mock_pooled)

    # Verify output matches the mock NSP head output
    assert torch.equal(output, mock_nsp_output)


def test_tokenizer_integration(bert_nsp, mock_hooked_encoder):
    """Test that tokenizer is properly integrated and called"""
    input_sentences = ["First sentence.", "Second sentence."]

    mock_hooked_encoder.tokenizer = Mock()

    # Mock tokenizer output
    mock_encodings = {
        "input_ids": torch.tensor([[101, 2034, 102, 2035, 102]]),
        "token_type_ids": torch.tensor([[0, 0, 0, 1, 1]]),
        "attention_mask": torch.tensor([[1, 1, 1, 1, 1]]),
    }
    mock_hooked_encoder.tokenizer.return_value = mock_encodings

    # Call to_tokens
    tokens, type_ids, mask = bert_nsp.to_tokens(input_sentences)

    # Verify tokenizer was called correctly
    mock_hooked_encoder.tokenizer.assert_called_once_with(
        input_sentences[0],
        input_sentences[1],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=mock_hooked_encoder.cfg.n_ctx,
    )

    # Verify outputs match tokenizer output
    assert torch.equal(tokens, mock_encodings["input_ids"])
    assert torch.equal(type_ids, mock_encodings["token_type_ids"])
    assert torch.equal(mask, mock_encodings["attention_mask"])


@pytest.mark.skipif(not torch.cuda.is_available(), reason="Requires a CUDA device")
def test_device_handling_to_tokens(bert_nsp, mock_hooked_encoder):
    """Test proper device handling in to_tokens"""
    mock_hooked_encoder.cfg.device = "cuda"  # Mock GPU device

    input_data = ["First sentence.", "Second sentence."]

    # Mock tokenizer output
    mock_encodings = {
        "input_ids": torch.tensor([[101, 2034, 102, 2035, 102]]),
        "token_type_ids": torch.tensor([[0, 0, 0, 1, 1]]),
        "attention_mask": torch.tensor([[1, 1, 1, 1, 1]]),
    }
    mock_hooked_encoder.tokenizer.return_value = mock_encodings

    # Call to_tokens with move_to_device=True
    tokens, type_ids, mask = bert_nsp.to_tokens(input_data, move_to_device=True)

    # Verify each tensor was moved to the correct device
    for tensor in [tokens, type_ids, mask]:
        assert tensor.device.type == mock_hooked_encoder.cfg.device

    # Call with move_to_device=False
    tokens, type_ids, mask = bert_nsp.to_tokens(input_data, move_to_device=False)

    # Verify tensors remained on CPU
    for tensor in [tokens, type_ids, mask]:
        assert tensor.device.type == "cpu"


def test_output_for_prediction_return_type(bert_nsp, mock_hooked_encoder):
    """Test that output for return_type='predictions' is correct based on NSP head output"""
    input_data = ["First sentence.", "Second sentence."]

    # Test case 1: Sequential prediction
    mock_hooked_encoder.nsp_head.return_value = torch.tensor([[0.9, 0.1]])
    pred = bert_nsp.forward(input_data, return_type="predictions")
    assert pred == "The sentences are sequential"

    # Test case 2: Non-sequential prediction
    mock_hooked_encoder.nsp_head.return_value = torch.tensor([[0.2, 0.8]])
    pred = bert_nsp.forward(input_data, return_type="predictions")
    assert pred == "The sentences are NOT sequential"


@pytest.mark.parametrize("input_type", ["str_list", "tensor"])
def test_forward_input_types(bert_nsp, encodings, input_type):
    """Test forward pass with different input types"""
    sentence_a = "She went to the grocery store."
    sentence_b = "She bought an apple."
    if input_type == "str_list":
        output = bert_nsp([sentence_a, sentence_b])
    else:
        output = bert_nsp.forward(
            encodings.input_ids,
            token_type_ids=encodings.token_type_ids,
            one_zero_attention_mask=encodings.attention_mask,
        )

    assert isinstance(output, torch.Tensor)
    assert output.shape == (1, 2)  # Batch size 1, binary classification


def test_forward_tokens_as_input_without_token_type_ids_error(bert_nsp):
    """Test that forward raises error when tokens are input directly but no token_type_ids are provided"""
    with pytest.raises(
        ValueError, match="You are using the NSP task without specifying token_type_ids"
    ):
        bert_nsp.forward(torch.tensor([[1, 2, 768]]))


@pytest.mark.parametrize("return_type", [None, "logits", "predictions"])
def test_forward_return_types(bert_nsp, mock_hooked_encoder, return_type):
    """Test different return types from forward pass"""
    # Setup mock logits that favor sequential prediction
    mock_logits = torch.tensor([[0.7, 0.3]])
    mock_hooked_encoder.nsp_head.return_value = mock_logits

    input_data = ["She went to the grocery store.", "She bought an apple."]
    output = bert_nsp.forward(input_data, return_type=return_type)

    if return_type is None:
        assert output is None
    elif return_type == "logits":
        assert isinstance(output, torch.Tensor)
        assert output.shape == (1, 2)
        assert torch.equal(output, mock_logits)
    elif return_type == "predictions":
        assert isinstance(output, str)
        assert output == "The sentences are sequential"  # Based on mock logits


def test_to_tokens_validation(bert_nsp):
    """Test input validation in to_tokens method"""
    with pytest.raises(
        ValueError, match="Next sentence prediction task requires exactly two sentences"
    ):
        bert_nsp.to_tokens(["Single sentence"])

    with pytest.raises(
        ValueError, match="Next sentence prediction task requires exactly two sentences"
    ):
        bert_nsp.to_tokens(["One", "Two", "Three"])


def test_run_with_cache(bert_nsp, mock_hooked_encoder):
    """Test run_with_cache for correct handling of cache"""
    # Set up mock returns
    mock_resid = torch.randn(1, 3, 768)
    mock_cache = {"resid_pre": torch.randn(1, 3, 768), "attn_output": torch.randn(1, 3, 768)}
    mock_hooked_encoder.run_with_cache.return_value = (
        torch.tensor([[0.6, 0.4]]),  # Mock logits
        mock_cache,
    )

    input_data = ["First sentence.", "Second sentence."]

    # Run with cache
    output, cache = bert_nsp.run_with_cache(
        input_data, return_type="logits", return_cache_object=True
    )

    # Verify output shape and values
    assert output.shape == (1, 2)
    assert isinstance(cache, dict) or hasattr(cache, "cache_dict")

    # Verify the cache contains expected keys
    if hasattr(cache, "cache_dict"):
        cache_dict = cache.cache_dict
    else:
        cache_dict = cache

    assert "resid_pre" in cache_dict
    assert "attn_output" in cache_dict


def test_return_type_consistency(bert_nsp, mock_hooked_encoder):
    """Test consistency between logits and prediction outputs"""
    # Setup mock logits that favor non-sequential prediction
    mock_logits = torch.tensor([[0.2, 0.8]])
    mock_hooked_encoder.nsp_head.return_value = mock_logits

    input_data = ["She went to the grocery store.", "She bought an apple."]

    # Get predictions using different return types
    logits = bert_nsp.forward(input_data, return_type="logits")
    prediction_str = bert_nsp.forward(input_data, return_type="predictions")

    # Calculate predicted class from logits
    predicted_class = logits.argmax(dim=-1).item()
    expected_prediction = ["The sentences are sequential", "The sentences are NOT sequential"][
        predicted_class
    ]

    assert prediction_str == expected_prediction  # Based on mock logits



---
File: /tests/unit/test_split_qkv.py
---

import torch

from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_split_qkv_normal_attn_correct():
    """Verifies that the split_qkv_input flag does not change the output for models with normal attention."""
    d_model = 128
    d_head = 8
    n_heads = 16
    n_ctx = 128
    n_layers = 1
    d_vocab = 10

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_layers=n_layers,
        attn_only=True,
        d_vocab=d_vocab,
    )

    model = HookedTransformer(cfg)
    assert model.cfg.use_split_qkv_input is False

    x = torch.arange(1, 9).unsqueeze(0)
    normal_output = model(x)

    model.set_use_split_qkv_input(True)
    assert model.cfg.use_split_qkv_input is True

    split_output = model(x)

    assert torch.allclose(normal_output, split_output, atol=1e-6)


def test_split_qkv_grouped_query_attn_correct():
    """Verifies that the split_qkv_input flag does not change the output for models with grouped query attention."""

    d_model = 128
    d_head = 8
    n_heads = 16
    n_ctx = 128
    n_key_value_heads = 2
    n_layers = 1
    d_vocab = 10

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        attn_only=True,
        d_vocab=d_vocab,
    )

    model = HookedTransformer(cfg)
    assert model.cfg.use_split_qkv_input is False

    x = torch.arange(1, 9).unsqueeze(0)
    normal_output = model(x)

    model.set_use_split_qkv_input(True)
    assert model.cfg.use_split_qkv_input is True

    split_output = model(x)

    assert torch.allclose(normal_output, split_output, atol=1e-6)



---
File: /tests/unit/test_svd_interpreter.py
---

import pytest
import torch
from beartype.roar import BeartypeCallHintParamViolation

from transformer_lens import HookedTransformer, SVDInterpreter

MODEL = "solu-2l"
VECTOR_TYPES = ["OV", "w_in", "w_out"]
ATOL = 2e-4  # Absolute tolerance - how far does a float have to be before we consider it no longer equal?


@pytest.fixture(scope="module")
def model():
    return HookedTransformer.from_pretrained(MODEL)


@pytest.fixture(scope="module")
def unfolded_model():
    return HookedTransformer.from_pretrained(MODEL, fold_ln=False)


@pytest.fixture(scope="module")
def second_model():
    return HookedTransformer.from_pretrained("solu-3l")


expected_OV_match = torch.Tensor(
    [[[0.6597, 0.8689, 0.6344, 0.7345]], [[0.5244, 0.6705, 0.5940, 0.7240]]]
)

expected_w_in_match = torch.Tensor(
    [[[0.7714, 0.6608, 0.6452, 0.6933]], [[0.7647, 0.6466, 0.6406, 0.6458]]]
)

expected_w_in_unfolded_match = torch.Tensor(
    [[[0.3639, 0.3164, 0.3095, 0.3430]], [[0.3614, 0.3050, 0.3041, 0.3140]]]
)

expected_w_out_match = torch.Tensor(
    [[[0.5097, 0.5389, 0.7906, 0.7178]], [[0.5076, 0.5350, 0.7674, 0.7106]]]
)

# Successes


def test_svd_interpreter(model):
    svd_interpreter = SVDInterpreter(model)
    ov = svd_interpreter.get_singular_vectors(
        "OV", num_vectors=4, layer_index=0, head_index=0
    ).abs()
    w_in = svd_interpreter.get_singular_vectors(
        "w_in", num_vectors=4, layer_index=0, head_index=0
    ).abs()
    w_out = svd_interpreter.get_singular_vectors(
        "w_out", num_vectors=4, layer_index=0, head_index=0
    ).abs()

    ov, w_in, w_out = (
        ov.topk(2, dim=0).values,
        w_in.topk(2, dim=0).values,
        w_out.topk(2, dim=0).values,
    )
    assert ov.shape == w_in.shape == w_out.shape == expected_OV_match.shape
    assert torch.allclose(ov.cpu(), expected_OV_match, atol=ATOL)
    assert torch.allclose(w_in.cpu(), expected_w_in_match, atol=ATOL)
    assert torch.allclose(w_out.cpu(), expected_w_out_match, atol=ATOL)


def test_w_in_when_fold_ln_is_false(unfolded_model):
    svd_interpreter = SVDInterpreter(unfolded_model)
    w_in = svd_interpreter.get_singular_vectors(
        "w_in", num_vectors=4, layer_index=0, head_index=0
    ).abs()
    w_in = w_in.topk(2, dim=0).values
    assert torch.allclose(w_in.cpu(), expected_w_in_unfolded_match, atol=ATOL)


def test_svd_interpreter_returns_different_answers_for_different_layers(model):
    svd_interpreter = SVDInterpreter(model)
    ov = svd_interpreter.get_singular_vectors(
        "OV", layer_index=1, num_vectors=4, head_index=0
    ).abs()
    w_in = svd_interpreter.get_singular_vectors(
        "w_in", layer_index=1, num_vectors=4, head_index=0
    ).abs()
    w_out = svd_interpreter.get_singular_vectors(
        "w_out", layer_index=1, num_vectors=4, head_index=0
    ).abs()

    ov, w_in, w_out = (
        ov.topk(2, dim=0).values,
        w_in.topk(2, dim=0).values,
        w_out.topk(2, dim=0).values,
    )
    assert ov.shape == w_in.shape == w_out.shape == expected_OV_match.shape
    assert not torch.allclose(ov.cpu(), expected_OV_match, atol=ATOL)
    assert not torch.allclose(w_in.cpu(), expected_w_in_match, atol=ATOL)
    assert not torch.allclose(w_out.cpu(), expected_w_out_match, atol=ATOL)


def test_svd_interpreter_returns_different_answers_for_different_models(second_model):
    svd_interpreter = SVDInterpreter(second_model)
    ov = svd_interpreter.get_singular_vectors(
        "OV", layer_index=1, num_vectors=4, head_index=0
    ).abs()
    w_in = svd_interpreter.get_singular_vectors(
        "w_in", layer_index=1, num_vectors=4, head_index=0
    ).abs()
    w_out = svd_interpreter.get_singular_vectors(
        "w_out", layer_index=1, num_vectors=4, head_index=0
    ).abs()

    ov, w_in, w_out = (
        ov.topk(2, dim=0).values,
        w_in.topk(2, dim=0).values,
        w_out.topk(2, dim=0).values,
    )
    assert not torch.allclose(ov.cpu(), expected_OV_match, atol=ATOL)
    assert not torch.allclose(w_in.cpu(), expected_w_in_match, atol=ATOL)
    assert not torch.allclose(w_out.cpu(), expected_w_out_match, atol=ATOL)


# Failures


def test_svd_interpreter_fails_on_invalid_vector_type(model):
    svd_interpreter = SVDInterpreter(model)
    with pytest.raises(BeartypeCallHintParamViolation) as e:
        svd_interpreter.get_singular_vectors("test", layer_index=0, num_vectors=4, head_index=0)


def test_svd_interpreter_fails_on_not_passing_required_head_index(model):
    svd_interpreter = SVDInterpreter(model)
    with pytest.raises(AssertionError) as e:
        svd_interpreter.get_singular_vectors("OV", layer_index=0, num_vectors=4)
        assert str(e.value) == "Head index optional only for w_in and w_out, got OV"


def test_svd_interpreter_fails_on_invalid_layer_index(model):
    svd_interpreter = SVDInterpreter(model)
    for vector in VECTOR_TYPES:
        with pytest.raises(AssertionError) as e:
            svd_interpreter.get_singular_vectors(vector, layer_index=2, num_vectors=4, head_index=0)
        assert str(e.value) == "Layer index must be between 0 and 1 but got 2"


def test_svd_interpreter_fails_on_invalid_head_index(model):
    # Only OV uses head index.
    svd_interpreter = SVDInterpreter(model)
    with pytest.raises(AssertionError) as e:
        svd_interpreter.get_singular_vectors("OV", layer_index=0, num_vectors=4, head_index=8)
    assert str(e.value) == "Head index must be between 0 and 7 but got 8"



---
File: /tests/unit/test_use_attn_result.py
---

import torch

from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def test_atten_result_normal_attn_correct():
    """Verifies that the attn_result flag does not change the output for models with normal attention."""
    d_model = 128
    d_head = 8
    n_heads = 16
    n_ctx = 128
    n_layers = 1
    d_vocab = 10

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_layers=n_layers,
        attn_only=True,
        d_vocab=d_vocab,
    )

    model = HookedTransformer(cfg)
    assert model.cfg.use_split_qkv_input is False

    x = torch.arange(1, 9).unsqueeze(0)
    normal_output = model(x)

    model.set_use_attn_result(True)
    assert model.cfg.use_attn_result is True

    split_output = model(x)

    assert torch.allclose(normal_output, split_output, atol=1e-6)


def test_atten_result_grouped_query_attn_correct():
    """Verifies that the atten_result flag does not change the output for models with grouped query attention."""

    d_model = 128
    d_head = 8
    n_heads = 16
    n_ctx = 128
    n_key_value_heads = 2
    n_layers = 1
    d_vocab = 10

    cfg = HookedTransformerConfig(
        d_model=d_model,
        d_head=d_head,
        n_heads=n_heads,
        n_ctx=n_ctx,
        n_key_value_heads=n_key_value_heads,
        n_layers=n_layers,
        attn_only=True,
        d_vocab=d_vocab,
    )

    model = HookedTransformer(cfg)
    assert model.cfg.use_split_qkv_input is False

    x = torch.arange(1, 9).unsqueeze(0)
    normal_output = model(x)

    model.set_use_attn_result(True)
    assert model.cfg.use_attn_result is True

    split_output = model(x)

    assert torch.allclose(normal_output, split_output, atol=1e-6)



---
File: /tests/unit/test_utils.py
---

import numpy as np
import pytest
import torch
from torch import nn

import transformer_lens.utils as utils
from transformer_lens import HookedTransformer

ref_tensor = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
shape = ref_tensor.shape
tensor_row0_dim2 = torch.tensor([[1, 2, 3, 4, 5]])
shape_2 = tensor_row0_dim2.shape
tensor_row0_dim1 = torch.tensor([1, 2, 3, 4, 5])
shape_1 = tensor_row0_dim1.shape


class TestSlice:
    @pytest.mark.parametrize(
        "input_slice, expected_shape",
        [
            (
                [
                    0,
                ],
                shape_2,
            ),
            ((1,), shape_2),
            (
                torch.tensor(
                    [
                        0,
                    ]
                ),
                shape_2,
            ),
            (
                np.array(
                    [
                        0,
                    ]
                ),
                shape_2,
            ),
            (0, shape_1),
            (torch.tensor(0), shape_1),
            (None, shape),
        ],
    )
    def test_modularity_shape(self, input_slice, expected_shape):
        slc = utils.Slice(input_slice=input_slice)
        sliced_tensor = slc.apply(ref_tensor)
        assert sliced_tensor.shape == expected_shape

    @pytest.mark.parametrize(
        "input_slice, expected_tensor",
        [
            (
                [
                    0,
                ],
                tensor_row0_dim2,
            ),
            (
                torch.tensor(
                    [
                        0,
                    ]
                ),
                tensor_row0_dim2,
            ),
            (
                np.array(
                    [
                        0,
                    ]
                ),
                tensor_row0_dim1,
            ),
            (0, tensor_row0_dim1),
            (torch.tensor(0), tensor_row0_dim1),
            (None, ref_tensor),
        ],
    )
    def test_modularity_tensor(self, input_slice, expected_tensor):
        slc = utils.Slice(input_slice=input_slice)
        sliced_tensor = slc.apply(ref_tensor)
        assert (sliced_tensor == expected_tensor).all()

    @pytest.mark.parametrize(
        "input_slice, expected_indices",
        [
            ([0, 1], np.array([0, 1])),
            (0, np.array(0)),
            (torch.tensor(0), np.array(0)),
        ],
    )
    def test_indices(self, input_slice, expected_indices):
        slc = utils.Slice(input_slice=input_slice)
        indices = slc.indices(2)
        assert (indices == expected_indices).all()

    def test_indices_error(self):
        with pytest.raises(ValueError):
            _ = utils.Slice(input_slice=[0, 2, 5]).indices()


class Test_is_square:
    failed_cases = (
        torch.tensor([]),
        torch.tensor(2),
        torch.ones(2, 3),
        torch.zeros(2),
        torch.ones(3, 3, 3),
        torch.ones(1, 1, 1, 1),
    )

    @pytest.mark.parametrize("x", (torch.ones(3, 3), torch.zeros(1, 1)))
    def test_pass(self, x: torch.Tensor):
        assert utils.is_square(x)

    @pytest.mark.parametrize("x", failed_cases)
    def test_fail(self, x: torch.Tensor):
        assert not utils.is_square(x)


class Test_lower_triangular:
    @pytest.mark.parametrize(
        "x",
        (
            torch.eye(4),
            torch.ones(4, 4).tril(),
            torch.ones(4, 4).triu().T,
            torch.zeros(4, 4),
        ),
    )
    def test_pass(self, x: torch.Tensor):
        assert utils.is_lower_triangular(x)

    @pytest.mark.parametrize(
        "x",
        (
            *Test_is_square.failed_cases,
            torch.ones(4, 4).triu(),
            torch.ones(4, 4).tril().T,
            torch.ones(3, 3),
        ),
    )
    def test_fail(self, x: torch.Tensor):
        assert not utils.is_lower_triangular(x)


def test_override_or_use_default_value():
    # Case when override is not None
    assert utils.override_or_use_default_value(default_flag=True, override=True) == True
    assert utils.override_or_use_default_value(default_flag=True, override=False) == False
    assert utils.override_or_use_default_value(default_flag=False, override=True) == True
    assert utils.override_or_use_default_value(default_flag=False, override=False) == False

    # Case when override is None
    assert utils.override_or_use_default_value(default_flag=True, override=None) == True
    assert utils.override_or_use_default_value(default_flag=False, override=None) == False

    # Case when override is not passed
    assert utils.override_or_use_default_value(default_flag=True) == True
    assert utils.override_or_use_default_value(default_flag=False) == False


class TestAttentionMask:
    prompts = [
        "Hello world!",
        "How are you today?",
        "I'm fine, thank you.",
        "I am happy.",
    ]

    prompts_with_sep = [
        "I like cats<|endoftext|>Cats are so cute",
        "Hello world!",
        "How are you<|endoftext|>I am fine, thanks",
    ]

    # fixtures
    @pytest.fixture(scope="class", params=["gpt2-small", "facebook/opt-125m"])
    def model_name(self, request):
        return request.param

    @pytest.fixture(scope="class")
    def model(self, model_name):
        return HookedTransformer.from_pretrained(model_name)

    # tests
    @pytest.mark.parametrize("padding_side", ["left", "right"])
    @pytest.mark.parametrize("prepend_bos", [True, False])
    @pytest.mark.parametrize("prompts_with_sep", [True, False])
    def test_get_attention_mask(self, model, padding_side, prepend_bos, prompts_with_sep):
        # setup
        model.tokenizer.padding_side = padding_side
        model.tokenizer.sep_token_id = model.tokenizer.pad_token_id
        prepend_bos = prepend_bos

        prompts = self.prompts_with_sep if prompts_with_sep else self.prompts
        tokens = model.to_tokens(prompts, prepend_bos=prepend_bos)

        attention_mask = utils.get_attention_mask(
            model.tokenizer, tokens, prepend_bos=prepend_bos
        )  # [batch pos]

        # dimension should be the same
        assert attention_mask.shape == tokens.shape

        # number of attended tokens for each sequence
        # should be the same as the number of 1s in the attention mask for that sequence
        str_tokens = model.to_str_tokens(prompts, prepend_bos=prepend_bos)
        intended_num_attended_tokens = torch.tensor(
            [len(t) for t in str_tokens], device=attention_mask.device
        )
        assert (intended_num_attended_tokens == attention_mask.sum(dim=1)).all()

        # all the masked tokens should be the padding token
        assert (tokens[attention_mask == 0] == model.tokenizer.pad_token_id).all()

        if padding_side == "right":
            # the first token is always attended
            assert (attention_mask[:, 0] == 1).all()

            # attended tokens are at the beginning of the sequence
            for i, num in enumerate(intended_num_attended_tokens.tolist()):
                assert (attention_mask[i, 0:num] == 1).all()

        else:  # left padding case
            # the last token is always attended
            assert (attention_mask[:, -1] == 1).all()

            # attended tokens are at the end of the sequence
            for i, num in enumerate(intended_num_attended_tokens.tolist()):
                assert (attention_mask[i, -num:] == 1).all()

        # the following tests make sense only when the prompts do not contain the separator token
        if not prompts_with_sep:
            non_pad_token_mask = (tokens != model.tokenizer.pad_token_id).int()
            attended_but_non_pad_mask = attention_mask != non_pad_token_mask
            if model.tokenizer.bos_token == model.tokenizer.pad_token and prepend_bos:
                # if bos_token is the same as pad_token and prepend_bos is True,
                # then there is one attended but non-pad token (bos token) in each sequence
                assert attended_but_non_pad_mask.sum() == tokens.shape[0]
            else:
                # otherwise, there should be no attended but non-pad token
                assert attended_but_non_pad_mask.sum() == 0


def test_calc_fan_in_fan_out():
    """
    Test for the calc_fan_in_and_fan_out function in the utils module.
    """
    # Test for the case when the tensor is 1D
    tensor_1d = torch.tensor([1, 2, 3, 4, 5])
    fan_in, fan_out = utils.calc_fan_in_and_fan_out(tensor_1d)
    assert fan_in == 1
    assert fan_out == 5

    # Test for the case when the tensor is 2D
    tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])
    fan_in, fan_out = utils.calc_fan_in_and_fan_out(tensor_2d)
    assert fan_in == 2
    assert fan_out == 3

    # Test for the case when the tensor is 3D
    tensor_3d = nn.Parameter(torch.rand(2, 25, 5))  # 2 x 25 x 5, I'm not writing this out
    fan_in, fan_out = utils.calc_fan_in_and_fan_out(tensor_3d)
    assert fan_in == 25
    assert fan_out == 10

    # Test for the case when the tensor is 4D (should raise a ValueError)
    tensor_4d = torch.tensor([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]])
    with pytest.raises(ValueError):
        fan_in, fan_out = utils.calc_fan_in_and_fan_out(tensor_4d)

    # Test for the case when the tensor is 0D (also should raise a ValueError)
    tensor_0d = torch.tensor(1)
    with pytest.raises(ValueError):
        fan_in, fan_out = utils.calc_fan_in_and_fan_out(tensor_0d)


class TestInitKaiming:
    """Test cases for kaiming init."""

    @pytest.mark.parametrize(
        "d_model", [4096, 10_000]
    )  # this needs to be large so std and min/max estimates are accurate
    @pytest.mark.parametrize("d_mlp", [256, 512])
    @pytest.mark.parametrize("nonlinearity", ["linear", "relu"])
    def test_init_kaiming_uniform(self, d_model, d_mlp, nonlinearity):
        """
        Test init_kaiming_uniform function in the utils module on 3/2/1D tensors.
        """
        torch.manual_seed(1234)

        gain = np.sqrt(2.0) if nonlinearity == "relu" else 1.0

        x = nn.Parameter(torch.empty(2, d_model, 137))  # n_head and d_head don't matter
        utils.init_kaiming_uniform_(x, nonlinearity=nonlinearity)
        std = gain / np.sqrt(d_model)
        assert np.isclose(x.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(x.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(x.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        y = nn.Parameter(torch.empty(d_mlp, d_model))
        utils.init_kaiming_uniform_(y, nonlinearity=nonlinearity)
        std = gain / np.sqrt(d_mlp)
        assert np.isclose(y.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(y.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(y.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        z = nn.Parameter(torch.empty(d_model * 123))
        utils.init_kaiming_uniform_(z, nonlinearity=nonlinearity)
        std = gain  # bias has fan_in 1
        assert np.isclose(z.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(z.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(z.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        torch.manual_seed(1234)
        x_new = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_kaiming_uniform_(x_new, nonlinearity=nonlinearity)
        assert torch.allclose(x_new, x, rtol=1e-2)

    @pytest.mark.parametrize("d_model", [4096, 10_000])
    @pytest.mark.parametrize("d_mlp", [256, 512])
    @pytest.mark.parametrize("nonlinearity", ["linear", "relu"])
    def test_init_kaiming_normal(self, d_model, d_mlp, nonlinearity):
        """
        Test init_kaiming_normal function in the utils module on 3/2/1D tensors.
        """
        torch.manual_seed(1234)

        gain = np.sqrt(2.0) if nonlinearity == "relu" else 1.0

        x = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_kaiming_normal_(x, nonlinearity=nonlinearity)
        std = gain / np.sqrt(d_model)
        assert np.isclose(x.std().detach().numpy(), std, rtol=1e-2)

        y = nn.Parameter(torch.empty(d_mlp, d_model))
        utils.init_kaiming_normal_(y, nonlinearity=nonlinearity)
        std = gain / np.sqrt(d_mlp)
        assert np.isclose(y.std().detach().numpy(), std, rtol=1e-2)

        z = nn.Parameter(torch.empty(d_model * 123))
        utils.init_kaiming_normal_(z, nonlinearity=nonlinearity)
        std = gain  # bias has fan_in 1
        assert np.isclose(z.std().detach().numpy(), std, rtol=1e-2)

        torch.manual_seed(1234)
        x_new = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_kaiming_normal_(x_new, nonlinearity=nonlinearity)
        assert torch.allclose(x_new, x, rtol=1e-2)


class TestInitXavier:
    """Test cases for Xavier init. Std of distribution should be scaled to sqrt(2/(fan_in + fan_out))."""

    @pytest.mark.parametrize("d_model", [4096, 10_000])
    @pytest.mark.parametrize("d_mlp", [256, 512])
    def test_init_xavier_uniform(self, d_model, d_mlp):
        """Test init_xavier_uniform function in the utils module on 3/2/1D tensors."""
        torch.manual_seed(1234)

        x = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_xavier_uniform_(x)
        std = np.sqrt(2 / (d_model + 137 * 2))
        assert np.isclose(x.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(x.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(x.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        y = nn.Parameter(torch.empty(d_mlp, d_model))
        utils.init_xavier_uniform_(y)
        std = np.sqrt(2 / (d_mlp + d_model))
        assert np.isclose(y.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(y.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(y.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        z = nn.Parameter(torch.empty(d_model * 123))
        utils.init_xavier_uniform_(z)
        std = np.sqrt(2 / (1 + d_model * 123))
        assert np.isclose(z.std().detach().numpy(), std, rtol=1e-2)
        # for uniform distributions, min/max is sqrt(3) times the std
        assert np.isclose(z.max().detach().numpy(), np.sqrt(3) * std, rtol=1e-2)
        assert np.isclose(z.min().detach().numpy(), -np.sqrt(3) * std, rtol=1e-2)

        torch.manual_seed(1234)
        x_new = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_xavier_uniform_(x_new)
        assert torch.allclose(x_new, x, rtol=1e-2)

    @pytest.mark.parametrize("d_model", [4096, 10_000])
    @pytest.mark.parametrize("d_mlp", [256, 512])
    def test_init_xavier_normal(self, d_model, d_mlp):
        """Test init_xavier_normal function in the utils module on 3/2/1D tensors."""
        torch.manual_seed(1234)

        x = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_xavier_normal_(x)
        std = np.sqrt(2 / (d_model + 137 * 2))
        assert np.isclose(x.std().detach().numpy(), std, rtol=1e-2)

        y = nn.Parameter(torch.empty(d_mlp, d_model))
        utils.init_xavier_normal_(y)
        std = np.sqrt(2 / (d_mlp + d_model))
        assert np.isclose(y.std().detach().numpy(), std, rtol=1e-2)

        z = nn.Parameter(torch.empty(d_model * 123))  # need to make this larger so std is accurate
        utils.init_xavier_normal_(z)
        std = np.sqrt(2 / (1 + d_model * 123))
        assert np.isclose(z.std().detach().numpy(), std, rtol=1e-2)

        torch.manual_seed(1234)
        x_new = nn.Parameter(torch.empty(2, d_model, 137))
        utils.init_xavier_normal_(x_new)
        assert torch.allclose(x_new, x, rtol=1e-2)



---
File: /transformer_lens/components/mlps/can_be_used_as_mlp.py
---

"""Can Be Used as MLP component.

This module serves as the base for everything within TransformerLens that can be used like an MLP.
This does not necessarily mean that every component extending this class will be an MLP, but 
everything extending this class can be used interchangeably for an MLP.
"""
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components import LayerNorm, LayerNormPre
from transformer_lens.factories.activation_function_factory import (
    ActivationFunctionFactory,
)
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.activation_functions import ActivationFunction


class CanBeUsedAsMLP(nn.Module):
    # The actual activation function
    act_fn: ActivationFunction

    # The full config object for the model
    cfg: HookedTransformerConfig

    # The d mlp value pulled out of the config to make sure it always has a value
    d_mlp: int

    # The middle hook point will be None unless it specifically should be used
    hook_mid: Optional[HookPoint]  # [batch, pos, d_mlp]

    # The layer norm component if the activation function is a layer norm
    ln: Optional[nn.Module]

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        """The base init for all MLP like components

        Args:
            config (Union[Dict, HookedTransformerConfig]): The config for this instance

        Raises:
            ValueError: If there is a misconfiguration
        """
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        if self.cfg.d_mlp is None:
            raise ValueError("d_mlp must be set to use an MLP")

        self.d_mlp = self.cfg.d_mlp

    def forward(
        self, x: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        """The format for all forward functions for any MLP"""
        return x

    def select_activation_function(self) -> None:
        """This function should be called by all components in their init to get everything needed
        for activation functions setup.

        Raises:
            ValueError: If the configure activation function is not supported.
        """

        self.act_fn = ActivationFunctionFactory.pick_activation_function(self.cfg)

        if self.cfg.is_layer_norm_activation():
            self.hook_mid = HookPoint()
            if self.cfg.normalization_type == "LN":
                self.ln = LayerNorm(self.cfg, self.d_mlp)
            else:
                self.ln = LayerNormPre(self.cfg)



---
File: /transformer_lens/components/mlps/gated_mlp_4bit.py
---

"""Hooked Transformer Gated MLP Component.

This module contains all the component :class:`GatedMLP`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float
from transformers.utils import is_bitsandbytes_available

from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig

if is_bitsandbytes_available():
    import bitsandbytes as bnb
    from bitsandbytes.nn.modules import Params4bit


class GatedMLP4Bit(CanBeUsedAsMLP):
    """
    The equation of a gated MLP:
    pre = x @ W_gate
    pre_linear = x @ W_in
    post = Gelu(pre) * (pre_linear) + b_in
    mlp_out = post @ W_out + b_out

    In one equation, mlp_out = (Gelu(x @ W_gate) * (x @ W_in) + b_in) @ W_out + b_out
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__(cfg)
        self.select_activation_function()

        nq = int((self.cfg.d_model * self.d_mlp) / 2)
        self.W_in = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
        self.W_gate = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
        self.W_out = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)

        self.b_in = nn.Parameter(torch.zeros(self.d_mlp, dtype=self.cfg.dtype))
        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))

        # hook on gate output but before act_fn
        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]
        # hook on the linear component of the input
        self.hook_pre_linear = HookPoint()  # [batch, pos, d_mlp]
        # hook on act_fn(gate_output) * W_in(x) + b_in
        self.hook_post = HookPoint()  # [batch, pos, d_mlp]

    def forward(
        self, x: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        # Technically, all these einsums could be done with a single matmul, but this is more readable.
        pre_act = self.hook_pre(
            bnb.matmul_4bit(x, self.W_gate.t(), bias=None, quant_state=self.W_gate.quant_state)
        )

        if (
            self.cfg.is_layer_norm_activation()
            and self.hook_mid is not None
            and self.ln is not None
        ):
            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp]
            post_act = self.hook_post(self.ln(mid_act))
        else:
            pre_linear = self.hook_pre_linear(
                bnb.matmul_4bit(x, self.W_in.t(), bias=None, quant_state=self.W_in.quant_state)
            )

            post_act = self.hook_post(
                (self.act_fn(pre_act) * pre_linear) + self.b_in
            )  # [batch, pos, d_mlp]

        return bnb.matmul_4bit(
            post_act, self.W_out.t(), bias=None, quant_state=self.W_out.quant_state
        )



---
File: /transformer_lens/components/mlps/gated_mlp.py
---

"""Hooked Transformer Gated MLP Component.

This module contains all the component :class:`GatedMLP`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float
from transformers.utils import is_bitsandbytes_available

from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.addmm import batch_addmm

if is_bitsandbytes_available():
    pass


class GatedMLP(CanBeUsedAsMLP):
    """
    The equation of a gated MLP:
    pre = x @ W_gate
    pre_linear = x @ W_in
    post = Gelu(pre) * (pre_linear) + b_in
    mlp_out = post @ W_out + b_out

    In one equation, mlp_out = (Gelu(x @ W_gate) * (x @ W_in) + b_in) @ W_out + b_out
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__(cfg)
        self.select_activation_function()
        self.W_in = nn.Parameter(torch.empty(self.cfg.d_model, self.d_mlp, dtype=self.cfg.dtype))
        self.W_out = nn.Parameter(torch.empty(self.d_mlp, self.cfg.d_model, dtype=self.cfg.dtype))
        self.W_gate = nn.Parameter(torch.empty(self.cfg.d_model, self.d_mlp, dtype=self.cfg.dtype))

        self.b_in = nn.Parameter(torch.zeros(self.d_mlp, dtype=self.cfg.dtype))
        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))

        # hook on gate output but before act_fn
        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]
        # hook on the linear component of the input
        self.hook_pre_linear = HookPoint()  # [batch, pos, d_mlp]
        # hook on act_fn(gate_output) * W_in(x) + b_in
        self.hook_post = HookPoint()  # [batch, pos, d_mlp]

    def forward(
        self, x: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        # Technically, all these einsums could be done with a single matmul, but this is more readable.
        if self.W_gate.device != x.device:
            x = x.to(self.W_gate.device)
        pre_act = self.hook_pre(
            torch.matmul(x, self.W_gate)  # batch pos d_model, d_model d_mlp -> batch pos d_mlp
        )  # [batch, pos, d_mlp]

        if (
            self.cfg.is_layer_norm_activation()
            and self.hook_mid is not None
            and self.ln is not None
        ):
            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp]
            post_act = self.hook_post(self.ln(mid_act))
        else:
            pre_linear = self.hook_pre_linear(
                torch.matmul(x, self.W_in)  # batch pos d_model, d_model d_mlp -> batch pos d_mlp
            )

            post_act = self.hook_post(
                (self.act_fn(pre_act) * pre_linear) + self.b_in
            )  # [batch, pos, d_mlp]

        return batch_addmm(self.b_out, self.W_out, post_act)



---
File: /transformer_lens/components/mlps/mlp.py
---

"""Hooked Transformer MLP Component.

This module contains all the component :class:`MLP`.
"""

from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.addmm import batch_addmm


class MLP(CanBeUsedAsMLP):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__(cfg)
        self.select_activation_function()

        self.W_in = nn.Parameter(torch.empty(self.cfg.d_model, self.d_mlp, dtype=self.cfg.dtype))
        self.b_in = nn.Parameter(torch.zeros(self.d_mlp, dtype=self.cfg.dtype))

        self.W_out = nn.Parameter(torch.empty(self.d_mlp, self.cfg.d_model, dtype=self.cfg.dtype))
        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))

        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]
        self.hook_post = HookPoint()  # [batch, pos, d_mlp]

    def forward(
        self, x: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        # This is equivalent to (roughly) W_in @ x + b_in. It's important to
        # use a fused addmm to ensure it matches the Huggingface implementation
        # exactly.
        pre_act = self.hook_pre(batch_addmm(self.b_in, self.W_in, x))  # [batch, pos, d_mlp]

        if (
            self.cfg.is_layer_norm_activation()
            and self.hook_mid is not None
            and self.ln is not None
        ):
            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp]
            post_act = self.hook_post(self.ln(mid_act))
        else:
            post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp]
        return batch_addmm(self.b_out, self.W_out, post_act)



---
File: /transformer_lens/components/mlps/moe.py
---

from typing import Dict, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from jaxtyping import Float

from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.factories.activation_function_factory import (
    ActivationFunctionFactory,
)
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class MoEGatedMLP(nn.Module):
    """MoEGated MLP

    This MLP matches the implementation for Mixtral on HuggingFace. It is meant to stay within our
    MoE, since the format of this MLP is different from the standard MLPs throughout
    TransformerLens.

    It may be possible to rework this to follow the same interface as other MLPs, but for the
    time being it is being left as is to ensure accuracy.
    """

    def __init__(self, cfg: HookedTransformerConfig):
        super().__init__()
        self.cfg = cfg

        self.d_mlp = self.cfg.d_mlp

        if self.d_mlp is None:
            raise ValueError("d_mlp must be set to use an MLP")

        self.W_in = nn.Linear(self.cfg.d_model, self.d_mlp, bias=False)
        self.W_out = nn.Linear(self.d_mlp, self.cfg.d_model, bias=False)
        self.W_gate = nn.Linear(self.cfg.d_model, self.d_mlp, bias=False)

        # hook on gate output but before act_fn
        self.hook_gate = HookPoint()  # [batch, pos, d_mlp]
        # hook on the linear component of the input
        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]
        # hook on act_fn(gate_output) * W_in(x) + b_in
        self.hook_post = HookPoint()  # [batch, pos, d_mlp]

        self.act_fn = ActivationFunctionFactory.pick_activation_function(self.cfg)

    def forward(self, x: Float[torch.Tensor, "pos d_model"]) -> Float[torch.Tensor, "pos d_model"]:
        gated_x = self.hook_gate(self.W_gate(x))
        pre_act = self.hook_pre(self.W_in(x))
        post_act = self.hook_post(self.act_fn(gated_x) * pre_act)
        return self.W_out(post_act)


class MoE(CanBeUsedAsMLP):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__(cfg)

        # Ensure that num_experts and experts_per_token are specified and non-zero
        assert self.cfg.num_experts is not None, "num_experts must be specified for MoE layer"
        assert self.cfg.experts_per_token, "experts_per_token must be specified for MoE layer"

        self.num_experts: int = self.cfg.num_experts
        self.experts_per_token: int = self.cfg.experts_per_token

        assert (
            self.cfg.experts_per_token <= self.cfg.num_experts
        ), "experts_per_token must be less than or equal to num_experts"

        self.experts = nn.ModuleList([MoEGatedMLP(self.cfg) for _ in range(self.num_experts)])
        self.W_gate = nn.Linear(self.cfg.d_model, self.cfg.num_experts, bias=False)

        # Hook on the weights of selected experts [batch pos experts_per_token]
        self.hook_expert_weights = HookPoint()
        # Hook on the indices of selected experts [batch pos experts_per_token]
        self.hook_expert_indices = HookPoint()

    def forward(
        self, x: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        # [batch, pos, d_model] -> [batch, pos, num_experts]
        batch, pos, d_model = x.shape
        x = x.view(-1, d_model)
        gate_logits = self.W_gate(x)

        # choose the top k(=experts_per_token) experts to use
        # both are [batch, pos, experts_per_token]
        weights = self.hook_expert_weights(F.softmax(gate_logits, dim=1, dtype=torch.float))
        weights, expert_indices = torch.topk(weights, self.experts_per_token, dim=-1)
        weights /= weights.sum(dim=-1, keepdim=True)
        expert_indices = self.hook_expert_indices(expert_indices)
        weights = weights.to(x.dtype)

        results = torch.zeros((batch * pos, d_model), dtype=x.dtype, device=x.device)
        expert_mask = F.one_hot(expert_indices, num_classes=self.num_experts).permute(2, 1, 0)
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            idx, top_x = torch.where(expert_mask[expert_idx])

            # Index the correct hidden states and compute the expert hidden state for
            # the current expert. We need to make sure to multiply the output hidden
            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
            current_state = x[None, top_x].reshape(-1, d_model)

            current_hidden_states = expert_layer(current_state) * weights[top_x, idx, None]

            # However `index_add_` only support torch tensors for indexing so we'll use
            # the `top_x` tensor here.
            results.index_add_(0, top_x, current_hidden_states.to(x.dtype))

        results = results.reshape(batch, pos, d_model)
        return results



---
File: /transformer_lens/components/__init__.py
---

"""Hooked Transformer Components.

This module contains all the components (e.g. :class:`Attention`, :class:`MLP`, :class:`LayerNorm`)
needed to create many different types of generative language models. They are used by
:class:`transformer_lens.HookedTransformer`.
"""

# Independent classes
from .abstract_attention import AbstractAttention
from .layer_norm import LayerNorm
from .layer_norm_pre import LayerNormPre
from .pos_embed import PosEmbed
from .rms_norm import RMSNorm
from .rms_norm_pre import RMSNormPre
from .token_typed_embed import TokenTypeEmbed
from .unembed import Unembed

# Only dependent on independent modules
from .attention import Attention
from .bert_mlm_head import BertMLMHead
from .bert_nsp_head import BertNSPHead
from .bert_pooler import BertPooler
from .embed import Embed
from .grouped_query_attention import GroupedQueryAttention
from .mlps.gated_mlp import GatedMLP
from .mlps.mlp import MLP

# Interdependent modules
from .bert_block import BertBlock
from .bert_embed import BertEmbed
from .mlps.moe import MoE
from .transformer_block import TransformerBlock
from .t5_attention import T5Attention
from .t5_block import T5Block



---
File: /transformer_lens/components/abstract_attention.py
---

import math
from abc import ABC
from typing import Dict, Optional, Tuple, Union

import einops
import torch
import torch.nn as nn
import torch.nn.functional as F
from better_abc import abstract_attribute
from jaxtyping import Float, Int
from transformers.utils.import_utils import is_bitsandbytes_available

from transformer_lens.components.rms_norm import RMSNorm
from transformer_lens.FactoredMatrix import FactoredMatrix
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCacheEntry
from transformer_lens.utilities.attention import complex_attn_linear, simple_attn_linear
from transformer_lens.utils import get_offset_position_ids

if is_bitsandbytes_available():
    import bitsandbytes as bnb
    from bitsandbytes.nn.modules import Params4bit


class AbstractAttention(ABC, nn.Module):
    alibi: Union[torch.Tensor, None]
    q_norm: Optional[RMSNorm]
    k_norm: Optional[RMSNorm]
    mask: torch.Tensor
    IGNORE: torch.Tensor
    rotary_sin: torch.Tensor
    rotary_cos: torch.Tensor

    def __init__(
        self,
        cfg: Union[Dict, HookedTransformerConfig],
        attn_type: str = "global",
        layer_id: Optional[int] = None,
    ):
        """Abstract Base Class of Attention Blocks, featuring common functionality of both Attention and GroupedQueryAttention blocks.

        Query and Output projections are defined in this class as they are the same for regular and grouped query attention.
        Attributes related to Key and Value projections are abstract as their implementations may differ. For example, in GroupedQueryAttention there are less query and key heads than value heads.
        To enforce implementation of W_K, W_V, b_K, and b_V by child classes, the better_abc.abstract_attribute class is used. See here for details: https://stackoverflow.com/questions/23831510/abstract-attribute-not-property.

        Args:
            cfg (Union[Dict, HookedTransformerConfig]): Config
            attn_type (str, optional): "global" or "local", used by GPT-Neo. Local attention means the model can only attend back cfg.window_size tokens (here, 256). Not used by any other model at the moment. Defaults to "global".
            layer_id (int, optional): The index of the current layer. Used by the Mistral models (labelled here as stanford-gpt2) to scale down attention scores pre softmax for numerical stability reasons by 1/(layer_id+1). Defaults to None.
        """
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)

        if self.cfg.load_in_4bit:
            nq = int((self.cfg.d_model * self.cfg.d_head * self.cfg.n_heads) / 2)
            self.W_Q = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
            self.W_O = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
        else:
            self.W_Q = nn.Parameter(
                torch.empty(
                    self.cfg.n_heads,
                    self.cfg.d_model,
                    self.cfg.d_head,
                    dtype=self.cfg.dtype,
                )
            )
            self.W_O = nn.Parameter(
                torch.empty(
                    self.cfg.n_heads,
                    self.cfg.d_head,
                    self.cfg.d_model,
                    dtype=self.cfg.dtype,
                )
            )
        self.W_K = abstract_attribute()
        self.W_V = abstract_attribute()

        self.b_Q = nn.Parameter(
            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)
        )
        self.b_K: nn.Parameter = abstract_attribute()
        self.b_V: nn.Parameter = abstract_attribute()
        self.b_O = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))

        if self.cfg.use_qk_norm:
            self.q_norm = RMSNorm(self.cfg, length=self.cfg.d_head)
            self.k_norm = RMSNorm(self.cfg, length=self.cfg.d_head)
        else:
            self.q_norm = None
            self.k_norm = None

        self.attn_type = attn_type
        # Create a max_ctx x max_ctx mask, with True iff that query position
        # can attend to that key position (query is first axis, key is second axis)
        causal_mask = torch.tril(torch.ones((self.cfg.n_ctx, self.cfg.n_ctx)).bool())
        if self.attn_type == "global":
            # For global attention, this is a lower triangular matrix - key <= query
            self.register_buffer("mask", causal_mask)
        elif self.attn_type == "local":
            # For local, this is banded, query - window_size < key <= query
            if not isinstance(self.cfg.window_size, int):
                raise ValueError("Window size must be an integer for local attention")
            self.register_buffer("mask", torch.triu(causal_mask, 1 - self.cfg.window_size))
        else:
            raise ValueError(f"Invalid attention type: {self.attn_type}")

        self.register_buffer("IGNORE", torch.tensor(-torch.inf))

        self.layer_id = layer_id

        # attn_scale is a constant that we divide the attention scores by pre-softmax. I'm not entirely sure why it matters, but it's probably a mix of softmax not being scale invariant and numerical stability?
        if self.cfg.use_attn_scale:
            self.attn_scale = self.cfg.attn_scale  # Defaults to sqrt(d_head)
        else:
            self.attn_scale = 1.0
        if self.cfg.scale_attn_by_inverse_layer_idx:
            if self.layer_id is None:  # keep mypy happy
                raise ValueError("Layer ID must be provided to scale attention scores")
            self.attn_scale *= self.layer_id + 1

        self.hook_k = HookPoint()  # [batch, pos, head_index, d_head]
        self.hook_q = HookPoint()  # [batch, pos, head_index, d_head]
        self.hook_v = HookPoint()  # [batch, pos, head_index, d_head]
        self.hook_z = HookPoint()  # [batch, pos, head_index, d_head]
        self.hook_attn_scores = HookPoint()  # [batch, head_index, query_pos, key_pos]
        self.hook_pattern = HookPoint()  # [batch, head_index, query_pos, key_pos]
        self.hook_result = HookPoint()  # [batch, pos, head_index, d_model]

        # See HookedTransformerConfig for more details.
        if self.cfg.positional_embedding_type == "shortformer":
            # This tracks the input to the keys and queries, which is resid_pre + pos_embeds
            self.hook_attn_input = HookPoint()  # [batch, pos, d_model]
        elif self.cfg.positional_embedding_type == "rotary":
            # Applies a rotation to each two-element chunk of keys and queries pre dot producting to bake in relative position. See HookedTransformerConfig for details
            self.hook_rot_k = HookPoint()
            self.hook_rot_q = HookPoint()
            if self.cfg.rotary_dim is None:  # keep mypy happy
                raise ValueError("Rotary dim must be provided for rotary positional embeddings")
            sin, cos = self.calculate_sin_cos_rotary(
                self.cfg.rotary_dim,
                self.cfg.n_ctx,
                base=self.cfg.rotary_base,
                dtype=self.cfg.dtype,
            )
            self.register_buffer("rotary_sin", sin)
            self.register_buffer("rotary_cos", cos)
        elif self.cfg.positional_embedding_type == "alibi":
            # ALiBi bias wil be constructed on the first forward pass.
            # Note: While computationally efficient, initializing an bias with max n_ctx (16, 1024, 1024) of float32 will occupy ~256MiB of contiguous GPU memory, which may not be optimal for memory usage.
            self.alibi = None

        elif self.cfg.positional_embedding_type == "relative_positional_bias":
            # will be overwritten by the child T5Attention class
            self.has_relative_attention_bias = False

    @property
    def OV(self) -> FactoredMatrix:
        """
        OV-Circuit, as defined in A Mathematical Framework. Because there's no non-linearity between the value vector and the output of the layer, the output is purely determined by the matrix W_OV = W_V @ W_O, and not W_V or W_O individually. (Mathematically, for a single head, output == pattern @ residual @ W_V @ W_O, see the glossary for more)

        Done in the order W_V, W_O because the paper uses left-multiplying weight matrices, and TransformerLens uses right-multiplying, sorry!

        Returns a FactoredMatrix, with left matrix W_V [head_index, d_model, d_head] and right matrix W_O [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model]. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the OV circuit of a head k, attn.OV[k] works.
        """
        return FactoredMatrix(self.W_V, self.W_O)

    @property
    def QK(self) -> FactoredMatrix:
        """
        QK-Circuit, as defined in A Mathematical Framework. Because there's no non-linearity in the key-query dot product, the output is purely determined by the matrix W_QK = W_Q.T @ W_K, and not W_Q or W_K individually. (Mathematically, for a single head, pattern = destination_residual.T @ W_Q.T @ W_K @ source-residual, see the glossary for more).

        Done in the order Q on the left, K on the right, because the pattern has dimensions [destination_pos, source_pos]

        Returns a FactoredMatrix, with left matrix W_Q [head_index, d_model, d_head] and right matrix W_K.T [head_index, d_head, d_model] - this is a low rank factorisation of the underlying [head_index, d_model, d_model] matrix. FactoredMatrix has helper functions to deal with these large matrices efficiently. To get the QK circuit of a head k, attn.QK[k] works.
        """
        W_K_transpose = einops.rearrange(
            self.W_K, "head_index d_model d_head -> head_index d_head d_model"
        )
        return FactoredMatrix(self.W_Q, W_K_transpose)

    def forward(
        self,
        query_input: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos head_index d_model"],
        ],
        key_input: Union[
            Float[torch.Tensor, "batch kv_pos d_model"],
            Float[torch.Tensor, "batch kv_pos head_index d_model"],
            Float[torch.Tensor, "batch kv_pos kv_head_index d_model"],
        ],
        value_input: Union[
            Float[torch.Tensor, "batch kv_pos d_model"],
            Float[torch.Tensor, "batch kv_pos head_index d_model"],
            Float[torch.Tensor, "batch kv_pos kv_head_index d_model"],
        ],
        past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] = None,
        additive_attention_mask: Optional[Float[torch.Tensor, "batch 1 1 kv_pos"]] = None,
        attention_mask: Optional[Int[torch.Tensor, "batch offset_pos"]] = None,
        position_bias: Optional[Float[torch.Tensor, "1 head_index pos kv_pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        """
        shortformer_pos_embed is only used if self.cfg.positional_embedding_type == "shortformer", else defaults to None and is irrelevant. See HookedTransformerConfig for more details
        past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None
        additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.
        attention_mask is the attention mask for padded tokens. Defaults to None.
        """

        q, k, v = self.calculate_qkv_matrices(query_input, key_input, value_input)

        if past_kv_cache_entry is not None:
            # Appends the new keys and values to the cached values, and automatically updates the cache
            kv_cache_pos_offset = past_kv_cache_entry.past_keys.size(1)
            k, v = past_kv_cache_entry.append(k, v)
        else:
            # Not using a cache
            kv_cache_pos_offset = 0

        if self.cfg.positional_embedding_type == "rotary":
            q = self.hook_rot_q(self.apply_rotary(q, kv_cache_pos_offset, attention_mask))
            k = self.hook_rot_k(
                self.apply_rotary(k, 0, attention_mask)
            )  # keys are cached so no offset

        if self.cfg.dtype not in [torch.float32, torch.float64]:
            # If using 16 bits, increase the precision to avoid numerical instabilities
            q = q.to(torch.float32)
            k = k.to(torch.float32)

        attn_scores = self.calculate_attention_scores(
            q, k
        )  # [batch, head_index, query_pos, key_pos]

        if self.cfg.positional_embedding_type == "alibi":
            query_ctx = attn_scores.size(-2)
            # The key context length is the number of positions in the past - this includes all positions in the cache
            key_ctx = attn_scores.size(-1)

            # only recompute when necessary to increase efficiency.
            if self.alibi is None or key_ctx > self.alibi.size(-1):
                self.alibi = AbstractAttention.create_alibi_bias(
                    self.cfg.n_heads, key_ctx, self.cfg.device
                )

            # Take the last query_ctx positions so it also works with past_kv_cache
            attn_scores += self.alibi[
                :, -query_ctx:, :key_ctx
            ]  # [batch, head_index, query_pos, key_pos]
        elif self.cfg.positional_embedding_type == "relative_positional_bias":
            if position_bias is None:
                if self.has_relative_attention_bias:
                    raise ValueError("Positional bias is required for relative_positional_bias")
                else:
                    position_bias = torch.zeros(
                        1,
                        self.cfg.n_heads,
                        attn_scores.shape[2],
                        attn_scores.shape[3],
                        device=attn_scores.device,
                    )

            attn_scores += position_bias
        if self.cfg.attention_dir == "causal":
            # If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.
            attn_scores = self.apply_causal_mask(
                attn_scores, kv_cache_pos_offset, attention_mask
            )  # [batch, head_index, query_pos, key_pos]
        if additive_attention_mask is not None:
            attn_scores += additive_attention_mask

        attn_scores = self.hook_attn_scores(attn_scores)
        pattern = F.softmax(attn_scores, dim=-1)
        pattern = torch.where(torch.isnan(pattern), torch.zeros_like(pattern), pattern)
        pattern = self.hook_pattern(pattern)  # [batch, head_index, query_pos, key_pos]
        pattern = pattern.to(self.cfg.dtype)
        pattern = pattern.to(v.device)
        z = self.calculate_z_scores(v, pattern)  # [batch, pos, head_index, d_head]
        if not self.cfg.use_attn_result:
            if self.cfg.load_in_4bit:
                # call bitsandbytes method to dequantize and multiply
                out = (
                    bnb.matmul_4bit(
                        z.reshape(z.shape[0], z.shape[1], self.cfg.d_head * self.cfg.n_heads),
                        self.W_O.t(),
                        # bias=self.W_O.t(),
                        bias=None,
                        quant_state=self.W_O.quant_state,
                    )
                    + self.b_O
                )
            else:
                w = einops.rearrange(
                    self.W_O, "head_index d_head d_model -> d_model (head_index d_head)"
                )

                if self.b_O.device != w.device:
                    w = w.to(self.b_O.device)
                if self.b_O.device != z.device:
                    z = z.to(self.b_O.device)

                out = F.linear(
                    z.reshape(z.shape[0], z.shape[1], self.cfg.d_head * self.cfg.n_heads),
                    w,
                    self.b_O,
                )
        else:
            # Explicitly calculate the attention result so it can be accessed by a hook
            # This is off by default because it can easily eat through your GPU memory.
            if self.cfg.load_in_4bit:
                result = self.hook_result(
                    bnb.matmul_4bit(
                        z.reshape(z.shape[0], z.shape[1], self.cfg.d_head * self.cfg.n_heads),
                        self.W_O.t(),
                        bias=None,
                        quant_state=self.W_O.quant_state,
                    )
                )
            else:
                # Add singleton dimensions to make shapes compatible for broadcasting:
                w = einops.rearrange(
                    self.W_O,
                    "head_index d_head d_model -> 1 1 head_index d_head d_model",
                )
                z = einops.rearrange(
                    z, "batch pos head_index d_head -> batch pos head_index d_head 1"
                )

                # Multiply the z tensor by the W_O tensor, summing over the d_head dimension
                unhooked_result = (z * w).sum(-2)

                result = self.hook_result(unhooked_result)  # [batch, pos, head_index, d_model]
            out = (
                einops.reduce(result, "batch position index model->batch position model", "sum")
                + self.b_O
            )  # [batch, pos, d_model]
        return out

    def _apply_qk_norm(
        self, x: Float[torch.Tensor, "batch pos head_index d_head"], norm_module: RMSNorm
    ) -> Float[torch.Tensor, "batch pos head_index d_head"]:
        """Apply QK normalization with proper reshaping.

        Args:
            x: Input tensor with shape [batch, pos, head_index, d_head]
            norm_module: RMSNorm module to apply

        Returns:
            Normalized tensor with same shape as input
        """
        # Reshape from [batch, pos, head_index, d_head] to [batch * pos * head_index, d_head]
        d_head = x.shape[-1]
        x_normed = norm_module(x.reshape(-1, d_head))
        return x_normed.reshape(x.shape)

    def calculate_qkv_matrices(
        self,
        query_input: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos head_index d_model"],
        ],
        key_input: Union[
            Float[torch.Tensor, "batch kv_pos d_model"],
            Float[torch.Tensor, "batch kv_pos head_index d_model"],
        ],
        value_input: Union[
            Float[torch.Tensor, "batch kv_pos d_model"],
            Float[torch.Tensor, "batch kv_pos head_index d_model"],
        ],
    ) -> Tuple[
        Float[torch.Tensor, "batch pos head_index d_head"],
        Float[torch.Tensor, "batch kv_pos head_index d_head"],
        Float[torch.Tensor, "batch kv_pos head_index d_head"],
    ]:
        attn_fn = (
            complex_attn_linear
            if self.cfg.use_split_qkv_input or self.cfg.use_attn_in
            else simple_attn_linear
        )
        if self.cfg.load_in_4bit:
            q = self.hook_q(
                # call bitsandbytes method to dequantize and multiply
                bnb.matmul_4bit(
                    query_input,
                    self.W_Q.t(),
                    bias=None,
                    quant_state=self.W_Q.quant_state,
                ).reshape(
                    query_input.shape[0],
                    query_input.shape[1],
                    self.cfg.n_heads,
                    self.cfg.d_head,
                )
                + self.b_Q
            )
        else:
            q = self.hook_q(attn_fn(query_input, self.W_Q, self.b_Q))
        if self.cfg.load_in_4bit:
            if not isinstance(self.W_K, Params4bit):
                raise ValueError("W_K must be a Params4bit object if load_in_4bit is True")
            k = self.hook_k(
                # call bitsandbytes method to dequantize and multiply
                bnb.matmul_4bit(
                    key_input, self.W_K.t(), bias=None, quant_state=self.W_K.quant_state
                ).reshape(
                    key_input.shape[0],
                    key_input.shape[1],
                    self.cfg.n_heads,
                    self.cfg.d_head,
                )
                + self.b_K
            )
        else:
            k = self.hook_k(attn_fn(key_input, self.W_K, self.b_K))

        if self.cfg.load_in_4bit:
            if not isinstance(self.W_V, Params4bit):
                raise ValueError("W_V must be a Params4bit object if load_in_4bit is True")
            v = self.hook_v(
                # call bitsandbytes method to dequantize and multiply
                bnb.matmul_4bit(
                    value_input,
                    self.W_V.t(),
                    bias=None,
                    quant_state=self.W_V.quant_state,
                ).reshape(
                    value_input.shape[0],
                    value_input.shape[1],
                    self.cfg.n_heads,
                    self.cfg.d_head,
                )
                + self.b_V
            )
        else:
            v = self.hook_v(attn_fn(value_input, self.W_V, self.b_V))

        if self.cfg.use_qk_norm:
            assert self.q_norm is not None
            assert self.k_norm is not None
            q = self._apply_qk_norm(q, self.q_norm)
            k = self._apply_qk_norm(k, self.k_norm)

        return q, k, v

    def calculate_attention_scores(
        self,
        q: Float[torch.Tensor, "batch query_pos head_index d_head"],
        k: Float[torch.Tensor, "batch key_pos head_index d_head"],
    ) -> Float[torch.Tensor, "batch head_index query_pos key_pos"]:
        q_ = einops.rearrange(
            q, "batch query_pos head_index d_head -> batch head_index query_pos d_head"
        )
        k_ = einops.rearrange(
            k, "batch key_pos head_index d_head -> batch head_index d_head key_pos"
        )
        attn_scores = q_ @ k_ / self.attn_scale
        if self.cfg.attn_scores_soft_cap > 0:
            attn_scores = self.cfg.attn_scores_soft_cap * F.tanh(
                attn_scores / self.cfg.attn_scores_soft_cap
            )
        return attn_scores

    def calculate_z_scores(
        self,
        v: Float[torch.Tensor, "batch key_pos head_index d_head"],
        pattern: Float[torch.Tensor, "batch head_index query_pos key_pos"],
    ) -> Float[torch.Tensor, "batch query_pos head_index d_head"]:
        v_ = einops.rearrange(
            v, "batch key_pos head_index d_head -> batch head_index key_pos d_head"
        )
        pattern_ = einops.rearrange(
            pattern,
            "batch head_index query_pos key_pos -> batch head_index query_pos key_pos",
        )
        z = self.hook_z(
            einops.rearrange(
                pattern_ @ v_,
                "batch head_index query_pos d_head -> batch query_pos head_index d_head",
            )
        )
        return z

    def apply_causal_mask(
        self,
        attn_scores: Float[torch.Tensor, "batch head_index pos pos_plus_past_kv_pos_offset"],
        past_kv_pos_offset: int = 0,
        attention_mask: Optional[Int[torch.Tensor, "batch offset_pos"]] = None,
    ):
        # The query context length is the number of positions we take queries from - if not using a past_kv_cache this is just the context length (for the current prompt), but if we're caching it can be different.
        query_ctx_length = attn_scores.size(-2)
        # The key context length is the number of positions in the past - this includes all positions in the cache
        # If not caching, query_ctx_length == key_ctx_length
        key_ctx_length = attn_scores.size(-1)

        if query_ctx_length + past_kv_pos_offset != key_ctx_length:
            raise ValueError(
                f"query_ctx_length {query_ctx_length} + past_kv_pos_offset {past_kv_pos_offset} != key_ctx_length {key_ctx_length} - you likely have a bug."
            )

        # Index back to front to ensure local attention works
        final_mask = self.mask[None, None, -query_ctx_length:, -key_ctx_length:]  # [1, 1, pos, pos]
        if attention_mask is not None:
            # Apply a causal mask to the attention scores considering the padding

            # Add singleton dimensions to the attention mask to match the shape of the final mask
            attention_mask = einops.rearrange(
                attention_mask, "batch offset_pos -> batch 1 1 offset_pos"
            )

            final_mask = final_mask.to(attention_mask.device)

            # Element-wise multiplication of the final mask and the attention mask and cast to boolean
            final_mask = (final_mask * attention_mask).bool()  # [batch, head, pos, offset_pos]

        attn_scores = attn_scores.to(final_mask.device)
        return torch.where(final_mask, attn_scores, self.IGNORE)

    def calculate_sin_cos_rotary(
        self,
        rotary_dim: int,
        n_ctx: int,
        base: int = 10000,
        dtype: torch.dtype = torch.float32,
    ) -> Tuple[Float[torch.Tensor, "n_ctx rotary_dim"], Float[torch.Tensor, "n_ctx rotary_dim"]]:
        """
        Calculate the sine and cosine waves to use in a rotary embedding. See https://blog.eleuther.ai/rotary-embeddings/ for details

        Note: For some inexplicable reason, in GPT-J each ADJACENT pair of elements in k and q are rotated, in GPT-NeoX the pair of elements at k and k+n//2 are rotated (ie folding the full length in half, and then looking at pairs accordingly). I have absolutely no clue why, it should be completely equivalent.
        To resolve this, I've coded it to default to the GPT-J mode, but to explicitly check whether it's GPT-NeoX and then do the GPT-NeoX thing if it is.
        """
        high_precision = torch.float32 if dtype != torch.float64 else torch.float64
        pos = torch.arange(n_ctx, dtype=high_precision)
        dim = torch.arange(rotary_dim // 2, dtype=high_precision)

        # Llama-3.1 uses NTK-by-Parts Rotary Embedding introduced in Section 3.2 in https://arxiv.org/pdf/2309.00071
        # Implementation copied from https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/modeling_rope_utils.py#L310
        if self.cfg.use_NTK_by_parts_rope:
            inv_freq = 1.0 / (
                base ** (torch.arange(0, rotary_dim, 2, dtype=torch.int64).float() / rotary_dim)
            )
            factor = self.cfg.NTK_by_parts_factor
            low_freq_factor = self.cfg.NTK_by_parts_low_freq_factor
            high_freq_factor = self.cfg.NTK_by_parts_high_freq_factor
            old_context_len = self.cfg.NTK_original_ctx_len

            low_freq_wavelen = old_context_len / low_freq_factor
            high_freq_wavelen = old_context_len / high_freq_factor

            wavelen = 2 * math.pi / inv_freq
            inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)
            smooth_factor = (old_context_len / wavelen - low_freq_factor) / (
                high_freq_factor - low_freq_factor
            )
            smoothed_inv_freq = (
                1 - smooth_factor
            ) * inv_freq_llama / factor + smooth_factor * inv_freq_llama
            is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)
            inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)
            freq = 1 / inv_freq_llama
        else:
            freq = base ** (dim / (rotary_dim / 2))
        if self.cfg.rotary_adjacent_pairs:
            freq = einops.repeat(freq, "d -> (d 2)")
        else:
            freq = einops.repeat(freq, "d -> (2 d)")
        # Create a n_ctx x rotary_dim tensor, where each column is an arithmetic sequence of angles in that frequency
        angles = pos[:, None] / freq[None, :]
        return torch.sin(angles).to(dtype), torch.cos(angles).to(dtype)

    def rotate_every_two(
        self, x: Float[torch.Tensor, "... rotary_dim"]
    ) -> Float[torch.Tensor, "... rotary_dim"]:
        """
        Rotary helper function, splits x into blocks of size 2 along the final axis and maps [x0, x1] to [-x1, x0]

        The final axis of x must have even length.

        GPT-NeoX and GPT-J do rotary subtly differently, see calculate_sin_cos_rotary for details.
        """
        rot_x = x.clone()
        if self.cfg.rotary_adjacent_pairs:
            rot_x[..., ::2] = -x[..., 1::2]
            rot_x[..., 1::2] = x[..., ::2]
        else:
            n = x.size(-1) // 2
            rot_x[..., :n] = -x[..., n:]
            rot_x[..., n:] = x[..., :n]

        return rot_x

    def apply_rotary(
        self,
        x: Float[torch.Tensor, "batch pos head_index d_head"],
        past_kv_pos_offset: int = 0,
        attention_mask: Optional[Int[torch.Tensor, "batch offset_pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos head_index d_head"]:
        # Only apply rotary to first rotary_dim dimensions (eg, if rotary_dim=64 and d_head=256, only apply to first 1/4 of dimensions)

        if x.device != self.rotary_sin.device:
            x = x.to(self.rotary_sin.device)

        x_pos = x.size(1)
        x_rot = x[..., : self.cfg.rotary_dim]
        x_pass = x[..., self.cfg.rotary_dim :]
        x_flip = self.rotate_every_two(x_rot)

        if attention_mask is None:
            rotary_cos = self.rotary_cos[
                None, past_kv_pos_offset : past_kv_pos_offset + x_pos, None, :
            ]
            rotary_sin = self.rotary_sin[
                None, past_kv_pos_offset : past_kv_pos_offset + x_pos, None, :
            ]
            x_rotated = x_rot * rotary_cos + x_flip * rotary_sin
        else:
            offset_position_ids = get_offset_position_ids(past_kv_pos_offset, attention_mask)
            offset_position_ids = offset_position_ids.to(self.rotary_cos.device)
            mask_rotary_cos = self.rotary_cos[offset_position_ids, None, :]
            mask_rotary_sin = self.rotary_sin[offset_position_ids, None, :]
            x_rotated = x_rot * mask_rotary_cos + x_flip * mask_rotary_sin

        return torch.cat([x_rotated, x_pass], dim=-1)

    @staticmethod
    def create_alibi_slope(
        n_ctx: int, device: Optional[Union[str, torch.device]] = None
    ) -> Float[torch.Tensor, "query key"]:
        """Create an ALiBi Slope Matrix.

        Create the slope matrix used in ALiBi, before it is multiplied by the head-specific scalar.

        See :meth:`create_alibi_bias` for the full ALiBi bias calculation.

        Examples:

        >>> AbstractAttention.create_alibi_slope(3)
        tensor([[ 0.,  0.,  0.],
                [-1.,  0.,  0.],
                [-2., -1.,  0.]])

        >>> AbstractAttention.create_alibi_slope(4)
        tensor([[ 0.,  0.,  0.,  0.],
                [-1.,  0.,  0.,  0.],
                [-2., -1.,  0.,  0.],
                [-3., -2., -1.,  0.]])

        Args:
            n_ctx: The maximum number of tokens in a prompt.

        Returns:
            A tensor of shape (n_ctx, n_ctx), where the upper triangle is zero and the lower
            triangle is decreasing by a constant slope of 1 (towards the bottom left corner).
        """
        # set rows as [[0,1,2...]]
        rows = torch.arange(n_ctx, device=device).unsqueeze(0)

        # Set cols as [[0],[1],[2]...]
        cols = torch.arange(n_ctx, device=device).unsqueeze(1)

        # Use broadcasting to create the desired lower triangular part of the matrix
        slope_matrix = rows - cols

        # Use the clamp method to set all positive values (upper right triangle) to
        return slope_matrix.clamp(max=0).to(torch.float32)

    @staticmethod
    def create_alibi_multipliers(
        n_heads: int, device: Optional[Union[str, torch.device]] = None
    ) -> Float[torch.Tensor, "head_idx"]:
        """Create the ALiBi Scalar Multipliers for each Head.

        For n heads, the set of multipliers (m) is the geometric sequence that starts at 2^(-8/n), and
        uses that same value as its ratio. For example, with 8 heads the values would be [1/(2^1),
        1/(2^2), ... , 1/(2^8)]. With 16 heads the values would be [1/(2^0.5), 1/(2^1), ... , 1/(2^8)].

        See :meth:`create_alibi_bias` for the full ALiBi bias calculation.

        Examples:

        >>> AbstractAttention.create_alibi_multipliers(8)
        tensor([0.5000, 0.2500, 0.1250, 0.0625, 0.0312, 0.0156, 0.0078, 0.0039])

        >>> AbstractAttention.create_alibi_multipliers(16)
        tensor([0.7071, 0.5000, 0.3536, 0.2500, 0.1768, 0.1250, 0.0884, 0.0625, 0.0442, 0.0312,
                0.0221, 0.0156, 0.0110, 0.0078, 0.0055, 0.0039])

        Args:
            n_heads: The number of heads in a layer.
            device: The device to create the tensor on.

        Returns:
            A tensor of shape (n_heads,) containing the scalar multiplier for each head.
        """
        # Calculate the starting value
        start = 2 ** (-8 / n_heads)

        # Generate the indices [0, 1, ..., n_heads-1]
        indices = torch.arange(n_heads, device=device)

        # Compute the multipliers, with the starting value being the same as the ratio
        multipliers = start * (start**indices)

        return multipliers

    @staticmethod
    def create_alibi_bias(
        n_heads: int, n_ctx: int, device: Optional[Union[torch.device, str]] = None
    ) -> Float[torch.Tensor, "head_idx query key"]:
        """Create the ALiBi Bias for all Heads.

        Calculate the ALiBi bias (https://arxiv.org/pdf/2108.12409.pdf) for all heads in a layer.

        The broad idea behind ALiBi is to remove the positional encoding from the original transformer
        model, and instead apply a bias to each attention score. This bias is proportional to the
        distance between the query and key (i.e. it encourage paying less attention to more distant
        tokens), and is added to the attention scores before the softmax. It is used in models such as
        Bloom.

        Examples:

        >>> AbstractAttention.create_alibi_bias(2, 4, torch.device('cpu'))
        tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],
            [-0.0625,  0.0000,  0.0000,  0.0000],
            [-0.1250, -0.0625,  0.0000,  0.0000],
            [-0.1875, -0.1250, -0.0625,  0.0000]],
            [[ 0.0000,  0.0000,  0.0000,  0.0000],
            [-0.0039,  0.0000,  0.0000,  0.0000],
            [-0.0078, -0.0039,  0.0000,  0.0000],
            [-0.0117, -0.0078, -0.0039,  0.0000]]])

        Args:
            n_heads: The number of heads in a layer.
            n_ctx: The maximum number of tokens in a prompt.
            device: The device to create the tensor on.

        Returns:
            The ALiBi bias that should be added to the attention scores before the softmax.
        """
        # Create the slope matrix
        slope: Float[torch.Tensor, "query key"] = AbstractAttention.create_alibi_slope(
            n_ctx, device
        )

        # Create the scalar multiplier for each head.
        multipliers: Float[torch.Tensor, "head_idx"] = AbstractAttention.create_alibi_multipliers(
            n_heads, device
        )

        # Add singleton dimensions to make shapes compatible for broadcasting:
        slope = einops.rearrange(slope, "query key -> 1 query key")
        multipliers = einops.rearrange(multipliers, "head_idx -> head_idx 1 1")

        # Element-wise multiplication of the slope and multipliers
        alibi_bias = multipliers * slope

        return alibi_bias



---
File: /transformer_lens/components/attention.py
---

"""Hooked Transformer Attention Component.

This module contains all the component :class:`Attention`.
"""
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
from transformers.utils import is_bitsandbytes_available

from transformer_lens.components import AbstractAttention
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig

if is_bitsandbytes_available():
    from bitsandbytes.nn.modules import Params4bit


# Attention
class Attention(AbstractAttention):
    def __init__(
        self,
        cfg: Union[Dict, HookedTransformerConfig],
        attn_type: str = "global",
        layer_id: Optional[int] = None,
    ):
        """Attention Block - params have shape [head_index, d_model, d_head] (or [head_index, d_head, d_model] for W_O) and multiply on the right. attn_scores refers to query key dot product immediately before attention softmax

        Convention: All attention pattern-style matrices have shape [batch, head_index, query_pos, key_pos]

        Args:
            cfg (Union[Dict, HookedTransformerConfig]): Config
            attn_type (str, optional): "global" or "local", used by GPT-Neo. Local attention means the model can only attend back cfg.window_size tokens (here, 256). Not used by any other model at the moment. Defaults to "global".
            layer_id (int, optional): The index of the current layer. Used by the Mistal models (labelled here as stanford-gpt2) to scale down attention scores pre softmax for numerical stability reasons by 1/(layer_id+1). Defaults to None.
        """
        super().__init__(cfg, attn_type, layer_id)
        self.cfg = HookedTransformerConfig.unwrap(cfg)

        if self.cfg.load_in_4bit:
            # 4-bit quantization convention
            nq = int((self.cfg.d_model * self.cfg.d_model) / 2)
            self.W_K = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
            self.W_V = Params4bit(torch.empty(nq, 1, dtype=torch.uint8), requires_grad=False)
        else:
            self.W_K = nn.Parameter(
                torch.empty(
                    self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head, dtype=self.cfg.dtype
                )
            )
            self.W_V = nn.Parameter(
                torch.empty(
                    self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head, dtype=self.cfg.dtype
                )
            )
        self.b_K = nn.Parameter(
            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)
        )
        self.b_V = nn.Parameter(
            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)
        )



---
File: /transformer_lens/components/bert_block.py
---

"""Hooked Transformer Bert Block Component.

This module contains all the component :class:`BertBlock`.
"""
from typing import Optional

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components import Attention, LayerNorm
from transformer_lens.factories.mlp_factory import MLPFactory
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utils import repeat_along_head_dimension


class BertBlock(nn.Module):
    """
    BERT Block. Similar to the TransformerBlock, except that the LayerNorms are applied after the attention and MLP, rather than before.
    """

    def __init__(self, cfg: HookedTransformerConfig):
        super().__init__()
        self.cfg = cfg

        self.attn = Attention(cfg)
        self.ln1 = LayerNorm(cfg)
        self.mlp = MLPFactory.create_mlp(self.cfg)
        self.ln2 = LayerNorm(cfg)

        self.hook_q_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_k_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_v_input = HookPoint()  # [batch, pos, n_heads, d_model]

        self.hook_attn_out = HookPoint()  # [batch, pos, d_model]
        self.hook_mlp_in = HookPoint()  # [batch, pos, d_model]
        self.hook_mlp_out = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_pre = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_mid = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_post = HookPoint()  # [batch, pos, d_model]
        self.hook_normalized_resid_post = HookPoint()  # [batch, pos, d_model]

    def forward(
        self,
        resid_pre: Float[torch.Tensor, "batch pos d_model"],
        additive_attention_mask: Optional[Float[torch.Tensor, "batch 1 1 pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        resid_pre = self.hook_resid_pre(resid_pre)

        query_input = resid_pre
        key_input = resid_pre
        value_input = resid_pre

        if self.cfg.use_split_qkv_input:
            n_heads = self.cfg.n_heads
            query_input = self.hook_q_input(repeat_along_head_dimension(query_input, n_heads))
            key_input = self.hook_k_input(repeat_along_head_dimension(key_input, n_heads))
            value_input = self.hook_v_input(repeat_along_head_dimension(value_input, n_heads))

        attn_out = self.hook_attn_out(
            self.attn(
                query_input,
                key_input,
                value_input,
                additive_attention_mask=additive_attention_mask,
            )
        )
        resid_mid = self.hook_resid_mid(resid_pre + attn_out)

        mlp_in = resid_mid if not self.cfg.use_hook_mlp_in else self.hook_mlp_in(resid_mid.clone())
        normalized_resid_mid = self.ln1(mlp_in)
        mlp_out = self.hook_mlp_out(self.mlp(normalized_resid_mid))
        resid_post = self.hook_resid_post(normalized_resid_mid + mlp_out)
        normalized_resid_post = self.hook_normalized_resid_post(self.ln2(resid_post))

        return normalized_resid_post



---
File: /transformer_lens/components/bert_embed.py
---

"""Hooked Transformer Bert Embed Component.

This module contains all the component :class:`BertEmbed`.
"""
from typing import Dict, Optional, Union

import einops
import torch
import torch.nn as nn
from jaxtyping import Float, Int

from transformer_lens.components import Embed, LayerNorm, PosEmbed, TokenTypeEmbed
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class BertEmbed(nn.Module):
    """
    Custom embedding layer for a BERT-like model. This module computes the sum of the token, positional and token-type embeddings and takes the layer norm of the result.
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.embed = Embed(self.cfg)
        self.pos_embed = PosEmbed(self.cfg)
        self.token_type_embed = TokenTypeEmbed(self.cfg)
        self.ln = LayerNorm(self.cfg)

        self.hook_embed = HookPoint()
        self.hook_pos_embed = HookPoint()
        self.hook_token_type_embed = HookPoint()

    def forward(
        self,
        input_ids: Int[torch.Tensor, "batch pos"],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        base_index_id = torch.arange(input_ids.shape[1], device=input_ids.device)
        index_ids = einops.repeat(base_index_id, "pos -> batch pos", batch=input_ids.shape[0])
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        word_embeddings_out = self.hook_embed(self.embed(input_ids))
        position_embeddings_out = self.hook_pos_embed(self.pos_embed(index_ids))
        token_type_embeddings_out = self.hook_token_type_embed(
            self.token_type_embed(token_type_ids)
        )

        embeddings_out = word_embeddings_out + position_embeddings_out + token_type_embeddings_out
        layer_norm_out = self.ln(embeddings_out)
        return layer_norm_out



---
File: /transformer_lens/components/bert_mlm_head.py
---

"""Hooked Encoder Bert MLM Head Component.

This module contains all the component :class:`BertMLMHead`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components import LayerNorm
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class BertMLMHead(nn.Module):
    """
    Transforms BERT embeddings into logits. The purpose of this module is to predict masked tokens in a sentence.
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_model, dtype=self.cfg.dtype))
        self.b = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))
        self.act_fn = nn.GELU()
        self.ln = LayerNorm(self.cfg)

    def forward(
        self, resid: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        resid = torch.matmul(resid, self.W) + self.b
        resid = self.act_fn(resid)
        resid = self.ln(resid)
        return resid



---
File: /transformer_lens/components/bert_nsp_head.py
---

"""Hooked Encoder Bert NSP Head Component.

This module contains all the component :class:`BertNSPHead`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class BertNSPHead(nn.Module):
    """
    Transforms BERT embeddings into logits. The purpose of this module is to predict whether or not sentence B follows sentence A.
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W = nn.Parameter(torch.empty(self.cfg.d_model, 2, dtype=self.cfg.dtype))
        self.b = nn.Parameter(torch.zeros(2, dtype=self.cfg.dtype))
        self.hook_nsp_out = HookPoint()

    def forward(
        self, resid: Float[torch.Tensor, "batch d_model"]
    ) -> Float[torch.Tensor, "batch 2"]:
        nsp_logits = torch.matmul(resid, self.W) + self.b
        return self.hook_nsp_out(nsp_logits)



---
File: /transformer_lens/components/bert_pooler.py
---

"""Hooked Encoder Bert Pooler Component.

This module contains all the component :class:`BertPooler`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class BertPooler(nn.Module):
    """
    Transforms the [CLS] token representation into a fixed-size sequence embedding.
    The purpose of this module is to convert variable-length sequence inputs into a single vector representation suitable for downstream tasks.
    (e.g. Next Sentence Prediction)
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W = nn.Parameter(torch.empty(self.cfg.d_model, self.cfg.d_model, dtype=self.cfg.dtype))
        self.b = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))
        self.activation = nn.Tanh()
        self.hook_pooler_out = HookPoint()

    def forward(
        self, resid: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch d_model"]:
        first_token_tensor = resid[:, 0]
        pooled_output = torch.matmul(first_token_tensor, self.W) + self.b
        pooled_output = self.hook_pooler_out(self.activation(pooled_output))
        return pooled_output



---
File: /transformer_lens/components/embed.py
---

"""Hooked Transformer Embed Component.

This module contains all the component :class:`Embed`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float, Int

from transformer_lens.components import LayerNorm
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


# Embed & Unembed
class Embed(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W_E: Float[torch.Tensor, "d_vocab d_model"] = nn.Parameter(
            torch.empty(self.cfg.d_vocab, self.cfg.d_model, dtype=self.cfg.dtype)
        )
        # Some models (e.g. Bloom) need post embedding layer norm
        if self.cfg.post_embedding_ln:
            self.ln = LayerNorm(self.cfg)

    def forward(
        self, tokens: Int[torch.Tensor, "batch pos"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]
        # B acts as a tensor of indices into the second dimension (so >=0 and <b)
        if self.cfg.post_embedding_ln:
            return self.ln(self.W_E[tokens, :])
        return self.W_E[tokens, :]



---
File: /transformer_lens/components/grouped_query_attention.py
---

from typing import Dict, Tuple, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components import AbstractAttention
from transformer_lens.components.rms_norm import RMSNorm
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.attention import complex_attn_linear, simple_attn_linear


class GroupedQueryAttention(AbstractAttention):
    def __init__(
        self,
        cfg: Union[Dict, HookedTransformerConfig],
        attn_type: str = "global",
        layer_id: Union[int, None] = None,
    ):
        """Grouped Query Attention Block - see https://arxiv.org/abs/2305.13245 for details.
        Similar to regular attention, W_Q, W_K, and W_V all have shape [head_index, d_model, d_head].
        However, under the hood the key and value weights _W_K and _W_V are stored with shape [n_key_value_heads, d_model, d_head] and are expanded when the corresponding properties' getter is called.
        Similarly, during a forward pass, initially K and V are kept in shapes [batch, pos, n_key_value_heads, d_head] and will only be expanded to shapes [batch, pos, n_heads, d_head]
        using torch.repeat_interleave when the attention pattern and z-scores are calculated.

        Args:
            cfg (Union[Dict, HookedTransformerConfig]): Config
            attn_type (str, optional): "global" or "local", used by GPT-Neo. Local attention means the model can only attend back cfg.window_size tokens (here, 256). Not used by any other model at the moment. Defaults to "global".
            layer_id (int, optional): The index of the current layer. Used by the Mistal models (labelled here as stanford-gpt2) to scale down attention scores pre softmax for numerical stability reasons by 1/(layer_id+1). Defaults to None.
        """
        cfg = HookedTransformerConfig.unwrap(cfg)
        assert cfg.n_key_value_heads is not None
        super().__init__(cfg, attn_type, layer_id)
        self.repeat_kv_heads = cfg.n_heads // cfg.n_key_value_heads
        self._W_K = nn.Parameter(
            torch.empty(
                cfg.n_key_value_heads,
                self.cfg.d_model,
                self.cfg.d_head,
                dtype=cfg.dtype,
            )
        )
        self._W_V = nn.Parameter(
            torch.empty(
                cfg.n_key_value_heads,
                self.cfg.d_model,
                self.cfg.d_head,
                dtype=cfg.dtype,
            )
        )
        self._b_K = nn.Parameter(
            torch.zeros(cfg.n_key_value_heads, self.cfg.d_head, dtype=cfg.dtype)
        )
        self._b_V = nn.Parameter(
            torch.zeros(cfg.n_key_value_heads, self.cfg.d_head, dtype=cfg.dtype)
        )

    @property
    def W_K(self):
        return torch.repeat_interleave(self._W_K, dim=0, repeats=self.repeat_kv_heads)

    @W_K.setter
    def W_K(self, value):
        self._W_K = value

    @property
    def W_V(self):
        return torch.repeat_interleave(self._W_V, dim=0, repeats=self.repeat_kv_heads)

    @W_V.setter
    def W_V(self, value):
        self._W_V = value

    @property
    def b_K(self):
        return torch.repeat_interleave(self._b_K, dim=0, repeats=self.repeat_kv_heads)

    @b_K.setter
    def b_K(self, value):
        self._b_K = value

    @property
    def b_V(self):
        return torch.repeat_interleave(self._b_V, dim=0, repeats=self.repeat_kv_heads)

    @b_V.setter
    def b_V(self, value):
        self._b_V = value

    def calculate_qkv_matrices(
        self,
        query_input: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos head_index d_model"],
        ],
        key_input: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos kv_head_index d_model"],
        ],
        value_input: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos kv_head_index d_model"],
        ],
    ) -> Tuple[
        Float[torch.Tensor, "batch pos head_index d_head"],
        Float[torch.Tensor, "batch pos kv_head_index d_head"],
        Float[torch.Tensor, "batch pos kv_head_index d_head"],
    ]:
        """Calculate the Q, K, and V matrices for grouped query attention.
        This function uses the unexpanded weights _W_K and _W_V to calculate K and V.

        Args:
        query_input (Union[Float[torch.Tensor, "batch pos d_model"], Float[torch.Tensor, "batch pos head_index d_model"]]): The input tensor for the query projection.
        key_input (Union[Float[torch.Tensor, "batch pos d_model"], Float[torch.Tensor, "batch pos kv_head_index d_model"]]): The input tensor for the key projection. Note that is has as many head dimensions as the GPA block has key-value heads.
        value_input (Union[Float[torch.Tensor, "batch pos d_model"], Float[torch.Tensor, "batch pos kv_head_index d_model"]]): The input tensor for the value projection. Note that is has as many head dimensions as the GPA block has key-value heads.

        Returns:
        Tuple[Float[torch.Tensor, "batch pos head_index d_head"], Float[torch.Tensor, "batch pos kv_head_index d_head"], Float[torch.Tensor, "batch pos kv_head_index d_head"]]:
        A tuple containing the Q, K, and V matrices with the specified shapes.
        """
        attn_fn = (
            complex_attn_linear
            if self.cfg.use_split_qkv_input or self.cfg.use_attn_in
            else simple_attn_linear
        )

        q = self.hook_q(
            attn_fn(query_input, self.W_Q, self.b_Q)
        )  # [batch, pos, head_index, d_head]

        k = self.hook_k(
            attn_fn(key_input, self.W_K, self.b_K)
            if self.cfg.ungroup_grouped_query_attention
            else attn_fn(key_input, self._W_K, self._b_K)
        )  # [batch, pos, head_index, d_head]
        v = self.hook_v(
            attn_fn(value_input, self.W_V, self.b_V)
            if self.cfg.ungroup_grouped_query_attention
            else attn_fn(value_input, self._W_V, self._b_V)
        )  # [batch, pos, head_index, d_head]

        if self.cfg.use_qk_norm:
            assert self.q_norm is not None
            assert self.k_norm is not None
            q = self._apply_qk_norm(q, self.q_norm)
            k = self._apply_qk_norm(k, self.k_norm)

        return q, k, v

    def calculate_attention_scores(
        self,
        q: Float[torch.Tensor, "batch query_pos head_index d_head"],
        k: Float[torch.Tensor, "batch key_pos kv_head_index d_head"],
    ) -> Float[torch.Tensor, "batch head_index query_pos key_pos"]:
        """Calculate attention scores from Q and the unexpanded K matrix.
        K will be expaned from [batch, pos, n_key_value_head, d_head] to [batch, pos, n_query_heads, d_head] using torch.repeat_interleave.

        Args:
        q (Float[torch.Tensor, "batch query_pos head_index d_head"]): The Q tensor.
        k (Float[torch.Tensor, "batch key_pos kv_head_index d_head"]): The K tensor.

        Returns:
            Float[torch.Tensor, "batch head_index query_pos key_pos"]: The attention scores.
        """
        if not self.cfg.ungroup_grouped_query_attention:
            k = torch.repeat_interleave(k, dim=2, repeats=self.repeat_kv_heads)
        return super().calculate_attention_scores(q, k)

    def calculate_z_scores(
        self,
        v: Float[torch.Tensor, "batch key_pos kv_head_index d_head"],
        pattern: Float[torch.Tensor, "batch head_index query_pos key_pos"],
    ) -> Float[torch.Tensor, "batch query_pos head_index d_head"]:
        """Calculate z scores from the attention pattern and the unexpanded V matrix.
        V will be expaned from [batch, pos, n_key_value_head, d_head] to [batch, pos, n_query_heads, d_head] using torch.repeat_interleave.

        Args:
        v (Float[torch.Tensor, "batch query_pos head_index d_head"]): The V tensor.
        pattern (Float[torch.Tensor, "batch key_pos kv_head_index d_head"]): The attention pattern.

        Returns:
            Float[torch.Tensor, "batch head_index query_pos key_pos"]: The z scores.
        """
        if not self.cfg.ungroup_grouped_query_attention:
            v = torch.repeat_interleave(v, dim=2, repeats=self.repeat_kv_heads)
        return super().calculate_z_scores(v, pattern)

    def _apply_qk_norm(
        self, x: Float[torch.Tensor, "batch pos head_index d_head"], norm_module: RMSNorm
    ) -> Float[torch.Tensor, "batch pos head_index d_head"]:
        """Apply QK normalization with proper reshaping.

        Args:
            x: Input tensor with shape [batch, pos, head_index, d_head]
            norm_module: RMSNorm module to apply

        Returns:
            Normalized tensor with same shape as input
        """
        # Reshape from [batch, pos, head_index, d_head] to [batch * pos * head_index, d_head]
        batch, pos, n_heads, d_head = x.shape
        x_reshaped = x.reshape(-1, d_head)
        x_normed = norm_module(x_reshaped)
        return x_normed.reshape(batch, pos, n_heads, d_head)



---
File: /transformer_lens/components/layer_norm_pre.py
---

"""Hooked Transformer Layer Norm Pre Component.

This module contains all the component :class:`LayerNormPre`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


# LayerNormPre
# I fold the LayerNorm weights and biases into later weights and biases.
# This is just the 'center and normalise' part of LayerNorm
# Centering is equivalent to just deleting one direction of residual space,
# and is equivalent to centering the weight matrices of everything writing to the residual stream
# Normalising is a funkier non-linear operation, that projects the residual stream onto the unit hypersphere
class LayerNormPre(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        """LayerNormPre - the 'center and normalise' part of LayerNorm. Length is
        normally d_model, but is d_mlp for softmax. Not needed as a parameter. This
        should only be used in inference mode after folding in LayerNorm weights"""
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.eps = self.cfg.eps

        # Adds a hook point for the normalisation scale factor
        self.hook_scale = HookPoint()  # [batch, pos]
        # Hook Normalized captures LN output - here it's a vector with std 1 and mean 0
        self.hook_normalized = HookPoint()  # [batch, pos, length]

    def forward(
        self,
        x: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos head_index d_model"],
        ],
    ) -> Union[
        Float[torch.Tensor, "batch pos d_model"],
        Float[torch.Tensor, "batch pos head_index d_model"],
    ]:
        if self.cfg.dtype not in [torch.float32, torch.float64]:
            x = x.to(torch.float32)

        x = x - x.mean(-1, keepdim=True)  # [batch, pos, length]
        scale: Union[
            Float[torch.Tensor, "batch pos 1"],
            Float[torch.Tensor, "batch pos head_index 1"],
        ] = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt())
        return self.hook_normalized(x / scale).to(self.cfg.dtype)



---
File: /transformer_lens/components/layer_norm.py
---

"""Hooked Transformer Layer Norm Component.

This module contains all the component :class:`LayerNorm`.
"""
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class LayerNorm(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig], length: Optional[int] = None):
        """
        LayerNorm with optional length parameter

        length (Optional[int]): If the dimension of the LayerNorm. If not provided, assumed to be d_model
        """
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.eps = self.cfg.eps
        if length is None:
            self.length = self.cfg.d_model
        else:
            self.length = length

        self.w = nn.Parameter(torch.ones(self.length, dtype=self.cfg.dtype))
        self.b = nn.Parameter(torch.zeros(self.length, dtype=self.cfg.dtype))

        # Adds a hook point for the normalisation scale factor
        self.hook_scale = HookPoint()  # [batch, pos, 1]
        # Hook_normalized is on the LN output
        self.hook_normalized = HookPoint()  # [batch, pos, length]

    def forward(
        self,
        x: Union[
            Float[torch.Tensor, "batch pos d_model"],
            Float[torch.Tensor, "batch pos head_index d_model"],
        ],
    ) -> Union[
        Float[torch.Tensor, "batch pos d_model"],
        Float[torch.Tensor, "batch pos head_index d_model"],
    ]:
        if self.cfg.dtype not in [torch.float32, torch.float64]:
            x = x.to(torch.float32)

        x = x - x.mean(-1, keepdim=True)  # [batch, pos, length]
        scale: Float[torch.Tensor, "batch pos 1"] = self.hook_scale(
            (x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt()
        )
        x = x / scale  # [batch, pos, length]
        return self.hook_normalized(x * self.w + self.b).to(self.cfg.dtype)



---
File: /transformer_lens/components/pos_embed.py
---

"""Hooked Transformer POS Embed Component.

This module contains all the component :class:`PosEmbed`.
"""
from typing import Dict, Optional, Union

import einops
import torch
import torch.nn as nn
from jaxtyping import Float, Int

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utils import get_offset_position_ids


# Positional Embeddings
class PosEmbed(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W_pos = nn.Parameter(
            torch.empty(self.cfg.n_ctx, self.cfg.d_model, dtype=self.cfg.dtype)
        )

    def forward(
        self,
        tokens: Int[torch.Tensor, "batch pos"],
        past_kv_pos_offset: int = 0,
        attention_mask: Optional[Int[torch.Tensor, "batch offset_pos"]] = None,
    ) -> Float[torch.Tensor, "batch new_pos d_model"]:
        """
        Forward pass for positional embeddings.

        Args:
            tokens (Int[torch.Tensor, "batch pos"]): Input tokens.
            past_kv_pos_offset (int, optional): The length of tokens in the past_kv_cache. Defaults to 0.
            attention_mask (Int[torch.Tensor, "batch pos"], optional): The attention mask for padded tokens.
                 Defaults to None.

        Returns:
            Float[torch.Tensor, "batch pos d_model"]: Absolute position embeddings.
        """
        tokens_length = tokens.size(-1)

        if attention_mask is None:
            pos_embed = self.W_pos[
                past_kv_pos_offset : tokens_length + past_kv_pos_offset, :
            ]  # [pos, d_model]
            batch_pos_embed = einops.repeat(
                pos_embed, "pos d_model -> batch pos d_model", batch=tokens.size(0)
            )

        else:
            # Separated from the no padding case for computational efficiency
            # (this code is a bit slower than the code above)

            offset_position_ids = get_offset_position_ids(past_kv_pos_offset, attention_mask)
            pos_embed = self.W_pos[offset_position_ids]  # [batch, pos, d_model]

            # Set the position embeddings to 0 for pad tokens (this is an arbitrary choice)
            padding_mask = ~attention_mask.bool()  # [batch, tokens_length]
            offset_padding_mask = padding_mask[
                :, past_kv_pos_offset : tokens_length + past_kv_pos_offset
            ].unsqueeze(
                -1
            )  # [batch, pos, 1]
            batch_pos_embed = torch.where(offset_padding_mask, 0, pos_embed)

        return batch_pos_embed.clone()



---
File: /transformer_lens/components/rms_norm_pre.py
---

"""Hooked Transformer RMS Norm Pre Component.

This module contains all the component :class:`RMSNormPre`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class RMSNormPre(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        """RMSNormPre - LayerNormPre without the centering and bias (RMS = Root Mean Square)"""
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.eps = self.cfg.eps

        # Adds a hook point for the normalisation scale factor
        self.hook_scale = HookPoint()  # [batch, pos]
        self.hook_normalized = HookPoint()  # [batch, pos, length]

    def forward(
        self, x: Float[torch.Tensor, "batch pos length"]
    ) -> Float[torch.Tensor, "batch pos length"]:
        if self.cfg.dtype not in [torch.float32, torch.float64]:
            x = x.to(torch.float32)

        scale: Float[torch.Tensor, "batch pos 1"] = self.hook_scale(
            (x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt()
        )
        return self.hook_normalized(x / scale).to(self.cfg.dtype)  # [batch, pos, length]



---
File: /transformer_lens/components/rms_norm.py
---

"""Hooked Transformer RMS Norm Component.

This module contains all the component :class:`RMSNorm`.
"""
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class RMSNorm(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig], length: Optional[int] = None):
        """
        RMSNorm - LayerNorm without the centering and bias (RMS = Root Mean Square)

        length (Optional[int]): If the dimension of the RMSNorm. If not provided, assumed to be d_model
        """
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.eps = self.cfg.eps
        if length is None:
            self.length = self.cfg.d_model
        else:
            self.length = length

        self.w = nn.Parameter(torch.ones(self.length, dtype=self.cfg.dtype))

        # Adds a hook point for the normalisation scale factor
        self.hook_scale = HookPoint()  # [batch, pos, 1]
        self.hook_normalized = HookPoint()  # [batch, pos, length]

    def forward(
        self, x: Float[torch.Tensor, "batch pos length"]
    ) -> Float[torch.Tensor, "batch pos length"]:
        if self.cfg.dtype not in [torch.float32, torch.float64]:
            x = x.to(torch.float32)
        scale: Float[torch.Tensor, "batch pos 1"] = self.hook_scale(
            (x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt()
        )
        x = self.hook_normalized(x / scale).to(self.cfg.dtype)  # [batch, pos, length]

        if x.device != self.w.device:
            self.to(x.device)

        return x * self.w



---
File: /transformer_lens/components/t5_attention.py
---

import math
from typing import Dict, Optional, Union

import torch
import torch.nn as nn
from jaxtyping import Float, Int

from transformer_lens.components.abstract_attention import AbstractAttention
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class T5Attention(AbstractAttention):
    r"""
    T5 attention - with relative attention bias and cross-attention support
    This realisation expects you to precompute relative positional bias, and then feed it to forward
    like
    ```python
    attn = T5Attention(cfg, has_relative_attention_bias=True)
    positional_bias = attn.compute_relative_attention_bias(query_len, key_len, device=device)
    result = attn(query, key, value, position_bias=positional_bias)
    ```
    """

    def __init__(
        self,
        cfg: Union[Dict, HookedTransformerConfig],
        has_relative_attention_bias: bool = False,
        attn_type: str = "global",
        layer_id: Optional[int] = None,
    ):
        super().__init__(cfg, attn_type, layer_id)
        if isinstance(cfg, Dict):
            cfg = HookedTransformerConfig.from_dict(cfg)
        self.cfg = cfg
        self.has_relative_attention_bias: bool = has_relative_attention_bias

        if self.has_relative_attention_bias:
            if (
                cfg.relative_attention_num_buckets is None
                or cfg.relative_attention_max_distance is None
            ):
                raise ValueError(
                    "You need to specify relative_attention_num_buckets and relative_attention_max_distance  in config to use relative attention bias"
                )

            self.relative_attention_num_buckets = cfg.relative_attention_num_buckets
            self.relative_attention_max_distance = cfg.relative_attention_max_distance
            self.rel_pos_bias = nn.Embedding(self.relative_attention_num_buckets, self.cfg.n_heads)
            self.rel_pos_hook = HookPoint()

        self.W_K = nn.Parameter(
            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head, dtype=cfg.dtype)
        )
        self.W_V = nn.Parameter(
            torch.empty(self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head, dtype=cfg.dtype)
        )
        self.b_K = nn.Parameter(torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=cfg.dtype))
        self.b_V = nn.Parameter(torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=cfg.dtype))

    @staticmethod
    def _relative_position_bucket(
        relative_position: Int[torch.Tensor, "query_pos kv_pos"],
        bidirectional=True,
        num_buckets=32,
        max_distance=128,
    ) -> Int[torch.Tensor, "query_pos kv_pos"]:
        """
        added from
        https://github.com/huggingface/transformers/blob/e0c3cee17085914bbe505c159beeb8ae39bc37dd/src/transformers/models/t5/modeling_t5.py#L382
        which is adapted from
        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593


        Translate relative position to a bucket number for relative attention. The relative position is defined as
        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to
        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for
        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative
        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.
        This should allow for more graceful generalization to longer sequences than the model has been trained on

        Args:
            relative_position: an int32 Tensor
            bidirectional: a boolean - whether the attention is bidirectional
            num_buckets: an integer
            max_distance: an integer

        Returns:
            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)
        """
        relative_buckets = torch.zeros_like(relative_position)

        if bidirectional:
            num_buckets //= 2
            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets
            relative_position = torch.abs(relative_position)
        else:
            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))
        # now relative_position is in the range [0, inf)

        # half of the buckets are for exact increments in positions
        max_exact = num_buckets // 2
        is_small = relative_position < max_exact

        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
        relative_position_if_large = max_exact + (
            torch.log(relative_position.float() / max_exact)
            / math.log(max_distance / max_exact)
            * (num_buckets - max_exact)
        ).to(torch.long)
        relative_position_if_large = torch.min(
            relative_position_if_large,
            torch.full_like(relative_position_if_large, num_buckets - 1),
        )

        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)
        return relative_buckets

    def compute_relative_attention_bias(
        self, query_length: int, key_length: int, device=None
    ) -> Float[torch.Tensor, "1 head_index pos kv_pos"]:
        """Compute binned relative position bias"""
        if device is None:
            device = self.rel_pos_bias.weight.device
        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
        relative_position = memory_position - context_position  # shape (query_length, key_length)
        relative_position_bucket = self._relative_position_bucket(
            relative_position,  # shape (query_length, key_length)
            bidirectional=True,
            num_buckets=self.relative_attention_num_buckets,
            max_distance=self.relative_attention_max_distance,
        )
        values = self.rel_pos_bias(
            relative_position_bucket
        )  # shape (query_length, key_length, num_heads)
        values = values.permute([2, 0, 1]).unsqueeze(
            0
        )  # shape (1, num_heads, query_length, key_length)
        return values



---
File: /transformer_lens/components/t5_block.py
---

from typing import Optional

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.components import RMSNorm, T5Attention
from transformer_lens.factories.mlp_factory import MLPFactory
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCacheEntry
from transformer_lens.utils import repeat_along_head_dimension


class T5Block(nn.Module):
    """
    T5 decoder Block. Uses T5Layernorm, and T5attention insted of usual ones.
    Also uses cross attention if is_decoder is True.
    """

    def __init__(self, cfg: HookedTransformerConfig, block_index: int, is_decoder: bool):
        super().__init__()
        self.cfg = cfg
        self.is_decoder = is_decoder

        self.ln1 = RMSNorm(cfg)
        self.attn = T5Attention(cfg, has_relative_attention_bias=block_index == 0)
        self.ln2 = RMSNorm(cfg)
        if self.is_decoder:
            self.cross_attn = T5Attention(cfg)
            self.ln3 = RMSNorm(cfg)
        self.mlp = MLPFactory.create_mlp(self.cfg)  # [batch, pos, n_heads]

        self.hook_q_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_k_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_v_input = HookPoint()  # [batch, pos, n_heads, d_model]

        self.hook_attn_in = HookPoint()  # [batch, pos, d_model]
        self.hook_attn_out = HookPoint()  # [batch, pos, d_model]
        if self.is_decoder:
            self.hook_cross_attn_in = HookPoint()  # [batch, pos, d_model]
            self.hook_cross_attn_out = HookPoint()  # [batch, pos, d_model]
            self.hook_resid_mid_cross = HookPoint()  # [batch, pos, d_model]

        self.hook_mlp_in = HookPoint()  # [batch, pos, d_model]
        self.hook_mlp_out = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_pre = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_mid = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_post = HookPoint()  # [batch, pos, d_model]

    def forward(
        self,
        resid_pre: Float[torch.Tensor, "batch pos d_model"],
        additive_attention_mask: Optional[Float[torch.Tensor, "batch 1 1 pos"]] = None,
        encoder_additive_attention_mask: Optional[
            Float[torch.Tensor, "batch 1 1 encoder_pos"]
        ] = None,
        position_bias: Optional[Float[torch.Tensor, "1 head_index pos kv_pos"]] = None,
        encoder_hidden_states: Optional[Float[torch.Tensor, "batch encoder_pos d_model"]] = None,
        past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] = None,
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        """A single Transformer block.

        Args:
            resid_pre (torch.Tensor): The residual stream - shape [batch, pos, d_model]
            encoder_hidden_states (torch.Tensor): The hidden states of the encoder for cross attention - shape [batch, encoder_pos, d_model]
            cache (HookedTransformerKeyValueCache): A cache of previous keys and values, used only when generating text. Defaults to None.
            attention_mask (torch.Tensor, optional): The attention mask for padded tokens. Defaults to None.

        Returns:
            _type_: _description_
        """
        resid_pre = self.hook_resid_pre(resid_pre)  # [batch, pos, d_model]

        attn_in = resid_pre

        if self.cfg.use_attn_in:
            attn_in = self.hook_attn_in(
                repeat_along_head_dimension(resid_pre, n_heads=self.cfg.n_heads)
            )

        if self.cfg.use_split_qkv_input:
            n_kv_heads = (
                self.cfg.n_key_value_heads
                if self.cfg.n_key_value_heads is not None
                else self.cfg.n_heads
            )
            query_input = self.hook_q_input(
                repeat_along_head_dimension(resid_pre, n_heads=self.cfg.n_heads)
            )
            key_input = self.hook_k_input(
                repeat_along_head_dimension(resid_pre, n_heads=n_kv_heads)
            )
            value_input = self.hook_v_input(
                repeat_along_head_dimension(resid_pre, n_heads=n_kv_heads)
            )
        else:
            query_input = attn_in
            key_input = attn_in
            value_input = attn_in

        attn_out = self.hook_attn_out(
            # hook the residual stream states that are used to calculate the
            # queries, keys and values, independently.
            # Then take the layer norm of these inputs, and pass these to the attention module.
            self.attn(
                query_input=self.ln1(query_input),
                key_input=self.ln1(key_input),
                value_input=self.ln1(value_input),
                past_kv_cache_entry=past_kv_cache_entry,
                additive_attention_mask=additive_attention_mask,
                position_bias=position_bias,
            )
        )

        # [batch, pos, d_model]

        resid_mid = self.hook_resid_mid(resid_pre + attn_out)  # [batch, pos, d_model]

        if self.is_decoder:
            cross_attn_in = (
                resid_mid
                if not self.cfg.use_attn_in
                else self.hook_cross_attn_in(resid_mid.clone())
            )

            if encoder_hidden_states is None:
                raise ValueError("Encoder hidden states must be provided for cross attention!")

            cross_attn_out = self.hook_cross_attn_out(
                self.cross_attn(
                    query_input=self.ln2(cross_attn_in),
                    key_input=encoder_hidden_states,
                    value_input=encoder_hidden_states,
                    additive_attention_mask=encoder_additive_attention_mask,
                )
            )
            resid_mid_cross = self.hook_resid_mid_cross(resid_mid + cross_attn_out)

            mlp_in = (
                resid_mid_cross
                if not self.cfg.use_hook_mlp_in
                else self.hook_mlp_in(resid_mid_cross.clone())
            )

            normalized_resid_mid = self.ln3(mlp_in)
        else:
            mlp_in = (
                resid_mid if not self.cfg.use_hook_mlp_in else self.hook_mlp_in(resid_mid.clone())
            )
            normalized_resid_mid = self.ln2(mlp_in)

        mlp_out = self.hook_mlp_out(self.mlp(normalized_resid_mid))  # [batch, pos, d_model]
        resid_post = self.hook_resid_post(mlp_in + mlp_out)  # [batch, pos, d_model]

        return resid_post



---
File: /transformer_lens/components/token_typed_embed.py
---

"""Hooked Transformer Token Typed Embed Component.

This module contains all the component :class:`TokenTypeEmbed`.
"""
from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Int

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class TokenTypeEmbed(nn.Module):
    """
    The token-type embed is a binary ids indicating whether a token belongs to sequence A or B. For example, for two sentences: "[CLS] Sentence A [SEP] Sentence B [SEP]", token_type_ids would be [0, 0, ..., 0, 1, ..., 1, 1]. `0` represents tokens from Sentence A, `1` from Sentence B. If not provided, BERT assumes a single sequence input. Typically, shape is (batch_size, sequence_length).

    See the BERT paper for more information: https://arxiv.org/pdf/1810.04805.pdf
    """

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        self.W_token_type = nn.Parameter(torch.empty(2, self.cfg.d_model, dtype=self.cfg.dtype))

    def forward(self, token_type_ids: Int[torch.Tensor, "batch pos"]):
        return self.W_token_type[token_type_ids, :]



---
File: /transformer_lens/components/transformer_block.py
---

"""Hooked Transformer Transformer Block Component.

This module contains all the component :class:`TransformerBlock`.
"""

from typing import Callable, Dict, Optional, Union

import torch
import torch.nn as nn
from jaxtyping import Float, Int

from transformer_lens.components import (
    Attention,
    GroupedQueryAttention,
    LayerNorm,
    LayerNormPre,
    RMSNorm,
    RMSNormPre,
)
from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.factories.mlp_factory import MLPFactory
from transformer_lens.hook_points import HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCacheEntry
from transformer_lens.utils import repeat_along_head_dimension


# Transformer Block
class TransformerBlock(nn.Module):
    ln1: nn.Module
    ln2: nn.Module
    mlp: CanBeUsedAsMLP

    def __init__(self, cfg: Union[Dict, HookedTransformerConfig], block_index):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        normalization_layer: Callable  # type: ignore
        normalization_layer_after: Callable  # type: ignore

        self.normalization_type = self.cfg.normalization_type

        if self.normalization_type == "LN":
            normalization_layer = LayerNorm
        elif self.normalization_type == "LNPre":
            # We've folded in LayerNorm weights, so just need the center + scale parts
            normalization_layer = LayerNormPre
        elif self.normalization_type == "RMS":
            normalization_layer = RMSNorm
        elif self.normalization_type == "RMSPre":
            normalization_layer = RMSNormPre
        elif self.normalization_type is None:
            # This should just be the identity.
            # We need to make this a lambda so we can call it on the config, just like the others
            normalization_layer = lambda cfg: nn.Identity()
        else:
            raise ValueError(f"Invalid normalization_type passed in: {self.normalization_type}")

        if self.cfg.use_normalization_before_and_after:
            # If we use LN before and after, we do *not* fold in the weights to the LN
            # after, though we can fold for the one before.
            if self.normalization_type is None:
                normalization_layer_after = lambda cfg: nn.Identity()
            elif self.normalization_type.startswith("RMS"):
                normalization_layer_after = RMSNorm
            elif self.normalization_type.startswith("LayerNorm"):
                normalization_layer_after = LayerNorm

        self.ln1 = normalization_layer(cfg)
        if self.cfg.use_normalization_before_and_after:
            self.ln1_post = normalization_layer_after(cfg)
        if not self.cfg.attn_only:
            self.ln2 = normalization_layer(cfg)
            if self.cfg.use_normalization_before_and_after:
                self.ln2_post = normalization_layer_after(cfg)

        attention = Attention if self.cfg.n_key_value_heads is None else GroupedQueryAttention
        if not self.cfg.use_local_attn:
            self.attn = attention(self.cfg, "global", block_index)
        else:
            if self.cfg.attn_types is None:
                raise ValueError("attn_types must be set when using local attention")
            attn_type = self.cfg.attn_types[block_index]
            self.attn = attention(self.cfg, attn_type, block_index)
        if not self.cfg.attn_only:
            self.mlp = MLPFactory.create_mlp(self.cfg)

        self.hook_attn_in = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_q_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_k_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_v_input = HookPoint()  # [batch, pos, n_heads, d_model]
        self.hook_mlp_in = HookPoint()  # [batch, pos, d_model]

        self.hook_attn_out = HookPoint()  # [batch, pos, d_model]
        self.hook_mlp_out = HookPoint()  # [batch, pos, d_model]

        self.hook_resid_pre = HookPoint()  # [batch, pos, d_model]
        if not self.cfg.attn_only and not self.cfg.parallel_attn_mlp:
            self.hook_resid_mid = HookPoint()  # [batch, pos, d_model]
        self.hook_resid_post = HookPoint()  # [batch, pos, d_model]

    def forward(
        self,
        resid_pre: Float[torch.Tensor, "batch pos d_model"],
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] = None,
        attention_mask: Optional[Int[torch.Tensor, "batch offset_pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        """A single Transformer block.

        Args:
            resid_pre (torch.Tensor): The residual stream - shape [batch, pos, d_model]
            cache (HookedTransformerKeyValueCache): A cache of previous keys and values, used only when generating text. Defaults to None.
            shortformer_pos_embed (torch.Tensor, optional): Only used for positional_embeddings_type == "shortformer". The positional embeddings. See HookedTransformerConfig for details. Defaults to None.
            attention_mask (torch.Tensor, optional): The attention mask for padded tokens. Defaults to None.

        Returns:
            Float[torch.Tensor, "batch pos d_model"]: Our resulting tensor
        """
        resid_pre = self.hook_resid_pre(resid_pre)  # [batch, pos, d_model]

        if self.cfg.use_attn_in or self.cfg.use_split_qkv_input:
            # We're adding a head dimension
            if shortformer_pos_embed is not None:
                shortformer_pos_embed = repeat_along_head_dimension(
                    shortformer_pos_embed, n_heads=self.cfg.n_heads
                )
        else:
            attn_in = resid_pre

        if self.cfg.use_attn_in:
            attn_in = self.hook_attn_in(
                repeat_along_head_dimension(resid_pre, n_heads=self.cfg.n_heads)
            )

        if self.cfg.use_split_qkv_input:
            n_kv_heads = (
                self.cfg.n_key_value_heads
                if self.cfg.n_key_value_heads is not None
                and not self.cfg.ungroup_grouped_query_attention
                else self.cfg.n_heads
            )
            query_input = self.hook_q_input(
                repeat_along_head_dimension(resid_pre, n_heads=self.cfg.n_heads)
            )
            key_input = self.hook_k_input(
                repeat_along_head_dimension(resid_pre, n_heads=n_kv_heads)
            )
            value_input = self.hook_v_input(
                repeat_along_head_dimension(resid_pre, n_heads=n_kv_heads)
            )
        else:
            query_input = attn_in
            key_input = attn_in
            value_input = attn_in

        attn_out = (
            # hook the residual stream states that are used to calculate the
            # queries, keys and values, independently.
            # Then take the layer norm of these inputs, and pass these to the attention module.
            self.attn(
                query_input=self.ln1(query_input)
                + (0.0 if shortformer_pos_embed is None else shortformer_pos_embed),
                key_input=self.ln1(key_input)
                + (0.0 if shortformer_pos_embed is None else shortformer_pos_embed),
                value_input=self.ln1(value_input),
                past_kv_cache_entry=past_kv_cache_entry,
                attention_mask=attention_mask,
            )
        )  # [batch, pos, d_model]
        if self.cfg.use_normalization_before_and_after:
            # If we use LayerNorm both before and after, then apply the second LN after the layer
            # and before the hook. We do it before the hook so hook_attn_out captures "that which
            # is added to the residual stream"
            attn_out = self.ln1_post(attn_out)
        attn_out = self.hook_attn_out(attn_out)

        if resid_pre.device != attn_out.device:
            resid_pre = resid_pre.to(attn_out.device)

        if not self.cfg.attn_only and not self.cfg.parallel_attn_mlp:
            resid_mid = self.hook_resid_mid(resid_pre + attn_out)  # [batch, pos, d_model]
            mlp_in = (
                resid_mid if not self.cfg.use_hook_mlp_in else self.hook_mlp_in(resid_mid.clone())
            )
            normalized_resid_mid = self.ln2(mlp_in)
            mlp_out = self.apply_mlp(normalized_resid_mid)
            resid_post = self.hook_resid_post(resid_mid + mlp_out)  # [batch, pos, d_model]
        elif self.cfg.parallel_attn_mlp:
            # Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.
            # In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.
            normalized_resid_pre_2 = self.ln2(
                resid_pre if not self.cfg.use_hook_mlp_in else self.hook_mlp_in(resid_pre.clone())
            )
            mlp_out = self.apply_mlp(normalized_resid_pre_2)
            resid_post = self.hook_resid_post(
                resid_pre + attn_out + mlp_out
            )  # [batch, pos, d_model]
        else:
            resid_post = self.hook_resid_post(resid_pre + attn_out)  # [batch, pos, d_model]
        return resid_post

    def apply_mlp(
        self, normalized_resid: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_model"]:
        """Centralized point where the MLP is applied to the forward pass

        Returns:
            Float[torch.Tensor, "batch pos d_model"]: Our resulting tensor
        """
        mlp_out = self.mlp(normalized_resid)  # [batch, pos, d_model]
        if self.cfg.use_normalization_before_and_after:
            mlp_out = self.ln2_post(mlp_out)
        return self.hook_mlp_out(mlp_out)



---
File: /transformer_lens/components/unembed.py
---

"""Hooked Transformer Unembed Component.

This module contains all the component :class:`Unembed`.
"""

from typing import Dict, Union

import torch
import torch.nn as nn
from jaxtyping import Float

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.addmm import batch_addmm


class Unembed(nn.Module):
    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):
        super().__init__()
        self.cfg = HookedTransformerConfig.unwrap(cfg)
        # Note that there's a separate variable for d_vocab_out and d_vocab (the input vocab size). For language tasks these are always the same, but for algorithmic tasks we may want them to be different.
        self.W_U: Float[torch.Tensor, "d_model d_vocab_out"] = nn.Parameter(
            torch.empty(self.cfg.d_model, self.cfg.d_vocab_out, dtype=self.cfg.dtype)
        )
        self.b_U: Float[torch.Tensor, "d_vocab_out"] = nn.Parameter(
            torch.zeros(self.cfg.d_vocab_out, dtype=self.cfg.dtype)
        )

    def forward(
        self, residual: Float[torch.Tensor, "batch pos d_model"]
    ) -> Float[torch.Tensor, "batch pos d_vocab_out"]:
        return batch_addmm(self.b_U, self.W_U, residual)



---
File: /transformer_lens/factories/activation_function_factory.py
---

"""Activation Function Factory

Centralized location for selection supported activation functions throughout TransformerLens
"""

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.activation_functions import (
    SUPPORTED_ACTIVATIONS,
    ActivationFunction,
)


class ActivationFunctionFactory:
    @staticmethod
    def pick_activation_function(cfg: HookedTransformerConfig) -> ActivationFunction:
        """Use this to select what activation function is needed based on configuration.

        Args:
            cfg (HookedTransformerConfig): The already created hooked transformer config

        Raises:
            ValueError: If there is a problem with the requested activation function.

        Returns:
            ActivationFunction: The activation function based on the dictionary of supported activations.
        """
        act_fn = cfg.act_fn

        if act_fn is None:
            raise ValueError("act_fn not set when trying to select Activation Function")

        activation_function = SUPPORTED_ACTIVATIONS.get(act_fn)

        if activation_function is None:
            raise ValueError(f"Invalid activation function name: {act_fn}")

        return activation_function



---
File: /transformer_lens/factories/mlp_factory.py
---

"""MLP Factory

Centralized location for creating any MLP needed within TransformerLens
"""
from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP
from transformer_lens.components.mlps.gated_mlp import GatedMLP
from transformer_lens.components.mlps.gated_mlp_4bit import GatedMLP4Bit
from transformer_lens.components.mlps.mlp import MLP
from transformer_lens.components.mlps.moe import MoE
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


class MLPFactory:
    @staticmethod
    def create_mlp(cfg: HookedTransformerConfig) -> CanBeUsedAsMLP:
        if cfg.num_experts:
            return MoE(cfg)
        elif cfg.gated_mlp:
            return GatedMLP(cfg) if not cfg.load_in_4bit else GatedMLP4Bit(cfg)
        else:
            return MLP(cfg)



---
File: /transformer_lens/pretrained/weight_conversions/__init__.py
---

from .neo import convert_neo_weights
from .gpt2 import convert_gpt2_weights
from .opt import convert_opt_weights
from .gptj import convert_gptj_weights
from .neox import convert_neox_weights
from .llama import convert_llama_weights
from .bert import convert_bert_weights
from .mistral import convert_mistral_weights
from .mixtral import convert_mixtral_weights
from .bloom import convert_bloom_weights
from .coder import convert_coder_weights
from .qwen import convert_qwen_weights
from .qwen2 import convert_qwen2_weights
from .qwen3 import convert_qwen3_weights
from .phi import convert_phi_weights
from .phi3 import convert_phi3_weights
from .gemma import convert_gemma_weights
from .mingpt import convert_mingpt_weights
from .nanogpt import convert_nanogpt_weights
from .t5 import convert_t5_weights
from .neel_solu_old import convert_neel_solu_old_weights



---
File: /transformer_lens/pretrained/weight_conversions/bert.py
---

import einops

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_bert_weights(bert, cfg: HookedTransformerConfig):
    embeddings = bert.bert.embeddings
    state_dict = {
        "embed.embed.W_E": embeddings.word_embeddings.weight,
        "embed.pos_embed.W_pos": embeddings.position_embeddings.weight,
        "embed.token_type_embed.W_token_type": embeddings.token_type_embeddings.weight,
        "embed.ln.w": embeddings.LayerNorm.weight,
        "embed.ln.b": embeddings.LayerNorm.bias,
    }

    for l in range(cfg.n_layers):
        block = bert.bert.encoder.layer[l]
        state_dict[f"blocks.{l}.attn.W_Q"] = einops.rearrange(
            block.attention.self.query.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.b_Q"] = einops.rearrange(
            block.attention.self.query.bias, "(i h) -> i h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.W_K"] = einops.rearrange(
            block.attention.self.key.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.b_K"] = einops.rearrange(
            block.attention.self.key.bias, "(i h) -> i h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.W_V"] = einops.rearrange(
            block.attention.self.value.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.b_V"] = einops.rearrange(
            block.attention.self.value.bias, "(i h) -> i h", i=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.W_O"] = einops.rearrange(
            block.attention.output.dense.weight,
            "m (i h) -> i h m",
            i=cfg.n_heads,
        )
        state_dict[f"blocks.{l}.attn.b_O"] = block.attention.output.dense.bias
        state_dict[f"blocks.{l}.ln1.w"] = block.attention.output.LayerNorm.weight
        state_dict[f"blocks.{l}.ln1.b"] = block.attention.output.LayerNorm.bias
        state_dict[f"blocks.{l}.mlp.W_in"] = einops.rearrange(
            block.intermediate.dense.weight, "mlp model -> model mlp"
        )
        state_dict[f"blocks.{l}.mlp.b_in"] = block.intermediate.dense.bias
        state_dict[f"blocks.{l}.mlp.W_out"] = einops.rearrange(
            block.output.dense.weight, "model mlp -> mlp model"
        )
        state_dict[f"blocks.{l}.mlp.b_out"] = block.output.dense.bias
        state_dict[f"blocks.{l}.ln2.w"] = block.output.LayerNorm.weight
        state_dict[f"blocks.{l}.ln2.b"] = block.output.LayerNorm.bias

    pooler = bert.bert.pooler
    state_dict["pooler.W"] = pooler.dense.weight.T
    state_dict["pooler.b"] = pooler.dense.bias

    mlm_head = bert.cls.predictions
    state_dict["mlm_head.W"] = mlm_head.transform.dense.weight.T
    state_dict["mlm_head.b"] = mlm_head.transform.dense.bias
    state_dict["mlm_head.ln.w"] = mlm_head.transform.LayerNorm.weight
    state_dict["mlm_head.ln.b"] = mlm_head.transform.LayerNorm.bias

    # The NSP head does not have an unembedding
    # so we are only using weights from the MLM head
    # Note: BERT uses tied embeddings
    state_dict["unembed.W_U"] = mlm_head.decoder.weight.T
    state_dict["unembed.b_U"] = mlm_head.decoder.bias

    nsp_head = bert.cls.seq_relationship
    state_dict["nsp_head.W"] = nsp_head.weight.T
    state_dict["nsp_head.b"] = nsp_head.bias

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/bloom.py
---

import einops

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_bloom_weights(bloom, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = bloom.transformer.word_embeddings.weight

    # Bloom uses post embedding layer norm
    state_dict["embed.ln.w"] = bloom.transformer.word_embeddings_layernorm.weight
    state_dict["embed.ln.b"] = bloom.transformer.word_embeddings_layernorm.bias

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = bloom.transformer.h[l].input_layernorm.weight
        state_dict[f"blocks.{l}.ln1.b"] = bloom.transformer.h[l].input_layernorm.bias

        W = bloom.transformer.h[l].self_attention.query_key_value.weight

        W_split = W.T.reshape(cfg.d_model, cfg.n_heads, 3, cfg.d_head)

        W_Q, W_K, W_V = W_split[..., 0, :], W_split[..., 1, :], W_split[..., 2, :]
        W_Q = einops.rearrange(W_Q, "m n h ->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "m n h ->n m h", n=cfg.n_heads)
        W_V = einops.rearrange(W_V, "m n h ->n m h", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        qkv_bias = bloom.transformer.h[l].self_attention.query_key_value.bias
        qkv_bias = qkv_bias.reshape(cfg.n_heads, 3, cfg.d_head)

        state_dict[f"blocks.{l}.attn.b_Q"] = qkv_bias[:, 0, :]
        state_dict[f"blocks.{l}.attn.b_K"] = qkv_bias[:, 1, :]
        state_dict[f"blocks.{l}.attn.b_V"] = qkv_bias[:, 2, :]

        W_O = bloom.transformer.h[l].self_attention.dense.weight.T  # [1024, 1024]
        W_O = einops.rearrange(W_O, "(n h) m->n h m", n=cfg.n_heads)  # [n_heads, d_head, d_model]
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = bloom.transformer.h[l].self_attention.dense.bias

        state_dict[f"blocks.{l}.ln2.w"] = bloom.transformer.h[l].post_attention_layernorm.weight
        state_dict[f"blocks.{l}.ln2.b"] = bloom.transformer.h[l].post_attention_layernorm.bias

        W_in = bloom.transformer.h[l].mlp.dense_h_to_4h.weight.T
        state_dict[f"blocks.{l}.mlp.W_in"] = W_in
        state_dict[f"blocks.{l}.mlp.b_in"] = bloom.transformer.h[l].mlp.dense_h_to_4h.bias

        W_out = bloom.transformer.h[l].mlp.dense_4h_to_h.weight.T
        state_dict[f"blocks.{l}.mlp.W_out"] = W_out
        state_dict[f"blocks.{l}.mlp.b_out"] = bloom.transformer.h[l].mlp.dense_4h_to_h.bias
    state_dict["unembed.W_U"] = bloom.lm_head.weight.T

    state_dict["ln_final.w"] = bloom.transformer.ln_f.weight
    state_dict["ln_final.b"] = bloom.transformer.ln_f.bias
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/coder.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_coder_weights(model, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = model.transformer.wte.weight
    state_dict["pos_embed.W_pos"] = model.transformer.wpe.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = model.transformer.h[l].ln_1.weight
        state_dict[f"blocks.{l}.ln1.b"] = model.transformer.h[l].ln_1.bias

        # In GPT-2, q,k,v are produced by one big linear map, whose output is
        # concat([q, k, v])
        W_KV = model.transformer.h[l].attn.kv_attn.weight  # [d_model, 2 * d_head]
        W_K, W_V = torch.tensor_split(W_KV, 2, dim=1)
        W_Q = model.transformer.h[l].attn.q_attn.weight  # [d_model, d_model]
        W_Q = einops.rearrange(W_Q, "m (i h)->i m h", i=cfg.n_heads)
        W_K = einops.repeat(W_K, "m h -> i m h", i=cfg.n_heads)
        W_V = einops.repeat(W_V, "m h -> i m h", i=cfg.n_heads)

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        b_Q = einops.rearrange(
            model.transformer.h[l].attn.q_attn.bias,
            "(index head)-> index head",
            index=cfg.n_heads,
            head=cfg.d_head,
        )
        b_KV = model.transformer.h[l].attn.kv_attn.bias  # [2 * d_head]
        b_K, b_V = torch.tensor_split(b_KV, 2, dim=0)
        b_K = einops.repeat(b_K, "head -> index head", index=cfg.n_heads)
        b_V = einops.repeat(b_V, "head -> index head", index=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.b_Q"] = b_Q
        state_dict[f"blocks.{l}.attn.b_K"] = b_K
        state_dict[f"blocks.{l}.attn.b_V"] = b_V

        W_O = model.transformer.h[l].attn.c_proj.weight
        W_O = einops.rearrange(W_O, "(i h) m->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = model.transformer.h[l].attn.c_proj.bias

        state_dict[f"blocks.{l}.ln2.w"] = model.transformer.h[l].ln_2.weight
        state_dict[f"blocks.{l}.ln2.b"] = model.transformer.h[l].ln_2.bias

        W_in = model.transformer.h[l].mlp.c_fc.weight
        state_dict[f"blocks.{l}.mlp.W_in"] = W_in
        state_dict[f"blocks.{l}.mlp.b_in"] = model.transformer.h[l].mlp.c_fc.bias

        W_out = model.transformer.h[l].mlp.c_proj.weight
        state_dict[f"blocks.{l}.mlp.W_out"] = W_out
        state_dict[f"blocks.{l}.mlp.b_out"] = model.transformer.h[l].mlp.c_proj.bias
    state_dict["unembed.W_U"] = model.lm_head.weight.T

    state_dict["ln_final.w"] = model.transformer.ln_f.weight
    state_dict["ln_final.b"] = model.transformer.ln_f.bias
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/gemma.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_gemma_weights(gemma, cfg: HookedTransformerConfig):
    state_dict = {}

    assert cfg.n_key_value_heads is not None  # keep mypy happy
    assert cfg.d_mlp is not None  # keep mypy happy

    # Gemma Models scale embeddings by multiplying by sqrt(d_model), use hidden state type to match
    # HF implementation
    state_dict["embed.W_E"] = gemma.model.embed_tokens.weight * torch.tensor(
        cfg.d_model**0.5, dtype=cfg.dtype
    )

    # Gemma has no biases anywhere
    for l in range(cfg.n_layers):
        # GemmaRMSNorm adds 1 to weights before multiplying by input, keep RMS calcs in float32
        state_dict[f"blocks.{l}.ln1.w"] = gemma.model.layers[
            l
        ].input_layernorm.weight.float() + torch.ones_like(
            gemma.model.layers[l].input_layernorm.weight, dtype=torch.float32
        )
        if cfg.use_normalization_before_and_after:
            # Only applies for Gemma 2
            state_dict[f"blocks.{l}.ln1_post.w"] = gemma.model.layers[
                l
            ].post_attention_layernorm.weight.float() + torch.ones_like(
                gemma.model.layers[l].input_layernorm.weight, dtype=torch.float32
            )

        W_Q = gemma.model.layers[l].self_attn.q_proj.weight
        W_K = gemma.model.layers[l].self_attn.k_proj.weight
        W_V = gemma.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=cfg.n_key_value_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=cfg.n_key_value_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn._W_K"] = W_K
        state_dict[f"blocks.{l}.attn._W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn._b_K"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )
        state_dict[f"blocks.{l}.attn._b_V"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )

        W_O = gemma.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        # GemmaRMSNorm adds 1 to weights before multiplying by input, keep RMS calcs in float32
        if not cfg.use_normalization_before_and_after:
            # Only applies for Gemma 1. Confusingly post_attention_layernorm is applied to mlp_input in Gemma 1 and attn_out in Gemma 2
            state_dict[f"blocks.{l}.ln2.w"] = gemma.model.layers[
                l
            ].post_attention_layernorm.weight.float() + torch.ones_like(
                gemma.model.norm.weight, dtype=torch.float32
            )
        else:
            # Only applies for Gemma 2
            state_dict[f"blocks.{l}.ln2.w"] = gemma.model.layers[
                l
            ].pre_feedforward_layernorm.weight.float() + torch.ones_like(
                gemma.model.layers[l].pre_feedforward_layernorm.weight, dtype=torch.float32
            )
            state_dict[f"blocks.{l}.ln2_post.w"] = gemma.model.layers[
                l
            ].post_feedforward_layernorm.weight.float() + torch.ones_like(
                gemma.model.layers[l].post_feedforward_layernorm.weight, dtype=torch.float32
            )

        state_dict[f"blocks.{l}.mlp.W_in"] = gemma.model.layers[l].mlp.up_proj.weight.T
        state_dict[f"blocks.{l}.mlp.W_gate"] = gemma.model.layers[l].mlp.gate_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(cfg.d_mlp, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.mlp.W_out"] = gemma.model.layers[l].mlp.down_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

    # GemmaRMSNorm adds 1 to weights before multiplying by input, keep RMS calcs in float32
    state_dict["ln_final.w"] = gemma.model.norm.weight.float() + torch.ones_like(
        gemma.model.norm.weight, dtype=torch.float32
    )

    state_dict["unembed.W_U"] = gemma.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/gpt2.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_gpt2_weights(gpt2, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = gpt2.transformer.wte.weight
    state_dict["pos_embed.W_pos"] = gpt2.transformer.wpe.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = gpt2.transformer.h[l].ln_1.weight
        state_dict[f"blocks.{l}.ln1.b"] = gpt2.transformer.h[l].ln_1.bias

        # In GPT-2, q,k,v are produced by one big linear map, whose output is
        # concat([q, k, v])
        W = gpt2.transformer.h[l].attn.c_attn.weight
        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)
        W_Q = einops.rearrange(W_Q, "m (i h)->i m h", i=cfg.n_heads)
        W_K = einops.rearrange(W_K, "m (i h)->i m h", i=cfg.n_heads)
        W_V = einops.rearrange(W_V, "m (i h)->i m h", i=cfg.n_heads)

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias
        qkv_bias = einops.rearrange(
            qkv_bias,
            "(qkv index head)->qkv index head",
            qkv=3,
            index=cfg.n_heads,
            head=cfg.d_head,
        )
        state_dict[f"blocks.{l}.attn.b_Q"] = qkv_bias[0]
        state_dict[f"blocks.{l}.attn.b_K"] = qkv_bias[1]
        state_dict[f"blocks.{l}.attn.b_V"] = qkv_bias[2]

        W_O = gpt2.transformer.h[l].attn.c_proj.weight
        W_O = einops.rearrange(W_O, "(i h) m->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = gpt2.transformer.h[l].attn.c_proj.bias

        state_dict[f"blocks.{l}.ln2.w"] = gpt2.transformer.h[l].ln_2.weight
        state_dict[f"blocks.{l}.ln2.b"] = gpt2.transformer.h[l].ln_2.bias

        W_in = gpt2.transformer.h[l].mlp.c_fc.weight
        state_dict[f"blocks.{l}.mlp.W_in"] = W_in
        state_dict[f"blocks.{l}.mlp.b_in"] = gpt2.transformer.h[l].mlp.c_fc.bias

        W_out = gpt2.transformer.h[l].mlp.c_proj.weight
        state_dict[f"blocks.{l}.mlp.W_out"] = W_out
        state_dict[f"blocks.{l}.mlp.b_out"] = gpt2.transformer.h[l].mlp.c_proj.bias
    state_dict["unembed.W_U"] = gpt2.lm_head.weight.T

    state_dict["ln_final.w"] = gpt2.transformer.ln_f.weight
    state_dict["ln_final.b"] = gpt2.transformer.ln_f.bias
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/gptj.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_gptj_weights(gptj, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = gptj.transformer.wte.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = gptj.transformer.h[l].ln_1.weight
        state_dict[f"blocks.{l}.ln1.b"] = gptj.transformer.h[l].ln_1.bias

        W_Q = gptj.transformer.h[l].attn.q_proj.weight
        W_K = gptj.transformer.h[l].attn.k_proj.weight
        W_V = gptj.transformer.h[l].attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)
        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn.b_K"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn.b_V"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)

        W_O = gptj.transformer.h[l].attn.out_proj.weight
        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        # Layer Norm 1 and 2 are tied.
        state_dict[f"blocks.{l}.ln2.w"] = state_dict[f"blocks.{l}.ln1.w"]
        state_dict[f"blocks.{l}.ln2.b"] = state_dict[f"blocks.{l}.ln1.b"]

        state_dict[f"blocks.{l}.mlp.W_in"] = gptj.transformer.h[l].mlp.fc_in.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = gptj.transformer.h[l].mlp.fc_in.bias

        state_dict[f"blocks.{l}.mlp.W_out"] = gptj.transformer.h[l].mlp.fc_out.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = gptj.transformer.h[l].mlp.fc_out.bias
    state_dict["ln_final.w"] = gptj.transformer.ln_f.weight
    state_dict["ln_final.b"] = gptj.transformer.ln_f.bias

    state_dict["unembed.W_U"] = gptj.lm_head.weight.T
    # Contains a bias, for some reason?
    state_dict["unembed.b_U"] = gptj.lm_head.bias
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/llama.py
---

from typing import cast

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_llama_weights(llama, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = llama.model.embed_tokens.weight

    # Some models with the Llama architecture use Grouped Query Attention, and so for these we need to modify
    # the state dict keys for the K/V attention weight/biases, prepending "_" to the key names.
    using_gqa = cfg.n_key_value_heads is not None
    gqa_uscore = "_" if using_gqa else ""
    # need a cast since MyPy isn't smart enough to realize that using_gqa implies n_key_value_heads is not None
    n_kv_heads = cast(int, cfg.n_key_value_heads if using_gqa else cfg.n_heads)

    # llama has no biases anywhere and deals with everything else roughly like
    # GPTNeoX with different names

    assert cfg.d_mlp is not None  # keep mypy happy

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = llama.model.layers[l].input_layernorm.weight

        W_Q = llama.model.layers[l].self_attn.q_proj.weight
        W_K = llama.model.layers[l].self_attn.k_proj.weight
        W_V = llama.model.layers[l].self_attn.v_proj.weight

        # in case of quantization,
        # parameters should stay as bitsandbytes.nn.modules.Params4bit
        if not cfg.load_in_4bit:
            W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
            W_K = einops.rearrange(W_K, "(n h) m->n m h", n=n_kv_heads)
            W_V = einops.rearrange(W_V, "(n h) m->n m h", n=n_kv_heads)

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_K"] = W_K
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(
            cfg.n_heads, cfg.d_head, dtype=cfg.dtype, device=cfg.device
        )
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_K"] = torch.zeros(
            n_kv_heads,
            cfg.d_head,
            dtype=cfg.dtype,
            device=cfg.device,
        )
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_V"] = torch.zeros(
            n_kv_heads,
            cfg.d_head,
            dtype=cfg.dtype,
            device=cfg.device,
        )

        W_O = llama.model.layers[l].self_attn.o_proj.weight

        if not cfg.load_in_4bit:
            W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)

        state_dict[f"blocks.{l}.attn.W_O"] = W_O.to(device=cfg.device)

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(
            cfg.d_model, dtype=cfg.dtype, device=cfg.device
        )

        state_dict[f"blocks.{l}.ln2.w"] = llama.model.layers[l].post_attention_layernorm.weight

        # in case of quantization,
        # parameters should stay as bitsandbytes.nn.modules.Params4bit
        if not cfg.load_in_4bit:
            state_dict[f"blocks.{l}.mlp.W_in"] = llama.model.layers[l].mlp.up_proj.weight.T
            state_dict[f"blocks.{l}.mlp.W_gate"] = llama.model.layers[l].mlp.gate_proj.weight.T
            state_dict[f"blocks.{l}.mlp.W_out"] = llama.model.layers[l].mlp.down_proj.weight.T
        else:
            state_dict[f"blocks.{l}.mlp.W_in"] = llama.model.layers[l].mlp.up_proj.weight
            state_dict[f"blocks.{l}.mlp.W_gate"] = llama.model.layers[l].mlp.gate_proj.weight
            state_dict[f"blocks.{l}.mlp.W_out"] = llama.model.layers[l].mlp.down_proj.weight

        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(
            cfg.d_mlp, dtype=cfg.dtype, device=cfg.device
        )
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(
            cfg.d_model, dtype=cfg.dtype, device=cfg.device
        )

    state_dict["ln_final.w"] = llama.model.norm.weight

    state_dict["unembed.W_U"] = llama.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype, device=cfg.device)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/mingpt.py
---

import einops

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_mingpt_weights(old_state_dict, cfg: HookedTransformerConfig):
    # mingpt (https://github.com/karpathy/minGPT) is mostly similar to GPT-2,
    # but doesn't concat the QKV matrices.
    state_dict = {}

    state_dict["embed.W_E"] = old_state_dict["tok_emb.weight"]
    state_dict["pos_embed.W_pos"] = old_state_dict["pos_emb"].squeeze()

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = old_state_dict[f"blocks.{l}.ln1.weight"]
        state_dict[f"blocks.{l}.ln1.b"] = old_state_dict[f"blocks.{l}.ln1.bias"]

        W_Q = old_state_dict[f"blocks.{l}.attn.query.weight"]
        W_K = old_state_dict[f"blocks.{l}.attn.key.weight"]
        W_V = old_state_dict[f"blocks.{l}.attn.value.weight"]
        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)
        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        q_bias = einops.rearrange(
            old_state_dict[f"blocks.{l}.attn.query.bias"], "(i h)->i h", i=cfg.n_heads
        )
        k_bias = einops.rearrange(
            old_state_dict[f"blocks.{l}.attn.key.bias"], "(i h)->i h", i=cfg.n_heads
        )
        v_bias = einops.rearrange(
            old_state_dict[f"blocks.{l}.attn.value.bias"], "(i h)->i h", i=cfg.n_heads
        )

        state_dict[f"blocks.{l}.attn.b_Q"] = q_bias
        state_dict[f"blocks.{l}.attn.b_K"] = k_bias
        state_dict[f"blocks.{l}.attn.b_V"] = v_bias

        W_O = old_state_dict[f"blocks.{l}.attn.proj.weight"]
        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = old_state_dict[f"blocks.{l}.attn.proj.bias"]

        state_dict[f"blocks.{l}.ln2.w"] = old_state_dict[f"blocks.{l}.ln2.weight"]
        state_dict[f"blocks.{l}.ln2.b"] = old_state_dict[f"blocks.{l}.ln2.bias"]

        W_in = old_state_dict[f"blocks.{l}.mlp.0.weight"]
        state_dict[f"blocks.{l}.mlp.W_in"] = W_in.T
        state_dict[f"blocks.{l}.mlp.b_in"] = old_state_dict[f"blocks.{l}.mlp.0.bias"]

        W_out = old_state_dict[f"blocks.{l}.mlp.2.weight"]
        state_dict[f"blocks.{l}.mlp.W_out"] = W_out.T
        state_dict[f"blocks.{l}.mlp.b_out"] = old_state_dict[f"blocks.{l}.mlp.2.bias"]

    state_dict["unembed.W_U"] = old_state_dict["head.weight"].T

    state_dict["ln_final.w"] = old_state_dict["ln_f.weight"]
    state_dict["ln_final.b"] = old_state_dict["ln_f.bias"]

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/mistral.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_mistral_weights(mistral, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = mistral.model.embed_tokens.weight

    assert cfg.n_key_value_heads is not None  # keep mypy happy
    assert cfg.d_mlp is not None  # keep mypy happy

    # Mistral has no biases anywhere
    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = mistral.model.layers[l].input_layernorm.weight

        W_Q = mistral.model.layers[l].self_attn.q_proj.weight
        W_K = mistral.model.layers[l].self_attn.k_proj.weight
        W_V = mistral.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=cfg.n_key_value_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=cfg.n_key_value_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn._W_K"] = W_K
        state_dict[f"blocks.{l}.attn._W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn._b_K"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )
        state_dict[f"blocks.{l}.attn._b_V"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )

        W_O = mistral.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = mistral.model.layers[l].post_attention_layernorm.weight

        state_dict[f"blocks.{l}.mlp.W_in"] = mistral.model.layers[l].mlp.up_proj.weight.T
        state_dict[f"blocks.{l}.mlp.W_gate"] = mistral.model.layers[l].mlp.gate_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(cfg.d_mlp, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.mlp.W_out"] = mistral.model.layers[l].mlp.down_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

    state_dict["ln_final.w"] = mistral.model.norm.weight

    state_dict["unembed.W_U"] = mistral.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/mixtral.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_mixtral_weights(mixtral, cfg: HookedTransformerConfig):
    # The same as Mistral, but with the MLP replaced with MoE
    # As with Mistral, Mixtral has no biases

    state_dict = {}

    assert cfg.n_key_value_heads is not None  # keep mypy happy
    assert cfg.d_mlp is not None
    assert cfg.num_experts is not None

    state_dict["embed.W_E"] = mixtral.model.embed_tokens.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = mixtral.model.layers[l].input_layernorm.weight

        W_Q = mixtral.model.layers[l].self_attn.q_proj.weight
        W_K = mixtral.model.layers[l].self_attn.k_proj.weight
        W_V = mixtral.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=cfg.n_key_value_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=cfg.n_key_value_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn._W_K"] = W_K
        state_dict[f"blocks.{l}.attn._W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn._b_K"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )
        state_dict[f"blocks.{l}.attn._b_V"] = torch.zeros(
            cfg.n_key_value_heads, cfg.d_head, dtype=cfg.dtype
        )

        W_O = mixtral.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = mixtral.model.layers[l].post_attention_layernorm.weight

        state_dict[f"blocks.{l}.mlp.W_gate.weight"] = mixtral.model.layers[
            l
        ].block_sparse_moe.gate.weight

        # The mapping here from wn to W_{in/out/gate} is a bit confusing:
        # w1 -> W_gate
        # w2 -> W_out
        # w3 -> W_in
        # See https://github.com/mistralai/mistral-inference/blob/8598cf582091a596671be31990448e0620017851/mistral/model.py#L128 for reference
        for e in range(cfg.num_experts):
            state_dict[f"blocks.{l}.mlp.experts.{e}.W_in.weight"] = (
                mixtral.model.layers[l].block_sparse_moe.experts[e].w3.weight
            )
            state_dict[f"blocks.{l}.mlp.experts.{e}.W_gate.weight"] = (
                mixtral.model.layers[l].block_sparse_moe.experts[e].w1.weight
            )
            state_dict[f"blocks.{l}.mlp.experts.{e}.W_out.weight"] = (
                mixtral.model.layers[l].block_sparse_moe.experts[e].w2.weight
            )

    state_dict["ln_final.w"] = mixtral.model.norm.weight.data

    state_dict["unembed.W_U"] = mixtral.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/nanogpt.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_nanogpt_weights(old_state_dict, cfg: HookedTransformerConfig):
    """For https://github.com/karpathy/nanoGPT
    There are two complications with converting nanogpt models:
    The first is that some state dicts have an unwanted prefix on keys that needs to be removed.
    The second is that the models can be saved with or without bias. By default, there
    is no bias. This function can handle both cases."""
    # Nanogpt models saved after torch.compile() have this unwanted prefix
    # This is a simple way to remove it
    unwanted_prefix = "_orig_mod."
    for k, v in list(old_state_dict.items()):
        if k.startswith(unwanted_prefix):
            old_state_dict[k[len(unwanted_prefix) :]] = old_state_dict.pop(k)

    new_state_dict = {}
    new_state_dict["pos_embed.W_pos"] = old_state_dict["transformer.wpe.weight"]
    new_state_dict["embed.W_E"] = old_state_dict["transformer.wte.weight"]

    new_state_dict["ln_final.w"] = old_state_dict["transformer.ln_f.weight"]
    new_state_dict["ln_final.b"] = torch.zeros_like(old_state_dict["transformer.ln_f.weight"])
    new_state_dict["unembed.W_U"] = old_state_dict["lm_head.weight"].T

    bias = False
    if "transformer.ln_f.bias" in old_state_dict:
        bias = True
        new_state_dict["ln_final.b"] = old_state_dict["transformer.ln_f.bias"]

    for layer in range(cfg.n_layers):
        layer_key = f"transformer.h.{layer}"

        new_state_dict[f"blocks.{layer}.ln1.w"] = old_state_dict[f"{layer_key}.ln_1.weight"]
        # A bias of zeros is required for folding layer norm
        new_state_dict[f"blocks.{layer}.ln1.b"] = torch.zeros_like(
            old_state_dict[f"{layer_key}.ln_1.weight"]
        )
        new_state_dict[f"blocks.{layer}.ln2.w"] = old_state_dict[f"{layer_key}.ln_2.weight"]
        new_state_dict[f"blocks.{layer}.ln2.b"] = torch.zeros_like(
            old_state_dict[f"{layer_key}.ln_2.weight"]
        )

        W = old_state_dict[f"{layer_key}.attn.c_attn.weight"]
        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=0)
        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)
        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)
        new_state_dict[f"blocks.{layer}.attn.W_Q"] = W_Q
        new_state_dict[f"blocks.{layer}.attn.W_K"] = W_K
        new_state_dict[f"blocks.{layer}.attn.W_V"] = W_V

        W_O = old_state_dict[f"{layer_key}.attn.c_proj.weight"]
        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)
        new_state_dict[f"blocks.{layer}.attn.W_O"] = W_O

        new_state_dict[f"blocks.{layer}.mlp.W_in"] = old_state_dict[
            f"{layer_key}.mlp.c_fc.weight"
        ].T
        new_state_dict[f"blocks.{layer}.mlp.W_out"] = old_state_dict[
            f"{layer_key}.mlp.c_proj.weight"
        ].T

        if bias:
            new_state_dict[f"blocks.{layer}.ln1.b"] = old_state_dict[f"{layer_key}.ln_1.bias"]
            new_state_dict[f"blocks.{layer}.ln2.b"] = old_state_dict[f"{layer_key}.ln_2.bias"]
            new_state_dict[f"blocks.{layer}.mlp.b_in"] = old_state_dict[
                f"{layer_key}.mlp.c_fc.bias"
            ]
            new_state_dict[f"blocks.{layer}.mlp.b_out"] = old_state_dict[
                f"{layer_key}.mlp.c_proj.bias"
            ]

            B = old_state_dict[f"{layer_key}.attn.c_attn.bias"]
            B_Q, B_K, B_V = torch.tensor_split(B, 3, dim=0)
            B_Q = einops.rearrange(B_Q, "(i h)->i h", i=cfg.n_heads)
            B_K = einops.rearrange(B_K, "(i h)->i h", i=cfg.n_heads)
            B_V = einops.rearrange(B_V, "(i h)->i h", i=cfg.n_heads)
            new_state_dict[f"blocks.{layer}.attn.b_Q"] = B_Q
            new_state_dict[f"blocks.{layer}.attn.b_K"] = B_K
            new_state_dict[f"blocks.{layer}.attn.b_V"] = B_V
            new_state_dict[f"blocks.{layer}.attn.b_O"] = old_state_dict[
                f"{layer_key}.attn.c_proj.bias"
            ]

    return new_state_dict



---
File: /transformer_lens/pretrained/weight_conversions/neel_solu_old.py
---

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_neel_solu_old_weights(state_dict: dict, cfg: HookedTransformerConfig):
    """
    Converts the weights of my old SoLU models to the HookedTransformer format.
    Takes as input a state dict, *not* a model object.

    There are a bunch of dumb bugs in the original code, sorry!

    Models 1L, 2L, 4L and 6L have left facing weights (ie, weights have shape
    [dim_out, dim_in]) while HookedTransformer does right facing (ie [dim_in,
    dim_out]).

    8L has *just* a left facing W_pos, the rest right facing.

    And some models were trained with
    """
    # Early models have left facing W_pos
    reverse_pos = cfg.n_layers <= 8

    # Models prior to 8L have left facing everything (8L has JUST left facing W_pos - sorry! Stupid bug)
    reverse_weights = cfg.n_layers <= 6

    new_state_dict = {}
    for k, v in state_dict.items():
        k = k.replace("norm", "ln")
        if k.startswith("ln."):
            k = k.replace("ln.", "ln_final.")
        new_state_dict[k] = v

    if reverse_pos:
        new_state_dict["pos_embed.W_pos"] = new_state_dict["pos_embed.W_pos"].T
    if reverse_weights:
        for k, v in new_state_dict.items():
            if "W_" in k and "W_pos" not in k:
                new_state_dict[k] = v.transpose(-2, -1)
    return new_state_dict



---
File: /transformer_lens/pretrained/weight_conversions/neo.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_neo_weights(neo, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = neo.transformer.wte.weight
    state_dict["pos_embed.W_pos"] = neo.transformer.wpe.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = neo.transformer.h[l].ln_1.weight
        state_dict[f"blocks.{l}.ln1.b"] = neo.transformer.h[l].ln_1.bias

        W_Q = neo.transformer.h[l].attn.attention.q_proj.weight
        W_K = neo.transformer.h[l].attn.attention.k_proj.weight
        W_V = neo.transformer.h[l].attn.attention.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(i h) m->i m h", i=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(i h) m->i m h", i=cfg.n_heads)
        W_V = einops.rearrange(W_V, "(i h) m->i m h", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn.b_K"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn.b_V"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)

        W_O = neo.transformer.h[l].attn.attention.out_proj.weight
        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = neo.transformer.h[l].attn.attention.out_proj.bias

        state_dict[f"blocks.{l}.ln2.w"] = neo.transformer.h[l].ln_2.weight
        state_dict[f"blocks.{l}.ln2.b"] = neo.transformer.h[l].ln_2.bias

        state_dict[f"blocks.{l}.mlp.W_in"] = neo.transformer.h[l].mlp.c_fc.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = neo.transformer.h[l].mlp.c_fc.bias

        state_dict[f"blocks.{l}.mlp.W_out"] = neo.transformer.h[l].mlp.c_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = neo.transformer.h[l].mlp.c_proj.bias
    state_dict["ln_final.w"] = neo.transformer.ln_f.weight
    state_dict["ln_final.b"] = neo.transformer.ln_f.bias

    state_dict["unembed.W_U"] = neo.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/neox.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_neox_weights(neox, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = neox.gpt_neox.embed_in.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = neox.gpt_neox.layers[l].input_layernorm.weight
        state_dict[f"blocks.{l}.ln1.b"] = neox.gpt_neox.layers[l].input_layernorm.bias

        # For some inexplicable reason, NeoX both uses the concatenated QKV
        # matmul of GPT-2 (afaict this has a neglible performance impact) AND
        # has the flattened axis in the DIFFERENT order of (head_index qkv
        # d_head) - this took me an hour to debug...
        W = neox.gpt_neox.layers[l].attention.query_key_value.weight
        W = einops.rearrange(W, "(i qkv h) m->qkv i m h", i=cfg.n_heads, qkv=3)

        # Fold in layer norm weights
        state_dict[f"blocks.{l}.attn.W_Q"] = W[0]
        state_dict[f"blocks.{l}.attn.W_K"] = W[1]
        state_dict[f"blocks.{l}.attn.W_V"] = W[2]

        qkv_bias = neox.gpt_neox.layers[l].attention.query_key_value.bias
        qkv_bias = einops.rearrange(
            qkv_bias,
            "(index qkv head)->qkv index head",
            qkv=3,
            index=cfg.n_heads,
            head=cfg.d_head,
        )
        # Fold in layer norm biases
        state_dict[f"blocks.{l}.attn.b_Q"] = qkv_bias[0]
        state_dict[f"blocks.{l}.attn.b_K"] = qkv_bias[1]
        state_dict[f"blocks.{l}.attn.b_V"] = qkv_bias[2]

        W_O = neox.gpt_neox.layers[l].attention.dense.weight
        W_O = einops.rearrange(W_O, "m (i h)->i h m", i=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = neox.gpt_neox.layers[l].attention.dense.bias

        state_dict[f"blocks.{l}.ln2.w"] = neox.gpt_neox.layers[l].post_attention_layernorm.weight
        state_dict[f"blocks.{l}.ln2.b"] = neox.gpt_neox.layers[l].post_attention_layernorm.bias

        state_dict[f"blocks.{l}.mlp.W_in"] = neox.gpt_neox.layers[l].mlp.dense_h_to_4h.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = neox.gpt_neox.layers[l].mlp.dense_h_to_4h.bias

        state_dict[f"blocks.{l}.mlp.W_out"] = neox.gpt_neox.layers[l].mlp.dense_4h_to_h.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = neox.gpt_neox.layers[l].mlp.dense_4h_to_h.bias
    state_dict["ln_final.w"] = neox.gpt_neox.final_layer_norm.weight
    state_dict["ln_final.b"] = neox.gpt_neox.final_layer_norm.bias

    state_dict["unembed.W_U"] = neox.embed_out.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/opt.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_opt_weights(opt, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = opt.model.decoder.embed_tokens.weight
    state_dict["pos_embed.W_pos"] = opt.model.decoder.embed_positions.weight[2:, :]

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = opt.model.decoder.layers[l].self_attn_layer_norm.weight
        state_dict[f"blocks.{l}.ln1.b"] = opt.model.decoder.layers[l].self_attn_layer_norm.bias

        W_Q = opt.model.decoder.layers[l].self_attn.q_proj.weight
        W_K = opt.model.decoder.layers[l].self_attn.k_proj.weight
        W_V = opt.model.decoder.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(
            W_Q,
            "(index d_head) d_model->index d_model d_head",
            index=cfg.n_heads,
        )
        W_K = einops.rearrange(
            W_K,
            "(index d_head) d_model->index d_model d_head",
            index=cfg.n_heads,
        )
        W_V = einops.rearrange(
            W_V,
            "(index d_head) d_model->index d_model d_head",
            index=cfg.n_heads,
        )

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        q_bias = einops.rearrange(
            opt.model.decoder.layers[l].self_attn.q_proj.bias,
            "(head_index d_head)->head_index d_head",
            head_index=cfg.n_heads,
            d_head=cfg.d_head,
        )
        k_bias = einops.rearrange(
            opt.model.decoder.layers[l].self_attn.k_proj.bias,
            "(head_index d_head)->head_index d_head",
            head_index=cfg.n_heads,
            d_head=cfg.d_head,
        )
        v_bias = einops.rearrange(
            opt.model.decoder.layers[l].self_attn.v_proj.bias,
            "(head_index d_head)->head_index d_head",
            head_index=cfg.n_heads,
            d_head=cfg.d_head,
        )

        state_dict[f"blocks.{l}.attn.b_Q"] = q_bias
        state_dict[f"blocks.{l}.attn.b_K"] = k_bias
        state_dict[f"blocks.{l}.attn.b_V"] = v_bias

        W_O = opt.model.decoder.layers[l].self_attn.out_proj.weight
        W_O = einops.rearrange(
            W_O,
            "d_model (index d_head)->index d_head d_model",
            index=cfg.n_heads,
        )
        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = opt.model.decoder.layers[l].self_attn.out_proj.bias

        state_dict[f"blocks.{l}.ln2.w"] = opt.model.decoder.layers[l].final_layer_norm.weight
        state_dict[f"blocks.{l}.ln2.b"] = opt.model.decoder.layers[l].final_layer_norm.bias

        state_dict[f"blocks.{l}.mlp.W_in"] = opt.model.decoder.layers[l].fc1.weight.T
        state_dict[f"blocks.{l}.mlp.W_out"] = opt.model.decoder.layers[l].fc2.weight.T

        state_dict[f"blocks.{l}.mlp.b_in"] = opt.model.decoder.layers[l].fc1.bias
        state_dict[f"blocks.{l}.mlp.b_out"] = opt.model.decoder.layers[l].fc2.bias
    state_dict["ln_final.w"] = opt.model.decoder.final_layer_norm.weight
    state_dict["ln_final.b"] = opt.model.decoder.final_layer_norm.bias
    state_dict["unembed.W_U"] = opt.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)
    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/phi.py
---

import einops

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_phi_weights(phi, cfg: HookedTransformerConfig):
    state_dict = {}

    state_dict["embed.W_E"] = phi.model.embed_tokens.weight

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = phi.model.layers[l].input_layernorm.weight
        state_dict[f"blocks.{l}.ln1.b"] = phi.model.layers[l].input_layernorm.bias

        W_Q = phi.model.layers[l].self_attn.q_proj.weight
        W_K = phi.model.layers[l].self_attn.k_proj.weight
        W_V = phi.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(
            W_Q, "(n_head d_head) d_model -> n_head d_model d_head", n_head=cfg.n_heads
        )
        W_K = einops.rearrange(
            W_K, "(n_head d_head) d_model  -> n_head d_model d_head", n_head=cfg.n_heads
        )
        W_V = einops.rearrange(
            W_V, "(n_head d_head) d_model  -> n_head d_model d_head", n_head=cfg.n_heads
        )
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        b_Q = phi.model.layers[l].self_attn.q_proj.bias
        b_K = phi.model.layers[l].self_attn.k_proj.bias
        b_V = phi.model.layers[l].self_attn.v_proj.bias
        b_Q = einops.rearrange(b_Q, "(n_head d_head) -> n_head d_head", n_head=cfg.n_heads)
        b_K = einops.rearrange(b_K, "(n_head d_head) -> n_head d_head", n_head=cfg.n_heads)
        b_V = einops.rearrange(b_V, "(n_head d_head) -> n_head d_head", n_head=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.b_Q"] = b_Q
        state_dict[f"blocks.{l}.attn.b_K"] = b_K
        state_dict[f"blocks.{l}.attn.b_V"] = b_V

        W_O = phi.model.layers[l].self_attn.dense.weight
        W_O = einops.rearrange(
            W_O, "d_model (n_head d_head) -> n_head d_head d_model", n_head=cfg.n_heads
        )

        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = phi.model.layers[l].self_attn.dense.bias

        # Layer Norm 1 and 2 are tied.
        state_dict[f"blocks.{l}.ln2.w"] = state_dict[f"blocks.{l}.ln1.w"]
        state_dict[f"blocks.{l}.ln2.b"] = state_dict[f"blocks.{l}.ln1.b"]

        state_dict[f"blocks.{l}.mlp.W_in"] = phi.model.layers[l].mlp.fc1.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = phi.model.layers[l].mlp.fc1.bias
        state_dict[f"blocks.{l}.mlp.W_out"] = phi.model.layers[l].mlp.fc2.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = phi.model.layers[l].mlp.fc2.bias

    state_dict["ln_final.w"] = phi.model.final_layernorm.weight
    state_dict["ln_final.b"] = phi.model.final_layernorm.bias

    state_dict["unembed.W_U"] = phi.lm_head.weight.T
    state_dict["unembed.b_U"] = phi.lm_head.bias

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/phi3.py
---

from typing import cast

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_phi3_weights(phi, cfg: HookedTransformerConfig):
    state_dict = {}
    state_dict["embed.W_E"] = phi.model.embed_tokens.weight

    # Some models with this architecture use Grouped Query Attention, and so for these we need to modify
    # the state dict keys for the K/V attention weight/biases, prepending "_" to the key names.
    using_gqa = cfg.n_key_value_heads is not None
    gqa_uscore = "_" if using_gqa else ""
    # need a cast since MyPy isn't smart enough to realize that using_gqa implies n_key_value_heads is not None
    n_kv_heads = cast(int, cfg.n_key_value_heads if using_gqa else cfg.n_heads)

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = phi.model.layers[l].input_layernorm.weight
        state_dict[f"blocks.{l}.ln1.b"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

        W = phi.model.layers[l].self_attn.qkv_proj.weight
        q_dim = cfg.n_heads * cfg.d_head
        kv_dim = n_kv_heads * cfg.d_head
        W_Q, W_K, W_V = W.split([q_dim, kv_dim, kv_dim], dim=0)

        W_Q = einops.rearrange(
            W_Q, "(n_head d_head) d_model -> n_head d_model d_head", n_head=cfg.n_heads
        )
        W_K = einops.rearrange(
            W_K, "(n_kv_head d_head) d_model -> n_kv_head d_model d_head", n_kv_head=n_kv_heads
        )
        W_V = einops.rearrange(
            W_V, "(n_kv_head d_head) d_model -> n_kv_head d_model d_head", n_kv_head=n_kv_heads
        )
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_K"] = W_K
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_V"] = W_V

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(
            cfg.n_heads, cfg.d_head, dtype=cfg.dtype, device=cfg.device
        )
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_K"] = torch.zeros(
            n_kv_heads,
            cfg.d_head,
            dtype=cfg.dtype,
        )
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_V"] = torch.zeros(
            n_kv_heads,
            cfg.d_head,
            dtype=cfg.dtype,
        )

        W_O = phi.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(
            W_O, "d_model (n_head d_head) -> n_head d_head d_model", n_head=cfg.n_heads
        )

        state_dict[f"blocks.{l}.attn.W_O"] = W_O
        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = phi.model.layers[l].post_attention_layernorm.weight
        state_dict[f"blocks.{l}.ln2.b"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

        W = phi.model.layers[l].mlp.gate_up_proj.weight.T
        W_gate, W_in = torch.tensor_split(W, 2, dim=1)
        state_dict[f"blocks.{l}.mlp.W_in"] = W_in
        state_dict[f"blocks.{l}.mlp.W_gate"] = W_gate
        state_dict[f"blocks.{l}.mlp.W_out"] = phi.model.layers[l].mlp.down_proj.weight.T

    state_dict["ln_final.w"] = phi.model.norm.weight

    state_dict["unembed.W_U"] = phi.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/qwen.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_qwen_weights(qwen, cfg: HookedTransformerConfig):
    state_dict = {}
    model = qwen.transformer
    state_dict["embed.W_E"] = model.wte.weight

    assert cfg.d_mlp is not None  # keep mypy happy

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = model.h[l].ln_1.weight

        W_Q, W_K, W_V = model.h[l].attn.c_attn.weight.split(split_size=cfg.d_model, dim=0)
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=cfg.n_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.W_K"] = W_K
        state_dict[f"blocks.{l}.attn.W_V"] = W_V

        b_Q, b_K, b_V = model.h[l].attn.c_attn.bias.split(split_size=cfg.d_model, dim=0)
        b_Q = einops.rearrange(
            b_Q,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_heads,
        )
        b_K = einops.rearrange(
            b_K,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_heads,
        )
        b_V = einops.rearrange(
            b_V,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_heads,
        )
        state_dict[f"blocks.{l}.attn.b_Q"] = b_Q
        state_dict[f"blocks.{l}.attn.b_K"] = b_K
        state_dict[f"blocks.{l}.attn.b_V"] = b_V

        W_O = model.h[l].attn.c_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = model.h[l].ln_2.weight

        state_dict[f"blocks.{l}.mlp.W_in"] = model.h[l].mlp.w1.weight.T
        state_dict[f"blocks.{l}.mlp.W_gate"] = model.h[l].mlp.w2.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(cfg.d_mlp, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.mlp.W_out"] = model.h[l].mlp.c_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

    state_dict["ln_final.w"] = model.ln_f.weight

    state_dict["unembed.W_U"] = qwen.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/qwen2.py
---

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_qwen2_weights(qwen, cfg: HookedTransformerConfig):
    # Note that this method is also applied for Qwen1.5 models, since they
    # have architecture type Qwen2ForCausalLM.

    state_dict = {}

    state_dict["embed.W_E"] = qwen.model.embed_tokens.weight

    assert cfg.d_mlp is not None  # keep mypy happy

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = qwen.model.layers[l].input_layernorm.weight

        W_Q = qwen.model.layers[l].self_attn.q_proj.weight
        W_K = qwen.model.layers[l].self_attn.k_proj.weight
        W_V = qwen.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=cfg.n_key_value_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=cfg.n_key_value_heads)

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn._W_K"] = W_K
        state_dict[f"blocks.{l}.attn._W_V"] = W_V

        b_Q = qwen.model.layers[l].self_attn.q_proj.bias
        b_Q = einops.rearrange(
            b_Q,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_heads,
        )

        b_K = qwen.model.layers[l].self_attn.k_proj.bias
        b_K = einops.rearrange(
            b_K,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_key_value_heads,
        )

        b_V = qwen.model.layers[l].self_attn.v_proj.bias
        b_V = einops.rearrange(
            b_V,
            "(n_head d_head) -> n_head d_head",
            n_head=cfg.n_key_value_heads,
        )

        state_dict[f"blocks.{l}.attn.b_Q"] = b_Q
        state_dict[f"blocks.{l}.attn._b_K"] = b_K
        state_dict[f"blocks.{l}.attn._b_V"] = b_V

        W_O = qwen.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = qwen.model.layers[l].post_attention_layernorm.weight

        state_dict[f"blocks.{l}.mlp.W_in"] = qwen.model.layers[l].mlp.up_proj.weight.T
        state_dict[f"blocks.{l}.mlp.W_gate"] = qwen.model.layers[l].mlp.gate_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(cfg.d_mlp, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.mlp.W_out"] = qwen.model.layers[l].mlp.down_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

    state_dict["ln_final.w"] = qwen.model.norm.weight

    state_dict["unembed.W_U"] = qwen.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/qwen3.py
---

from typing import Any

import einops
import torch

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_qwen3_weights(qwen: Any, cfg: HookedTransformerConfig):
    """Convert Qwen3 weights to TransformerLens format."""
    state_dict = {}

    state_dict["embed.W_E"] = qwen.model.embed_tokens.weight

    if cfg.n_key_value_heads is None:
        gqa_uscore = ""
        n_kv_heads = cfg.n_heads
    else:
        gqa_uscore = "_"
        n_kv_heads = cfg.n_key_value_heads

    assert cfg.d_mlp is not None  # keep mypy happy

    for l in range(cfg.n_layers):
        state_dict[f"blocks.{l}.ln1.w"] = qwen.model.layers[l].input_layernorm.weight

        W_Q = qwen.model.layers[l].self_attn.q_proj.weight
        W_K = qwen.model.layers[l].self_attn.k_proj.weight
        W_V = qwen.model.layers[l].self_attn.v_proj.weight
        W_Q = einops.rearrange(W_Q, "(n h) m->n m h", n=cfg.n_heads)
        W_K = einops.rearrange(W_K, "(n h) m->n m h", n=n_kv_heads)
        W_V = einops.rearrange(W_V, "(n h) m->n m h", n=n_kv_heads)

        state_dict[f"blocks.{l}.attn.W_Q"] = W_Q
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_K"] = W_K
        state_dict[f"blocks.{l}.attn.{gqa_uscore}W_V"] = W_V

        # Load weights into RMSNorm modules
        state_dict[f"blocks.{l}.attn.q_norm.w"] = qwen.model.layers[l].self_attn.q_norm.weight
        state_dict[f"blocks.{l}.attn.k_norm.w"] = qwen.model.layers[l].self_attn.k_norm.weight

        state_dict[f"blocks.{l}.attn.b_Q"] = torch.zeros(cfg.n_heads, cfg.d_head, dtype=cfg.dtype)
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_K"] = torch.zeros(
            n_kv_heads, cfg.d_head, dtype=cfg.dtype
        )
        state_dict[f"blocks.{l}.attn.{gqa_uscore}b_V"] = torch.zeros(
            n_kv_heads, cfg.d_head, dtype=cfg.dtype
        )

        W_O = qwen.model.layers[l].self_attn.o_proj.weight
        W_O = einops.rearrange(W_O, "m (n h)->n h m", n=cfg.n_heads)
        state_dict[f"blocks.{l}.attn.W_O"] = W_O

        state_dict[f"blocks.{l}.attn.b_O"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.ln2.w"] = qwen.model.layers[l].post_attention_layernorm.weight

        state_dict[f"blocks.{l}.mlp.W_in"] = qwen.model.layers[l].mlp.up_proj.weight.T
        state_dict[f"blocks.{l}.mlp.W_gate"] = qwen.model.layers[l].mlp.gate_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_in"] = torch.zeros(cfg.d_mlp, dtype=cfg.dtype)

        state_dict[f"blocks.{l}.mlp.W_out"] = qwen.model.layers[l].mlp.down_proj.weight.T
        state_dict[f"blocks.{l}.mlp.b_out"] = torch.zeros(cfg.d_model, dtype=cfg.dtype)

    state_dict["ln_final.w"] = qwen.model.norm.weight

    state_dict["unembed.W_U"] = qwen.lm_head.weight.T
    state_dict["unembed.b_U"] = torch.zeros(cfg.d_vocab, dtype=cfg.dtype)

    return state_dict



---
File: /transformer_lens/pretrained/weight_conversions/t5.py
---

import einops

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig


def convert_t5_weights(t5, cfg: HookedTransformerConfig):
    state_dict = {
        "embed.W_E": t5.encoder.embed_tokens.weight,
        "unembed.W_U": t5.encoder.embed_tokens.weight.T,
        "encoder.0.attn.rel_pos_bias.weight": t5.encoder.block[0]
        .layer[0]
        .SelfAttention.relative_attention_bias.weight,
    }

    for l in range(cfg.n_layers):
        block = t5.encoder.block[l]
        state_dict[f"encoder.{l}.attn.W_Q"] = einops.rearrange(
            block.layer[0].SelfAttention.q.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"encoder.{l}.attn.W_K"] = einops.rearrange(
            block.layer[0].SelfAttention.k.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"encoder.{l}.attn.W_V"] = einops.rearrange(
            block.layer[0].SelfAttention.v.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"encoder.{l}.attn.W_O"] = einops.rearrange(
            block.layer[0].SelfAttention.o.weight,
            "m (i h) -> i h m",
            i=cfg.n_heads,
        )
        state_dict[f"encoder.{l}.ln1.w"] = block.layer[0].layer_norm.weight

        # fixme DenseReluDense may be T5DenseGatedActDense instead
        state_dict[f"encoder.{l}.mlp.W_in"] = einops.rearrange(
            block.layer[1].DenseReluDense.wi.weight, "mlp model -> model mlp"
        )

        state_dict[f"encoder.{l}.mlp.W_out"] = einops.rearrange(
            block.layer[1].DenseReluDense.wo.weight, "model mlp -> mlp model"
        )
        state_dict[f"encoder.{l}.ln2.w"] = block.layer[1].layer_norm.weight

    state_dict["encoder_final_ln.w"] = t5.encoder.final_layer_norm.weight

    state_dict["decoder.0.attn.rel_pos_bias.weight"] = (
        t5.decoder.block[0].layer[0].SelfAttention.relative_attention_bias.weight
    )

    for l in range(cfg.n_layers):
        block = t5.decoder.block[l]
        state_dict[f"decoder.{l}.attn.W_Q"] = einops.rearrange(
            block.layer[0].SelfAttention.q.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"decoder.{l}.attn.W_K"] = einops.rearrange(
            block.layer[0].SelfAttention.k.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"decoder.{l}.attn.W_V"] = einops.rearrange(
            block.layer[0].SelfAttention.v.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"decoder.{l}.attn.W_O"] = einops.rearrange(
            block.layer[0].SelfAttention.o.weight,
            "m (i h) -> i h m",
            i=cfg.n_heads,
        )

        state_dict[f"decoder.{l}.ln1.w"] = block.layer[0].layer_norm.weight

        state_dict[f"decoder.{l}.cross_attn.W_Q"] = einops.rearrange(
            block.layer[1].EncDecAttention.q.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"decoder.{l}.cross_attn.W_K"] = einops.rearrange(
            block.layer[1].EncDecAttention.k.weight, "(i h) m -> i m h", i=cfg.n_heads
        )

        state_dict[f"decoder.{l}.cross_attn.W_V"] = einops.rearrange(
            block.layer[1].EncDecAttention.v.weight, "(i h) m -> i m h", i=cfg.n_heads
        )
        state_dict[f"decoder.{l}.cross_attn.W_O"] = einops.rearrange(
            block.layer[1].EncDecAttention.o.weight,
            "m (i h) -> i h m",
            i=cfg.n_heads,
        )
        state_dict[f"decoder.{l}.ln2.w"] = block.layer[1].layer_norm.weight

        # fixme DenseReluDense may be T5DenseGatedActDense instead
        state_dict[f"decoder.{l}.mlp.W_in"] = einops.rearrange(
            block.layer[2].DenseReluDense.wi.weight, "mlp model -> model mlp"
        )
        state_dict[f"decoder.{l}.mlp.W_out"] = einops.rearrange(
            block.layer[2].DenseReluDense.wo.weight, "model mlp -> mlp model"
        )
        state_dict[f"decoder.{l}.ln3.w"] = block.layer[2].layer_norm.weight

    state_dict["decoder_final_ln.w"] = t5.decoder.final_layer_norm.weight

    return state_dict



---
File: /transformer_lens/pretrained/__init__.py
---




---
File: /transformer_lens/utilities/__init__.py
---




---
File: /transformer_lens/utilities/activation_functions.py
---

"""Activation Functions.

Utilities for interacting with all supported activation functions.
"""
from typing import Callable, Dict

import torch
import torch.nn.functional as F

from transformer_lens.utils import gelu_fast, gelu_new, solu

# Convenient type for the format of each activation function
ActivationFunction = Callable[..., torch.Tensor]

# All currently supported activation functions. To add a new function, simply
# put the name of the function as the key, and the value as the actual callable.
SUPPORTED_ACTIVATIONS: Dict[str, ActivationFunction] = {
    "solu": solu,
    "solu_ln": solu,
    "gelu_new": gelu_new,
    "gelu_fast": gelu_fast,
    "silu": F.silu,
    "relu": F.relu,
    "gelu": F.gelu,
    "gelu_pytorch_tanh": lambda tensor: F.gelu(tensor, approximate="tanh"),
}



---
File: /transformer_lens/utilities/addmm.py
---

"""Addmm

Implementations of Addmm functions matching Huggingface implementations.
"""
import torch
from jaxtyping import Float


def vanilla_addmm(
    input: Float[torch.Tensor, "... #o"],  # Must be broadcastable to "m o"
    mat1: Float[torch.Tensor, "m n"],
    mat2: Float[torch.Tensor, "n o"],
) -> Float[torch.Tensor, "m o"]:
    """Typechecked version of torch.addmm.

    Note that both mat1 and mat2 *must* be 2d matrices.
    """
    return torch.addmm(input, mat1, mat2)


def batch_addmm(
    bias: Float[torch.Tensor, "... #d_out"],  # Must be broadcastable to "... d_out"
    weight: Float[torch.Tensor, "d_in d_out"],
    x: Float[torch.Tensor, "... d_in"],
) -> Float[torch.Tensor, "... d_out"]:
    """Fused add-multiply with support for batch dimensions.

    Must match the Huggingface Conv1D implementation exactly.
    https://github.com/huggingface/transformers/blob/9ba9369a2557e53a01378199a9839ec6e82d8bc7/src/transformers/pytorch_utils.py#L102-L106
    """
    n_output_features = weight.shape[-1]
    size_out = x.size()[:-1] + (n_output_features,)
    x = vanilla_addmm(bias, x.view(-1, x.size(-1)), weight)
    x = x.view(size_out)
    return x



---
File: /transformer_lens/utilities/attention.py
---

"""Attention.

Utilities for attention components.
"""

import einops
import torch
import torch.nn.functional as F
from jaxtyping import Float


def simple_attn_linear(
    input: Float[torch.Tensor, "batch pos d_model"],
    w: Float[torch.Tensor, "head_index d_model d_head"],
    b: Float[torch.Tensor, "head_index d_head"],
) -> Float[torch.Tensor, "batch pos head_index d_head"]:
    """Linear layer for attention calculation."""

    if input.device != w.device:
        w = w.to(input.device)
    if input.device != b.device:
        b = b.to(input.device)

    w = einops.rearrange(w, "head_index d_model d_head -> (head_index d_head) d_model")
    b_ = einops.rearrange(b, "head_index d_head -> (head_index d_head)")

    return F.linear(input, w, b_).reshape(input.shape[0], input.shape[1], b.shape[0], b.shape[1])


def complex_attn_linear(
    input: Float[torch.Tensor, "batch pos head_index d_model"],
    w: Float[torch.Tensor, "head_index d_model d_head"],
    b: Float[torch.Tensor, "head_index d_head"],
) -> Float[torch.Tensor, "batch pos head_index d_head"]:
    """Linear layer for attention calculation.

    This is almost the same as simple_attn_linear, but the input tensor has an extra head_index dimension, used when calculating the input of each attention head separately.
    """

    result = einops.einsum(
        input,
        w,
        "batch pos head_index d_model, head_index d_model d_head -> batch pos head_index d_head",
    )
    return result + b



---
File: /transformer_lens/utilities/devices.py
---

"""Devices.

Utilities to get the correct device, and assist in distributing model layers across multiple
devices.
"""

from __future__ import annotations

from typing import Optional, Union

import torch
from torch import nn

import transformer_lens

AvailableDeviceMemory = list[tuple[int, int]]
"""
This type is passed around between different CUDA memory operations.
The first entry of each tuple will be the device index.
The second entry will be how much memory is currently available.
"""


def calculate_available_device_cuda_memory(i: int) -> int:
    """Calculates how much memory is available at this moment for the device at the indicated index

    Args:
        i (int): The index we are looking at

    Returns:
        int: How memory is available
    """
    total = torch.cuda.get_device_properties(i).total_memory
    allocated = torch.cuda.memory_allocated(i)
    return total - allocated


def determine_available_memory_for_available_devices(max_devices: int) -> AvailableDeviceMemory:
    """Gets all available CUDA devices with their current memory calculated

    Returns:
        AvailableDeviceMemory: The list of all available devices with memory precalculated
    """
    devices = []
    for i in range(max_devices):
        devices.append((i, calculate_available_device_cuda_memory(i)))

    return devices


def sort_devices_based_on_available_memory(devices: AvailableDeviceMemory) -> AvailableDeviceMemory:
    """Sorts all available devices with devices with the most available memory returned first

    Args:
        devices (AvailableDeviceMemory): All available devices with memory calculated

    Returns:
        AvailableDeviceMemory: The same list of passed through devices sorted with devices with most
        available memory first
    """
    return sorted(devices, key=lambda x: x[1], reverse=True)


def get_best_available_cuda_device(max_devices: Optional[int] = None) -> torch.device:
    """Gets whichever cuda device has the most available amount of memory for use

    Raises:
        EnvironmentError: If there are no available devices, this will error out

    Returns:
        torch.device: The specific device that should be used
    """
    max_devices = max_devices if max_devices is not None else torch.cuda.device_count()
    devices = determine_available_memory_for_available_devices(max_devices)

    if len(devices) <= 0:
        raise EnvironmentError(
            "TransformerLens has been configured to use CUDA, but no available devices are present"
        )

    sorted_devices = sort_devices_based_on_available_memory(devices=devices)

    return torch.device("cuda", sorted_devices[0][0])


def get_best_available_device(cfg: "transformer_lens.HookedTransformerConfig") -> torch.device:
    """Gets the best available device to be used based on the passed in arguments

    Args:
        device (Union[torch.device, str]): Either the existing torch device or the string identifier

    Returns:
        torch.device: The best available device
    """
    assert cfg.device is not None
    device = torch.device(cfg.device)

    if device.type == "cuda" and cfg.n_devices > 1:
        return get_best_available_cuda_device(cfg.n_devices)
    else:
        return device


def get_device_for_block_index(
    index: int,
    cfg: "transformer_lens.HookedTransformerConfig",
    device: Optional[Union[torch.device, str]] = None,
):
    """
    Determine the device for a given layer index based on the model configuration.

    This function assists in distributing model layers across multiple devices. The distribution
    is based on the configuration's number of layers (cfg.n_layers) and devices (cfg.n_devices).


    Args:
        index (int): Model layer index.
        cfg (HookedTransformerConfig): Model and device configuration.
        device (Optional[Union[torch.device, str]], optional): Initial device used for determining the target device.
            If not provided, the function uses the device specified in the configuration (cfg.device).

    Returns:
        torch.device: The device for the specified layer index.

    Deprecated:
        This function did not take into account a few factors for multi-GPU support. You should now
        use get_best_available_device in order to properly run models on multiple devices.
        This will be removed in 3.0
    """
    assert cfg.device is not None
    layers_per_device = cfg.n_layers // cfg.n_devices
    if device is None:
        device = cfg.device
    device = torch.device(device)
    if device.type == "cpu":
        return device
    device_index = (device.index or 0) + (index // layers_per_device)
    return torch.device(device.type, device_index)


def move_to_and_update_config(
    model: Union[
        "transformer_lens.HookedTransformer",
        "transformer_lens.HookedEncoder",
        "transformer_lens.HookedEncoderDecoder",
    ],
    device_or_dtype: Union[torch.device, str, torch.dtype],
    print_details=True,
):
    """
    Wrapper around `to` that also updates `model.cfg`.
    """
    if isinstance(device_or_dtype, torch.device):
        model.cfg.device = device_or_dtype.type
        if print_details:
            print("Moving model to device: ", model.cfg.device)
    elif isinstance(device_or_dtype, str):
        model.cfg.device = device_or_dtype
        if print_details:
            print("Moving model to device: ", model.cfg.device)
    elif isinstance(device_or_dtype, torch.dtype):
        model.cfg.dtype = device_or_dtype
        if print_details:
            print("Changing model dtype to", device_or_dtype)
        # change state_dict dtypes
        for k, v in model.state_dict().items():
            model.state_dict()[k] = v.to(device_or_dtype)
    return nn.Module.to(model, device_or_dtype)



---
File: /transformer_lens/__init__.py
---

from . import hook_points
from . import utils
from . import evals
from .past_key_value_caching import (
    HookedTransformerKeyValueCache,
    HookedTransformerKeyValueCacheEntry,
)
from . import components
from . import factories
from .HookedTransformerConfig import HookedTransformerConfig
from .FactoredMatrix import FactoredMatrix
from .ActivationCache import ActivationCache
from .HookedTransformer import HookedTransformer
from .SVDInterpreter import SVDInterpreter
from .HookedEncoder import HookedEncoder
from .HookedEncoderDecoder import HookedEncoderDecoder
from .BertNextSentencePrediction import BertNextSentencePrediction
from . import head_detector
from . import loading_from_pretrained as loading
from . import patching
from . import train

from .past_key_value_caching import (
    HookedTransformerKeyValueCache as EasyTransformerKeyValueCache,
    HookedTransformerKeyValueCacheEntry as EasyTransformerKeyValueCacheEntry,
)
from .HookedTransformer import HookedTransformer as EasyTransformer
from .HookedTransformerConfig import HookedTransformerConfig as EasyTransformerConfig



---
File: /transformer_lens/ActivationCache.py
---

"""Activation Cache.

The :class:`ActivationCache` is at the core of Transformer Lens. It is a wrapper that stores all
important activations from a forward pass of the model, and provides a variety of helper functions
to investigate them.

Getting Started:

When reading these docs for the first time, we recommend reading the main :class:`ActivationCache`
class first, including the examples, and then skimming the available methods. You can then refer
back to these docs depending on what you need to do.
"""

from __future__ import annotations

import logging
import warnings
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union, cast

import einops
import numpy as np
import torch
from jaxtyping import Float, Int
from typing_extensions import Literal

import transformer_lens.utils as utils
from transformer_lens.utils import Slice, SliceInput


class ActivationCache:
    """Activation Cache.

    A wrapper that stores all important activations from a forward pass of the model, and provides a
    variety of helper functions to investigate them.

    The :class:`ActivationCache` is at the core of Transformer Lens. It is a wrapper that stores all
    important activations from a forward pass of the model, and provides a variety of helper
    functions to investigate them. The common way to access it is to run the model with
    :meth:`transformer_lens.HookedTransformer.HookedTransformer.run_with_cache`.

    Examples:

    When investigating a particular behaviour of a model, a very common first step is to try and
    understand which components of the model are most responsible for that behaviour. For example,
    if you're investigating the prompt "Why did the chicken cross the" -> " road", you might want to
    understand if there is a specific sublayer (mlp or multi-head attention) that is responsible for
    the model predicting "road". This kind of analysis commonly falls under the category of "logit
    attribution" or "direct logit attribution" (DLA).

    >>> from transformer_lens import HookedTransformer
    >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")
    Loaded pretrained model tiny-stories-1M into HookedTransformer

    >>> _logits, cache = model.run_with_cache("Why did the chicken cross the")
    >>> residual_stream, labels = cache.decompose_resid(return_labels=True, mode="attn")
    >>> print(labels[0:3])
    ['embed', 'pos_embed', '0_attn_out']

    >>> answer = " road" # Note the proceeding space to match the model's tokenization
    >>> logit_attrs = cache.logit_attrs(residual_stream, answer)
    >>> print(logit_attrs.shape) # Attention layers
    torch.Size([10, 1, 7])

    >>> most_important_component_idx = torch.argmax(logit_attrs)
    >>> print(labels[most_important_component_idx])
    3_attn_out

    You can also dig in with more granularity, using :meth:`get_full_resid_decomposition` to get the
    residual stream by individual component (mlp neurons and individual attention heads). This
    creates a larger residual stack, but the approach of using :meth"`logit_attrs` remains the same.

    Equally you might want to find out if the model struggles to construct such excellent jokes
    until the very last layers, or if it is trivial and the first few layers are enough. This kind
    of analysis is called "logit lens", and you can find out more about how to do that with
    :meth:`ActivationCache.accumulated_resid`.

    Warning:

    :class:`ActivationCache` is designed to be used with
    :class:`transformer_lens.HookedTransformer`, and will not work with other models. It's also
    designed to be used with all activations of :class:`transformer_lens.HookedTransformer` being
    cached, and some internal methods will break without that.

    The biggest footgun and source of bugs in this code will be keeping track of indexes,
    dimensions, and the numbers of each. There are several kinds of activations:

    * Internal attn head vectors: q, k, v, z. Shape [batch, pos, head_index, d_head].
    * Internal attn pattern style results: pattern (post softmax), attn_scores (pre-softmax). Shape
      [batch, head_index, query_pos, key_pos].
    * Attn head results: result. Shape [batch, pos, head_index, d_model].
    * Internal MLP vectors: pre, post, mid (only used for solu_ln - the part between activation +
      layernorm). Shape [batch, pos, d_mlp].
    * Residual stream vectors: resid_pre, resid_mid, resid_post, attn_out, mlp_out, embed,
      pos_embed, normalized (output of each LN or LNPre). Shape [batch, pos, d_model].
    * LayerNorm Scale: scale. Shape [batch, pos, 1].

    Sometimes the batch dimension will be missing because we applied `remove_batch_dim` (used when
    batch_size=1), and as such all library functions *should* be robust to that.

    Type annotations are in the following form:

    * layers_covered is the number of layers queried in functions that stack the residual stream.
    * batch_and_pos_dims is the set of dimensions from batch and pos - by default this is ["batch",
      "pos"], but is only ["pos"] if we've removed the batch dimension and is [()] if we've removed
      batch dimension and are applying a pos slice which indexes a specific position.

    Args:
        cache_dict:
            A dictionary of cached activations from a model run.
        model:
            The model that the activations are from.
        has_batch_dim:
            Whether the activations have a batch dimension.
    """

    def __init__(self, cache_dict: Dict[str, torch.Tensor], model, has_batch_dim: bool = True):
        self.cache_dict = cache_dict
        self.model = model
        self.has_batch_dim = has_batch_dim
        self.has_embed = "hook_embed" in self.cache_dict
        self.has_pos_embed = "hook_pos_embed" in self.cache_dict

    def remove_batch_dim(self) -> ActivationCache:
        """Remove the Batch Dimension (if a single batch item).

        Returns:
            The ActivationCache with the batch dimension removed.
        """
        if self.has_batch_dim:
            for key in self.cache_dict:
                assert (
                    self.cache_dict[key].size(0) == 1
                ), f"Cannot remove batch dimension from cache with batch size > 1, \
                    for key {key} with shape {self.cache_dict[key].shape}"
                self.cache_dict[key] = self.cache_dict[key][0]
            self.has_batch_dim = False
        else:
            logging.warning("Tried removing batch dimension after already having removed it.")
        return self

    def __repr__(self) -> str:
        """Representation of the ActivationCache.

        Special method that returns a string representation of an object. It's normally used to give
        a string that can be used to recreate the object, but here we just return a string that
        describes the object.
        """
        return f"ActivationCache with keys {list(self.cache_dict.keys())}"

    def __getitem__(self, key) -> torch.Tensor:
        """Retrieve Cached Activations by Key or Shorthand.

        Enables direct access to cached activations via dictionary-style indexing using keys or
        shorthand naming conventions.

        It also supports tuples for advanced indexing, with the dimension order as (name, layer_index, layer_type).
        See :func:`transformer_lens.utils.get_act_name` for how shorthand is converted to a full name.


        Args:
            key:
                The key or shorthand name for the activation to retrieve.

        Returns:
            The cached activation tensor corresponding to the given key.
        """
        if key in self.cache_dict:
            return self.cache_dict[key]
        elif type(key) == str:
            return self.cache_dict[utils.get_act_name(key)]
        else:
            if len(key) > 1 and key[1] is not None:
                if key[1] < 0:
                    # Supports negative indexing on the layer dimension
                    key = (key[0], self.model.cfg.n_layers + key[1], *key[2:])
            return self.cache_dict[utils.get_act_name(*key)]

    def __len__(self) -> int:
        """Length of the ActivationCache.

        Special method that returns the length of an object (in this case the number of different
        activations in the cache).
        """
        return len(self.cache_dict)

    def to(self, device: Union[str, torch.device], move_model=False) -> ActivationCache:
        """Move the Cache to a Device.

        Mostly useful for moving the cache to the CPU after model computation finishes to save GPU
        memory. Note however that operations will be much slower on the CPU. Note also that some
        methods will break unless the model is also moved to the same device, eg
        `compute_head_results`.

        Args:
            device:
                The device to move the cache to (e.g. `torch.device.cpu`).
            move_model:
                Whether to also move the model to the same device. @deprecated

        """
        # Move model is deprecated as we plan on de-coupling the classes
        if move_model is not None:
            warnings.warn(
                "The 'move_model' parameter is deprecated.",
                DeprecationWarning,
            )

        self.cache_dict = {key: value.to(device) for key, value in self.cache_dict.items()}

        if move_model:
            self.model.to(device)

        return self

    def toggle_autodiff(self, mode: bool = False):
        """Toggle Autodiff Globally.

        Applies `torch.set_grad_enabled(mode)` to the global state (not just TransformerLens).

        Warning:

        This is pretty dangerous, since autodiff is global state - this turns off torch's
        ability to take gradients completely and it's easy to get a bunch of errors if you don't
        realise what you're doing.

        But autodiff consumes a LOT of GPU memory (since every intermediate activation is cached
        until all downstream activations are deleted - this means that computing the loss and
        storing it in a list will keep every activation sticking around!). So often when you're
        analysing a model's activations, and don't need to do any training, autodiff is more trouble
        than its worth.

        If you don't want to mess with global state, using torch.inference_mode as a context manager
        or decorator achieves similar effects:

        >>> with torch.inference_mode():
        ...     y = torch.Tensor([1., 2, 3])
        >>> y.requires_grad
        False
        """
        logging.warning("Changed the global state, set autodiff to %s", mode)
        torch.set_grad_enabled(mode)

    def keys(self):
        """Keys of the ActivationCache.

        Examples:

            >>> from transformer_lens import HookedTransformer
            >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")
            Loaded pretrained model tiny-stories-1M into HookedTransformer
            >>> _logits, cache = model.run_with_cache("Some prompt")
            >>> list(cache.keys())[0:3]
            ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre']

        Returns:
            List of all keys.
        """
        return self.cache_dict.keys()

    def values(self):
        """Values of the ActivationCache.

        Returns:
            List of all values.
        """
        return self.cache_dict.values()

    def items(self):
        """Items of the ActivationCache.

        Returns:
            List of all items ((key, value) tuples).
        """
        return self.cache_dict.items()

    def __iter__(self) -> Iterator[str]:
        """ActivationCache Iterator.

        Special method that returns an iterator over the keys in the ActivationCache. Allows looping over the
        cache.

        Examples:

            >>> from transformer_lens import HookedTransformer
            >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")
            Loaded pretrained model tiny-stories-1M into HookedTransformer
            >>> _logits, cache = model.run_with_cache("Some prompt")
            >>> cache_interesting_names = []
            >>> for key in cache:
            ...     if not key.startswith("blocks.") or key.startswith("blocks.0"):
            ...         cache_interesting_names.append(key)
            >>> print(cache_interesting_names[0:3])
            ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre']

        Returns:
            Iterator over the cache.
        """
        return self.cache_dict.__iter__()

    def apply_slice_to_batch_dim(self, batch_slice: Union[Slice, SliceInput]) -> ActivationCache:
        """Apply a Slice to the Batch Dimension.

        Args:
            batch_slice:
                The slice to apply to the batch dimension.

        Returns:
            The ActivationCache with the batch dimension sliced.
        """
        if not isinstance(batch_slice, Slice):
            batch_slice = Slice(batch_slice)
        batch_slice = cast(Slice, batch_slice)  # mypy can't seem to infer this
        assert (
            self.has_batch_dim or batch_slice.mode == "empty"
        ), "Cannot index into a cache without a batch dim"
        still_has_batch_dim = (batch_slice.mode != "int") and self.has_batch_dim
        new_cache_dict = {
            name: batch_slice.apply(param, dim=0) for name, param in self.cache_dict.items()
        }
        return ActivationCache(new_cache_dict, self.model, has_batch_dim=still_has_batch_dim)

    def accumulated_resid(
        self,
        layer: Optional[int] = None,
        incl_mid: bool = False,
        apply_ln: bool = False,
        pos_slice: Optional[Union[Slice, SliceInput]] = None,
        mlp_input: bool = False,
        return_labels: bool = False,
    ) -> Union[
        Float[torch.Tensor, "layers_covered *batch_and_pos_dims d_model"],
        Tuple[Float[torch.Tensor, "layers_covered *batch_and_pos_dims d_model"], List[str]],
    ]:
        """Accumulated Residual Stream.

        Returns the accumulated residual stream at each layer/sub-layer. This is useful for `Logit
        Lens <https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens>`
        style analysis, where it can be thought of as what the model "believes" at each point in the
        residual stream.

        To project this into the vocabulary space, remember that there is a final layer norm in most
        decoder-only transformers. Therefore, you need to first apply the final layer norm (which
        can be done with `apply_ln`), and then multiply by the unembedding matrix (:math:`W_U`).

        If you instead want to look at contributions to the residual stream from each component
        (e.g. for direct logit attribution), see :meth:`decompose_resid` instead, or
        :meth:`get_full_resid_decomposition` if you want contributions broken down further into each
        MLP neuron.

        Examples:

        Logit Lens analysis can be done as follows:

        >>> from transformer_lens import HookedTransformer
        >>> from einops import einsum
        >>> import torch
        >>> import pandas as pd

        >>> model = HookedTransformer.from_pretrained("tiny-stories-1M", device="cpu")
        Loaded pretrained model tiny-stories-1M into HookedTransformer

        >>> prompt = "Why did the chicken cross the"
        >>> answer = " road"
        >>> logits, cache = model.run_with_cache("Why did the chicken cross the")
        >>> answer_token = model.to_single_token(answer)
        >>> print(answer_token)
        2975

        >>> accum_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)
        >>> last_token_accum = accum_resid[:, 0, -1, :]  # layer, batch, pos, d_model
        >>> print(last_token_accum.shape)  # layer, d_model
        torch.Size([9, 64])

        >>> W_U = model.W_U
        >>> print(W_U.shape)
        torch.Size([64, 50257])

        >>> layers_unembedded = einsum(
        ...         last_token_accum,
        ...         W_U,
        ...         "layer d_model, d_model d_vocab -> layer d_vocab"
        ...     )
        >>> print(layers_unembedded.shape)
        torch.Size([9, 50257])

        >>> # Get the rank of the correct answer by layer
        >>> sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)
        >>> rank_answer = (sorted_indices == 2975).nonzero(as_tuple=True)[1]
        >>> print(pd.Series(rank_answer, index=labels))
        0_pre         4442
        1_pre          382
        2_pre          982
        3_pre         1160
        4_pre          408
        5_pre          145
        6_pre           78
        7_pre          387
        final_post       6
        dtype: int64

        Args:
            layer:
                The layer to take components up to - by default includes resid_pre for that layer
                and excludes resid_mid and resid_post for that layer. If set as `n_layers`, `-1` or
                `None` it will return all residual streams, including the final one (i.e.
                immediately pre logits). The indices are taken such that this gives the accumulated
                streams up to the input to layer l.
            incl_mid:
                Whether to return `resid_mid` for all previous layers.
            apply_ln:
                Whether to apply LayerNorm to the stack.
            pos_slice:
                A slice object to apply to the pos dimension. Defaults to None, do nothing.
            mlp_input:
                Whether to include resid_mid for the current layer. This essentially gives the MLP
                input rather than the attention input.
            return_labels:
                Whether to return a list of labels for the residual stream components. Useful for
                labelling graphs.

        Returns:
            A tensor of the accumulated residual streams. If `return_labels` is True, also returns a
            list of labels for the components (as a tuple in the form `(components, labels)`).
        """
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)
        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers
        assert isinstance(layer, int)
        labels = []
        components_list = []
        for l in range(layer + 1):
            if l == self.model.cfg.n_layers:
                components_list.append(self[("resid_post", self.model.cfg.n_layers - 1)])
                labels.append("final_post")
                continue
            components_list.append(self[("resid_pre", l)])
            labels.append(f"{l}_pre")
            if (incl_mid and l < layer) or (mlp_input and l == layer):
                components_list.append(self[("resid_mid", l)])
                labels.append(f"{l}_mid")
        components_list = [pos_slice.apply(c, dim=-2) for c in components_list]
        components = torch.stack(components_list, dim=0)
        if apply_ln:
            components = self.apply_ln_to_stack(
                components, layer, pos_slice=pos_slice, mlp_input=mlp_input
            )
        if return_labels:
            return components, labels
        else:
            return components

    def logit_attrs(
        self,
        residual_stack: Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"],
        tokens: Union[
            str,
            int,
            Int[torch.Tensor, ""],
            Int[torch.Tensor, "batch"],
            Int[torch.Tensor, "batch position"],
        ],
        incorrect_tokens: Optional[
            Union[
                str,
                int,
                Int[torch.Tensor, ""],
                Int[torch.Tensor, "batch"],
                Int[torch.Tensor, "batch position"],
            ]
        ] = None,
        pos_slice: Union[Slice, SliceInput] = None,
        batch_slice: Union[Slice, SliceInput] = None,
        has_batch_dim: bool = True,
    ) -> Float[torch.Tensor, "num_components *batch_and_pos_dims_out"]:
        """Logit Attributions.

        Takes a residual stack (typically the residual stream decomposed by components), and
        calculates how much each item in the stack "contributes" to specific tokens.

        It does this by:
            1. Getting the residual directions of the tokens (i.e. reversing the unembed)
            2. Taking the dot product of each item in the residual stack, with the token residual
               directions.

        Note that if incorrect tokens are provided, it instead takes the difference between the
        correct and incorrect tokens (to calculate the residual directions). This is useful as
        sometimes we want to know e.g. which components are most responsible for selecting the
        correct token rather than an incorrect one. For example in the `Interpretability in the Wild
        paper <https://arxiv.org/abs/2211.00593>` prompts such as "John and Mary went to the shops,
        John gave a bag to" were investigated, and it was therefore useful to calculate attribution
        for the :math:`\\text{Mary} - \\text{John}` residual direction.

        Warning:

        Choosing the correct `tokens` and `incorrect_tokens` is both important and difficult. When
        investigating specific components it's also useful to look at it's impact on all tokens
        (i.e. :math:`\\text{final_ln}(\\text{residual_stack_item}) W_U`).

        Args:
            residual_stack:
                Stack of components of residual stream to get logit attributions for.
            tokens:
                Tokens to compute logit attributions on.
            incorrect_tokens:
                If provided, compute attributions on logit difference between tokens and
                incorrect_tokens. Must have the same shape as tokens.
            pos_slice:
                The slice to apply layer norm scaling on. Defaults to None, do nothing.
            batch_slice:
                The slice to take on the batch dimension during layer norm scaling. Defaults to
                None, do nothing.
            has_batch_dim:
                Whether residual_stack has a batch dimension. Defaults to True.

        Returns:
            A tensor of the logit attributions or logit difference attributions if incorrect_tokens
            was provided.
        """
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)

        if not isinstance(batch_slice, Slice):
            batch_slice = Slice(batch_slice)

        if isinstance(tokens, str):
            tokens = torch.as_tensor(self.model.to_single_token(tokens))

        elif isinstance(tokens, int):
            tokens = torch.as_tensor(tokens)

        logit_directions = self.model.tokens_to_residual_directions(tokens)

        if incorrect_tokens is not None:
            if isinstance(incorrect_tokens, str):
                incorrect_tokens = torch.as_tensor(self.model.to_single_token(incorrect_tokens))

            elif isinstance(incorrect_tokens, int):
                incorrect_tokens = torch.as_tensor(incorrect_tokens)

            if tokens.shape != incorrect_tokens.shape:
                raise ValueError(
                    f"tokens and incorrect_tokens must have the same shape! \
                        (tokens.shape={tokens.shape}, \
                        incorrect_tokens.shape={incorrect_tokens.shape})"
                )

            # If incorrect_tokens was provided, take the logit difference
            logit_directions = logit_directions - self.model.tokens_to_residual_directions(
                incorrect_tokens
            )

        scaled_residual_stack = self.apply_ln_to_stack(
            residual_stack,
            layer=-1,
            pos_slice=pos_slice,
            batch_slice=batch_slice,
            has_batch_dim=has_batch_dim,
        )

        # Element-wise multiplication and sum over the d_model dimension
        logit_attrs = (scaled_residual_stack * logit_directions).sum(dim=-1)
        return logit_attrs

    def decompose_resid(
        self,
        layer: Optional[int] = None,
        mlp_input: bool = False,
        mode: Literal["all", "mlp", "attn"] = "all",
        apply_ln: bool = False,
        pos_slice: Union[Slice, SliceInput] = None,
        incl_embeds: bool = True,
        return_labels: bool = False,
    ) -> Union[
        Float[torch.Tensor, "layers_covered *batch_and_pos_dims d_model"],
        Tuple[Float[torch.Tensor, "layers_covered *batch_and_pos_dims d_model"], List[str]],
    ]:
        """Decompose the Residual Stream.

        Decomposes the residual stream input to layer L into a stack of the output of previous
        layers. The sum of these is the input to layer L (plus embedding and pos embedding). This is
        useful for attributing model behaviour to different components of the residual stream

        Args:
            layer:
                The layer to take components up to - by default includes
                resid_pre for that layer and excludes resid_mid and resid_post for that layer.
                layer==n_layers means to return all layer outputs incl in the final layer, layer==0
                means just embed and pos_embed. The indices are taken such that this gives the
                accumulated streams up to the input to layer l
            mlp_input:
                Whether to include attn_out for the current
                layer - essentially decomposing the residual stream that's input to the MLP input
                rather than the Attn input.
            mode:
                Values are "all", "mlp" or "attn". "all" returns all
                components, "mlp" returns only the MLP components, and "attn" returns only the
                attention components. Defaults to "all".
            apply_ln:
                Whether to apply LayerNorm to the stack.
            pos_slice:
                A slice object to apply to the pos dimension.
                Defaults to None, do nothing.
            incl_embeds:
                Whether to include embed & pos_embed
            return_labels:
                Whether to return a list of labels for the residual stream components.
                Useful for labelling graphs.

        Returns:
            A tensor of the accumulated residual streams. If `return_labels` is True, also returns
            a list of labels for the components (as a tuple in the form `(components, labels)`).
        """
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)
        pos_slice = cast(Slice, pos_slice)  # mypy can't seem to infer this
        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers
        assert isinstance(layer, int)

        incl_attn = mode != "mlp"
        incl_mlp = mode != "attn" and not self.model.cfg.attn_only
        components_list = []
        labels = []
        if incl_embeds:
            if self.has_embed:
                components_list = [self["hook_embed"]]
                labels.append("embed")
            if self.has_pos_embed:
                components_list.append(self["hook_pos_embed"])
                labels.append("pos_embed")

        for l in range(layer):
            if incl_attn:
                components_list.append(self[("attn_out", l)])
                labels.append(f"{l}_attn_out")
            if incl_mlp:
                components_list.append(self[("mlp_out", l)])
                labels.append(f"{l}_mlp_out")
        if mlp_input and incl_attn:
            components_list.append(self[("attn_out", layer)])
            labels.append(f"{layer}_attn_out")
        components_list = [pos_slice.apply(c, dim=-2) for c in components_list]
        components = torch.stack(components_list, dim=0)
        if apply_ln:
            components = self.apply_ln_to_stack(
                components, layer, pos_slice=pos_slice, mlp_input=mlp_input
            )
        if return_labels:
            return components, labels
        else:
            return components

    def compute_head_results(
        self,
    ):
        """Compute Head Results.

        Computes and caches the results for each attention head, ie the amount contributed to the
        residual stream from that head. attn_out for a layer is the sum of head results plus b_O.
        Intended use is to enable use_attn_results when running and caching the model, but this can
        be useful if you forget.
        """
        if "blocks.0.attn.hook_result" in self.cache_dict:
            logging.warning("Tried to compute head results when they were already cached")
            return
        for layer in range(self.model.cfg.n_layers):
            # Note that we haven't enabled set item on this object so we need to edit the underlying
            # cache_dict directly.

            # Add singleton dimension to match W_O's shape for broadcasting
            z = einops.rearrange(
                self[("z", layer, "attn")],
                "... head_index d_head -> ... head_index d_head 1",
            )

            # Element-wise multiplication of z and W_O (with shape [head_index, d_head, d_model])
            result = z * self.model.blocks[layer].attn.W_O

            # Sum over d_head to get the contribution of each head to the residual stream
            self.cache_dict[f"blocks.{layer}.attn.hook_result"] = result.sum(dim=-2)

    def stack_head_results(
        self,
        layer: int = -1,
        return_labels: bool = False,
        incl_remainder: bool = False,
        pos_slice: Union[Slice, SliceInput] = None,
        apply_ln: bool = False,
    ) -> Union[
        Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"],
        Tuple[Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"], List[str]],
    ]:
        """Stack Head Results.

        Returns a stack of all head results (ie residual stream contribution) up to layer L. A good
        way to decompose the outputs of attention layers into attribution by specific heads. Note
        that the num_components axis has length layer x n_heads ((layer head_index) in einops
        notation).

        Args:
            layer:
                Layer index - heads at all layers strictly before this are included. layer must be
                in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer.
            return_labels:
                Whether to also return a list of labels of the form "L0H0" for the heads.
            incl_remainder:
                Whether to return a final term which is "the rest of the residual stream".
            pos_slice:
                A slice object to apply to the pos dimension. Defaults to None, do nothing.
            apply_ln:
                Whether to apply LayerNorm to the stack.
        """
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)
        pos_slice = cast(Slice, pos_slice)  # mypy can't seem to infer this
        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers

        if "blocks.0.attn.hook_result" not in self.cache_dict:
            print(
                "Tried to stack head results when they weren't cached. Computing head results now"
            )
            self.compute_head_results()

        components: Any = []
        labels = []
        for l in range(layer):
            # Note that this has shape batch x pos x head_index x d_model
            components.append(pos_slice.apply(self[("result", l, "attn")], dim=-3))
            labels.extend([f"L{l}H{h}" for h in range(self.model.cfg.n_heads)])
        if components:
            components = torch.cat(components, dim=-2)
            components = einops.rearrange(
                components,
                "... concat_head_index d_model -> concat_head_index ... d_model",
            )
            if incl_remainder:
                remainder = pos_slice.apply(
                    self[("resid_post", layer - 1)], dim=-2
                ) - components.sum(dim=0)
                components = torch.cat([components, remainder[None]], dim=0)
                labels.append("remainder")
        elif incl_remainder:
            # There are no components, so the remainder is the entire thing.
            components = torch.cat(
                [pos_slice.apply(self[("resid_post", layer - 1)], dim=-2)[None]], dim=0
            )
            labels.append("remainder")
        else:
            # If this is called with layer 0, we return an empty tensor of the right shape to be
            # stacked correctly. This uses the shape of hook_embed, which is pretty janky since it
            # assumes embed is in the cache. But it's hard to explicitly code the shape, since it
            # depends on the pos slice, whether we have a batch dim, etc. And it's pretty messy!
            components = torch.zeros(
                0,
                *pos_slice.apply(self["hook_embed"], dim=-2).shape,
                device=self.model.cfg.device,
            )

        if apply_ln:
            components = self.apply_ln_to_stack(components, layer, pos_slice=pos_slice)

        if return_labels:
            return components, labels
        else:
            return components

    def stack_activation(
        self,
        activation_name: str,
        layer: int = -1,
        sublayer_type: Optional[str] = None,
    ) -> Float[torch.Tensor, "layers_covered ..."]:
        """Stack Activations.

        Flexible way to stack activations with a given name.

        Args:
            activation_name:
                The name of the activation to be stacked
            layer:
                'Layer index - heads' at all layers strictly before this are included. layer must be
                in [1, n_layers-1], or any of (n_layers, -1, None), which all mean the final layer.
            sublayer_type:
                The sub layer type of the activation, passed to utils.get_act_name. Can normally be
                inferred.
            incl_remainder:
                Whether to return a final term which is "the rest of the residual stream".
        """
        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers

        components = []
        for l in range(layer):
            components.append(self[(activation_name, l, sublayer_type)])

        return torch.stack(components, dim=0)

    def get_neuron_results(
        self,
        layer: int,
        neuron_slice: Union[Slice, SliceInput] = None,
        pos_slice: Union[Slice, SliceInput] = None,
    ) -> Float[torch.Tensor, "*batch_and_pos_dims num_neurons d_model"]:
        """Get Neuron Results.

        Get the results of for neurons in a specific layer (i.e, how much each neuron contributes to
        the residual stream). Does it for the subset of neurons specified by neuron_slice, defaults
        to all of them. Does *not* cache these because it's expensive in space and cheap to compute.

        Args:
            layer:
                Layer index.
            neuron_slice:
                Slice of the neuron.
            pos_slice:
                Slice of the positions.

        Returns:
            Tensor of the results.
        """
        if not isinstance(neuron_slice, Slice):
            neuron_slice = Slice(neuron_slice)
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)

        neuron_acts = self[("post", layer, "mlp")]
        W_out = self.model.blocks[layer].mlp.W_out
        if pos_slice is not None:
            # Note - order is important, as Slice.apply *may* collapse a dimension, so this ensures
            # that position dimension is -2 when we apply position slice
            neuron_acts = pos_slice.apply(neuron_acts, dim=-2)
        if neuron_slice is not None:
            neuron_acts = neuron_slice.apply(neuron_acts, dim=-1)
            W_out = neuron_slice.apply(W_out, dim=0)
        return neuron_acts[..., None] * W_out

    def stack_neuron_results(
        self,
        layer: int,
        pos_slice: Union[Slice, SliceInput] = None,
        neuron_slice: Union[Slice, SliceInput] = None,
        return_labels: bool = False,
        incl_remainder: bool = False,
        apply_ln: bool = False,
    ) -> Union[
        Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"],
        Tuple[Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"], List[str]],
    ]:
        """Stack Neuron Results

        Returns a stack of all neuron results (ie residual stream contribution) up to layer L - ie
        the amount each individual neuron contributes to the residual stream. Also returns a list of
        labels of the form "L0N0" for the neurons. A good way to decompose the outputs of MLP layers
        into attribution by specific neurons.

        Note that doing this for all neurons is SUPER expensive on GPU memory and only works for
        small models or short inputs.

        Args:
            layer:
                Layer index - heads at all layers strictly before this are included. layer must be
                in [1, n_layers]
            pos_slice:
                Slice of the positions.
            neuron_slice:
                Slice of the neurons.
            return_labels:
                Whether to also return a list of labels of the form "L0H0" for the heads.
            incl_remainder:
                Whether to return a final term which is "the rest of the residual stream".
            apply_ln:
                Whether to apply LayerNorm to the stack.
        """

        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers

        components: Any = []  # TODO: fix typing properly
        labels = []

        if not isinstance(neuron_slice, Slice):
            neuron_slice = Slice(neuron_slice)
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)

        neuron_labels: Union[torch.Tensor, np.ndarray] = neuron_slice.apply(
            torch.arange(self.model.cfg.d_mlp), dim=0
        )
        if isinstance(neuron_labels, int):
            neuron_labels = np.array([neuron_labels])

        for l in range(layer):
            # Note that this has shape batch x pos x head_index x d_model
            components.append(
                self.get_neuron_results(l, pos_slice=pos_slice, neuron_slice=neuron_slice)
            )
            labels.extend([f"L{l}N{h}" for h in neuron_labels])
        if components:
            components = torch.cat(components, dim=-2)
            components = einops.rearrange(
                components,
                "... concat_neuron_index d_model -> concat_neuron_index ... d_model",
            )

            if incl_remainder:
                remainder = pos_slice.apply(
                    self[("resid_post", layer - 1)], dim=-2
                ) - components.sum(dim=0)
                components = torch.cat([components, remainder[None]], dim=0)
                labels.append("remainder")
        elif incl_remainder:
            components = torch.cat(
                [pos_slice.apply(self[("resid_post", layer - 1)], dim=-2)[None]], dim=0
            )
            labels.append("remainder")
        else:
            # Returning empty, give it the right shape to stack properly
            components = torch.zeros(
                0,
                *pos_slice.apply(self["hook_embed"], dim=-2).shape,
                device=self.model.cfg.device,
            )

        if apply_ln:
            components = self.apply_ln_to_stack(components, layer, pos_slice=pos_slice)

        if return_labels:
            return components, labels
        else:
            return components

    def apply_ln_to_stack(
        self,
        residual_stack: Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"],
        layer: Optional[int] = None,
        mlp_input: bool = False,
        pos_slice: Union[Slice, SliceInput] = None,
        batch_slice: Union[Slice, SliceInput] = None,
        has_batch_dim: bool = True,
    ) -> Float[torch.Tensor, "num_components *batch_and_pos_dims_out d_model"]:
        """Apply Layer Norm to a Stack.

        Takes a stack of components of the residual stream (eg outputs of decompose_resid or
        accumulated_resid), treats them as the input to a specific layer, and applies the layer norm
        scaling of that layer to them, using the cached scale factors - simulating what that
        component of the residual stream contributes to that layer's input.

        The layernorm scale is global across the entire residual stream for each layer, batch
        element and position, which is why we need to use the cached scale factors rather than just
        applying a new LayerNorm.

        If the model does not use LayerNorm or RMSNorm, it returns the residual stack unchanged.

        Args:
            residual_stack:
                A tensor, whose final dimension is d_model. The other trailing dimensions are
                assumed to be the same as the stored hook_scale - which may or may not include batch
                or position dimensions.
            layer:
                The layer we're taking the input to. In [0, n_layers], n_layers means the unembed.
                None maps to the n_layers case, ie the unembed.
            mlp_input:
                Whether the input is to the MLP or attn (ie ln2 vs ln1). Defaults to False, ie ln1.
                If layer==n_layers, must be False, and we use ln_final
            pos_slice:
                The slice to take of positions, if residual_stack is not over the full context, None
                means do nothing. It is assumed that pos_slice has already been applied to
                residual_stack, and this is only applied to the scale. See utils.Slice for details.
                Defaults to None, do nothing.
            batch_slice:
                The slice to take on the batch dimension. Defaults to None, do nothing.
            has_batch_dim:
                Whether residual_stack has a batch dimension.

        """
        if self.model.cfg.normalization_type not in ["LN", "LNPre", "RMS", "RMSPre"]:
            # The model does not use LayerNorm, so we don't need to do anything.
            return residual_stack
        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)
        if not isinstance(batch_slice, Slice):
            batch_slice = Slice(batch_slice)

        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers

        if has_batch_dim:
            # Apply batch slice to the stack
            residual_stack = batch_slice.apply(residual_stack, dim=1)

        # Center the stack onlny if the model uses LayerNorm
        if self.model.cfg.normalization_type in ["LN", "LNPre"]:
            residual_stack = residual_stack - residual_stack.mean(dim=-1, keepdim=True)

        if layer == self.model.cfg.n_layers or layer is None:
            scale = self["ln_final.hook_scale"]
        else:
            hook_name = f"blocks.{layer}.ln{2 if mlp_input else 1}.hook_scale"
            scale = self[hook_name]

        # The shape of scale is [batch, position, 1] or [position, 1] - final dimension is a dummy
        # thing to get broadcoasting to work nicely.
        scale = pos_slice.apply(scale, dim=-2)

        if self.has_batch_dim:
            # Apply batch slice to the scale
            scale = batch_slice.apply(scale)

        return residual_stack / scale

    def get_full_resid_decomposition(
        self,
        layer: Optional[int] = None,
        mlp_input: bool = False,
        expand_neurons: bool = True,
        apply_ln: bool = False,
        pos_slice: Union[Slice, SliceInput] = None,
        return_labels: bool = False,
    ) -> Union[
        Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"],
        Tuple[Float[torch.Tensor, "num_components *batch_and_pos_dims d_model"], List[str]],
    ]:
        """Get the full Residual Decomposition.

        Returns the full decomposition of the residual stream into embed, pos_embed, each head
        result, each neuron result, and the accumulated biases. We break down the residual stream
        that is input into some layer.

        Args:
            layer:
                The layer we're inputting into. layer is in [0, n_layers], if layer==n_layers (or
                None) we're inputting into the unembed (the entire stream), if layer==0 then it's
                just embed and pos_embed
            mlp_input:
                Are we inputting to the MLP in that layer or the attn? Must be False for final
                layer, since that's the unembed.
            expand_neurons:
                Whether to expand the MLP outputs to give every neuron's result or just return the
                MLP layer outputs.
            apply_ln:
                Whether to apply LayerNorm to the stack.
            pos_slice:
                Slice of the positions to take.
            return_labels:
                Whether to return the labels.
        """
        if layer is None or layer == -1:
            # Default to the residual stream immediately pre unembed
            layer = self.model.cfg.n_layers
        assert layer is not None  # keep mypy happy

        if not isinstance(pos_slice, Slice):
            pos_slice = Slice(pos_slice)
        head_stack, head_labels = self.stack_head_results(
            layer + (1 if mlp_input else 0), pos_slice=pos_slice, return_labels=True
        )
        labels = head_labels
        components = [head_stack]
        if not self.model.cfg.attn_only and layer > 0:
            if expand_neurons:
                neuron_stack, neuron_labels = self.stack_neuron_results(
                    layer, pos_slice=pos_slice, return_labels=True
                )
                labels.extend(neuron_labels)
                components.append(neuron_stack)
            else:
                # Get the stack of just the MLP outputs
                # mlp_input included for completeness, but it doesn't actually matter, since it's
                # just for MLP outputs
                mlp_stack, mlp_labels = self.decompose_resid(
                    layer,
                    mlp_input=mlp_input,
                    pos_slice=pos_slice,
                    incl_embeds=False,
                    mode="mlp",
                    return_labels=True,
                )
                labels.extend(mlp_labels)
                components.append(mlp_stack)

        if self.has_embed:
            labels.append("embed")
            components.append(pos_slice.apply(self["embed"], -2)[None])
        if self.has_pos_embed:
            labels.append("pos_embed")
            components.append(pos_slice.apply(self["pos_embed"], -2)[None])
        # If we didn't expand the neurons, the MLP biases are already included in the MLP outputs.
        bias = self.model.accumulated_bias(layer, mlp_input, include_mlp_biases=expand_neurons)
        bias = bias.expand((1,) + head_stack.shape[1:])
        labels.append("bias")
        components.append(bias)
        residual_stack = torch.cat(components, dim=0)
        if apply_ln:
            residual_stack = self.apply_ln_to_stack(
                residual_stack, layer, pos_slice=pos_slice, mlp_input=mlp_input
            )

        if return_labels:
            return residual_stack, labels
        else:
            return residual_stack



---
File: /transformer_lens/BertNextSentencePrediction.py
---

"""Next Sentence Prediction.

Contains a BERT style model specifically for Next Sentence Prediction. This is separate from
:class:`transformer_lens.HookedTransformer` because it has a significantly different architecture
to e.g. GPT style transformers.
"""

from typing import Dict, List, Optional, Tuple, Union, overload

import torch
from jaxtyping import Float, Int
from typing_extensions import Literal

from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.HookedEncoder import HookedEncoder


class BertNextSentencePrediction:
    """A BERT-style model for Next Sentence Prediction (NSP) that extends HookedEncoder.

    This class implements a BERT model specifically designed for the Next Sentence Prediction task,
    where the model predicts whether two input sentences naturally follow each other in the original text.
    It inherits from HookedEncoder and adds NSP-specific components like the NSP head and pooler layer.

    The model processes pairs of sentences and outputs either logits or human-readable predictions
    indicating whether the sentences are sequential. String inputs are automatically tokenized with
    appropriate token type IDs to distinguish between the two sentences.

    Note:
        This model expects inputs to be provided as pairs of sentences. Single sentence inputs
        or inputs without proper sentence separation will raise errors.
    """

    def __init__(self, model: HookedEncoder):
        self.model = model

    def __call__(
        self,
        input: Union[
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Optional[Union[Literal["logits"], Literal["predictions"]]] = "logits",
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Union[Float[torch.Tensor, "batch 2"], str]]:
        """Makes the NextSentencePrediction instance callable.

        This method delegates to the forward method, allowing the model to be called directly.
        The arguments and return types match the forward method exactly.
        """
        return self.forward(
            input,
            return_type=return_type,
            token_type_ids=token_type_ids,
            one_zero_attention_mask=one_zero_attention_mask,
        )

    def to_tokens(
        self,
        input: List[str],
        move_to_device: bool = True,
        truncate: bool = True,
    ) -> Tuple[
        Int[torch.Tensor, "batch pos"],
        Int[torch.Tensor, "batch pos"],
        Int[torch.Tensor, "batch pos"],
    ]:
        """Converts a string to a tensor of tokens.
        Taken mostly from the HookedTransformer implementation, but does not support default padding
        sides or prepend_bos.
        Args:
            input: List[str]]: The input to tokenize.
            move_to_device (bool): Whether to move the output tensor of tokens to the device the model lives on. Defaults to True
            truncate (bool): If the output tokens are too long, whether to truncate the output
            tokens to the model's max context window. Does nothing for shorter inputs. Defaults to
            True.
        """

        if len(input) != 2:
            raise ValueError(
                "Next sentence prediction task requires exactly two sentences, please provide a list of strings with each sentence as an element."
            )

        # We need to input the two sentences separately for NSP
        encodings = self.model.tokenizer(
            input[0],
            input[1],
            return_tensors="pt",
            padding=True,
            truncation=truncate,
            max_length=self.model.cfg.n_ctx if truncate else None,
        )

        tokens = encodings["input_ids"]
        token_type_ids = encodings["token_type_ids"]
        attention_mask = encodings["attention_mask"]

        if move_to_device:
            tokens = tokens.to(self.model.cfg.device)
            token_type_ids = token_type_ids.to(self.model.cfg.device)
            attention_mask = attention_mask.to(self.model.cfg.device)

        return tokens, token_type_ids, attention_mask

    @overload
    def forward(
        self,
        input: Union[
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Union[Literal["logits"], Literal["predictions"]],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Union[Float[torch.Tensor, "batch 2"], str]:
        ...

    @overload
    def forward(
        self,
        input: Union[
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Literal[None],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Union[Float[torch.Tensor, "batch 2"], str]]:
        ...

    def forward(
        self,
        input: Union[
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Optional[Union[Literal["logits"], Literal["predictions"]]] = "logits",
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Union[Float[torch.Tensor, "batch 2"], str]]:
        """Forward pass through the NextSentencePrediction module. Performs Next Sentence Prediction on a pair of sentences.

        Args:
            input: The input to process. Can be one of:
                - List[str]: A list of two strings representing the two sentences NSP should be performed on
                - torch.Tensor: Input tokens as integers with shape (batch, position)
            return_type: Optional[str]: The type of output to return. Can be one of:
                - None: Return nothing, don't calculate logits
                - 'logits': Return logits tensor
                - 'predictions': Return human-readable predictions
            token_type_ids: Optional[torch.Tensor]: Binary ids indicating whether a token belongs
                to sequence A or B. For example, for two sentences:
                "[CLS] Sentence A [SEP] Sentence B [SEP]", token_type_ids would be
                [0, 0, ..., 0, 1, ..., 1, 1]. `0` represents tokens from Sentence A,
                `1` from Sentence B. If not provided, BERT assumes a single sequence input.
                This parameter gets inferred from the the tokenizer if input is a string or list of strings.
                Shape is (batch_size, sequence_length).
            one_zero_attention_mask: Optional[torch.Tensor]: A binary mask which indicates
                which tokens should be attended to (1) and which should be ignored (0).
                Primarily used for padding variable-length sentences in a batch.
                For instance, in a batch with sentences of differing lengths, shorter
                sentences are padded with 0s on the right. If not provided, the model
                assumes all tokens should be attended to.
                This parameter gets inferred from the tokenizer if input is a string or list of strings.
                Shape is (batch_size, sequence_length).

        Returns:
            Optional[torch.Tensor]: Depending on return_type:
                - None: Returns None if return_type is None
                - torch.Tensor: Returns logits if return_type is 'logits' (or if return_type is not explicitly provided)
                    - Shape is (batch_size, 2)
                - str or List[str]: Returns string indicating if sentences are sequential if return_type is 'predictions'

        Raises:
            ValueError: If using NSP task without proper input format or token_type_ids
            AssertionError: If using string input without a tokenizer
        """

        if isinstance(input, list):
            assert self.model.tokenizer is not None, "Must provide a tokenizer if input is a string"
            tokens, token_type_ids_from_tokenizer, attention_mask = self.to_tokens(input)

            # If token_type_ids or attention mask are not provided, use the ones from the tokenizer
            token_type_ids = (
                token_type_ids_from_tokenizer if token_type_ids is None else token_type_ids
            )
            one_zero_attention_mask = (
                attention_mask if one_zero_attention_mask is None else one_zero_attention_mask
            )
        elif token_type_ids == None and isinstance(input, torch.Tensor):
            raise ValueError(
                "You are using the NSP task without specifying token_type_ids."
                "This means that the model will treat the input as a single sequence which will lead to incorrect results."
                "Please provide token_type_ids or use a string input."
            )
        else:
            tokens = input

        resid = self.model.encoder_output(tokens, token_type_ids, one_zero_attention_mask)

        # NSP requires pooling (for more information see BertPooler)
        resid = self.model.pooler(resid)
        logits = self.model.nsp_head(resid)

        if return_type == "predictions":
            logprobs = logits.log_softmax(dim=-1)
            predictions = [
                "The sentences are sequential",
                "The sentences are NOT sequential",
            ]
            return predictions[logprobs.argmax(dim=-1).item()]

        elif return_type == None:
            return None

        return logits

    @overload
    def run_with_cache(
        self, *model_args, return_cache_object: Literal[True] = True, **kwargs
    ) -> Tuple[Float[torch.Tensor, "batch 2"], ActivationCache,]:
        ...

    @overload
    def run_with_cache(
        self, *model_args, return_cache_object: Literal[False], **kwargs
    ) -> Tuple[Float[torch.Tensor, "batch 2"], Dict[str, torch.Tensor],]:
        ...

    def run_with_cache(
        self,
        *model_args,
        return_cache_object: bool = True,
        remove_batch_dim: bool = False,
        **kwargs,
    ) -> Tuple[Float[torch.Tensor, "batch 2"], Union[ActivationCache, Dict[str, torch.Tensor]],]:
        """
        Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True,
        this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods,
        otherwise it will return a dictionary of activations as in HookedRootModule.
        This function was copied directly from HookedTransformer.
        """

        # Create wrapper for forward function, such that run_with_cache uses
        # the forward function of this class and not HookedEncoder

        class ForwardWrapper:
            def __init__(self, nsp):
                self.nsp = nsp
                self.original_forward = nsp.model.forward

            def __enter__(self):
                # Store reference to wrapper function
                def wrapped_forward(*fargs, **fkwargs):
                    return self.nsp.forward(*fargs, **fkwargs)

                self.nsp.model.forward = wrapped_forward
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                # Restore original forward
                self.nsp.model.forward = self.original_forward

        with ForwardWrapper(self):
            out, cache_dict = self.model.run_with_cache(
                *model_args, remove_batch_dim=remove_batch_dim, **kwargs
            )
            if return_cache_object:
                cache = ActivationCache(cache_dict, self, has_batch_dim=not remove_batch_dim)
                return out, cache
            else:
                return out, cache_dict



---
File: /transformer_lens/evals.py
---

"""Evaluation Helpers.

This module contains some rough evals for models, but you are likely better off using the
HuggingFace evaluate library if you want to do anything properly. This is however here if you want
it and want to eg cheaply and roughly compare models you've trained to baselines.
"""

import random
from typing import Dict, List, Optional

import einops
import torch
import tqdm.auto as tqdm
from datasets import load_dataset
from torch.utils.data import DataLoader, Dataset

from transformer_lens import utils


# %%
def sanity_check(model):
    """
    Very basic eval - just feeds a string into the model (in this case, the first paragraph of Circuits: Zoom In), and returns the loss. It's a rough and quick sanity check - if the loss is <5 the model is probably OK, if the loss is >7 something's gone wrong.

    Note that this is a very basic eval, and doesn't really tell you much about the model's performance.
    """

    text = "Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens."

    return model(text, return_type="loss")


# %%
def make_wiki_data_loader(tokenizer, batch_size=8):
    """
    Evaluate on Wikitext 2, a dump of Wikipedia articles. (Using the train set because it's larger, I don't really expect anyone to bother with quarantining the validation set nowadays.)

    Note there's likely to be dataset leakage into training data (though I believe GPT-2 was explicitly trained on non-Wikipedia data)
    """
    wiki_data = load_dataset("wikitext", "wikitext-2-v1", split="train")
    print(len(wiki_data))
    dataset = utils.tokenize_and_concatenate(wiki_data, tokenizer)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    return data_loader


def make_owt_data_loader(tokenizer, batch_size=8):
    """
    Evaluate on OpenWebText an open source replication of the GPT-2 training corpus (Reddit links with >3 karma)

    I think the Mistral models were trained on this dataset, so they get very good performance.
    """
    owt_data = load_dataset("stas/openwebtext-10k", split="train")
    print(len(owt_data))
    dataset = utils.tokenize_and_concatenate(owt_data, tokenizer)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    return data_loader


def make_pile_data_loader(tokenizer, batch_size=8):
    """
    Evaluate on the first 10k texts from The Pile.

    The Pile is EleutherAI's general-purpose english dataset, made of 22 subsets
    including academic papers, books, internet content...
    """
    pile_data = load_dataset("NeelNanda/pile-10k", split="train")
    print(len(pile_data))
    dataset = utils.tokenize_and_concatenate(pile_data, tokenizer)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    return data_loader


def make_code_data_loader(tokenizer, batch_size=8):
    """
    Evaluate on the CodeParrot dataset, a dump of Python code.

    All models seem to get significantly lower loss here (even non-code trained models like GPT-2),
    presumably code is much easier to predict than natural language?
    """
    code_data = load_dataset("codeparrot/codeparrot-valid-v2-near-dedup", split="train")
    print(len(code_data))
    dataset = utils.tokenize_and_concatenate(code_data, tokenizer, column_name="content")
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    return data_loader


DATASET_NAMES = ["wiki", "owt", "pile", "code"]
DATASET_LOADERS = [
    make_wiki_data_loader,
    make_owt_data_loader,
    make_pile_data_loader,
    make_code_data_loader,
]


# %%
@torch.inference_mode()
def evaluate_on_dataset(model, data_loader, truncate=100, device="cuda"):
    running_loss = 0
    total = 0
    for batch in tqdm.tqdm(data_loader):
        loss = model(batch["tokens"].to(device), return_type="loss").mean()
        running_loss += loss.item()
        total += 1
        if total > truncate:
            break
    return running_loss / total


# %%
@torch.inference_mode()
def induction_loss(
    model, tokenizer=None, batch_size=4, subseq_len=384, prepend_bos=None, device="cuda"
):
    """
    Generates a batch of random sequences repeated twice, and measures model performance on the second half. Tests whether a model has induction heads.

    By default, prepends a beginning of string token (when prepend_bos flag defaults to None, model.cfg.default_prepend_bos is used
    whose default is True unless specified otherwise), which is useful to give models a resting position, and sometimes models were trained with this.
    """
    # Make the repeated sequence
    first_half_tokens = torch.randint(100, 20000, (batch_size, subseq_len)).to(device)
    repeated_tokens = einops.repeat(first_half_tokens, "b p -> b (2 p)")

    # Use the provided prepend_bos as an override if it's not None;
    # otherwise use model.cfg.default_prepend_bos (defaults to True)
    prepend_bos = utils.override_or_use_default_value(
        model.cfg.default_prepend_bos, override=prepend_bos
    )

    # Prepend a Beginning Of String token
    if prepend_bos:
        if tokenizer is None:
            tokenizer = model.tokenizer
        repeated_tokens[:, 0] = tokenizer.bos_token_id
    # Run the model, and extract the per token correct log prob
    logits = model(repeated_tokens, return_type="logits")
    correct_log_probs = utils.lm_cross_entropy_loss(logits, repeated_tokens, per_token=True)
    # Take the loss over the second half of the sequence
    return correct_log_probs[:, subseq_len + 1 :].mean()


# %%
@torch.inference_mode()
def evaluate(model, truncate=100, batch_size=8, tokenizer=None):
    if tokenizer is None:
        tokenizer = model.tokenizer
    losses = {}
    for data_name, data_loader_fn in zip(DATASET_NAMES, DATASET_LOADERS):
        data_loader = data_loader_fn(tokenizer=tokenizer, batch_size=batch_size)
        loss = evaluate_on_dataset(model, data_loader, truncate=truncate)
        print(f"{data_name}: {loss}")
        losses[f"{data_name}_loss"] = loss
    return losses


# %%
class IOIDataset(Dataset):
    """
    Dataset for Indirect Object Identification tasks.
    Paper: https://arxiv.org/pdf/2211.00593.pdf

    Example:

    .. code-block:: python

        >>> from transformer_lens.evals import ioi_eval, IOIDataset
        >>> from transformer_lens.HookedTransformer import HookedTransformer

        >>> model = HookedTransformer.from_pretrained('gpt2-small')
        Loaded pretrained model gpt2-small into HookedTransformer

        >>> # Evaluate like this, printing the logit difference
        >>> print(round(ioi_eval(model, num_samples=100)["Logit Difference"], 3))
        5.476

        >>> # Can use custom dataset
        >>> ds = IOIDataset(
        ...     tokenizer=model.tokenizer,
        ...     num_samples=100,
        ...     templates=['[A] met with [B]. [B] gave the [OBJECT] to [A]'],
        ...     names=['Alice', 'Bob', 'Charlie'],
        ...     nouns={'OBJECT': ['ball', 'book']},
        ... )
        >>> print(round(ioi_eval(model, dataset=ds)["Logit Difference"], 3))
        5.397
    """

    def __init__(
        self,
        tokenizer,
        templates: Optional[List[str]] = None,
        names: Optional[List[str]] = None,
        nouns: Optional[Dict[str, List[str]]] = None,
        num_samples: int = 1000,
        symmetric: bool = False,
        prepend_bos: bool = True,
    ):
        self.tokenizer = tokenizer
        self.prepend_bos = prepend_bos

        self.templates = templates if templates is not None else self.get_default_templates()
        self.names = names if names is not None else self.get_default_names()
        self.nouns = nouns if nouns is not None else self.get_default_nouns()

        self.samples = []
        for _ in range(num_samples // 2 if symmetric else num_samples):
            # If symmetric, get_sample will return two samples
            self.samples.extend(self.get_sample(symmetric=symmetric))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        prompt = self.tokenizer.encode(sample["text"])
        if self.prepend_bos:
            prompt = [self.tokenizer.bos_token_id] + prompt

        return {
            "prompt": torch.LongTensor(prompt),
            "IO": torch.LongTensor(self.tokenizer.encode(sample["IO"])),
            "S": torch.LongTensor(self.tokenizer.encode(sample["S"])),
        }

    def get_sample(self, symmetric=False) -> List[Dict[str, str]]:
        random.seed(42)
        template: str = random.choice(self.templates)
        for noun_type, noun_list in self.nouns.items():
            template = template.replace(f"[{noun_type}]", random.choice(noun_list))

        samples: List[Dict[str, str]] = []

        # Sample two names without replacement
        names = random.sample(self.names, 2)
        sample = template.replace("[A]", names[0])
        sample = sample.replace("[B]", names[1])
        # Prepend spaces to IO and S so that the target is e.g. " Mary" and not "Mary"
        samples.append({"text": sample, "IO": " " + names[0], "S": " " + names[1]})

        if symmetric:
            sample_2 = template.replace("[A]", names[1])
            sample_2 = sample_2.replace("[B]", names[0])
            samples.append({"text": sample_2, "IO": " " + names[1], "S": " " + names[0]})

        return samples

    @staticmethod
    def get_default_names():
        return ["John", "Mary"]

    @staticmethod
    def get_default_templates():
        return [
            "[A] and [B] went to the [LOCATION] to buy [OBJECT]. [B] handed the [OBJECT] to [A]",
            "Then, [B] and [A] went to the [LOCATION]. [B] gave the [OBJECT] to [A]",
        ]

    @staticmethod
    def get_default_nouns():
        return {
            "LOCATION": ["store", "market"],
            "OBJECT": ["milk", "eggs", "bread"],
        }


@torch.inference_mode()
def ioi_eval(model, dataset=None, batch_size=8, num_samples=1000, tokenizer=None, symmetric=False):
    """Evaluate the Model on the Indirect Object Identification Task.

    Args:
        model: HookedTransformer model.
        dataset: PyTorch Dataset that returns a dict with keys "prompt", "IO", and "S".
        batch_size: Batch size to use.
        num_samples: Number of samples to use.
        tokenizer: Tokenizer to use.
        symmetric: Whether to use the symmetric version of the task.

    Returns:
        Average logit difference and accuracy.
    """
    if tokenizer is None:
        tokenizer = model.tokenizer

    if dataset is None:
        dataset = IOIDataset(tokenizer, num_samples=num_samples, symmetric=symmetric)

    def collate(samples):
        prompts = [sample["prompt"] for sample in samples]
        padded_prompts = torch.nn.utils.rnn.pad_sequence(prompts, batch_first=True)
        return {
            "prompt": padded_prompts,
            "IO": [sample["IO"] for sample in samples],
            "S": [sample["S"] for sample in samples],
            "prompt_length": [p.shape[0] for p in prompts],
        }

    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)

    total_correct = 0
    total_logit_diff = 0
    for batch in tqdm.tqdm(data_loader):
        batch_logits = model(batch["prompt"], return_type="logits")

        for i in range(batch_logits.shape[0]):
            io = batch["IO"][i]
            s = batch["S"][i]
            prefix_length = batch["prompt_length"][i] - io.shape[0]

            # Trim io and s to the same length
            min_len = min(io.shape[0], s.shape[0])
            io = io[:min_len]
            s = s[:min_len]

            # Remove identical prefixes
            start_idx = torch.where(io != s)[0][0]
            io = io[start_idx]
            s = s[start_idx]
            logit_idx = prefix_length + start_idx - 1

            # Get the logits for the tokens we care about
            logits = batch_logits[i, logit_idx]
            correct_logit = logits[io]
            incorrect_logit = logits[s]

            # Compute stats
            logit_diff = correct_logit - incorrect_logit
            correct = logit_diff > 0
            total_correct += correct.item()
            total_logit_diff += logit_diff.item()

    return {
        "Logit Difference": total_logit_diff / len(dataset),
        "Accuracy": total_correct / len(dataset),
    }



---
File: /transformer_lens/FactoredMatrix.py
---

"""Factored Matrix.

Utilities for representing a matrix as a product of two matrices, and for efficient calculation of
eigenvalues, norm and SVD.
"""

from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple, Union, overload

import torch
from jaxtyping import Float

import transformer_lens.utils as utils


class FactoredMatrix:
    """
    Class to represent low rank factored matrices, where the matrix is represented as a product of two matrices. Has utilities for efficient calculation of eigenvalues, norm and SVD.
    """

    def __init__(
        self,
        A: Float[torch.Tensor, "... ldim mdim"],
        B: Float[torch.Tensor, "... mdim rdim"],
    ):
        self.A = A
        self.B = B
        assert self.A.size(-1) == self.B.size(
            -2
        ), f"Factored matrix must match on inner dimension, shapes were a: {self.A.shape}, b:{self.B.shape}"
        self.ldim = self.A.size(-2)
        self.rdim = self.B.size(-1)
        self.mdim = self.B.size(-2)
        self.has_leading_dims = (self.A.ndim > 2) or (self.B.ndim > 2)
        self.shape = torch.broadcast_shapes(self.A.shape[:-2], self.B.shape[:-2]) + (
            self.ldim,
            self.rdim,
        )
        self.A = self.A.broadcast_to(self.shape[:-2] + (self.ldim, self.mdim))
        self.B = self.B.broadcast_to(self.shape[:-2] + (self.mdim, self.rdim))

    @overload
    def __matmul__(
        self,
        other: Union[
            Float[torch.Tensor, "... rdim new_rdim"],
            "FactoredMatrix",
        ],
    ) -> "FactoredMatrix":
        ...

    @overload
    def __matmul__(  # type: ignore
        self,
        other: Float[torch.Tensor, "rdim"],
    ) -> Float[torch.Tensor, "... ldim"]:
        ...

    def __matmul__(
        self,
        other: Union[
            Float[torch.Tensor, "... rdim new_rdim"],
            Float[torch.Tensor, "rdim"],
            "FactoredMatrix",
        ],
    ) -> Union["FactoredMatrix", Float[torch.Tensor, "... ldim"]]:
        if isinstance(other, torch.Tensor):
            if other.ndim < 2:
                # It's a vector, so we collapse the factorisation and just return a vector
                # Squeezing/Unsqueezing is to preserve broadcasting working nicely
                return (self.A @ (self.B @ other.unsqueeze(-1))).squeeze(-1)
            else:
                assert (
                    other.size(-2) == self.rdim
                ), f"Right matrix must match on inner dimension, shapes were self: {self.shape}, other:{other.shape}"
                if self.rdim > self.mdim:
                    return FactoredMatrix(self.A, self.B @ other)
                else:
                    return FactoredMatrix(self.AB, other)
        elif isinstance(other, FactoredMatrix):
            return (self @ other.A) @ other.B

    @overload
    def __rmatmul__(  # type: ignore
        self,
        other: Union[
            Float[torch.Tensor, "... new_rdim ldim"],
            "FactoredMatrix",
        ],
    ) -> "FactoredMatrix":
        ...

    @overload
    def __rmatmul__(  # type: ignore
        self,
        other: Float[torch.Tensor, "ldim"],
    ) -> Float[torch.Tensor, "... rdim"]:
        ...

    def __rmatmul__(  # type: ignore
        self,
        other: Union[
            Float[torch.Tensor, "... new_rdim ldim"],
            Float[torch.Tensor, "ldim"],
            "FactoredMatrix",
        ],
    ) -> Union["FactoredMatrix", Float[torch.Tensor, "... rdim"]]:
        if isinstance(other, torch.Tensor):
            assert (
                other.size(-1) == self.ldim
            ), f"Left matrix must match on inner dimension, shapes were self: {self.shape}, other:{other.shape}"
            if other.ndim < 2:
                # It's a vector, so we collapse the factorisation and just return a vector
                return ((other.unsqueeze(-2) @ self.A) @ self.B).squeeze(-2)
            elif self.ldim > self.mdim:
                return FactoredMatrix(other @ self.A, self.B)
            else:
                return FactoredMatrix(other, self.AB)
        elif isinstance(other, FactoredMatrix):
            return other.A @ (other.B @ self)

    def __mul__(self, scalar: Union[int, float, torch.Tensor]) -> FactoredMatrix:
        """
        Left scalar multiplication. Scalar multiplication distributes over matrix multiplication, so we can just multiply one of the factor matrices by the scalar.
        """
        if isinstance(scalar, torch.Tensor):
            assert (
                scalar.numel() == 1
            ), f"Tensor must be a scalar for use with * but was of shape {scalar.shape}. For matrix multiplication, use @ instead."
        return FactoredMatrix(self.A * scalar, self.B)

    def __rmul__(self, scalar: Union[int, float, torch.Tensor]) -> FactoredMatrix:  # type: ignore
        """
        Right scalar multiplication. For scalar multiplication from the right, we can reuse the __mul__ method.
        """
        return self * scalar

    @property
    def AB(self) -> Float[torch.Tensor, "*leading_dims ldim rdim"]:
        """The product matrix - expensive to compute, and can consume a lot of GPU memory"""
        return self.A @ self.B

    @property
    def BA(self) -> Float[torch.Tensor, "*leading_dims rdim ldim"]:
        """The reverse product. Only makes sense when ldim==rdim"""
        assert (
            self.rdim == self.ldim
        ), f"Can only take ba if ldim==rdim, shapes were self: {self.shape}"
        return self.B @ self.A

    @property
    def T(self) -> FactoredMatrix:
        return FactoredMatrix(self.B.transpose(-2, -1), self.A.transpose(-2, -1))

    @lru_cache(maxsize=None)
    def svd(
        self,
    ) -> Tuple[
        Float[torch.Tensor, "*leading_dims ldim mdim"],
        Float[torch.Tensor, "*leading_dims mdim"],
        Float[torch.Tensor, "*leading_dims rdim mdim"],
    ]:
        """
        Efficient algorithm for finding Singular Value Decomposition, a tuple (U, S, Vh) for matrix M st S is a vector and U, Vh are orthogonal matrices, and U @ S.diag() @ Vh.T == M

        (Note that Vh is given as the transpose of the obvious thing)
        """
        Ua, Sa, Vha = torch.svd(self.A)
        Ub, Sb, Vhb = torch.svd(self.B)
        middle = Sa[..., :, None] * utils.transpose(Vha) @ Ub * Sb[..., None, :]
        Um, Sm, Vhm = torch.svd(middle)
        U = Ua @ Um
        Vh = Vhb @ Vhm
        S = Sm
        return U, S, Vh

    @property
    def U(self) -> Float[torch.Tensor, "*leading_dims ldim mdim"]:
        return self.svd()[0]

    @property
    def S(self) -> Float[torch.Tensor, "*leading_dims mdim"]:
        return self.svd()[1]

    @property
    def Vh(self) -> Float[torch.Tensor, "*leading_dims rdim mdim"]:
        return self.svd()[2]

    @property
    def eigenvalues(self) -> Float[torch.Tensor, "*leading_dims mdim"]:
        """Eigenvalues of AB are the same as for BA (apart from trailing zeros), because if BAv=kv ABAv = A(BAv)=kAv, so Av is an eigenvector of AB with eigenvalue k."""
        return torch.linalg.eig(self.BA).eigenvalues

    def _convert_to_slice(self, sequence: Union[Tuple, List], idx: int) -> Tuple:
        """
        e.g. if sequence = (1, 2, 3) and idx = 1, return (1, slice(2, 3), 3). This only edits elements if they are ints.
        """
        if isinstance(idx, int):
            sequence = list(sequence)
            if isinstance(sequence[idx], int):
                sequence[idx] = slice(sequence[idx], sequence[idx] + 1)
            sequence = tuple(sequence)

        return sequence

    def __getitem__(self, idx: Union[int, Tuple]) -> FactoredMatrix:
        """Indexing - assumed to only apply to the leading dimensions."""
        if not isinstance(idx, tuple):
            idx = (idx,)
        length = len([i for i in idx if i is not None])
        if length <= len(self.shape) - 2:
            return FactoredMatrix(self.A[idx], self.B[idx])
        elif length == len(self.shape) - 1:
            idx = self._convert_to_slice(idx, -1)
            return FactoredMatrix(self.A[idx], self.B[idx[:-1]])
        elif length == len(self.shape):
            idx = self._convert_to_slice(idx, -1)
            idx = self._convert_to_slice(idx, -2)
            return FactoredMatrix(self.A[idx[:-1]], self.B[idx[:-2] + (slice(None), idx[-1])])
        else:
            raise ValueError(
                f"{idx} is too long an index for a FactoredMatrix with shape {self.shape}"
            )

    def norm(self) -> Float[torch.Tensor, "*leading_dims"]:
        """
        Frobenius norm is sqrt(sum of squared singular values)
        """
        return self.S.pow(2).sum(-1).sqrt()

    def __repr__(self):
        return f"FactoredMatrix: Shape({self.shape}), Hidden Dim({self.mdim})"

    def make_even(self) -> FactoredMatrix:
        """
        Returns the factored form of (U @ S.sqrt().diag(), S.sqrt().diag() @ Vh) where U, S, Vh are the SVD of the matrix. This is an equivalent factorisation, but more even - each half has half the singular values, and orthogonal rows/cols
        """
        return FactoredMatrix(
            self.U * self.S.sqrt()[..., None, :],
            self.S.sqrt()[..., :, None] * utils.transpose(self.Vh),
        )

    def get_corner(self, k=3):
        return utils.get_corner(self.A[..., :k, :] @ self.B[..., :, :k], k)

    @property
    def ndim(self) -> int:
        return len(self.shape)

    def collapse_l(self) -> Float[torch.Tensor, "*leading_dims mdim rdim"]:
        """
        Collapses the left side of the factorization by removing the orthogonal factor (given by self.U). Returns a (..., mdim, rdim) tensor
        """
        return self.S[..., :, None] * utils.transpose(self.Vh)

    def collapse_r(self) -> Float[torch.Tensor, "*leading_dims ldim mdim"]:
        """
        Analogous to collapse_l, returns a (..., ldim, mdim) tensor
        """
        return self.U * self.S[..., None, :]

    def unsqueeze(self, k: int) -> FactoredMatrix:
        return FactoredMatrix(self.A.unsqueeze(k), self.B.unsqueeze(k))

    @property
    def pair(
        self,
    ) -> Tuple[
        Float[torch.Tensor, "*leading_dims ldim mdim"],
        Float[torch.Tensor, "*leading_dims mdim rdim"],
    ]:
        return (self.A, self.B)



---
File: /transformer_lens/head_detector.py
---

"""Head Detector.

Utilities for detecting specific types of heads (e.g. previous token heads).
"""

import logging
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Union, cast

import numpy as np
import torch
from typing_extensions import Literal, get_args

from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.HookedTransformer import HookedTransformer
from transformer_lens.utils import is_lower_triangular, is_square

HeadName = Literal["previous_token_head", "duplicate_token_head", "induction_head"]
HEAD_NAMES = cast(List[HeadName], get_args(HeadName))
ErrorMeasure = Literal["abs", "mul"]

LayerHeadTuple = Tuple[int, int]
LayerToHead = Dict[int, List[int]]

INVALID_HEAD_NAME_ERR = (
    f"detection_pattern must be a Tensor or one of head names: {HEAD_NAMES}; got %s"
)

SEQ_LEN_ERR = "The sequence must be non-empty and must fit within the model's context window."

DET_PAT_NOT_SQUARE_ERR = "The detection pattern must be a lower triangular matrix of shape (sequence_length, sequence_length); sequence_length=%d; got detection patern of shape %s"


def detect_head(
    model: HookedTransformer,
    seq: Union[str, List[str]],
    detection_pattern: Union[torch.Tensor, HeadName],
    heads: Optional[Union[List[LayerHeadTuple], LayerToHead]] = None,
    cache: Optional[ActivationCache] = None,
    *,
    exclude_bos: bool = False,
    exclude_current_token: bool = False,
    error_measure: ErrorMeasure = "mul",
) -> torch.Tensor:
    """Search for a Particular Type of Attention Head.

    Searches the model (or a set of specific heads, for circuit analysis) for a particular type of
    attention head. This head is specified by a detection pattern, a (sequence_length,
    sequence_length) tensor representing the attention pattern we expect that type of attention head
    to show. The detection pattern can be also passed not as a tensor, but as a name of one of
    pre-specified types of attention head (see `HeadName` for available patterns), in which case the
    tensor is computed within the function itself.

    There are two error measures available for quantifying the match between the detection pattern
    and the actual attention pattern.

    1. `"mul"` (default) multiplies both tensors element-wise and divides the sum of the result by
        the sum of the attention pattern. Typically, the detection pattern should in this case
        contain only ones and zeros, which allows a straightforward interpretation of the score: how
        big fraction of this head's attention is allocated to these specific query-key pairs? Using
        values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled,
        of course).

    2. `"abs"` calculates the mean element-wise absolute difference between the detection pattern
        and the actual attention pattern. The "raw result" ranges from 0 to 2 where lower score
        corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval,
        with 1 being perfect match and -1 perfect mismatch.

    Which one should you use?

    `"mul"` is likely better for quick or exploratory investigations. For precise examinations where
    you're trying to reproduce as much functionality as possible or really test your understanding
    of the attention head, you probably want to switch to `"abs"`.

    The advantage of `"abs"` is that you can make more precise predictions, and have that measured
    in the score. You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and
    your score will be better if your prediction is closer. The "mul" metric does not allow this,
    you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.

    Args:
        model: Model being used.
        seq: String or list of strings being fed to the model.
        head_name: Name of an existing head in HEAD_NAMES we want to check. Must pass either a
            head_name or a detection_pattern, but not both!
        detection_pattern: (sequence_length, sequence_length)nTensor representing what attention
            pattern corresponds to the head we're looking for or the name of a pre-specified head.
            Currently available heads are: `["previous_token_head", "duplicate_token_head",
            "induction_head"]`.
        heads: If specific attention heads is given here, all other heads' score is set to -1.
            Useful for IOI-style circuit analysis. Heads can be spacified as a list tuples (layer,
            head) or a dictionary mapping a layer to heads within that layer that we want to
            analyze. cache: Include the cache to save time if you want.
        exclude_bos: Exclude attention paid to the beginning of sequence token.
        exclude_current_token: Exclude attention paid to the current token.
        error_measure: `"mul"` for using element-wise multiplication. `"abs"` for using absolute
            values of element-wise differences as the error measure.

    Returns:
        Tensor representing the score for each attention head.
    """

    cfg = model.cfg
    tokens = model.to_tokens(seq).to(cfg.device)
    seq_len = tokens.shape[-1]

    # Validate error_measure

    assert error_measure in get_args(
        ErrorMeasure
    ), f"Invalid error_measure={error_measure}; valid values are {get_args(ErrorMeasure)}"

    # Validate detection pattern if it's a string
    if isinstance(detection_pattern, str):
        assert detection_pattern in HEAD_NAMES, INVALID_HEAD_NAME_ERR % detection_pattern
        if isinstance(seq, list):
            batch_scores = [detect_head(model, seq, detection_pattern) for seq in seq]
            return torch.stack(batch_scores).mean(0)
        detection_pattern = cast(
            torch.Tensor,
            eval(f"get_{detection_pattern}_detection_pattern(tokens.cpu())"),
        ).to(cfg.device)

    # if we're using "mul", detection_pattern should consist of zeros and ones
    if error_measure == "mul" and not set(detection_pattern.unique().tolist()).issubset({0, 1}):
        logging.warning(
            "Using detection pattern with values other than 0 or 1 with error_measure 'mul'"
        )

    # Validate inputs and detection pattern shape
    assert 1 < tokens.shape[-1] < cfg.n_ctx, SEQ_LEN_ERR
    assert (
        is_lower_triangular(detection_pattern) and seq_len == detection_pattern.shape[0]
    ), DET_PAT_NOT_SQUARE_ERR % (seq_len, detection_pattern.shape)

    if cache is None:
        _, cache = model.run_with_cache(tokens, remove_batch_dim=True)

    if heads is None:
        layer2heads = {layer_i: list(range(cfg.n_heads)) for layer_i in range(cfg.n_layers)}
    elif isinstance(heads, list):
        layer2heads = defaultdict(list)
        for layer, head in heads:
            layer2heads[layer].append(head)
    else:
        layer2heads = heads

    matches = -torch.ones(cfg.n_layers, cfg.n_heads, dtype=cfg.dtype)

    for layer, layer_heads in layer2heads.items():
        # [n_heads q_pos k_pos]
        layer_attention_patterns = cache["pattern", layer, "attn"]
        for head in layer_heads:
            head_attention_pattern = layer_attention_patterns[head, :, :]
            head_score = compute_head_attention_similarity_score(
                head_attention_pattern,
                detection_pattern=detection_pattern,
                exclude_bos=exclude_bos,
                exclude_current_token=exclude_current_token,
                error_measure=error_measure,
            )
            matches[layer, head] = head_score
    return matches


# Previous token head
def get_previous_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [previous token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc).

    Args:
      tokens: Tokens being fed to the model.
    """
    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])
    # Adds a diagonal of 1's below the main diagonal.
    detection_pattern[1:, :-1] = torch.eye(tokens.shape[-1] - 1)
    return torch.tril(detection_pattern)


# Duplicate token head
def get_duplicate_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [duplicate token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo).

    Args:
      sequence: String being fed to the model.
    """
    # [pos x pos]
    token_pattern = tokens.repeat(tokens.shape[-1], 1).numpy()

    # If token_pattern[i][j] matches its transpose, then token j and token i are duplicates.
    eq_mask = np.equal(token_pattern, token_pattern.T).astype(int)

    np.fill_diagonal(eq_mask, 0)  # Current token is always a duplicate of itself. Ignore that.
    detection_pattern = eq_mask.astype(int)
    return torch.tril(torch.as_tensor(detection_pattern).float())


# Induction head
def get_induction_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY).

    Args:
      sequence: String being fed to the model.
    """
    duplicate_pattern = get_duplicate_token_head_detection_pattern(tokens)

    # Shift all items one to the right
    shifted_tensor = torch.roll(duplicate_pattern, shifts=1, dims=1)

    # Replace first column with 0's
    # we don't care about bos but shifting to the right moves the last column to the first,
    # and the last column might contain non-zero values.
    zeros_column = torch.zeros(duplicate_pattern.shape[0], 1)
    result_tensor = torch.cat((zeros_column, shifted_tensor[:, 1:]), dim=1)
    return torch.tril(result_tensor)


def get_supported_heads() -> None:
    """Returns a list of supported heads."""
    print(f"Supported heads: {HEAD_NAMES}")


def compute_head_attention_similarity_score(
    attention_pattern: torch.Tensor,  # [q_pos k_pos]
    detection_pattern: torch.Tensor,  # [seq_len seq_len] (seq_len == q_pos == k_pos)
    *,
    exclude_bos: bool,
    exclude_current_token: bool,
    error_measure: ErrorMeasure,
) -> float:
    """Compute the similarity between `attention_pattern` and `detection_pattern`.

    Args:
      attention_pattern: Lower triangular matrix (Tensor) representing the attention pattern of a particular attention head.
      detection_pattern: Lower triangular matrix (Tensor) representing the attention pattern we are looking for.
      exclude_bos: `True` if the beginning-of-sentence (BOS) token should be omitted from comparison. `False` otherwise.
      exclude_bcurrent_token: `True` if the current token at each position should be omitted from comparison. `False` otherwise.
      error_measure: "abs" for using absolute values of element-wise differences as the error measure. "mul" for using element-wise multiplication (legacy code).
    """
    assert is_square(
        attention_pattern
    ), f"Attention pattern is not square; got shape {attention_pattern.shape}"

    # mul

    if error_measure == "mul":
        if exclude_bos:
            attention_pattern[:, 0] = 0
        if exclude_current_token:
            attention_pattern.fill_diagonal_(0)
        score = attention_pattern * detection_pattern
        return (score.sum() / attention_pattern.sum()).item()

    # abs

    abs_diff = (attention_pattern - detection_pattern).abs()
    assert (abs_diff - torch.tril(abs_diff).to(abs_diff.device)).sum() == 0

    size = len(abs_diff)
    if exclude_bos:
        abs_diff[:, 0] = 0
    if exclude_current_token:
        abs_diff.fill_diagonal_(0)

    return 1 - round((abs_diff.mean() * size).item(), 3)
    return 1 - round((abs_diff.mean() * size).item(), 3)



---
File: /transformer_lens/hook_points.py
---

from __future__ import annotations

"""Hook Points.

Helpers to access activations in models.
"""

import logging
from collections.abc import Callable, Iterable, Sequence
from contextlib import contextmanager
from dataclasses import dataclass
from functools import partial
from typing import Any, Literal, Optional, Protocol, Union, runtime_checkable

import torch
import torch.nn as nn
import torch.utils.hooks as hooks
from torch import Tensor

from transformer_lens.utils import Slice, SliceInput


@dataclass
class LensHandle:
    """Dataclass that holds information about a PyTorch hook."""

    hook: hooks.RemovableHandle
    """Reference to the Hook's Removable Handle."""

    is_permanent: bool = False
    """Indicates if the Hook is Permanent."""

    context_level: Optional[int] = None
    """Context level associated with the hooks context manager for the given hook."""


# Define type aliases
NamesFilter = Optional[Union[Callable[[str], bool], Sequence[str], str]]


@runtime_checkable
class _HookFunctionProtocol(Protocol):
    """Protocol for hook functions."""

    def __call__(self, tensor: Tensor, *, hook: "HookPoint") -> Union[Any, None]:
        ...


HookFunction = _HookFunctionProtocol  # Callable[..., _HookFunctionProtocol]

DeviceType = Optional[torch.device]
_grad_t = Union[tuple[Tensor, ...], Tensor]


class HookPoint(nn.Module):
    """
    A helper class to access intermediate activations in a PyTorch model (inspired by Garcon).

    HookPoint is a dummy module that acts as an identity function by default. By wrapping any
    intermediate activation in a HookPoint, it provides a convenient way to add PyTorch hooks.
    """

    def __init__(self):
        super().__init__()
        self.fwd_hooks: list[LensHandle] = []
        self.bwd_hooks: list[LensHandle] = []
        self.ctx = {}

        # A variable giving the hook's name (from the perspective of the root
        # module) - this is set by the root module at setup.
        self.name: Optional[str] = None

    def add_perma_hook(self, hook: HookFunction, dir: Literal["fwd", "bwd"] = "fwd") -> None:
        self.add_hook(hook, dir=dir, is_permanent=True)

    def add_hook(
        self,
        hook: HookFunction,
        dir: Literal["fwd", "bwd"] = "fwd",
        is_permanent: bool = False,
        level: Optional[int] = None,
        prepend: bool = False,
    ) -> None:
        """
        Hook format is fn(activation, hook_name)
        Change it into PyTorch hook format (this includes input and output,
        which are the same for a HookPoint)
        If prepend is True, add this hook before all other hooks
        """

        def full_hook(
            module: torch.nn.Module,
            module_input: Any,
            module_output: Any,
        ):
            if (
                dir == "bwd"
            ):  # For a backwards hook, module_output is a tuple of (grad,) - I don't know why.
                module_output = module_output[0]
            return hook(module_output, hook=self)

        # annotate the `full_hook` with the string representation of the `hook` function
        if isinstance(hook, partial):
            # partial.__repr__() can be extremely slow if arguments contain large objects, which
            # is common when caching tensors.
            full_hook.__name__ = f"partial({hook.func.__repr__()},...)"
        else:
            full_hook.__name__ = hook.__repr__()

        if dir == "fwd":
            pt_handle = self.register_forward_hook(full_hook, prepend=prepend)
            visible_hooks = self.fwd_hooks
        elif dir == "bwd":
            pt_handle = self.register_full_backward_hook(full_hook, prepend=prepend)
            visible_hooks = self.bwd_hooks
        else:
            raise ValueError(f"Invalid direction {dir}")

        handle = LensHandle(pt_handle, is_permanent, level)

        if prepend:
            # we could just pass this as an argument in PyTorch 2.0, but for now we manually do this...
            visible_hooks.insert(0, handle)

        else:
            visible_hooks.append(handle)

    def remove_hooks(
        self,
        dir: Literal["fwd", "bwd", "both"] = "fwd",
        including_permanent: bool = False,
        level: Optional[int] = None,
    ) -> None:
        def _remove_hooks(handles: list[LensHandle]) -> list[LensHandle]:
            output_handles = []
            for handle in handles:
                if including_permanent:
                    handle.hook.remove()
                elif (not handle.is_permanent) and (level is None or handle.context_level == level):
                    handle.hook.remove()
                else:
                    output_handles.append(handle)
            return output_handles

        if dir == "fwd" or dir == "both":
            self.fwd_hooks = _remove_hooks(self.fwd_hooks)
        if dir == "bwd" or dir == "both":
            self.bwd_hooks = _remove_hooks(self.bwd_hooks)
        if dir not in ["fwd", "bwd", "both"]:
            raise ValueError(f"Invalid direction {dir}")

    def clear_context(self):
        del self.ctx
        self.ctx = {}

    def forward(self, x: Tensor) -> Tensor:
        return x

    def layer(self):
        # Returns the layer index if the name has the form 'blocks.{layer}.{...}'
        # Helper function that's mainly useful on HookedTransformer
        # If it doesn't have this form, raises an error -
        if self.name is None:
            raise ValueError("Name cannot be None")
        split_name = self.name.split(".")
        return int(split_name[1])


# %%
class HookedRootModule(nn.Module):
    """A class building on nn.Module to interface nicely with HookPoints.

    Adds various nice utilities, most notably run_with_hooks to run the model with temporary hooks,
    and run_with_cache to run the model on some input and return a cache of all activations.

    Notes:

    The main footgun with PyTorch hooking is that hooks are GLOBAL state. If you add a hook to the
    module, and then run it a bunch of times, the hooks persist. If you debug a broken hook and add
    the fixed version, the broken one is still there. To solve this, run_with_hooks will remove
    hooks at the end by default, and I recommend using the API of this and run_with_cache. If you
    want to add hooks into global state, I recommend being intentional about this, and I recommend
    using reset_hooks liberally in your code to remove any accidentally remaining global state.

    The main time this goes wrong is when you want to use backward hooks (to cache or intervene on
    gradients). In this case, you need to keep the hooks around as global state until you've run
    loss.backward() (and so need to disable the reset_hooks_end flag on run_with_hooks)
    """

    name: Optional[str]
    mod_dict: dict[str, nn.Module]
    hook_dict: dict[str, HookPoint]

    def __init__(self, *args: Any):
        super().__init__()
        self.is_caching = False
        self.context_level = 0

    def setup(self):
        """
        Sets up model.

        This function must be called in the model's `__init__` method AFTER defining all layers. It
        adds a parameter to each module containing its name, and builds a dictionary mapping module
        names to the module instances. It also initializes a hook dictionary for modules of type
        "HookPoint".
        """
        self.mod_dict = {}
        self.hook_dict = {}
        for name, module in self.named_modules():
            if name == "":
                continue
            module.name = name
            self.mod_dict[name] = module
            # TODO: is the bottom line the same as "if "HookPoint" in str(type(module)):"
            if isinstance(module, HookPoint):
                self.hook_dict[name] = module

    def hook_points(self):
        return self.hook_dict.values()

    def remove_all_hook_fns(
        self,
        direction: Literal["fwd", "bwd", "both"] = "both",
        including_permanent: bool = False,
        level: Optional[int] = None,
    ):
        for hp in self.hook_points():
            hp.remove_hooks(direction, including_permanent=including_permanent, level=level)

    def clear_contexts(self):
        for hp in self.hook_points():
            hp.clear_context()

    def reset_hooks(
        self,
        clear_contexts: bool = True,
        direction: Literal["fwd", "bwd", "both"] = "both",
        including_permanent: bool = False,
        level: Optional[int] = None,
    ):
        if clear_contexts:
            self.clear_contexts()
        self.remove_all_hook_fns(direction, including_permanent, level=level)
        self.is_caching = False

    def check_and_add_hook(
        self,
        hook_point: HookPoint,
        hook_point_name: str,
        hook: HookFunction,
        dir: Literal["fwd", "bwd"] = "fwd",
        is_permanent: bool = False,
        level: Optional[int] = None,
        prepend: bool = False,
    ) -> None:
        """Runs checks on the hook, and then adds it to the hook point"""

        self.check_hooks_to_add(
            hook_point,
            hook_point_name,
            hook,
            dir=dir,
            is_permanent=is_permanent,
            prepend=prepend,
        )
        hook_point.add_hook(hook, dir=dir, is_permanent=is_permanent, level=level, prepend=prepend)

    def check_hooks_to_add(
        self,
        hook_point: HookPoint,
        hook_point_name: str,
        hook: HookFunction,
        dir: Literal["fwd", "bwd"] = "fwd",
        is_permanent: bool = False,
        prepend: bool = False,
    ) -> None:
        """Override this function to add checks on which hooks should be added"""
        pass

    def add_hook(
        self,
        name: Union[str, Callable[[str], bool]],
        hook: HookFunction,
        dir: Literal["fwd", "bwd"] = "fwd",
        is_permanent: bool = False,
        level: Optional[int] = None,
        prepend: bool = False,
    ) -> None:
        if isinstance(name, str):
            hook_point = self.mod_dict[name]
            assert isinstance(
                hook_point, HookPoint
            )  # TODO does adding assert meaningfully slow down performance? I've added them for type checking purposes.
            self.check_and_add_hook(
                hook_point,
                name,
                hook,
                dir=dir,
                is_permanent=is_permanent,
                level=level,
                prepend=prepend,
            )
        else:
            # Otherwise, name is a Boolean function on names
            for hook_point_name, hp in self.hook_dict.items():
                if name(hook_point_name):
                    self.check_and_add_hook(
                        hp,
                        hook_point_name,
                        hook,
                        dir=dir,
                        is_permanent=is_permanent,
                        level=level,
                        prepend=prepend,
                    )

    def add_perma_hook(
        self,
        name: Union[str, Callable[[str], bool]],
        hook: HookFunction,
        dir: Literal["fwd", "bwd"] = "fwd",
    ) -> None:
        self.add_hook(name, hook, dir=dir, is_permanent=True)

    def _enable_hook_with_name(self, name: str, hook: Callable, dir: Literal["fwd", "bwd"]):
        """This function takes a key for the mod_dict and enables the related hook for that module

        Args:
            name (str): The module name
            hook (Callable): The hook to add
            dir (Literal[&quot;fwd&quot;, &quot;bwd&quot;]): The direction for the hook
        """
        self.mod_dict[name].add_hook(hook, dir=dir, level=self.context_level)  # type: ignore[operator]

    def _enable_hooks_for_points(
        self,
        hook_points: Iterable[tuple[str, HookPoint]],
        enabled: Callable,
        hook: Callable,
        dir: Literal["fwd", "bwd"],
    ):
        """Enables hooks for a list of points

        Args:
            hook_points (Dict[str, HookPoint]): The hook points
            enabled (Callable): _description_
            hook (Callable): _description_
            dir (Literal[&quot;fwd&quot;, &quot;bwd&quot;]): _description_
        """
        for hook_name, hook_point in hook_points:
            if enabled(hook_name):
                hook_point.add_hook(hook, dir=dir, level=self.context_level)

    def _enable_hook(self, name: Union[str, Callable], hook: Callable, dir: Literal["fwd", "bwd"]):
        """Enables an individual hook on a hook point

        Args:
            name (str): The name of the hook
            hook (Callable): The actual hook
            dir (Literal[&quot;fwd&quot;, &quot;bwd&quot;], optional): The direction of the hook. Defaults to "fwd".
        """
        if isinstance(name, str):
            self._enable_hook_with_name(name=name, hook=hook, dir=dir)
        else:
            self._enable_hooks_for_points(
                hook_points=self.hook_dict.items(), enabled=name, hook=hook, dir=dir
            )

    @contextmanager
    def hooks(
        self,
        fwd_hooks: list[tuple[Union[str, Callable], Callable]] = [],
        bwd_hooks: list[tuple[Union[str, Callable], Callable]] = [],
        reset_hooks_end: bool = True,
        clear_contexts: bool = False,
    ):
        """
        A context manager for adding temporary hooks to the model.

        Args:
            fwd_hooks: List[Tuple[name, hook]], where name is either the name of a hook point or a
                Boolean function on hook names and hook is the function to add to that hook point.
            bwd_hooks: Same as fwd_hooks, but for the backward pass.
            reset_hooks_end (bool): If True, removes all hooks added by this context manager when the context manager exits.
            clear_contexts (bool): If True, clears hook contexts whenever hooks are reset.

        Example:

        .. code-block:: python

            with model.hooks(fwd_hooks=my_hooks):
                hooked_loss = model(text, return_type="loss")
        """
        try:
            self.context_level += 1

            for name, hook in fwd_hooks:
                self._enable_hook(name=name, hook=hook, dir="fwd")
            for name, hook in bwd_hooks:
                self._enable_hook(name=name, hook=hook, dir="bwd")
            yield self
        finally:
            if reset_hooks_end:
                self.reset_hooks(
                    clear_contexts, including_permanent=False, level=self.context_level
                )
            self.context_level -= 1

    def run_with_hooks(
        self,
        *model_args: Any,  # TODO: unsure about whether or not this Any typing is correct or not; may need to be replaced with something more specific?
        fwd_hooks: list[tuple[Union[str, Callable], Callable]] = [],
        bwd_hooks: list[tuple[Union[str, Callable], Callable]] = [],
        reset_hooks_end: bool = True,
        clear_contexts: bool = False,
        **model_kwargs: Any,
    ):
        """
        Runs the model with specified forward and backward hooks.

        Args:
            fwd_hooks (List[Tuple[Union[str, Callable], Callable]]): A list of (name, hook), where name is
                either the name of a hook point or a boolean function on hook names, and hook is the
                function to add to that hook point. Hooks with names that evaluate to True are added
                respectively.
            bwd_hooks (List[Tuple[Union[str, Callable], Callable]]): Same as fwd_hooks, but for the
                backward pass.
            reset_hooks_end (bool): If True, all hooks are removed at the end, including those added
                during this run. Default is True.
            clear_contexts (bool): If True, clears hook contexts whenever hooks are reset. Default is
                False.
            *model_args: Positional arguments for the model.
            **model_kwargs: Keyword arguments for the model's forward function. See your related
                models forward pass for details as to what sort of arguments you can pass through.

        Note:
            If you want to use backward hooks, set `reset_hooks_end` to False, so the backward hooks
            remain active. This function only runs a forward pass.
        """
        if len(bwd_hooks) > 0 and reset_hooks_end:
            logging.warning(
                "WARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur."
            )

        with self.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) as hooked_model:
            return hooked_model.forward(*model_args, **model_kwargs)

    def add_caching_hooks(
        self,
        names_filter: NamesFilter = None,
        incl_bwd: bool = False,
        device: DeviceType = None,  # TODO: unsure about whether or not this device typing is correct or not?
        remove_batch_dim: bool = False,
        cache: Optional[dict] = None,
    ) -> dict:
        """Adds hooks to the model to cache activations. Note: It does NOT actually run the model to get activations, that must be done separately.

        Args:
            names_filter (NamesFilter, optional): Which activations to cache. Can be a list of strings (hook names) or a filter function mapping hook names to booleans. Defaults to lambda name: True.
            incl_bwd (bool, optional): Whether to also do backwards hooks. Defaults to False.
            device (_type_, optional): The device to store on. Defaults to same device as model.
            remove_batch_dim (bool, optional): Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.
            cache (Optional[dict], optional): The cache to store activations in, a new dict is created by default. Defaults to None.

        Returns:
            cache (dict): The cache where activations will be stored.
        """
        if cache is None:
            cache = {}

        if names_filter is None:
            names_filter = lambda name: True
        elif isinstance(names_filter, str):
            filter_str = names_filter
            names_filter = lambda name: name == filter_str
        elif isinstance(names_filter, list):
            filter_list = names_filter
            names_filter = lambda name: name in filter_list

        assert callable(names_filter), "names_filter must be a callable"

        self.is_caching = True

        def save_hook(tensor: Tensor, hook: HookPoint, is_backward: bool):
            assert hook.name is not None
            hook_name = hook.name
            if is_backward:
                hook_name += "_grad"
            if remove_batch_dim:
                cache[hook_name] = tensor.detach().to(device)[0]
            else:
                cache[hook_name] = tensor.detach().to(device)

        for name, hp in self.hook_dict.items():
            if names_filter(name):
                hp.add_hook(partial(save_hook, is_backward=False), "fwd")
                if incl_bwd:
                    hp.add_hook(partial(save_hook, is_backward=True), "bwd")
        return cache

    def run_with_cache(
        self,
        *model_args: Any,
        names_filter: NamesFilter = None,
        device: DeviceType = None,
        remove_batch_dim: bool = False,
        incl_bwd: bool = False,
        reset_hooks_end: bool = True,
        clear_contexts: bool = False,
        pos_slice: Optional[Union[Slice, SliceInput]] = None,
        **model_kwargs: Any,
    ):
        """
        Runs the model and returns the model output and a Cache object.

        Args:
            *model_args: Positional arguments for the model.
            names_filter (NamesFilter, optional): A filter for which activations to cache. Accepts None, str,
                list of str, or a function that takes a string and returns a bool. Defaults to None, which
                means cache everything.
            device (str or torch.Device, optional): The device to cache activations on. Defaults to the
                model device. WARNING: Setting a different device than the one used by the model leads to
                significant performance degradation.
            remove_batch_dim (bool, optional): If True, removes the batch dimension when caching. Only
                makes sense with batch_size=1 inputs. Defaults to False.
            incl_bwd (bool, optional): If True, calls backward on the model output and caches gradients
                as well. Assumes that the model outputs a scalar (e.g., return_type="loss"). Custom loss
                functions are not supported. Defaults to False.
            reset_hooks_end (bool, optional): If True, removes all hooks added by this function at the
                end of the run. Defaults to True.
            clear_contexts (bool, optional): If True, clears hook contexts whenever hooks are reset.
                Defaults to False.
            pos_slice:
                The slice to apply to the cache output. Defaults to None, do nothing.
            **model_kwargs: Keyword arguments for the model's forward function. See your related
                models forward pass for details as to what sort of arguments you can pass through.

        Returns:
            tuple: A tuple containing the model output and a Cache object.

        """

        pos_slice = Slice.unwrap(pos_slice)

        cache_dict, fwd, bwd = self.get_caching_hooks(
            names_filter,
            incl_bwd,
            device,
            remove_batch_dim=remove_batch_dim,
            pos_slice=pos_slice,
        )

        with self.hooks(
            fwd_hooks=fwd,
            bwd_hooks=bwd,
            reset_hooks_end=reset_hooks_end,
            clear_contexts=clear_contexts,
        ):
            model_out = self(*model_args, **model_kwargs)
            if incl_bwd:
                model_out.backward()

        return model_out, cache_dict

    def get_caching_hooks(
        self,
        names_filter: NamesFilter = None,
        incl_bwd: bool = False,
        device: DeviceType = None,
        remove_batch_dim: bool = False,
        cache: Optional[dict] = None,
        pos_slice: Optional[Union[Slice, SliceInput]] = None,
    ) -> tuple[dict, list, list]:
        """Creates hooks to cache activations. Note: It does not add the hooks to the model.

        Args:
            names_filter (NamesFilter, optional): Which activations to cache. Can be a list of strings (hook names) or a filter function mapping hook names to booleans. Defaults to lambda name: True.
            incl_bwd (bool, optional): Whether to also do backwards hooks. Defaults to False.
            device (_type_, optional): The device to store on. Keeps on the same device as the layer if None.
            remove_batch_dim (bool, optional): Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.
            cache (Optional[dict], optional): The cache to store activations in, a new dict is created by default. Defaults to None.

        Returns:
            cache (dict): The cache where activations will be stored.
            fwd_hooks (list): The forward hooks.
            bwd_hooks (list): The backward hooks. Empty if incl_bwd is False.
        """
        if cache is None:
            cache = {}

        pos_slice = Slice.unwrap(pos_slice)

        if names_filter is None:
            names_filter = lambda name: True
        elif isinstance(names_filter, str):
            filter_str = names_filter
            names_filter = lambda name: name == filter_str
        elif isinstance(names_filter, list):
            filter_list = names_filter
            names_filter = lambda name: name in filter_list
        elif callable(names_filter):
            names_filter = names_filter
        else:
            raise ValueError("names_filter must be a string, list of strings, or function")
        assert callable(names_filter)  # Callable[[str], bool]

        self.is_caching = True

        def save_hook(tensor: Tensor, hook: HookPoint, is_backward: bool = False):
            # for attention heads the pos dimension is the third from last
            if hook.name is None:
                raise RuntimeError("Hook should have been provided a name")

            hook_name = hook.name
            if is_backward:
                hook_name += "_grad"
            resid_stream = tensor.detach().to(device)
            if remove_batch_dim:
                resid_stream = resid_stream[0]

            if (
                hook.name.endswith("hook_q")
                or hook.name.endswith("hook_k")
                or hook.name.endswith("hook_v")
                or hook.name.endswith("hook_z")
                or hook.name.endswith("hook_result")
            ):
                pos_dim = -3
            else:
                # for all other components the pos dimension is the second from last
                # including the attn scores where the dest token is the second from last
                pos_dim = -2

            if (
                tensor.dim() >= -pos_dim
            ):  # check if the residual stream has a pos dimension before trying to slice
                resid_stream = pos_slice.apply(resid_stream, dim=pos_dim)
            cache[hook_name] = resid_stream

        fwd_hooks = []
        bwd_hooks = []
        for name, _ in self.hook_dict.items():
            if names_filter(name):
                fwd_hooks.append((name, partial(save_hook, is_backward=False)))
                if incl_bwd:
                    bwd_hooks.append((name, partial(save_hook, is_backward=True)))

        return cache, fwd_hooks, bwd_hooks

    def cache_all(
        self,
        cache: Optional[dict],
        incl_bwd: bool = False,
        device: DeviceType = None,
        remove_batch_dim: bool = False,
    ):
        logging.warning(
            "cache_all is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache"
        )
        self.add_caching_hooks(
            names_filter=lambda name: True,
            cache=cache,
            incl_bwd=incl_bwd,
            device=device,
            remove_batch_dim=remove_batch_dim,
        )

    def cache_some(
        self,
        cache: Optional[dict],
        names: Callable[[str], bool],
        incl_bwd: bool = False,
        device: DeviceType = None,
        remove_batch_dim: bool = False,
    ):
        """Cache a list of hook provided by names, Boolean function on names"""
        logging.warning(
            "cache_some is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache"
        )
        self.add_caching_hooks(
            names_filter=names,
            cache=cache,
            incl_bwd=incl_bwd,
            device=device,
            remove_batch_dim=remove_batch_dim,
        )


# %%



---
File: /transformer_lens/HookedEncoder.py
---

"""Hooked Encoder.

Contains a BERT style model. This is separate from :class:`transformer_lens.HookedTransformer`
because it has a significantly different architecture to e.g. GPT style transformers.
"""

from __future__ import annotations

import logging
import os
from typing import Any, Dict, List, Optional, Tuple, TypeVar, Union, overload

import torch
import torch.nn as nn
from einops import repeat
from jaxtyping import Float, Int
from transformers.models.auto.tokenization_auto import AutoTokenizer
from typing_extensions import Literal

import transformer_lens.loading_from_pretrained as loading
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.components import (
    MLP,
    Attention,
    BertBlock,
    BertEmbed,
    BertMLMHead,
    BertNSPHead,
    BertPooler,
    Unembed,
)
from transformer_lens.FactoredMatrix import FactoredMatrix
from transformer_lens.hook_points import HookedRootModule, HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities import devices

T = TypeVar("T", bound="HookedEncoder")


class HookedEncoder(HookedRootModule):
    """
    This class implements a BERT-style encoder using the components in ./components.py, with HookPoints on every interesting activation. It inherits from HookedRootModule.

    Limitations:
    - The model does not include dropouts, which may lead to inconsistent results from training or fine-tuning.

    Like HookedTransformer, it can have a pretrained Transformer's weights loaded via `.from_pretrained`. There are a few features you might know from HookedTransformer which are not yet supported:
        - There is no preprocessing (e.g. LayerNorm folding) when loading a pretrained model
    """

    def __init__(
        self,
        cfg: Union[HookedTransformerConfig, Dict],
        tokenizer: Optional[Any] = None,
        move_to_device: bool = True,
        **kwargs: Any,
    ):
        super().__init__()
        if isinstance(cfg, Dict):
            cfg = HookedTransformerConfig(**cfg)
        elif isinstance(cfg, str):
            raise ValueError(
                "Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a pretrained model, use HookedEncoder.from_pretrained() instead."
            )
        self.cfg = cfg

        assert self.cfg.n_devices == 1, "Multiple devices not supported for HookedEncoder"
        if tokenizer is not None:
            self.tokenizer = tokenizer
        elif self.cfg.tokenizer_name is not None:
            huggingface_token = os.environ.get("HF_TOKEN", "")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.cfg.tokenizer_name,
                token=huggingface_token if len(huggingface_token) > 0 else None,
            )
        else:
            self.tokenizer = None

        if self.cfg.d_vocab == -1:
            # If we have a tokenizer, vocab size can be inferred from it.
            assert self.tokenizer is not None, "Must provide a tokenizer if d_vocab is not provided"
            self.cfg.d_vocab = max(self.tokenizer.vocab.values()) + 1
        if self.cfg.d_vocab_out == -1:
            self.cfg.d_vocab_out = self.cfg.d_vocab

        self.embed = BertEmbed(self.cfg)
        self.blocks = nn.ModuleList([BertBlock(self.cfg) for _ in range(self.cfg.n_layers)])
        self.mlm_head = BertMLMHead(self.cfg)
        self.unembed = Unembed(self.cfg)
        self.nsp_head = BertNSPHead(self.cfg)
        self.pooler = BertPooler(self.cfg)

        self.hook_full_embed = HookPoint()

        if move_to_device:
            if self.cfg.device is None:
                raise ValueError("Cannot move to device when device is None")
            self.to(self.cfg.device)

        self.setup()

    def to_tokens(
        self,
        input: Union[str, List[str]],
        move_to_device: bool = True,
        truncate: bool = True,
    ) -> Tuple[
        Int[torch.Tensor, "batch pos"],
        Int[torch.Tensor, "batch pos"],
        Int[torch.Tensor, "batch pos"],
    ]:
        """Converts a string to a tensor of tokens.
        Taken mostly from the HookedTransformer implementation, but does not support default padding
        sides or prepend_bos.
        Args:
            input (Union[str, List[str]]): The input to tokenize.
            move_to_device (bool): Whether to move the output tensor of tokens to the device the model lives on. Defaults to True
            truncate (bool): If the output tokens are too long, whether to truncate the output
            tokens to the model's max context window. Does nothing for shorter inputs. Defaults to
            True.
        """

        assert self.tokenizer is not None, "Cannot use to_tokens without a tokenizer"

        encodings = self.tokenizer(
            input,
            return_tensors="pt",
            padding=True,
            truncation=truncate,
            max_length=self.cfg.n_ctx if truncate else None,
        )

        tokens = encodings.input_ids
        token_type_ids = encodings.token_type_ids
        attention_mask = encodings.attention_mask

        if move_to_device:
            tokens = tokens.to(self.cfg.device)
            token_type_ids = token_type_ids.to(self.cfg.device)
            attention_mask = attention_mask.to(self.cfg.device)

        return tokens, token_type_ids, attention_mask

    def encoder_output(
        self,
        tokens: Int[torch.Tensor, "batch pos"],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_vocab"]:
        """Processes input through the encoder layers and returns the resulting residual stream.

        Args:
            input: Input tokens as integers with shape (batch, position)
            token_type_ids: Optional binary ids indicating segment membership.
                Shape (batch_size, sequence_length). For example, with input
                "[CLS] Sentence A [SEP] Sentence B [SEP]", token_type_ids would be
                [0, 0, ..., 0, 1, ..., 1, 1] where 0 marks tokens from sentence A
                and 1 marks tokens from sentence B.
            one_zero_attention_mask: Optional binary mask of shape (batch_size, sequence_length)
                where 1 indicates tokens to attend to and 0 indicates tokens to ignore.
                Used primarily for handling padding in batched inputs.

        Returns:
            resid: Final residual stream tensor of shape (batch, position, d_model)

        Raises:
            AssertionError: If using string input without a tokenizer
        """

        if tokens.device.type != self.cfg.device:
            tokens = tokens.to(self.cfg.device)
            if one_zero_attention_mask is not None:
                one_zero_attention_mask = one_zero_attention_mask.to(self.cfg.device)

        resid = self.hook_full_embed(self.embed(tokens, token_type_ids))

        large_negative_number = -torch.inf
        mask = (
            repeat(1 - one_zero_attention_mask, "batch pos -> batch 1 1 pos")
            if one_zero_attention_mask is not None
            else None
        )
        additive_attention_mask = (
            torch.where(mask == 1, large_negative_number, 0) if mask is not None else None
        )

        for block in self.blocks:
            resid = block(resid, additive_attention_mask)

        return resid

    @overload
    def forward(
        self,
        input: Union[
            str,
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Union[Literal["logits"], Literal["predictions"]],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Union[Float[torch.Tensor, "batch pos d_vocab"], str, List[str]]:
        ...

    @overload
    def forward(
        self,
        input: Union[
            str,
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Literal[None],
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Union[Float[torch.Tensor, "batch pos d_vocab"], str, List[str]]]:
        ...

    def forward(
        self,
        input: Union[
            str,
            List[str],
            Int[torch.Tensor, "batch pos"],
        ],
        return_type: Optional[Union[Literal["logits"], Literal["predictions"]]] = "logits",
        token_type_ids: Optional[Int[torch.Tensor, "batch pos"]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Union[Float[torch.Tensor, "batch pos d_vocab"], str, List[str]]]:
        """Forward pass through the HookedEncoder. Performs Masked Language Modelling on the given input.

        Args:
            input: The input to process. Can be one of:
                - str: A single text string
                - List[str]: A list of text strings
                - torch.Tensor: Input tokens as integers with shape (batch, position)
            return_type: Optional[str]: The type of output to return. Can be one of:
                - None: Return nothing, don't calculate logits
                - 'logits': Return logits tensor
                - 'predictions': Return human-readable predictions
            token_type_ids: Optional[torch.Tensor]: Binary ids indicating whether a token belongs
                to sequence A or B. For example, for two sentences:
                "[CLS] Sentence A [SEP] Sentence B [SEP]", token_type_ids would be
                [0, 0, ..., 0, 1, ..., 1, 1]. `0` represents tokens from Sentence A,
                `1` from Sentence B. If not provided, BERT assumes a single sequence input.
                This parameter gets inferred from the the tokenizer if input is a string or list of strings.
                Shape is (batch_size, sequence_length).
            one_zero_attention_mask: Optional[torch.Tensor]: A binary mask which indicates
                which tokens should be attended to (1) and which should be ignored (0).
                Primarily used for padding variable-length sentences in a batch.
                For instance, in a batch with sentences of differing lengths, shorter
                sentences are padded with 0s on the right. If not provided, the model
                assumes all tokens should be attended to.
                This parameter gets inferred from the tokenizer if input is a string or list of strings.
                Shape is (batch_size, sequence_length).

        Returns:
            Optional[torch.Tensor]: Depending on return_type:
                - None: Returns None if return_type is None
                - torch.Tensor: Returns logits if return_type is 'logits' (or if return_type is not explicitly provided)
                    - Shape is (batch_size, sequence_length, d_vocab)
                - str or List[str]: Returns predicted words for masked tokens if return_type is 'predictions'.
                    Returns a list of strings if input is a list of strings, otherwise a single string.

        Raises:
            AssertionError: If using string input without a tokenizer
        """

        if isinstance(input, str) or isinstance(input, list):
            assert self.tokenizer is not None, "Must provide a tokenizer if input is a string"
            tokens, token_type_ids_from_tokenizer, attention_mask = self.to_tokens(input)

            # If token_type_ids or attention mask are not provided, use the ones from the tokenizer
            token_type_ids = (
                token_type_ids_from_tokenizer if token_type_ids is None else token_type_ids
            )
            one_zero_attention_mask = (
                attention_mask if one_zero_attention_mask is None else one_zero_attention_mask
            )

        else:
            tokens = input

        resid = self.encoder_output(tokens, token_type_ids, one_zero_attention_mask)

        # MLM requires an unembedding step
        resid = self.mlm_head(resid)
        logits = self.unembed(resid)

        if return_type == "predictions":
            assert (
                self.tokenizer is not None
            ), "Must have a tokenizer to use return_type='predictions'"
            # Get predictions for masked tokens
            logprobs = logits[tokens == self.tokenizer.mask_token_id].log_softmax(dim=-1)
            predictions = self.tokenizer.decode(logprobs.argmax(dim=-1))

            # If input was a list of strings, split predictions into a list
            if " " in predictions:
                # Split along space
                predictions = predictions.split(" ")
                predictions = [f"Prediction {i}: {p}" for i, p in enumerate(predictions)]
            return predictions

        elif return_type == None:
            return None

        return logits

    @overload
    def run_with_cache(
        self, *model_args: Any, return_cache_object: Literal[True] = True, **kwargs: Any
    ) -> Tuple[Float[torch.Tensor, "batch pos d_vocab"], ActivationCache]:
        ...

    @overload
    def run_with_cache(
        self, *model_args: Any, return_cache_object: Literal[False], **kwargs: Any
    ) -> Tuple[Float[torch.Tensor, "batch pos d_vocab"], Dict[str, torch.Tensor]]:
        ...

    def run_with_cache(
        self,
        *model_args: Any,
        return_cache_object: bool = True,
        remove_batch_dim: bool = False,
        **kwargs: Any,
    ) -> Tuple[
        Float[torch.Tensor, "batch pos d_vocab"],
        Union[ActivationCache, Dict[str, torch.Tensor]],
    ]:
        """
        Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule. This function was copied directly from HookedTransformer.
        """
        out, cache_dict = super().run_with_cache(
            *model_args, remove_batch_dim=remove_batch_dim, **kwargs
        )
        if return_cache_object:
            cache = ActivationCache(cache_dict, self, has_batch_dim=not remove_batch_dim)
            return out, cache
        else:
            return out, cache_dict

    def to(  # type: ignore
        self,
        device_or_dtype: Union[torch.device, str, torch.dtype],
        print_details: bool = True,
    ):
        return devices.move_to_and_update_config(self, device_or_dtype, print_details)

    def cuda(self: T, device: Optional[Union[int, torch.device]] = None) -> T:
        if isinstance(device, int):
            return self.to(f"cuda:{device}")
        elif device is None:
            return self.to("cuda")
        else:
            return self.to(device)

    def cpu(self: T) -> T:
        return self.to("cpu")

    def mps(self: T) -> T:
        return self.to(torch.device("mps"))

    @classmethod
    def from_pretrained(
        cls,
        model_name: str,
        checkpoint_index: Optional[int] = None,
        checkpoint_value: Optional[int] = None,
        hf_model: Optional[Any] = None,
        device: Optional[str] = None,
        tokenizer: Optional[Any] = None,
        move_to_device: bool = True,
        dtype: torch.dtype = torch.float32,
        **from_pretrained_kwargs: Any,
    ) -> HookedEncoder:
        """Loads in the pretrained weights from huggingface. Currently supports loading weight from HuggingFace BertForMaskedLM. Unlike HookedTransformer, this does not yet do any preprocessing on the model."""
        logging.warning(
            "Support for BERT in TransformerLens is currently experimental, until such a time when it has feature "
            "parity with HookedTransformer and has been tested on real research tasks. Until then, backward "
            "compatibility is not guaranteed. Please see the docs for information on the limitations of the current "
            "implementation."
            "\n"
            "If using BERT for interpretability research, keep in mind that BERT has some significant architectural "
            "differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning "
            "that the last LayerNorm in a block cannot be folded."
        )

        assert not (
            from_pretrained_kwargs.get("load_in_8bit", False)
            or from_pretrained_kwargs.get("load_in_4bit", False)
        ), "Quantization not supported"

        if "torch_dtype" in from_pretrained_kwargs:
            dtype = from_pretrained_kwargs["torch_dtype"]

        official_model_name = loading.get_official_model_name(model_name)

        cfg = loading.get_pretrained_model_config(
            official_model_name,
            checkpoint_index=checkpoint_index,
            checkpoint_value=checkpoint_value,
            fold_ln=False,
            device=device,
            n_devices=1,
            dtype=dtype,
            **from_pretrained_kwargs,
        )

        state_dict = loading.get_pretrained_state_dict(
            official_model_name, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs
        )

        model = cls(cfg, tokenizer, move_to_device=False)

        model.load_state_dict(state_dict, strict=False)

        if move_to_device:
            model.to(cfg.device)

        print(f"Loaded pretrained model {model_name} into HookedEncoder")

        return model

    @property
    def W_U(self) -> Float[torch.Tensor, "d_model d_vocab"]:
        """
        Convenience to get the unembedding matrix (ie the linear map from the final residual stream to the output logits)
        """
        return self.unembed.W_U

    @property
    def b_U(self) -> Float[torch.Tensor, "d_vocab"]:
        """
        Convenience to get the unembedding bias
        """
        return self.unembed.b_U

    @property
    def W_E(self) -> Float[torch.Tensor, "d_vocab d_model"]:
        """
        Convenience to get the embedding matrix
        """
        return self.embed.embed.W_E

    @property
    def W_pos(self) -> Float[torch.Tensor, "n_ctx d_model"]:
        """
        Convenience function to get the positional embedding. Only works on models with absolute positional embeddings!
        """
        return self.embed.pos_embed.W_pos

    @property
    def W_E_pos(self) -> Float[torch.Tensor, "d_vocab+n_ctx d_model"]:
        """
        Concatenated W_E and W_pos. Used as a full (overcomplete) basis of the input space, useful for full QK and full OV circuits.
        """
        return torch.cat([self.W_E, self.W_pos], dim=0)

    @property
    def W_K(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the key weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.W_K for block in self.blocks], dim=0)

    @property
    def W_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the query weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.W_Q for block in self.blocks], dim=0)

    @property
    def W_V(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the value weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.W_V for block in self.blocks], dim=0)

    @property
    def W_O(self) -> Float[torch.Tensor, "n_layers n_heads d_head d_model"]:
        """Stacks the attn output weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.W_O for block in self.blocks], dim=0)

    @property
    def W_in(self) -> Float[torch.Tensor, "n_layers d_model d_mlp"]:
        """Stacks the MLP input weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.mlp, MLP)
        return torch.stack([block.mlp.W_in for block in self.blocks], dim=0)

    @property
    def W_out(self) -> Float[torch.Tensor, "n_layers d_mlp d_model"]:
        """Stacks the MLP output weights across all layers"""
        for block in self.blocks:
            assert isinstance(block.mlp, MLP)
        return torch.stack([block.mlp.W_out for block in self.blocks], dim=0)

    @property
    def b_K(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the key biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.b_K for block in self.blocks], dim=0)

    @property
    def b_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the query biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.b_Q for block in self.blocks], dim=0)

    @property
    def b_V(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the value biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.b_V for block in self.blocks], dim=0)

    @property
    def b_O(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stacks the attn output biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.attn, Attention)
        return torch.stack([block.attn.b_O for block in self.blocks], dim=0)

    @property
    def b_in(self) -> Float[torch.Tensor, "n_layers d_mlp"]:
        """Stacks the MLP input biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.mlp, MLP)
        return torch.stack([block.mlp.b_in for block in self.blocks], dim=0)

    @property
    def b_out(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stacks the MLP output biases across all layers"""
        for block in self.blocks:
            assert isinstance(block.mlp, MLP)
        return torch.stack([block.mlp.b_out for block in self.blocks], dim=0)

    @property
    def QK(self) -> FactoredMatrix:  # [n_layers, n_heads, d_model, d_model]
        """Returns a FactoredMatrix object with the product of the Q and K matrices for each layer and head.
        Useful for visualizing attention patterns."""
        return FactoredMatrix(self.W_Q, self.W_K.transpose(-2, -1))

    @property
    def OV(self) -> FactoredMatrix:  # [n_layers, n_heads, d_model, d_model]
        """Returns a FactoredMatrix object with the product of the O and V matrices for each layer and head."""
        return FactoredMatrix(self.W_V, self.W_O)

    def all_head_labels(self) -> List[str]:
        """Returns a list of strings with the format "L{l}H{h}", where l is the layer index and h is the head index."""
        return [f"L{l}H{h}" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)]



---
File: /transformer_lens/HookedEncoderDecoder.py
---

"""Hooked EncoderDecoder

Contains a T5 style model. This is separate from :class:`transformer_lens.HookedTransformer`
because it has a significantly different architecture to e.g. GPT style transformers.
"""

from __future__ import annotations

import logging
import os
from itertools import chain
from pathlib import Path
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Tuple,
    Type,
    TypeVar,
    Union,
    cast,
    overload,
)

import torch
import tqdm
from einops import repeat
from jaxtyping import Float, Int
from torch import nn
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from typing_extensions import Literal

import transformer_lens.loading_from_pretrained as loading
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.components import MLP, Embed, GatedMLP, RMSNorm, T5Block, Unembed
from transformer_lens.FactoredMatrix import FactoredMatrix
from transformer_lens.hook_points import HookedRootModule, HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities import devices
from transformer_lens.utils import sample_logits

T = TypeVar("T", bound="HookedEncoderDecoder")


class HookedEncoderDecoder(HookedRootModule):
    """
    This class implements a T5 encoder-decoder using the components in ./components.py, with HookPoints on every interesting activation. It inherits from HookedRootModule.

    Limitations:
    - Also note that model does not include dropouts, which may lead to inconsistent results from training or fine-tuning.

    Like HookedTransformer, it can have a pretrained Transformer's weights loaded via `.from_pretrained`. There are a few features you might know from HookedTransformer which are not yet supported:
        - There is no preprocessing (e.g. LayerNorm folding) when loading a pretrained model
        - The model only accepts tokens as inputs, and not strings, or lists of strings
    """

    tokenizer: Optional[PreTrainedTokenizerBase]

    def __init__(
        self,
        cfg: Union[HookedTransformerConfig, Dict],
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        move_to_device: bool = True,
        **kwargs: Any,
    ):
        super().__init__()
        if isinstance(cfg, Dict):
            cfg = HookedTransformerConfig(**cfg)
        elif isinstance(cfg, str):
            raise ValueError(
                "Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a pretrained model, use HookedEncoderDecoder.from_pretrained() instead."
            )
        self.cfg: HookedTransformerConfig = cfg

        if self.cfg.n_devices != 1:
            raise ValueError("Multiple devices not supported for HookedEncoderDecoder")
        if tokenizer is not None:
            self.tokenizer = tokenizer
        elif self.cfg.tokenizer_name is not None:
            huggingface_token = os.environ.get("HF_TOKEN", "")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.cfg.tokenizer_name,
                token=huggingface_token if len(huggingface_token) > 0 else None,
            )
        else:
            self.tokenizer = None

        if self.cfg.d_vocab == -1:
            # If we have a tokenizer, vocab size can be inferred from it.
            if self.tokenizer is None:
                raise ValueError("Must provide a tokenizer if d_vocab is not provided")

            self.cfg.d_vocab = len(self.tokenizer)
        if self.cfg.d_vocab_out == -1:
            self.cfg.d_vocab_out = self.cfg.d_vocab

        self.embed = Embed(self.cfg)
        self.encoder = nn.ModuleList(
            [
                T5Block(self.cfg, num_layer, is_decoder=False)
                for num_layer in range(self.cfg.n_layers)
            ]
        )
        self.encoder_final_ln = RMSNorm(self.cfg)
        self.decoder = nn.ModuleList(
            [
                T5Block(self.cfg, num_layer, is_decoder=True)
                for num_layer in range(self.cfg.n_layers)
            ]
        )
        self.decoder_final_ln = RMSNorm(self.cfg)
        # self.lm_head = nn.Linear(self.cfg.d_model, self.cfg.d_vocab_out)
        self.unembed = Unembed(self.cfg)

        self.hook_embed = HookPoint()

        if move_to_device:
            self.to(self.cfg.device)

        self.setup()

    def to_tokens(
        self,
        input: Union[str, List[str]],
        move_to_device: bool = True,
        truncate: bool = True,
    ) -> Tuple[Int[torch.Tensor, "batch pos"], Int[torch.Tensor, "batch pos"]]:
        """Converts a string to a tensor of tokens.
        Taken mostly from the HookedTransformer implementation, but does not support default padding
        sides or prepend_bos.

        Args:
            input (Union[str, List[str]]): The input to tokenize.
            move_to_device (bool): Whether to move the output tensor of tokens to the device the
                model lives on. Defaults to True
            truncate (bool): If the output tokens are too long, whether to truncate the output
                tokens to the model's max context window. Does nothing for shorter inputs.
                Defaults to True.
        """

        assert self.tokenizer is not None, "Cannot use to_tokens without a tokenizer"

        encodings = self.tokenizer(
            input,
            return_tensors="pt",
            padding=True,
            truncation=truncate,
            max_length=self.cfg.n_ctx if truncate else None,
        )

        tokens = encodings.input_ids
        attention_mask = encodings.attention_mask

        if move_to_device:
            tokens = tokens.to(self.cfg.device)
            attention_mask = attention_mask.to(self.cfg.device)
        return tokens, attention_mask

    @overload
    def forward(
        self,
        input: Union[str, List[str], Int[torch.Tensor, "batch pos"]],
        decoder_input: Optional[Int[torch.Tensor, "batch decoder_pos"]] = None,
        return_type: Literal["logits"] = "logits",
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Float[torch.Tensor, "batch pos d_vocab"]:
        ...

    @overload
    def forward(
        self,
        input: Union[str, List[str], Int[torch.Tensor, "batch pos"]],
        decoder_input: Optional[Int[torch.Tensor, "batch decoder_pos"]] = None,
        return_type: Optional[Literal[None]] = None,
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Float[torch.Tensor, "batch pos d_vocab"]]:
        ...

    def forward(
        self,
        input: Union[str, List[str], Int[torch.Tensor, "batch pos"]],
        decoder_input: Optional[Int[torch.Tensor, "batch decoder_pos"]] = None,
        return_type: Optional[str] = "logits",
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    ) -> Optional[Float[torch.Tensor, "batch decoder_pos d_vocab"]]:
        """Forward pass of the T5 model.

        Args:
            input: Input to be processed. Can be one of:
                - str: A single string input
                - List[str]: A batch of string inputs
                - Int[torch.Tensor, "batch pos"]: A batch of token IDs
            decoder_input: Tensor of shape (batch, decoder_pos) containing the decoder input sequence.
                If None and input is of type str or List[str], starts with batch of beginning-of-sequence (BOS) tokens.
            return_type: Specifies the model output type:
                - "logits": Return logits tensor
                - None: Returns nothing
            one_zero_attention_mask: A binary mask which indicates
                which tokens should be attended to (1) and which should be ignored (0).
                Primarily used for padding variable-length sentences in a batch.
                For instance, in a batch with sentences of differing lengths, shorter
                sentences are padded with 0s on the right. If not provided, the model
                assumes all tokens should be attended to.
                This parameter gets inferred from the tokenizer if input is a string or list of strings.
                Shape is (batch_size, sequence_length).

        Returns:
            Optional[Float[torch.Tensor, "batch decoder_pos d_vocab"]]:
                If return_type="logits": Returns logits tensor of shape (batch, decoder_pos, vocab_size)
                If return_type=None: Returns None
        """

        if isinstance(input, (str, list)):
            tokens, attention_mask = self.to_tokens(input)

            # If attention mask is not provided, use the ones from the tokenizer
            one_zero_attention_mask = (
                attention_mask if one_zero_attention_mask is None else one_zero_attention_mask
            )

            # If decoder_input is not provided, start with tensor of PAD tokens of shape (batch, 1)
            if decoder_input is None:
                assert self.tokenizer is not None
                decoder_input = torch.full(
                    (tokens.shape[0], 1),
                    self.tokenizer.pad_token_id,
                    device=self.cfg.device,
                )
        else:
            tokens = input

            if one_zero_attention_mask is None:
                logging.warning(
                    "No attention mask provided. Assuming all tokens should be attended to."
                )

            if decoder_input is None:
                raise ValueError(
                    "Must provide decoder_input if input is not a string or list of strings"
                )

        if tokens.device.type != self.cfg.device:
            tokens = tokens.to(self.cfg.device)

        if one_zero_attention_mask is not None:
            one_zero_attention_mask = one_zero_attention_mask.to(self.cfg.device)

        resid = self.hook_embed(self.embed(tokens))

        if one_zero_attention_mask is not None:
            additive_attention_mask = (
                repeat(1 - one_zero_attention_mask, "batch pos -> batch 1 1 pos")
            ) * torch.finfo(self.cfg.dtype).min
        else:
            additive_attention_mask = None

        query_len = key_len = tokens.shape[1]

        encoder_positional_bias = cast(
            T5Block, self.encoder[0]
        ).attn.compute_relative_attention_bias(query_len, key_len, device=self.cfg.device)

        for encoder_block in self.encoder:
            resid = encoder_block(
                resid_pre=resid,
                additive_attention_mask=additive_attention_mask,
                position_bias=encoder_positional_bias,
            )

        encoder_resid = self.encoder_final_ln(resid)

        if decoder_input is None:
            raise ValueError("decoder_input cannot be None when input is not a string")
        decoder_resid = self.embed(decoder_input)
        decoder_query_len = decoder_key_len = decoder_input.shape[1]
        decoder_positional_bias = cast(
            T5Block, self.decoder[0]
        ).attn.compute_relative_attention_bias(
            decoder_query_len, decoder_key_len, device=self.cfg.device
        )

        for decoder_block in self.decoder:
            decoder_resid = decoder_block(
                resid_pre=decoder_resid,
                position_bias=decoder_positional_bias,
                encoder_hidden_states=encoder_resid,
                encoder_additive_attention_mask=additive_attention_mask,
            )

        decoder_resid = self.decoder_final_ln(decoder_resid)

        if self.cfg.tie_word_embeddings:
            # Rescale output before projecting on vocab
            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586
            decoder_resid *= self.cfg.d_model**-0.5

        logits = self.unembed(decoder_resid)
        if return_type is None:
            return None
        return logits

    @torch.inference_mode()
    def generate(
        self,
        input: Union[str, Int[torch.Tensor, "batch pos"]] = "",
        one_zero_attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
        max_new_tokens: int = 10,
        stop_at_eos: bool = True,
        eos_token_id: Optional[Union[int, List[int]]] = None,
        do_sample: bool = True,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: float = 1.0,
        freq_penalty: float = 0.0,
        return_type: Optional[str] = "input",
        verbose: bool = True,
    ) -> Union[Int[torch.Tensor, "batch new_tokens"], str]:
        """Sample tokens from the T5 encoder-decoder model.

        Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.
        This function is primarily taken from HookedTransformer but adjusted for the HookedEncoderDecoder
        architecture.
        This function does not support key value caching and no default padding sides or prepend_bos.

        To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish
        (by producing an EOT token), we keep running the model on the entire batch, but throw away
        the output for a finished sequence and just keep adding EOTs to pad.

        This supports entering a single string, but not a list of strings - if the strings don't
        tokenize to exactly the same length, this gets messy. If that functionality is needed,
        convert them to a batch of tokens and input that instead.

        Args:
            input (Union[str, Int[torch.Tensor, "batch pos"])]): Either a batch of tokens ([batch,
                pos]) or a text string (this will be converted to a batch of tokens with batch size
                1).
            max_new_tokens (int): Maximum number of tokens to generate.
            stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token.
            eos_token_id (Optional[Union[int, Sequence]]): The token ID to use for end
                of sentence. If None, use the tokenizer's eos_token_id - required if using
                stop_at_eos. It's also possible to provide a list of token IDs (not just the
                eos_token_id), in which case the generation will stop when any of them are output
                (useful e.g. for stable_lm).
            do_sample (bool): If True, sample from the model's output distribution. Otherwise, use
                greedy search (take the max logit each time).
            top_k (int): Number of tokens to sample from. If None, sample from all tokens.
            top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0,
                we take the top tokens with cumulative probability >= top_p.
            temperature (float): Temperature for sampling. Higher values will make the model more
                random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is
                sampling from a uniform distribution).
            freq_penalty (float): Frequency penalty for sampling - how much to penalise previous
                tokens. Higher values will make the model more random.
            return_type (Optional[str]): The type of the output to return - either a string (str),
                a tensor of tokens (tensor) or whatever the format of the input was (input).
            verbose (bool): If True, show tqdm progress bars for generation.

        Returns:
            outputs (torch.Tensor): [batch, new_tokens], generated sequence of new tokens
                (by default returns same type as input).
        """

        if isinstance(input, str):
            # If text, convert to tokens (batch_size=1)
            assert (
                self.tokenizer is not None
            ), "Must provide a tokenizer if passing a string to the model"
            encoder_input, attention_mask = self.to_tokens(input)

            # If attention mask is not provided, use the one from the tokenizer
            one_zero_attention_mask = (
                attention_mask if one_zero_attention_mask is None else one_zero_attention_mask
            )
        else:
            assert isinstance(input, torch.Tensor)  # keep mypy happy
            encoder_input = input

            # If tokens are provided, user should be aware that attention mask will not be inferred
            if one_zero_attention_mask is None:
                logging.warning(
                    "No attention mask provided. Assuming all tokens should be attended to."
                )

        if return_type == "input":
            if isinstance(input, str):
                return_type = "str"
            else:
                return_type = "tensor"

        assert isinstance(encoder_input, torch.Tensor)
        batch_size = encoder_input.shape[0]
        device = devices.get_device_for_block_index(0, self.cfg)

        # For the decoder input, we start with a tensor of PAD tokens of shape (batch, 1)
        assert self.tokenizer is not None
        decoder_input = torch.full((batch_size, 1), self.tokenizer.pad_token_id).to(device)

        stop_tokens: List[int] = []
        eos_token_for_padding = 0
        if stop_at_eos:
            tokenizer_has_eos_token = self.tokenizer.eos_token_id is not None

            local_eos_token_id: Optional[Union[int, List[int]]] = eos_token_id
            if local_eos_token_id is None:
                assert (
                    tokenizer_has_eos_token
                ), "Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id"

                local_eos_token_id = self.tokenizer.eos_token_id

            if isinstance(local_eos_token_id, int):
                stop_tokens = [local_eos_token_id]
                eos_token_for_padding = local_eos_token_id
            else:
                # eos_token_id is a Sequence (e.g. list or tuple)
                if local_eos_token_id is None:
                    raise ValueError("eos_token_id cannot be None here")
                stop_tokens = local_eos_token_id
                eos_token_for_padding = (
                    self.tokenizer.eos_token_id
                    if tokenizer_has_eos_token
                    else local_eos_token_id[0]
                )

        # An array to track which sequences in the batch have finished.
        finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.cfg.device)

        # Currently nothing in HookedTransformer changes with eval, but this is here in case
        # that changes in the future.
        self.eval()
        for _ in tqdm.tqdm(range(max_new_tokens), disable=not verbose):
            # While generating, we keep generating logits, throw away all but the final logits,
            # and then use those logits to sample from the distribution We keep adding the
            # sampled tokens to the end of tokens.
            # We input the entire sequence, as a [batch, pos] tensor, since we aren't using
            # the cache.

            # Encoder input will be the same for all iterations
            # Decoder input will be appended with the new token each iteration
            logits = self.forward(
                encoder_input,
                decoder_input=decoder_input,
                one_zero_attention_mask=one_zero_attention_mask,
            )
            assert logits is not None
            final_logits = logits[:, -1, :]

            if do_sample:
                sampled_tokens = sample_logits(
                    final_logits,
                    top_k=top_k,
                    top_p=top_p,
                    temperature=temperature,
                    freq_penalty=freq_penalty,
                    tokens=decoder_input,
                ).to(devices.get_device_for_block_index(0, self.cfg))
            else:
                sampled_tokens = final_logits.argmax(-1).to(
                    devices.get_device_for_block_index(0, self.cfg)
                )

            if stop_at_eos:
                # For all unfinished sequences, add on the next token. If a sequence was
                # finished, throw away the generated token and add eos_token_for_padding
                # instead.
                sampled_tokens[finished_sequences] = eos_token_for_padding
                finished_sequences.logical_or_(
                    torch.isin(
                        sampled_tokens.to(self.cfg.device),
                        torch.tensor(stop_tokens).to(self.cfg.device),
                    )
                )

            # Append new token to the decoder input
            decoder_input = torch.cat([decoder_input, sampled_tokens.unsqueeze(-1)], dim=-1)

            if stop_at_eos and finished_sequences.all():
                break

        if return_type == "str":
            assert self.tokenizer is not None
            # Convert tokens to string
            return self.tokenizer.decode(decoder_input[0], skip_special_tokens=True)

        else:
            return decoder_input

    @overload
    def run_with_cache(
        self, *model_args: Any, return_cache_object: Literal[True] = True, **kwargs: Any
    ) -> Tuple[Float[torch.Tensor, "batch pos d_vocab"], ActivationCache]:
        ...

    @overload
    def run_with_cache(
        self, *model_args: Any, return_cache_object: Literal[False] = False, **kwargs: Any
    ) -> Tuple[Float[torch.Tensor, "batch pos d_vocab"], Dict[str, torch.Tensor]]:
        ...

    def run_with_cache(
        self,
        *model_args: Any,
        return_cache_object: bool = True,
        remove_batch_dim: bool = False,
        **kwargs: Any,
    ) -> Tuple[
        Float[torch.Tensor, "batch pos d_vocab"],
        Union[ActivationCache, Dict[str, torch.Tensor]],
    ]:
        """
        Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule. This function was copied directly from HookedTransformer.
        """
        out, cache_dict = super().run_with_cache(
            *model_args, remove_batch_dim=remove_batch_dim, **kwargs
        )
        if return_cache_object:
            cache = ActivationCache(cache_dict, self, has_batch_dim=not remove_batch_dim)
            return out, cache
        else:
            return out, cache_dict

    def to(self: T, *args: Any, **kwargs: Any) -> T:
        return super().to(*args, **kwargs)

    def cuda(self: T, device: Optional[Union[int, torch.device]] = None) -> T:
        if isinstance(device, int):
            return self.to(f"cuda:{device}")
        elif device is None:
            return self.to("cuda")
        else:
            return self.to(device)

    def cpu(self: T) -> T:
        return self.to("cpu")

    def mps(self: T) -> T:
        return self.to(torch.device("mps"))

    @classmethod
    def from_pretrained(
        cls: Type[T],
        model_name: str,
        checkpoint_index: Optional[int] = None,
        checkpoint_value: Optional[int] = None,
        hf_model: Optional[Any] = None,
        device: Optional[str] = None,
        tokenizer: Optional[Any] = None,
        move_to_device: bool = True,
        dtype: Optional[torch.dtype] = torch.float32,
        **from_pretrained_kwargs: Any,
    ) -> T:
        """Loads in the pretrained weights from huggingface. Currently supports loading weight from HuggingFace BertForMaskedLM. Unlike HookedTransformer, this does not yet do any preprocessing on the model."""
        logging.warning(
            "Support for T5 in TransformerLens is currently experimental, until such a time when it has feature "
            "parity with HookedTransformer and has been tested on real research tasks. Until then, backward "
            "compatibility is not guaranteed. Please see the docs for information on the limitations of the current "
            "implementation."
            "\n"
            "If using T5 for interpretability research, keep in mind that T5 has some significant architectural "
            "differences to GPT. The major one is that T5 is an Encoder-Decoder model"
            "Also, it uses relative positional embeddings, different types of Attention (without bias) and LayerNorm"
        )

        if from_pretrained_kwargs.get("load_in_8bit", False) or from_pretrained_kwargs.get(
            "load_in_4bit", False
        ):
            raise ValueError("Quantization not supported")

        if "torch_dtype" in from_pretrained_kwargs:
            dtype = from_pretrained_kwargs["torch_dtype"]

        if dtype is None:
            dtype = torch.float32

        name_or_path = (
            model_name if Path(model_name).exists() else loading.get_official_model_name(model_name)
        )

        cfg = loading.get_pretrained_model_config(
            name_or_path,
            checkpoint_index=checkpoint_index,
            checkpoint_value=checkpoint_value,
            fold_ln=False,
            device=device,
            n_devices=1,
            dtype=dtype,
            **from_pretrained_kwargs,
        )

        state_dict = loading.get_pretrained_state_dict(
            name_or_path, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs
        )

        model = cls(cfg, tokenizer, move_to_device=False)

        model.load_state_dict(state_dict, strict=False)

        if move_to_device:
            model.to(cfg.device)

        print(f"Loaded pretrained model {model_name} into HookedTransformer")

        return model

    @property
    def W_U(self) -> Float[torch.Tensor, "d_model d_vocab"]:
        """
        Convenience to get the unembedding matrix (ie the linear map from the final residual stream to the output logits)
        """
        return self.unembed.W_U

    @property
    def b_U(self) -> Float[torch.Tensor, "d_vocab"]:
        """
        Convenience to get the unembedding bias
        """
        return self.unembed.b_U

    @property
    def W_E(self) -> Float[torch.Tensor, "d_vocab d_model"]:
        """
        Convenience to get the embedding matrix
        """
        return self.embed.W_E

    @property
    def W_pos(self) -> None:
        """
        Convenience function to get the positional embedding. Only works on models with absolute positional embeddings!
        """
        raise NotImplementedError(
            "T5 does not have absolute positional embeddings. Uses relative positional embeddings instead."
        )

    @property
    def W_K(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the key weights across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.W_K for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def W_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the query weights across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.W_Q for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def W_V(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stacks the value weights across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.W_V for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def W_O(self) -> Float[torch.Tensor, "n_layers n_heads d_head d_model"]:
        """Stacks the attn output weights across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.W_O for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def W_in(self) -> Float[torch.Tensor, "n_layers d_model d_mlp"]:
        """Stacks the MLP input weights across all layers"""
        weights: List[torch.Tensor] = []
        for block in chain(self.encoder, self.decoder):
            mlp = cast(T5Block, block).mlp
            if isinstance(mlp, (MLP, GatedMLP)):
                weights.append(mlp.W_in)
            else:
                raise NotImplementedError(
                    f"W_in property is not supported for MLP of type {type(mlp).__name__}"
                )
        return torch.stack(weights, dim=0)

    @property
    def W_out(self) -> Float[torch.Tensor, "n_layers d_mlp d_model"]:
        """Stacks the MLP output weights across all layers"""
        weights: List[torch.Tensor] = []
        for block in chain(self.encoder, self.decoder):
            mlp = cast(T5Block, block).mlp
            if isinstance(mlp, (MLP, GatedMLP)):
                weights.append(mlp.W_out)
            else:
                raise NotImplementedError(
                    f"W_out property is not supported for MLP of type {type(mlp).__name__}"
                )
        return torch.stack(weights, dim=0)

    @property
    def b_K(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the key biases across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.b_K for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def b_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the query biases across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.b_Q for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def b_V(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stacks the value biases across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.b_V for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def b_O(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stacks the attn output biases across all layers"""
        return torch.stack(
            [cast(T5Block, block).attn.b_O for block in chain(self.encoder, self.decoder)],
            dim=0,
        )

    @property
    def b_in(self) -> Float[torch.Tensor, "n_layers d_mlp"]:
        """Stacks the MLP input biases across all layers"""
        biases: List[torch.Tensor] = []
        for block in chain(self.encoder, self.decoder):
            mlp = cast(T5Block, block).mlp
            if isinstance(mlp, (MLP, GatedMLP)):
                biases.append(mlp.b_in)
            else:
                raise NotImplementedError(
                    f"b_in property is not supported for MLP of type {type(mlp).__name__}"
                )
        return torch.stack(biases, dim=0)

    @property
    def b_out(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stacks the MLP output biases across all layers"""
        biases: List[torch.Tensor] = []
        for block in chain(self.encoder, self.decoder):
            mlp = cast(T5Block, block).mlp
            if isinstance(mlp, (MLP, GatedMLP)):
                biases.append(mlp.b_out)
            else:
                raise NotImplementedError(
                    f"b_out property is not supported for MLP of type {type(mlp).__name__}"
                )
        return torch.stack(biases, dim=0)

    @property
    def QK(self) -> FactoredMatrix:  # [n_layers, n_heads, d_model, d_model]
        """Returns a FactoredMatrix object with the product of the Q and K matrices for each layer and head.
        Useful for visualizing attention patterns."""
        return FactoredMatrix(self.W_Q, self.W_K.transpose(-2, -1))

    @property
    def OV(self) -> FactoredMatrix:  # [n_layers, n_heads, d_model, d_model]
        """Returns a FactoredMatrix object with the product of the O and V matrices for each layer and head."""
        return FactoredMatrix(self.W_V, self.W_O)

    def all_head_labels(self) -> List[str]:
        """Returns a list of strings with the format "L{l}H{h}", where l is the layer index and h is the head index."""
        return [f"EL{l}H{h}" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)] + [
            f"DL{l}H{h}" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)
        ]



---
File: /transformer_lens/HookedTransformer.py
---

"""Hooked Transformer.

The Hooked Transformer is the core part of TransformerLens.

In common PyTorch model implementations (e.g. ones from HuggingFace) it's fairly easy to extract
model weights, but much harder to extract activations. TransformerLens aims to simplify this task by
attaching hooks to every notable activation within the model. This enables the inspection and/or
alteration of activations in individual components like attention heads and MLP layers, facilitating
a deeper understanding of the internal workings of transformers like GPT-2.
"""

from __future__ import annotations

import logging
import os
from typing import (
    Any,
    Dict,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Type,
    TypeVar,
    Union,
    cast,
    overload,
)

import einops
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import tqdm.auto as tqdm
from jaxtyping import Float, Int
from packaging import version
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from transformers.models.auto.tokenization_auto import AutoTokenizer
from transformers.tokenization_utils_base import PreTrainedTokenizerBase
from typing_extensions import Literal

import transformer_lens.loading_from_pretrained as loading
import transformer_lens.utils as utils
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.components import (
    Embed,
    LayerNorm,
    LayerNormPre,
    PosEmbed,
    RMSNorm,
    RMSNormPre,
    TransformerBlock,
    Unembed,
)
from transformer_lens.FactoredMatrix import FactoredMatrix
from transformer_lens.hook_points import HookedRootModule, HookPoint
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.loading_from_pretrained import NON_HF_HOSTED_MODEL_NAMES

# Note - activation cache is used with run_with_cache, past_key_value_caching is used for
# generation.
from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache
from transformer_lens.utilities import devices
from transformer_lens.utils import (
    USE_DEFAULT_VALUE,
    init_kaiming_normal_,
    init_kaiming_uniform_,
    init_xavier_normal_,
    init_xavier_uniform_,
)

SingleLoss = Float[torch.Tensor, ""]  # Type alias for a single element tensor
LossPerToken = Float[torch.Tensor, "batch pos-1"]
Loss = Union[SingleLoss, LossPerToken]

DTYPE_FROM_STRING = {
    "float32": torch.float32,
    "fp32": torch.float32,
    "float16": torch.float16,
    "fp16": torch.float16,
    "bfloat16": torch.bfloat16,
    "bf16": torch.bfloat16,
}

T = TypeVar("T", bound="HookedTransformer")


class Output(NamedTuple):
    """Output Named Tuple.

    Named tuple object for if we want to output both logits and loss.
    """

    logits: Float[torch.Tensor, "batch pos d_vocab"]
    loss: Loss


class HookedTransformer(HookedRootModule):
    """Hooked Transformer.

    Implements a full Transformer using the components :doc:`here <transformer_lens.components>`,
    with a :class:`transformer_lens.hook_points.HookPoint` on every interesting activation.

    TransformerLens comes loaded with >50 GPT-style models. Typically you initialise it with one of
    these via :meth:`from_pretrained`, although it can also be instantiated with randomly
    initialized weights via :meth:`__init__`.

    Once you've initialized the model, a common next step is to test it can do the task you're
    investigating. This can be done with :func:`transformer_lens.utils.test_prompt`.
    """

    ln_final: nn.Module
    tokenizer: Optional[PreTrainedTokenizerBase]

    def __init__(
        self,
        cfg: Union[HookedTransformerConfig, Dict],
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        move_to_device: bool = True,
        default_padding_side: Literal["left", "right"] = "right",
    ):
        """Model initialization.

        Note that if you want to load the model from pretrained weights, you should use
        :meth:`from_pretrained` instead.

        Args:
            cfg: The config to use for the model.
            tokenizer: The tokenizer to use for the model. If not provided, it is inferred from
                `cfg.tokenizer_name` or initialized to `None`. If `None`, then the model cannot be
                passed strings, and d_vocab must be explicitly set.
            move_to_device: Whether to move the model to the device specified in cfg.
                device. Must be true if `n_devices` in the config is greater than 1, since the
                model's layers will be split across multiple devices.
            default_padding_side: Which side to pad on.
        """
        super().__init__()
        if isinstance(cfg, str):
            raise ValueError(
                "Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a "
                "pretrained model, use HookedTransformer.from_pretrained() instead."
            )

        self.cfg = HookedTransformerConfig.unwrap(cfg)

        if tokenizer is not None:
            self.set_tokenizer(tokenizer, default_padding_side=default_padding_side)
        elif self.cfg.tokenizer_name is not None:
            # If we have a tokenizer name, we can load it from HuggingFace
            if self.cfg.tokenizer_name in NON_HF_HOSTED_MODEL_NAMES:
                logging.warning(
                    "%s tokenizer not loaded. Please load manually.",
                    self.cfg.tokenizer_name,
                )
            else:
                # Hugging Face defaults to use_fast to True
                use_fast = True
                # Phi model's fast tokenizer does not support adding a BOS token, use_fast
                # should be False
                if "phi" in self.cfg.tokenizer_name.lower():
                    use_fast = False
                huggingface_token = os.environ.get("HF_TOKEN", "")
                self.set_tokenizer(
                    AutoTokenizer.from_pretrained(
                        self.cfg.tokenizer_name,
                        add_bos_token=True,
                        trust_remote_code=self.cfg.trust_remote_code,
                        use_fast=use_fast,
                        token=huggingface_token if len(huggingface_token) > 0 else None,
                    ),
                    default_padding_side=default_padding_side,
                )
        else:
            # If no tokenizer name is provided, we assume we're training on an algorithmic task and
            # will pass in tokens directly. In this case, we don't need a tokenizer.
            assert self.cfg.d_vocab != -1, "Must provide a tokenizer if d_vocab is not provided"
            self.tokenizer = None
            if default_padding_side != "right":
                logging.warning(
                    "default_padding_side is explictly given but ignored because tokenizer is not set."
                )

        self.embed = Embed(self.cfg)
        self.hook_embed = HookPoint()  # [batch, pos, d_model]

        if self.cfg.positional_embedding_type != "rotary":
            self.pos_embed = PosEmbed(self.cfg)
            self.hook_pos_embed = HookPoint()  # [batch, pos, d__dictmodel]

        if self.cfg.use_hook_tokens:
            self.hook_tokens = HookPoint()  # [batch, pos]

        self.blocks = nn.ModuleList(
            [TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg.n_layers)]
        )

        if self.cfg.normalization_type == "RMS":
            self.ln_final = RMSNorm(self.cfg)
        elif self.cfg.normalization_type == "RMSPre":
            self.ln_final = RMSNormPre(self.cfg)
        elif self.cfg.normalization_type == "LN":
            if self.cfg.final_rms:
                self.ln_final = RMSNorm(self.cfg)
            else:
                self.ln_final = LayerNorm(self.cfg)
        elif self.cfg.normalization_type == "LNPre":
            # We've folded in LayerNorm weights, so just need the center + scale parts
            if self.cfg.final_rms:
                self.ln_final = RMSNormPre(self.cfg)
            else:
                self.ln_final = LayerNormPre(self.cfg)
        elif self.cfg.normalization_type is None:
            # If it's None, don't create either layer
            pass
        else:
            logging.warning("Invalid normalization_type passed in %s", self.cfg.normalization_type)
        self.unembed = Unembed(self.cfg)

        if self.cfg.init_weights:
            self.init_weights()

        if move_to_device:
            # We load the devices in a pipeline manner - the first device gets the embed and
            # pos_embed layers and the first n_layers // n_devices blocks, the second gets the next
            # n_layers // n_devices blocks ... the last gets the last n_layers // n_devices blocks,
            # the final normalization layer (if it exists) and the unembed layer
            self.move_model_modules_to_device()

        # Helper variable to store a small (10K-20K) dataset of training data. Empty by default, can
        # be loaded with load_sample_training_dataset
        self.dataset = None

        # Gives each module a parameter with its name (relative to this root module)
        # Needed for HookPoints to work
        self.setup()

    def check_hooks_to_add(
        self,
        hook_point,
        hook_point_name,
        hook,
        dir="fwd",
        is_permanent=False,
        prepend=False,
    ) -> None:
        if hook_point_name.endswith("attn.hook_result"):
            assert (
                self.cfg.use_attn_result
            ), f"Cannot add hook {hook_point_name} if use_attn_result_hook is False"
        if hook_point_name.endswith(("hook_q_input", "hook_k_input", "hook_v_input")):
            assert (
                self.cfg.use_split_qkv_input
            ), f"Cannot add hook {hook_point_name} if use_split_qkv_input is False"
        if hook_point_name.endswith("mlp_in"):
            assert (
                self.cfg.use_hook_mlp_in
            ), f"Cannot add hook {hook_point_name} if use_hook_mlp_in is False"
        if hook_point_name.endswith("attn_in"):
            assert (
                self.cfg.use_attn_in
            ), f"Cannot add hook {hook_point_name} if use_attn_in is False"

    def get_pos_offset(self, past_kv_cache, batch_size):
        # If we're doing caching, then we reuse keys and values from previous runs, as that's the
        # only way that past activations will affect the final logits. The cache contains those so
        # we don't need to recompute them. This is useful for generating text. As we have absolute
        # positional encodings, to implement this we have a `pos_offset` variable, defaulting to
        # zero, which says to offset which positional encodings are used (cached keys and values
        # were calculated with their own positional encodings).
        if past_kv_cache is None:
            pos_offset = 0
        else:
            (
                cached_batch_size,
                cache_ctx_length,
                num_heads_in_cache,
                d_head_in_cache,
            ) = past_kv_cache[0].past_keys.shape
            assert cached_batch_size == batch_size
            if self.cfg.n_key_value_heads is None:
                assert num_heads_in_cache == self.cfg.n_heads
            else:
                assert num_heads_in_cache == self.cfg.n_key_value_heads
            assert d_head_in_cache == self.cfg.d_head
            pos_offset = cache_ctx_length
        return pos_offset

    def get_residual(
        self,
        embed,
        pos_offset,
        prepend_bos=USE_DEFAULT_VALUE,
        attention_mask=None,
        tokens=None,
        return_shortformer_pos_embed=True,
        device=None,
    ):
        if device is None:
            device = devices.get_device_for_block_index(0, self.cfg)

        if tokens is None:
            # Because tokens only need for defining batch size and sequence length, we can simply synthesize them
            tokens = torch.ones((embed.size(0), embed.size(1))).int().to(device)

        if self.cfg.positional_embedding_type == "standard":
            pos_embed = self.hook_pos_embed(
                self.pos_embed(tokens, pos_offset, attention_mask)
            )  # [batch, pos, d_model]
            residual = embed + pos_embed  # [batch, pos, d_model]
            shortformer_pos_embed = None
        elif self.cfg.positional_embedding_type == "shortformer":
            # If we're using shortformer style attention, we don't add the positional embedding to
            # the residual stream. See HookedTransformerConfig for details
            pos_embed = self.hook_pos_embed(
                self.pos_embed(tokens, pos_offset, attention_mask)
            )  # [batch, pos, d_model]
            residual = embed
            shortformer_pos_embed = pos_embed
        elif self.cfg.positional_embedding_type == "rotary":
            # Rotary doesn't use positional embeddings, instead they're applied when dot producting
            # keys and queries. See HookedTransformerConfig for details
            residual = embed
            shortformer_pos_embed = None
        elif self.cfg.positional_embedding_type == "alibi":
            # ALiBi does not add positional embeddings to word embeddings,instead it biases QK attention scores.
            residual = embed
            shortformer_pos_embed = None
        else:
            raise ValueError(
                f"Invalid positional_embedding_type passed in {self.cfg.positional_embedding_type}"
            )

        if return_shortformer_pos_embed:
            return residual, shortformer_pos_embed
        else:
            return residual

    def input_to_embed(
        self,
        input: Union[str, List[str], Int[torch.Tensor, "batch pos"]],
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        attention_mask: Optional[torch.Tensor] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> Tuple[
        Float[torch.Tensor, "batch pos d_model"],  # residual
        Optional[Int[torch.Tensor, "batch pos"]],  # tokens
        Optional[Float[torch.Tensor, "batch pos d_model"]],  # shortformer_pos_embed
        Optional[torch.Tensor],  # attention_mask [batch pos]
    ]:
        """Convert input to first residual stream.

        Args:
            input (Union[str, List[str], Int[torch.Tensor, "batch pos"]]): The input to the model.
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (only applies when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos which is set to True unless specified
                otherwise. Pass True or False to locally override the default.
            padding_side ([Literal["left", "right"], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing
                multiple strings of different lengths.
            past_kv_cache (HookedTransformerKeyValueCache, optional): If passed, we're doing caching
                and attention_mask will be stored in the cache.
        """
        if isinstance(input, str) or isinstance(input, list):
            # If text, convert to tokens (batch_size=1)
            assert (
                self.tokenizer is not None
            ), "Must provide a tokenizer if passing a string to the model"
            # This is only intended to support passing in a single string
            tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)
        else:
            tokens = input
        if len(tokens.shape) == 1:
            # If tokens are a rank 1 tensor, add a dummy batch dimension to avoid things breaking.
            tokens = tokens[None]
        if tokens.device.type != self.cfg.device:
            tokens = tokens.to(devices.get_device_for_block_index(0, self.cfg))

        if (
            (self.tokenizer and self.tokenizer.padding_side == "left")
            or attention_mask is not None
            or past_kv_cache is not None
        ):
            # This means we need to have an explicit attention mask.
            if attention_mask is None:
                # If the padding side is left or we are using caching, we need to compute the attention
                # mask for the adjustment of absolute positional embeddings and attention masking so
                # that pad tokens are not attended.
                if prepend_bos is USE_DEFAULT_VALUE:
                    prepend_bos = self.cfg.default_prepend_bos
                if self.tokenizer is None:
                    raise ValueError("Cannot compute attention mask without a tokenizer.")
                attention_mask = utils.get_attention_mask(self.tokenizer, tokens, prepend_bos)

            assert attention_mask.shape == tokens.shape, (
                f"Attention mask shape {attention_mask.shape} does not match tokens shape "
                f"{tokens.shape}"
            )
            attention_mask = attention_mask.to(devices.get_device_for_block_index(0, self.cfg))
            if past_kv_cache is not None:
                # past_kv_cache is not None, so we're doing caching.
                # We need to extend the previous attention_mask.
                # Update the past_kv_cache with the new attention_mask (unless it's frozen)
                attention_mask = past_kv_cache.append_attention_mask(attention_mask)
        else:
            # We separate this case from for computational efficiency.
            attention_mask = None

        batch_size = tokens.shape[0]
        pos_offset = self.get_pos_offset(past_kv_cache, batch_size)

        if self.cfg.use_hook_tokens:
            tokens = self.hook_tokens(tokens)

        embed = self.hook_embed(self.embed(tokens))  # [batch, pos, d_model]
        residual, shortformer_pos_embed = self.get_residual(
            embed,
            pos_offset,
            prepend_bos,
            attention_mask,
            tokens,
            return_shortformer_pos_embed=True,
        )
        return residual, tokens, shortformer_pos_embed, attention_mask

    @overload
    def forward(
        self,
        input,
        return_type: Literal["logits"],
        loss_per_token: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        start_at_layer: Optional[int] = None,
        tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]
        stop_at_layer: Optional[int] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> Loss:
        ...

    @overload
    def forward(
        self,
        input,
        return_type: Literal["loss"],
        loss_per_token: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        start_at_layer: Optional[int] = None,
        tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]
        stop_at_layer: Optional[int] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> Loss:
        ...

    @overload
    def forward(
        self,
        input,
        return_type: Literal["both"],
        loss_per_token: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        start_at_layer: Optional[int] = None,
        tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]
        stop_at_layer: Optional[int] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> Tuple[Float[torch.Tensor, "batch pos d_vocab"], Loss]:
        ...

    @overload
    def forward(
        self,
        input,
        return_type: Literal[None],
        loss_per_token: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        start_at_layer: Optional[int] = None,
        tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]
        stop_at_layer: Optional[int] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> None:
        ...

    def forward(
        self,
        input: Union[
            str,
            List[str],
            Int[torch.Tensor, "batch pos"],
            Float[torch.Tensor, "batch pos d_model"],
        ],
        return_type: Optional[str] = "logits",
        loss_per_token: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Literal["left", "right"]] = USE_DEFAULT_VALUE,
        start_at_layer: Optional[int] = None,
        tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
        shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]] = None,
        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]
        stop_at_layer: Optional[int] = None,
        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,
    ) -> Union[
        None,
        Float[torch.Tensor, "batch pos d_vocab"],
        Loss,
        Tuple[Float[torch.Tensor, "batch pos d_vocab"], Loss],
    ]:
        """Forward Pass.

        Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically
        tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a
        text string.

        Note that loss is the standard "predict the next token" cross-entropy loss for GPT-2 style
        language models - if you want a custom loss function, the recommended behaviour is returning
        the logits and then applying your custom loss function.

        Args:
            return_type Optional[str]: The type of output to return. Can be one of: None (return
                nothing, don't calculate logits), 'logits' (return logits), 'loss' (return
                cross-entropy loss), 'both' (return logits and loss).
            loss_per_token bool: Whether to return the (next token prediction) loss per token (True)
                or average (False). Average loss is a scalar (averaged over position *and* batch),
                per-token loss is a tensor ([batch, position-1]) - position-1 because we're
                predicting the next token, and there's no specified next token for the final token.
                Defaults to False.
            prepend_bos Optional[bool]: Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (only applies when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos which is set to True unless specified
                otherwise. (Even for models not explicitly trained with a prepended BOS token, heads
                often use the first position as a resting position and accordingly lose information
                from the first token, so this empirically seems to give better results.) Pass True
                or False to locally override the default.
            padding_side Optional[Literal["left", "right"]]: Overrides self.tokenizer.padding_side.
                Specifies which side to pad on when tokenizing multiple strings of different
                lengths.
            start_at_layer Optional[int]: If not None, start the forward pass at the specified
                layer. Requires input to be the residual stream before the specified layer with
                shape [batch, pos, d_model]. Inclusive - ie, start_at_layer = 0 skips the embedding
                then runs the rest of the model. Supports negative indexing. start_at_layer = -1
                only runs the final block and the unembedding. Defaults to None (run the full
                model).
            tokens: Optional[Int[torch.Tensor, "batch pos"]]: Tokenized input. Only use if
                start_at_layer is not None and return type is "loss" or "both".
            shortformer_pos_embed: Optional[Float[torch.Tensor, "batch pos d_model"]]: Positional
                embedding for shortformer models. Only use if start_at_layer is not None and
                self.cfg.positional_embedding_type == "shortformer".
            attention_mask: Optional[torch.Tensor]: Override the attention mask used to ignore
                padded tokens. If start_at_layer is not None and (self.tokenizer.padding_side ==
                "left" or past_kv_cache is not None), this should be passed as the attention mask
                is not computed automatically. Defaults to None.
            stop_at_layer Optional[int]: If not None, stop the forward pass at the specified layer.
                Exclusive - ie, stop_at_layer = 0 will only run the embedding layer, stop_at_layer =
                1 will run the embedding layer and the first transformer block, etc. Supports
                negative indexing. Useful for analysis of intermediate layers, eg finding neuron
                activations in layer 3 of a 24 layer model. Defaults to None (run the full model).
                If not None, we return the last residual stream computed.
            past_kv_cache Optional[HookedTransformerKeyValueCache]: If not None, keys and values
                will be stored for every attention head (unless the cache is frozen). If there are
                keys and values already in the cache, these will be prepended to the keys and values
                for the new input, so that the new tokens can pay attention to previous tokens. This
                is useful for generating text, because we don't need to repeat computation for
                tokens that have already been through the model. Also caches attention_mask so
                previous tokens are masked correctly (unless frozen). Padding should be ignored in
                all cases, so it's okay to eg. pass in left padded tokens twice in a row.
                Warning: Don't accidentally prepend_bos to the second half of a prompt.
                Defaults to None (don't use caching).
        """

        with utils.LocallyOverridenDefaults(
            self, prepend_bos=prepend_bos, padding_side=padding_side
        ):
            if start_at_layer is None:
                (
                    residual,
                    tokens,
                    shortformer_pos_embed,
                    attention_mask,
                ) = self.input_to_embed(
                    input,
                    prepend_bos=prepend_bos,
                    padding_side=padding_side,
                    attention_mask=attention_mask,
                    past_kv_cache=past_kv_cache,
                )
            else:
                assert type(input) == torch.Tensor
                residual = input

            if start_at_layer is None:
                start_at_layer = 0
            # If we explicitly want to start or stop at a layer, we only iterate through the blocks
            # between those indices. Note that start_at_layer is inclusive and stop_at_layer is
            # exclusive.
            # Eg: start_at_layer==None + stop_at_layer==0 means to only run the embed.
            # Eg: start_at_layer==3 + stop_at_layer==-1 means to run from layer 3 until the end of the PENULTIMATE layer
            blocks_and_idxs = list(zip(range(self.cfg.n_layers), self.blocks))
            for i, block in blocks_and_idxs[start_at_layer:stop_at_layer]:  # type: ignore
                # Note that each block includes skip connections, so we don't need
                # residual + block(residual)
                # If we're using multiple GPUs, we need to send the residual and shortformer_pos_embed to the correct GPU
                residual = residual.to(devices.get_device_for_block_index(i, self.cfg))
                if shortformer_pos_embed is not None:
                    shortformer_pos_embed = shortformer_pos_embed.to(
                        devices.get_device_for_block_index(i, self.cfg)
                    )

                residual = block(
                    residual,
                    # Cache contains a list of HookedTransformerKeyValueCache objects, one for each
                    # block
                    past_kv_cache_entry=past_kv_cache[i] if past_kv_cache is not None else None,
                    shortformer_pos_embed=shortformer_pos_embed,
                    attention_mask=attention_mask,
                )  # [batch, pos, d_model]

            if stop_at_layer is not None:
                # When we stop at an early layer, we end here rather than doing further computation
                return residual

            if self.cfg.normalization_type is not None:
                residual = self.ln_final(residual)  # [batch, pos, d_model]
            if return_type is None:
                return None
            else:
                logits = self.unembed(residual)  # [batch, pos, d_vocab]
                if self.cfg.output_logits_soft_cap > 0.0:
                    logits = self.cfg.output_logits_soft_cap * F.tanh(
                        logits / self.cfg.output_logits_soft_cap
                    )
                if return_type == "logits":
                    return logits
                else:
                    assert (
                        tokens is not None
                    ), "tokens must be passed in if return_type is 'loss' or 'both'"
                    loss = self.loss_fn(logits, tokens, attention_mask, per_token=loss_per_token)
                    if return_type == "loss":
                        return loss
                    elif return_type == "both":
                        return Output(logits, loss)
                    else:
                        logging.warning(f"Invalid return_type passed in: {return_type}")
                        return None

    def loss_fn(
        self,
        logits: Float[torch.Tensor, "batch pos d_vocab"],
        tokens: Int[torch.Tensor, "batch pos"],
        attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
        per_token: bool = False,
    ):
        """Wrapper around `utils.lm_cross_entropy_loss`.

        Used in forward() with return_type=="loss" or "both".
        """
        if tokens.device != logits.device:
            tokens = tokens.to(logits.device)
        return utils.lm_cross_entropy_loss(logits, tokens, attention_mask, per_token)

    @overload
    def run_with_cache(
        self, *model_args, return_cache_object: Literal[True] = True, **kwargs
    ) -> Tuple[Output, ActivationCache]:
        ...

    @overload
    def run_with_cache(
        self, *model_args, return_cache_object: Literal[False], **kwargs
    ) -> Tuple[Output, Dict[str, torch.Tensor]]:
        ...

    def run_with_cache(
        self, *model_args, return_cache_object=True, remove_batch_dim=False, **kwargs
    ) -> Tuple[
        Union[
            None,
            Float[torch.Tensor, "batch pos d_vocab"],
            Loss,
            Tuple[Float[torch.Tensor, "batch pos d_vocab"], Loss],
        ],
        Union[ActivationCache, Dict[str, torch.Tensor]],
    ]:
        """Wrapper around `run_with_cache` in HookedRootModule.

        If return_cache_object is True, this will return an ActivationCache object, with a bunch of
        useful HookedTransformer specific methods, otherwise it will return a dictionary of
        activations as in HookedRootModule.
        """
        out, cache_dict = super().run_with_cache(
            *model_args, remove_batch_dim=remove_batch_dim, **kwargs
        )
        if return_cache_object:
            cache = ActivationCache(cache_dict, self, has_batch_dim=not remove_batch_dim)
            return out, cache
        else:
            return out, cache_dict

    def set_tokenizer(
        self,
        tokenizer,
        default_padding_side="right",
    ):
        """Set the tokenizer to use for this model.

        Args:
            tokenizer (PreTrainedTokenizer): a pretrained HuggingFace tokenizer.
            default_padding_side (str): "right" or "left", which side to pad on.

        """
        assert isinstance(
            tokenizer, PreTrainedTokenizerBase
        ), f"{type(tokenizer)} is not a supported tokenizer, please use PreTrainedTokenizer or PreTrainedTokenizerFast"

        assert default_padding_side in [
            "right",
            "left",
        ], f"padding_side must be 'right' or 'left', got {default_padding_side}"

        # Use a tokenizer that is initialized with add_bos_token=True as the default tokenizer.
        # Such a tokenizer should be set as the default tokenizer because the tokenization of some
        # tokenizers like LlamaTokenizer are different when bos token is automatically/manually
        # prepended, and add_bos_token cannot be dynamically controlled after initialization
        # (https://github.com/huggingface/transformers/issues/25886).
        tokenizer_with_bos = utils.get_tokenizer_with_bos(tokenizer)
        self.tokenizer = tokenizer_with_bos
        self.tokenizer.padding_side = default_padding_side

        # Some tokenizers doesn't automatically prepend the BOS token even when they are initialized
        # with add_bos_token=True. Therefore, we need this information to dynamically control prepend_bos.
        self.cfg.tokenizer_prepends_bos = len(self.tokenizer.encode("")) > 0

        if self.tokenizer.eos_token is None:
            self.tokenizer.eos_token = "<|endoftext|>"
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        if self.tokenizer.bos_token is None:
            self.tokenizer.bos_token = self.tokenizer.eos_token

        # Infer vocab size from tokenizer
        if self.cfg.d_vocab == -1:
            self.cfg.d_vocab = max(self.tokenizer.vocab.values()) + 1
        if self.cfg.d_vocab_out == -1:
            self.cfg.d_vocab_out = self.cfg.d_vocab

    def to_tokens(
        self,
        input: Union[str, List[str]],
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
        move_to_device: bool = True,
        truncate: bool = True,
    ) -> Int[torch.Tensor, "batch pos"]:
        """Converts a string to a tensor of tokens.

        If prepend_bos is True, prepends the BOS token to the input - this is recommended when
        creating a sequence of tokens to be input to a model.

        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when
        inputting a prompt to the model as the first token is often treated weirdly, but should only
        be done at the START of the prompt. Make sure to turn it off if you're looking at the
        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS
        token, others (OPT and my models) were)

        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether
        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not
        careful!

        Args:
            input (Union[str, List[str]]): The input to tokenize.
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (only applies when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos which is set to True unless specified
                otherwise. Pass True or False to locally override the default.
            padding_side (Union[Literal["left", "right"], None], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing
                multiple strings of different lengths.
            move_to_device (bool): Whether to move the output tensor of tokens to the device the
                model lives on. Defaults to True
            truncate (bool): If the output tokens are too long,
                whether to truncate the output tokens to the model's max context window. Does nothing
                for shorter inputs. Defaults to True.
        """
        with utils.LocallyOverridenDefaults(
            self, prepend_bos=prepend_bos, padding_side=padding_side
        ):
            assert self.tokenizer is not None, "Cannot use to_tokens without a tokenizer"
            assert (
                self.cfg.tokenizer_prepends_bos is not None
            ), "Set the tokenizer for the model by calling set_tokenizer"

            if self.cfg.default_prepend_bos and not self.cfg.tokenizer_prepends_bos:
                # We want to prepend bos but the tokenizer doesn't automatically do it, so we add it manually
                input = utils.get_input_with_manually_prepended_bos(self.tokenizer, input)

            tokens = self.tokenizer(
                input,
                return_tensors="pt",
                padding=True,
                truncation=truncate,
                max_length=self.cfg.n_ctx if truncate else None,
            )["input_ids"]

            if not self.cfg.default_prepend_bos and self.cfg.tokenizer_prepends_bos:
                # We don't want to prepend bos but the tokenizer does it automatically, so we remove it manually
                tokens = utils.get_tokens_with_bos_removed(self.tokenizer, tokens)

            if move_to_device:
                tokens = tokens.to(self.cfg.device)
            return tokens

    def to_string(
        self,
        tokens: Union[
            List[int],
            Int[torch.Tensor, ""],
            Int[torch.Tensor, "batch pos"],
            Int[torch.Tensor, "pos"],
            np.ndarray,
            List[Int[torch.Tensor, "pos"]],
        ],
    ) -> Union[str, List[str]]:
        """Tokens to String(s).

        Converts a tensor of tokens to a string (if rank 1) or a list of strings (if rank 2).

        Accepts lists of tokens and numpy arrays as inputs too (and converts to tensors internally)
        """
        assert self.tokenizer is not None, "Cannot use to_string without a tokenizer"

        if not isinstance(tokens, torch.Tensor):
            # We allow lists to be input
            tokens = torch.tensor(tokens)

        # I'm not sure what exactly clean_up_tokenization_spaces does, but if
        # it's set, then tokenization is no longer invertible, and some tokens
        # with a bunch of whitespace get collapsed together
        if len(tokens.shape) == 2:
            return self.tokenizer.batch_decode(tokens, clean_up_tokenization_spaces=False)
        elif len(tokens.shape) <= 1:
            return self.tokenizer.decode(tokens, clean_up_tokenization_spaces=False)
        else:
            raise ValueError(f"Invalid shape passed in: {tokens.shape}")

    def to_str_tokens(
        self,
        input: Union[
            str,
            Int[torch.Tensor, "pos"],
            Int[torch.Tensor, "1 pos"],
            Int[np.ndarray, "pos"],
            Int[np.ndarray, "1 pos"],
            list,
        ],
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
    ) -> Union[List[str], List[List[str]]]:
        """Map text, a list of text or tokens to a list of tokens as strings.

        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when
        inputting a prompt to the model as the first token is often treated weirdly, but should only
        be done at the START of the prompt. If prepend_bos=None is passed, it implies the usage of
        self.cfg.default_prepend_bos which is set to True unless specified otherwise. Therefore,
        make sure to locally turn it off by passing prepend_bos=False if you're looking at the
        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS
        token, others (OPT and my models) were)

        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether
        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not
        careful!

        Gotcha3: If passing a string that exceeds the model's context length (model.cfg.n_ctx), it
        will be truncated.

        Args:
            input (Union[str, list, torch.Tensor]): The input - either a string or a tensor of
                tokens. If tokens, should be a tensor of shape [pos] or [1, pos].
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (only applies when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos which is set to True unless specified
                otherwise. Pass True or False to locally override the default.
            padding_side (Union[Literal["left", "right"], None], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple
                strings of different lengths.

        Returns:
            str_tokens: List of individual tokens as strings
        """
        with utils.LocallyOverridenDefaults(
            self, prepend_bos=prepend_bos, padding_side=padding_side
        ):
            assert self.tokenizer is not None  # keep mypy happy
            tokens: Union[np.ndarray, torch.Tensor]
            if isinstance(input, list):
                return list(
                    map(
                        lambda tokens: self.to_str_tokens(tokens, prepend_bos, padding_side),
                        input,
                    )
                )  # type: ignore
            elif isinstance(input, str):
                tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)[
                    0
                ]
                # Gemma tokenizer expects a batch dimension
                if "gemma" in self.tokenizer.name_or_path and tokens.ndim == 1:
                    tokens = tokens.unsqueeze(1)
            elif isinstance(input, torch.Tensor):
                tokens = input
                tokens = tokens.squeeze()  # Get rid of a trivial batch dimension
                if tokens.dim() == 0:
                    # Don't pass dimensionless tensor
                    tokens = tokens.unsqueeze(0)
                assert (
                    tokens.dim() == 1
                ), f"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}"
            elif isinstance(input, np.ndarray):
                tokens = input
                tokens = tokens.squeeze()  # Get rid of a trivial batch dimension
                if tokens.ndim == 0:
                    # Don't pass dimensionless tensor
                    tokens = np.expand_dims(tokens, axis=0)
                assert (
                    tokens.ndim == 1
                ), f"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}"
            else:
                raise ValueError(f"Invalid input type to to_str_tokens: {type(input)}")
            str_tokens = self.tokenizer.batch_decode(tokens, clean_up_tokenization_spaces=False)
            return str_tokens

    def to_single_token(self, string):
        """Map a string that makes up a single token to the id for that token.

        Raises an error for strings that are not a single token! If uncertain use to_tokens.
        """

        # We use the to_tokens method, do not append a BOS token
        token = self.to_tokens(string, prepend_bos=False).squeeze()
        # If token shape is non-empty, raise error
        assert not token.shape, f"Input string: {string} is not a single token!"
        return token.item()

    def to_single_str_token(self, int_token: int) -> str:
        # Gives the single token corresponding to an int in string form
        assert isinstance(int_token, int)
        token = self.to_str_tokens(torch.tensor([int_token]))
        assert len(token) == 1
        return cast(str, token[0])

    def get_token_position(
        self,
        single_token: Union[str, int],
        input: Union[str, Union[Float[torch.Tensor, "pos"], Float[torch.Tensor, "1 pos"]]],
        mode="first",
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Union[Literal["left", "right"], None]] = USE_DEFAULT_VALUE,
    ):
        """Get the position of a single_token in a string or sequence of tokens.

        Raises an error if the token is not present.

        Gotcha: If you're inputting a string, it'll automatically be tokenized. Be careful about the
        setting for prepend_bos! When a string is input to the model, a BOS (beginning of sequence)
        token is prepended by default when the string is tokenized because
        self.cfg.default_prepend_bos is set to True unless specified otherwise. But this should only
        be done at the START of the input, not when inputting part of the prompt. If you're getting
        weird off-by-one errors, check carefully for what the setting should be!

        Args:
            single_token (Union[str, int]): The token to search for. Can
                be a token index, or a string (but the string must correspond to a single token).
            input (Union[str, torch.Tensor]): The sequence to
                search in. Can be a string or a rank 1 tensor of tokens or a rank 2 tensor of tokens
                with a dummy batch dimension.
            mode (str, optional): If there are multiple matches, which match to return. Supports
                "first" or "last". Defaults to "first".
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (only applies when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos which is set to True unless specified
                otherwise. Pass True or False to locally override the default.
            padding_side (Union[Literal["left", "right"], None], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple
                strings of different lengths.
        """
        if isinstance(input, str):
            # If the input is a string, convert to tensor
            tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)
        else:
            tokens = input

        if len(tokens.shape) == 2:
            # If the tokens have shape [1, seq_len], flatten to [seq_len]
            assert (
                tokens.shape[0] == 1
            ), f"If tokens are rank two, they must have shape [1, seq_len], not {tokens.shape}"
            tokens = tokens[0]

        if isinstance(single_token, str):
            # If the single token is a string, convert to an integer
            single_token = self.to_single_token(single_token)
        elif isinstance(single_token, torch.Tensor):
            single_token = single_token.item()

        indices = torch.arange(len(tokens), device=tokens.device)[tokens == single_token]
        assert len(indices) > 0, "The token does not occur in the prompt"
        if mode == "first":
            return indices[0].item()
        elif mode == "last":
            return indices[-1].item()
        else:
            raise ValueError(f"mode must be 'first' or 'last', not {mode}")

    def tokens_to_residual_directions(
        self,
        tokens: Union[
            str,
            int,
            Int[torch.Tensor, ""],
            Int[torch.Tensor, "pos"],
            Int[torch.Tensor, "batch pos"],
        ],
    ) -> Union[
        Float[torch.Tensor, "d_model"],
        Float[torch.Tensor, "pos d_model"],
        Float[torch.Tensor, "batch pos d_model"],
    ]:
        """Map tokens to a tensor with the unembedding vector for those tokens.

        I.e. the vector in the residual stream that we dot with to the get the logit for that token.

        WARNING: If you use this without folding in LayerNorm, the results will be misleading and
        may be incorrect, as the LN weights change the unembed map. This is done automatically with
        the fold_ln flag on from_pretrained

        WARNING 2: LayerNorm scaling will scale up or down the effective direction in the residual
        stream for each output token on any given input token position.
        ActivationCache.apply_ln_to_stack will apply the appropriate scaling to these directions.

        Args:
            tokens (Union[str, int, torch.Tensor]): The token(s). If a single token, can be a single
                element tensor, an integer, or string. If string, will be mapped to a single token
                using to_single_token, and an error raised if it's multiple tokens. The method also
                works for a batch of input tokens.

        Returns:
            residual_direction torch.Tensor: The unembedding vector for the token(s), a stack of
                [d_model] tensor.
        """
        if isinstance(tokens, torch.Tensor) and tokens.numel() > 1:
            # If the tokens are a tensor, and have more than one element, assume they are a batch of
            # tokens.
            residual_directions = self.W_U[:, tokens]
            residual_directions = einops.rearrange(
                residual_directions, "d_model ... -> ... d_model"
            )
            return residual_directions
        else:
            # Otherwise there is a single token
            if isinstance(tokens, str):
                token = self.to_single_token(tokens)
            elif isinstance(tokens, int):
                token = tokens
            elif isinstance(tokens, torch.Tensor) and tokens.numel() == 1:
                token = tokens.item()
            else:
                raise ValueError(f"Invalid token type: {type(tokens)}")
            residual_direction = self.W_U[:, token]
            return residual_direction

    def to(  # type: ignore
        self,
        device_or_dtype: Union[torch.device, str, torch.dtype],
        print_details: bool = True,
    ):
        return devices.move_to_and_update_config(self, device_or_dtype, print_details)

    def cuda(self: T, device: Optional[Union[int, torch.device]] = None) -> T:
        # TODO: Add support for kwargs
        if isinstance(device, int):
            return self.to(f"cuda:{device}")
        elif device is None:
            return self.to("cuda")
        else:
            return self.to(device)

    def cpu(self: T) -> T:
        return self.to(torch.device("cpu"))

    def mps(self: T) -> T:
        return self.to(torch.device("mps"))

    def move_model_modules_to_device(self):
        self.embed.to(devices.get_best_available_device(self.cfg))
        self.hook_embed.to(devices.get_best_available_device(self.cfg))
        if self.cfg.positional_embedding_type != "rotary":
            self.pos_embed.to(devices.get_best_available_device(self.cfg))
            self.hook_pos_embed.to(devices.get_best_available_device(self.cfg))

        if hasattr(self, "ln_final"):
            self.ln_final.to(devices.get_best_available_device(self.cfg))
        self.unembed.to(devices.get_best_available_device(self.cfg))
        for i, block in enumerate(self.blocks):
            block.to(devices.get_best_available_device(self.cfg))

    @classmethod
    def from_pretrained(
        cls: Type[T],
        model_name: str,
        fold_ln: bool = True,
        center_writing_weights: bool = True,
        center_unembed: bool = True,
        refactor_factored_attn_matrices: bool = False,
        checkpoint_index: Optional[int] = None,
        checkpoint_value: Optional[int] = None,
        hf_model: Optional[Any] = None,
        device: Optional[Union[str, torch.device]] = None,
        n_devices: int = 1,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        move_to_device: bool = True,
        fold_value_biases: bool = True,
        default_prepend_bos: Optional[bool] = None,
        default_padding_side: Literal["left", "right"] = "right",
        dtype="float32",
        first_n_layers: Optional[int] = None,
        **from_pretrained_kwargs,
    ) -> T:
        """Load in a Pretrained Model.

        Load in pretrained model weights to the HookedTransformer format and optionally to do some
        processing to make the model easier to interpret. Currently supports loading from most
        autoregressive HuggingFace models (``gpt2``, ``neo``, ``gptj``, ``opt``...) and from a range
        of toy models and SoLU models trained by Neel Nanda. The full list is available in the docs
        under :doc:`model properties</generated/model_properties_table>`. Also supports loading from
        a checkpoint for checkpointed models (currently, models trained by NeelNanda and the
        stanford-crfm models (using parameters ``checkpoint_index`` and ``checkpoint_value``).

        See :meth:`load_and_process_state_dict` for details on the processing (folding layer norm,
        centering the unembedding and centering the writing weights).

        Example:

        >>> from transformer_lens import HookedTransformer
        >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")
        Loaded pretrained model tiny-stories-1M into HookedTransformer

        Args:
            model_name: The model name - must be an element of
                :const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES` or an alias
                of one. The full list of available models can be found in the docs under :doc:`model
                properties</generated/model_properties_table>`.
            fold_ln: Whether to fold in the LayerNorm weights to the
                subsequent linear layer. This does not change the computation.

                `LayerNorm
                <https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1>`_
                is a common regularization technique used in transformers. Unlike BatchNorm, it
                cannot be turned off at inference time, as it significantly alters the mathematical
                function implemented by the transformer.

                When `fold_ln` is set to True, LayerNorm (with weights :math:`w_{ln}` and
                :math:`b_{ln}`) followed by a linear layer (:math:`W + b`) is optimized to
                LayerNormPre (just centering & normalizing) followed by a new linear layer with
                :math:`W_{eff} = w[:, \text{None}] * W` (element-wise multiplication) and
                :math:`b_{eff} = b + b_{ln} @ W`. This transformation is computationally equivalent
                and simplifies the model's interpretability. It essentially merges LayerNorm weights
                into the subsequent linear layer's weights, which is handled by HookedTransformer
                when loading pre-trained weights. Set `fold_ln` to False when loading a state dict
                if you wish to turn this off.

                Mathematically, LayerNorm is defined as follows:

                .. math::
                    x_1 &= x_0 - \\text{mean}(x_0)

                    x_2 &= \\frac{x_1}{\\sqrt{\\text{mean}(x_1^2)}}

                    x_3 &= x_2 \\cdot w

                    x_4 &= x_3 + b

                For further details, refer to `this document
                <https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization>`_.
            center_writing_weights: Whether to center weights
                writing to the residual stream (ie set mean to be zero). Due to LayerNorm this
                doesn't change the computation.

                A related idea to folding layernorm (``fold_ln``) - *every* component reading an
                input from the residual stream is preceded by a LayerNorm, which means that the mean
                of a residual stream vector (ie the component in the direction of all ones) never
                matters. This means we can remove the all ones component of weights and biases whose
                output *writes* to the residual stream. Mathematically, ``W_writing -=
                W_writing.mean(dim=1, keepdim=True)``.
            center_unembed: Whether to center W_U (ie set mean
                to be zero). Softmax is translation invariant so this doesn't affect log probs or
                loss, but does change logits.

                The logits are fed into a softmax. Softmax is translation invariant (eg, adding 1 to
                every logit doesn't change the output), so we can simplify things by setting the
                mean of the logits to be zero. This is equivalent to setting the mean of every
                output vector of ``W_U`` to zero. In code, ``W_U -= W_U.mean(dim=-1,
                keepdim=True)``.
            refactor_factored_attn_matrices: Whether to convert the factored
                matrices (W_Q & W_K, and W_O & W_V) to be "even". Defaults to False
            checkpoint_index: If loading from a checkpoint, the index of
                the checkpoint to load.
            checkpoint_value: If loading from a checkpoint, the value of
                the checkpoint to load, ie the step or token number (each model has checkpoints
                labelled with exactly one of these). E.g. ``1000`` for a checkpoint taken at step
                1000 or after 1000 tokens. If `checkpoint_index` is also specified, this will be
                ignored.
            hf_model: If you have already loaded in the
                HuggingFace model, you can pass it in here rather than needing to recreate the
                object. Defaults to None.
            device: The device to load the model onto. By
                default will load to CUDA if available, else CPU.
            n_devices: The number of devices to split the model
                across. Defaults to 1. If greater than 1, `device` must be cuda.
            tokenizer: The tokenizer to use for the model. If not
                provided, it is inferred from cfg.tokenizer_name or initialized to None. If None,
                then the model cannot be passed strings, and d_vocab must be explicitly set.
            move_to_device: Whether to move the model to the device specified in
                cfg. device. Must be true if `n_devices` in the config is greater than 1, since the
                model's layers will be split across multiple devices.
            fold_value_biases: Each attention head has a value bias. Values are averaged to create
                mixed values (``z``), weighted by the attention pattern, but as the bias is
                constant, its contribution to ``z`` is exactly the same. The output of a head is ``z
                @ W_O``, and so the value bias just linearly adds to the output of the head. This
                means that the value bias of a head has nothing to do with the head, and is just a
                constant added to the attention layer outputs. We can take the sum across these and
                b_O to get an "effective bias" for the layer. In code, we set ``b_V=0``. and ``b_O =
                (b_V @ W_O).sum(dim=0) + b_O``.

                The technical derivation of this is as follows. ``v = residual @ W_V[h] +
                broadcast_b_V[h]`` for each head ``h`` (where ``b_V`` is broadcast up from shape
                ``d_head`` to shape ``[position, d_head]``). And ``z = pattern[h] @ v = pattern[h] @
                residual @ W_V[h] + pattern[h] @ broadcast_b_V[h]``. Because ``pattern[h]`` is
                ``[destination_position, source_position]`` and ``broadcast_b_V`` is constant along
                the ``(source_)position`` dimension, we're basically just multiplying it by the sum
                of the pattern across the ``source_position`` dimension, which is just ``1``. So it
                remains exactly the same, and so is just broadcast across the destination positions.
            default_prepend_bos: Default behavior of whether to prepend the BOS
                token when the methods of HookedTransformer process input text to tokenize (only
                when input is a string).
                Resolution order for default_prepend_bos:
                1. If user passes value explicitly, use that value
                2. Model-specific default from cfg_dict if it exists (e.g. for bloom models it's False)
                3. Global default (True)

                Even for models not explicitly trained with the BOS token, heads often use the first position as a resting position
                and accordingly lose information from the first token, so this empirically seems to give better
                results. Note that you can also locally override the default behavior by passing in
                prepend_bos=True/False when you call a method that processes the input string.
            from_pretrained_kwargs: Any other optional argument passed to
                HuggingFace's from_pretrained (e.g. "cache_dir" or "torch_dtype"). Also passed to
                other HuggingFace functions when compatible. For some models or arguments it doesn't
                work, especially for models that are not internally loaded with HuggingFace's
                from_pretrained (e.g. SoLU models).
            dtype: What data type to load the model in (also sets the dtype of
                the HuggingFace model). Set to bfloat16 or float16 if you get out of memory errors when loading
                the model.
            default_padding_side: Which side to pad on when tokenizing. Defaults to
                "right".
            first_n_layers: If specified, only load the first n layers of the model.
        """
        if model_name.lower().startswith("t5"):
            raise RuntimeError(
                "Execution stopped: Please use HookedEncoderDecoder to load T5 models instead of HookedTransformer."
            )

        assert not (
            from_pretrained_kwargs.get("load_in_8bit", False)
            or from_pretrained_kwargs.get("load_in_4bit", False)
        ), "Quantization not supported"

        if hf_model is not None:
            assert hf_model.config is not None
            hf_cfg = hf_model.config.to_dict()
            qc = hf_cfg.get("quantization_config", {})
            load_in_4bit = qc.get("load_in_4bit", False)
            load_in_8bit = qc.get("load_in_8bit", False)
            quant_method = qc.get("quant_method", "")
            assert not load_in_8bit, "8-bit quantization is not supported"
            assert not (
                load_in_4bit and (version.parse(torch.__version__) < version.parse("2.1.1"))
            ), "Quantization is only supported for torch versions >= 2.1.1"
            assert not (
                load_in_4bit and ("llama" not in model_name.lower())
            ), "Quantization is only supported for Llama models"
            if load_in_4bit:
                assert (
                    qc.get("quant_method", "") == "bitsandbytes"
                ), "Only bitsandbytes quantization is supported"
        else:
            hf_cfg = {}

        if isinstance(dtype, str):
            # Convert from string to a torch dtype
            dtype = DTYPE_FROM_STRING[dtype]
        if "torch_dtype" in from_pretrained_kwargs:
            # For backwards compatibility with the previous way to do low precision loading
            # This should maybe check the user did not explicitly set dtype *and* torch_dtype
            dtype = from_pretrained_kwargs["torch_dtype"]

        if (
            (from_pretrained_kwargs.get("torch_dtype", None) == torch.float16)
            or dtype == torch.float16
        ) and device in ["cpu", None]:
            logging.warning("float16 models may not work on CPU. Consider using a GPU or bfloat16.")

        # Get the model name used in HuggingFace, rather than the alias.
        official_model_name = loading.get_official_model_name(model_name)

        # Load the config into an HookedTransformerConfig object. If loading from a
        # checkpoint, the config object will contain the information about the
        # checkpoint
        cfg = loading.get_pretrained_model_config(
            official_model_name,
            hf_cfg=hf_cfg,
            checkpoint_index=checkpoint_index,
            checkpoint_value=checkpoint_value,
            fold_ln=fold_ln,
            device=device,
            n_devices=n_devices,
            default_prepend_bos=default_prepend_bos,
            dtype=dtype,
            first_n_layers=first_n_layers,
            **from_pretrained_kwargs,
        )

        if cfg.positional_embedding_type == "shortformer":
            if fold_ln:
                logging.warning(
                    "You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_"
                    "ln=False instead."
                )
                fold_ln = False
            if center_unembed:
                logging.warning(
                    "You tried to specify center_unembed=True for a shortformer model, but this can't be done! "
                    "Setting center_unembed=False instead."
                )
                center_unembed = False
            if center_writing_weights:
                logging.warning(
                    "You tried to specify center_writing_weights=True for a shortformer model, but this can't be done! "
                    "Setting center_writing_weights=False instead."
                )
                center_writing_weights = False
        if center_unembed and cfg.output_logits_soft_cap > 0.0:
            logging.warning(
                "You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant "
                "Setting center_unembed=False instead."
            )
            center_unembed = False

        # Get the state dict of the model (ie a mapping of parameter names to tensors), processed to
        # match the HookedTransformer parameter names.
        state_dict = loading.get_pretrained_state_dict(
            official_model_name, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs
        )

        # Create the HookedTransformer object
        model = cls(
            cfg,
            tokenizer,
            move_to_device=False,
            default_padding_side=default_padding_side,
        )

        model.load_and_process_state_dict(
            state_dict,
            fold_ln=fold_ln,
            center_writing_weights=center_writing_weights,
            center_unembed=center_unembed,
            fold_value_biases=fold_value_biases,
            refactor_factored_attn_matrices=refactor_factored_attn_matrices,
        )

        if move_to_device:
            model.move_model_modules_to_device()

        print(f"Loaded pretrained model {model_name} into HookedTransformer")

        return model

    @classmethod
    def from_pretrained_no_processing(
        cls,
        model_name: str,
        fold_ln=False,
        center_writing_weights=False,
        center_unembed=False,
        refactor_factored_attn_matrices=False,
        fold_value_biases=False,
        dtype=torch.float32,
        default_prepend_bos=None,
        default_padding_side="right",
        **from_pretrained_kwargs,
    ):
        """Wrapper for from_pretrained.

        Wrapper for from_pretrained with all boolean flags related to simplifying the model set to
        False. Refer to from_pretrained for details.
        """
        return cls.from_pretrained(
            model_name,
            fold_ln=fold_ln,
            center_writing_weights=center_writing_weights,
            center_unembed=center_unembed,
            fold_value_biases=fold_value_biases,
            refactor_factored_attn_matrices=refactor_factored_attn_matrices,
            dtype=dtype,
            default_prepend_bos=default_prepend_bos,
            default_padding_side=default_padding_side,
            **from_pretrained_kwargs,
        )

    def init_weights(self):
        """Initialize weights.

        LayerNorm weights are already initialized to 1.0, and all biases are initialized to 0.0
        (including LayerNorm), so this just initializes weight matrices.

        Weight matrices are set to empty by default (to save space + compute, since they're the bulk
        of the parameters), so it is important to call this if you are not loading in pretrained
        weights! Note that this function assumes that weight names being with `W_`.

        Set seed here to ensure determinism.

        This does NOT follow the PyTorch scheme, which as far as I can tell is super out of date but
        no one has gotten round to updating it? https://github.com/pytorch/pytorch/issues/18182

        The default PyTorch scheme is the following: all linear layers use uniform(-1/sqrt(fan_in),
        1/sqrt(fan_in)) for weights, and uniform(-1/sqrt(fan_in), 1/sqrt(fan_in)) for biases. For
        biases, fan_in is computed using the fan_in for the weight matrix of the linear layer. Note
        tha it *does not actually* use Kaiming initialization, despite the fact that it calls the
        function.

        However, for Transformer blocks, it instead initializes biases to zero and weights using Xavier uniform, that
        is: uniform(-sqrt(6 / (fan_in + fan_out)), sqrt(6 / (fan_in + fan_out))) for weights.

        PyTorch Transformers are especially bad - TransformerEncoder initializes all layers to the
        exact same weights?! https://github.com/pytorch/pytorch/issues/72253.

        The best paper I've found on transformer initialization is the muP paper, but haven't
        integrated those ideas yet: https://arxiv.org/abs/2203.03466

        We split off the initialization into separate functions because muP initialization handles
        different parts of the model differently.
        """

        if self.cfg.seed is not None:
            torch.manual_seed(self.cfg.seed)

        if self.cfg.init_mode == "gpt2":
            self._init_weights_gpt2()
        elif self.cfg.init_mode == "xavier_uniform":
            self._init_weights_xavier(dist_type="uniform")
        elif self.cfg.init_mode == "xavier_normal":
            self._init_weights_xavier(dist_type="normal")
        elif self.cfg.init_mode == "kaiming_uniform":
            self._init_weights_kaiming(dist_type="uniform")
        elif self.cfg.init_mode == "kaiming_normal":
            self._init_weights_kaiming(dist_type="normal")
        elif self.cfg.init_mode == "muP":
            self._init_weights_muP(dist_type="normal")  # muP uses normal initialization

    def _init_weights_gpt2(self):
        """Initialize weights with GPT-2 initialization. Biases are initialized to 0.0 and weights
        are initialized to N(0, 0.64/d_model) if initializer_range is not set, otherwise std is initializer_range.
        """
        for name, param in self.named_parameters():
            if "W_" in name:
                nn.init.normal_(param, std=self.cfg.initializer_range)

    def _init_weights_xavier(self, dist_type="normal"):
        """
        Initialize weights with Xavier initialization -- that is, scale the weights by sqrt(6 /
        (fan_in + fan_out)) for a [-1, 1] uniform distribution, or sqrt(2 / (fan_in + fan_out)) for a
        standard normal.

        Note that since TransformerLens implements the matrices in the opposite orientation to what
        torch does (e.g. it's d_in x d_out, not d_out x d_in as in torch), we need to calculate it
        ourselves.
        """
        gain = self.cfg.initializer_range
        for name, param in self.named_parameters():
            if "W_" in name:
                if dist_type == "uniform":
                    init_xavier_uniform_(param, gain=gain)
                elif dist_type == "normal":
                    init_xavier_normal_(param, gain=gain)

    def _init_weights_kaiming(self, dist_type="uniform"):
        """
        Initialize weights with Kaiming initialization -- that is, scale the weights by
        c / sqrt(fan_in), where c = sqrt(2) if the params were immediately preceded by a relu and 1 for
        everything else.

        Note that the numbers are actually incorrect here when you're using a nonlinearity other
        than relu, e.g. the correct c for SiLu is ~1.74, for tanh it's 5/3 ~= 1.67, and for GeLU it's ~1.57.
        But this is unlikely to matter in practice.

        I'm just using fan_mode = "fan_in" for now, but it should be trivial to add fan_out.

        Again, we have to implement it ourselves because of the orientation of the matrices.
        """
        gain = self.cfg.initializer_range
        for name, param in self.named_parameters():
            if "W_" in name:
                if dist_type == "uniform":
                    init_kaiming_uniform_(param, gain=gain, nonlinearity="relu", mode="fan_in")
                elif dist_type == "normal":
                    init_kaiming_normal_(param, gain=gain, nonlinearity="relu", mode="fan_in")

    def _init_weights_muP(self, dist_type="uniform"):
        """
        Initialize weights with muParameterization. This involves scaling output weights by a factor
        of 1/fan_in, input weights and biases by 1, everything else by a factor of 1/sqrt(fan_in).

        Also, you need to use muAdamW, which rescales the learning rate for output weights and
        hidden weights by a factor of 1/fan_in.

        All biases are still assumed to be initialized to 0.0, so we only need to change the
        weights.
        """
        for name, param in self.named_parameters():
            if "W_" in name:
                fan_in, _ = utils.calc_fan_in_and_fan_out(param)
                if "embed" in name:
                    scale = float(1)
                elif "unembed" in name:
                    scale = 1 / fan_in
                else:
                    scale = 1 / fan_in**0.5

                if dist_type == "uniform":
                    scale *= 3**0.5
                    nn.init.uniform_(param, -scale, scale)
                elif dist_type == "normal":
                    nn.init.normal_(param, std=scale)

    def load_and_process_state_dict(
        self,
        state_dict: Dict[str, torch.Tensor],
        fold_ln: bool = True,
        center_writing_weights: bool = True,
        center_unembed: bool = True,
        fold_value_biases: bool = True,
        refactor_factored_attn_matrices: bool = False,
    ):
        """Load & Process State Dict.

        Load a state dict into the model, and to apply processing to simplify it. The state dict is
        assumed to be in the HookedTransformer format.

        See the relevant method (same name as the flag) for more details on the folding, centering
        and processing flags.

        Args:
            state_dict (dict): The state dict of the model, in HookedTransformer format. fold_ln
            fold_ln (bool, optional): Whether to fold in the LayerNorm weights to the
                subsequent linear layer. This does not change the computation. Defaults to True.
            center_writing_weights (bool, optional): Whether to center weights writing to the
                residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the
                computation. Defaults to True.
            center_unembed (bool, optional): Whether to center W_U (ie set mean to be zero).
                Softmax is translation invariant so this doesn't affect log probs or loss, but does
                change logits. Defaults to True.
            fold_value_biases (bool, optional): Whether to fold the value biases into the output
                bias. Because attention patterns add up to 1, the value biases always have a
                constant effect on a layer's output, and it doesn't matter which head a bias is
                associated with. We can factor this all into a single output bias to the layer, and
                make it easier to interpret the head's output.
            refactor_factored_attn_matrices (bool, optional): Whether to convert the factored
                matrices (W_Q & W_K, and W_O & W_V) to be "even". Defaults to False.
            model_name (str, optional): checks the model name for special cases of state dict
                loading. Only used for Redwood 2L model currently.
        """
        if self.cfg.dtype not in [torch.float32, torch.float64] and fold_ln:
            logging.warning(
                "With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`."
            )

        if (
            self.cfg.dtype not in [torch.float32, torch.float64]
            and self.cfg.num_experts
            and self.cfg.num_experts > 1
        ):
            logging.warning(
                "When running MoE models, it is advised to use a higher precision data type. See docs for more info."
            )

        state_dict = self.fill_missing_keys(state_dict)
        if fold_ln:
            if self.cfg.num_experts and self.cfg.num_experts > 1:
                logging.warning(
                    "You are using MoE, so the layer norm weights can't be folded! Skipping"
                )
            elif self.cfg.normalization_type in ["LN", "LNPre"]:
                state_dict = self.fold_layer_norm(state_dict)
            elif self.cfg.normalization_type in ["RMS", "RMSPre"]:
                state_dict = self.fold_layer_norm(
                    state_dict, fold_biases=False, center_weights=False
                )
            else:
                logging.warning(
                    "You are not using LayerNorm or RMSNorm, so the layer norm weights can't be folded! Skipping"
                )

        if center_writing_weights:
            if self.cfg.normalization_type not in ["LN", "LNPre"]:
                logging.warning(
                    "You are not using LayerNorm, so the writing weights can't be centered! Skipping"
                )
            elif self.cfg.final_rms:
                logging.warning(
                    "This model is using final RMS normalization, so the writing weights can't be centered! Skipping"
                )
            else:
                state_dict = self.center_writing_weights(state_dict)

        if center_unembed:
            state_dict = self.center_unembed(state_dict)
        if fold_value_biases:
            state_dict = self.fold_value_biases(state_dict)
        if refactor_factored_attn_matrices:
            state_dict = self.refactor_factored_attn_matrices(state_dict)

        if self.cfg.load_in_4bit:
            # with quantization, parameters should be assigned
            # so that quantization settings are not lost
            self.load_state_dict(state_dict, assign=True, strict=False)
        else:
            state_dict_keys = list(state_dict.keys())
            for key in state_dict_keys:
                self.load_state_dict({key: state_dict[key]}, strict=False)
                del state_dict[key]

    def fill_missing_keys(self, state_dict):
        return loading.fill_missing_keys(self, state_dict)

    def fold_layer_norm(
        self, state_dict: Dict[str, torch.Tensor], fold_biases=True, center_weights=True
    ):
        """Fold Layer Norm. Can also be used to fold RMS Norm, when fold_biases and center_weights are set to False.

        Takes in a state dict from a pretrained model, formatted to be consistent with
        HookedTransformer but with LayerNorm weights and biases. Folds these into the neighbouring
        weights. See further_comments.md for more details.

        Args:
            state_dict (Dict[str, torch.Tensor]): State dict of pretrained model.
            fold_biases (bool): Enables folding of LN biases. Should be disabled when RMS Norm is used.
            center_weights (bool): Enables the centering of weights after folding in LN. Should be disabled when RMS Norm is used.
        """

        # Models that use Grouped Query Attention (Only Mistral at the time of writing) prefix their K/V weights and
        # biases with an underscore in order to distinguish them, but folding the LN into them still works the same,
        # so we just add the underscore if GQA is used (i.e. if `cfg.n_key_value_heads is specified`).
        gqa = "" if self.cfg.n_key_value_heads is None else "_"

        for l in range(self.cfg.n_layers):
            # Fold ln1 into attention - it's important to fold biases first, since biases depend on
            # weights but not vice versa The various indexing is just to broadcast ln.b and ln.w
            # along every axis other than d_model. Each weight matrix right multiplies. To fold in
            # the bias, we use the W_ matrix to map it to the hidden space of the layer, so we need
            # to sum along axis -2, which is the residual stream space axis.
            if fold_biases:
                state_dict[f"blocks.{l}.attn.b_Q"] = state_dict[f"blocks.{l}.attn.b_Q"] + (
                    state_dict[f"blocks.{l}.attn.W_Q"]
                    * state_dict[f"blocks.{l}.ln1.b"][None, :, None]
                ).sum(-2)
                state_dict[f"blocks.{l}.attn.{gqa}b_K"] = state_dict[
                    f"blocks.{l}.attn.{gqa}b_K"
                ] + (
                    state_dict[f"blocks.{l}.attn.{gqa}W_K"]
                    * state_dict[f"blocks.{l}.ln1.b"][None, :, None]
                ).sum(
                    -2
                )
                state_dict[f"blocks.{l}.attn.{gqa}b_V"] = state_dict[
                    f"blocks.{l}.attn.{gqa}b_V"
                ] + (
                    state_dict[f"blocks.{l}.attn.{gqa}W_V"]
                    * state_dict[f"blocks.{l}.ln1.b"][None, :, None]
                ).sum(
                    -2
                )
                del state_dict[f"blocks.{l}.ln1.b"]

            state_dict[f"blocks.{l}.attn.W_Q"] = (
                state_dict[f"blocks.{l}.attn.W_Q"] * state_dict[f"blocks.{l}.ln1.w"][None, :, None]
            )
            state_dict[f"blocks.{l}.attn.{gqa}W_K"] = (
                state_dict[f"blocks.{l}.attn.{gqa}W_K"]
                * state_dict[f"blocks.{l}.ln1.w"][None, :, None]
            )
            state_dict[f"blocks.{l}.attn.{gqa}W_V"] = (
                state_dict[f"blocks.{l}.attn.{gqa}W_V"]
                * state_dict[f"blocks.{l}.ln1.w"][None, :, None]
            )
            del state_dict[f"blocks.{l}.ln1.w"]

            # Finally, we center the weights reading from the residual stream. The output of the
            # first part of the LayerNorm is mean 0 and standard deviation 1, so the mean of any
            # input vector of the matrix doesn't matter and can be set to zero. Equivalently, the
            # output of LayerNormPre is orthogonal to the vector of all 1s (because dotting with
            # that gets the sum), so we can remove the component of the matrix parallel to this.
            if center_weights:
                state_dict[f"blocks.{l}.attn.W_Q"] -= einops.reduce(
                    state_dict[f"blocks.{l}.attn.W_Q"],
                    "head_index d_model d_head -> head_index 1 d_head",
                    "mean",
                )
                state_dict[f"blocks.{l}.attn.{gqa}W_K"] -= einops.reduce(
                    state_dict[f"blocks.{l}.attn.{gqa}W_K"],
                    "head_index d_model d_head -> head_index 1 d_head",
                    "mean",
                )
                state_dict[f"blocks.{l}.attn.{gqa}W_V"] -= einops.reduce(
                    state_dict[f"blocks.{l}.attn.{gqa}W_V"],
                    "head_index d_model d_head -> head_index 1 d_head",
                    "mean",
                )

            # Fold ln2 into MLP
            if not self.cfg.attn_only:
                if fold_biases:
                    state_dict[f"blocks.{l}.mlp.b_in"] = state_dict[f"blocks.{l}.mlp.b_in"] + (
                        state_dict[f"blocks.{l}.mlp.W_in"]
                        * state_dict[f"blocks.{l}.ln2.b"][:, None]
                    ).sum(-2)
                    del state_dict[f"blocks.{l}.ln2.b"]

                state_dict[f"blocks.{l}.mlp.W_in"] = (
                    state_dict[f"blocks.{l}.mlp.W_in"] * state_dict[f"blocks.{l}.ln2.w"][:, None]
                )

                if self.cfg.gated_mlp:
                    state_dict[f"blocks.{l}.mlp.W_gate"] = (
                        state_dict[f"blocks.{l}.mlp.W_gate"]
                        * state_dict[f"blocks.{l}.ln2.w"][:, None]
                    )

                del state_dict[f"blocks.{l}.ln2.w"]

                if center_weights:
                    # Center the weights that read in from the LayerNormPre
                    state_dict[f"blocks.{l}.mlp.W_in"] -= einops.reduce(
                        state_dict[f"blocks.{l}.mlp.W_in"],
                        "d_model d_mlp -> 1 d_mlp",
                        "mean",
                    )

                if self.cfg.act_fn is not None and self.cfg.act_fn.startswith("solu"):
                    # Fold ln3 into activation
                    if fold_biases:
                        state_dict[f"blocks.{l}.mlp.b_out"] = state_dict[
                            f"blocks.{l}.mlp.b_out"
                        ] + (
                            state_dict[f"blocks.{l}.mlp.W_out"]
                            * state_dict[f"blocks.{l}.mlp.ln.b"][:, None]
                        ).sum(
                            -2
                        )

                        del state_dict[f"blocks.{l}.mlp.ln.b"]

                    state_dict[f"blocks.{l}.mlp.W_out"] = (
                        state_dict[f"blocks.{l}.mlp.W_out"]
                        * state_dict[f"blocks.{l}.mlp.ln.w"][:, None]
                    )

                    if center_weights:
                        # Center the weights that read in from the LayerNormPre
                        state_dict[f"blocks.{l}.mlp.W_out"] -= einops.reduce(
                            state_dict[f"blocks.{l}.mlp.W_out"],
                            "d_mlp d_model -> 1 d_model",
                            "mean",
                        )

                    del state_dict[f"blocks.{l}.mlp.ln.w"]

        # Fold ln_final into Unembed
        if not self.cfg.final_rms and fold_biases:
            # Dumb bug from my old SoLU training code, some models have RMSNorm instead of LayerNorm
            # pre unembed.
            state_dict[f"unembed.b_U"] = state_dict[f"unembed.b_U"] + (
                state_dict[f"unembed.W_U"] * state_dict[f"ln_final.b"][:, None]
            ).sum(dim=-2)
            del state_dict[f"ln_final.b"]

        state_dict[f"unembed.W_U"] = state_dict[f"unembed.W_U"] * state_dict[f"ln_final.w"][:, None]
        del state_dict[f"ln_final.w"]

        if center_weights:
            # Center the weights that read in from the LayerNormPre
            state_dict[f"unembed.W_U"] -= einops.reduce(
                state_dict[f"unembed.W_U"], "d_model d_vocab -> 1 d_vocab", "mean"
            )

        return state_dict

    def center_writing_weights(self, state_dict: Dict[str, torch.Tensor]):
        """Center Writing Weights.

        Centers the weights of the model that write to the residual stream - W_out, W_E, W_pos and
        W_out. This is done by subtracting the mean of the weights from the weights themselves. This
        is done in-place. See fold_layer_norm for more details.
        """
        state_dict["embed.W_E"] = state_dict["embed.W_E"] - state_dict["embed.W_E"].mean(
            -1, keepdim=True
        )
        if self.cfg.positional_embedding_type != "rotary":
            state_dict["pos_embed.W_pos"] = state_dict["pos_embed.W_pos"] - state_dict[
                "pos_embed.W_pos"
            ].mean(-1, keepdim=True)
        for l in range(self.cfg.n_layers):
            state_dict[f"blocks.{l}.attn.W_O"] = state_dict[f"blocks.{l}.attn.W_O"] - state_dict[
                f"blocks.{l}.attn.W_O"
            ].mean(
                -1, keepdim=True
            )  # W_O is [head_index, d_model, d_head]
            state_dict[f"blocks.{l}.attn.b_O"] = (
                state_dict[f"blocks.{l}.attn.b_O"] - state_dict[f"blocks.{l}.attn.b_O"].mean()
            )  # b_O is [d_model]
            if not self.cfg.attn_only:
                state_dict[f"blocks.{l}.mlp.W_out"] = state_dict[
                    f"blocks.{l}.mlp.W_out"
                ] - state_dict[f"blocks.{l}.mlp.W_out"].mean(-1, keepdim=True)
                state_dict[f"blocks.{l}.mlp.b_out"] = (
                    state_dict[f"blocks.{l}.mlp.b_out"] - state_dict[f"blocks.{l}.mlp.b_out"].mean()
                )
        return state_dict

    def center_unembed(self, state_dict: Dict[str, torch.Tensor]):
        """Center the unembedding weights W_U.

        This is done by subtracting the mean of the weights from the weights themselves. This is
        done in-place. As softmax is translation invariant, this changes the logits but not the log
        probs, and makes the model logits (slightly) more interpretable - when trying to understand
        how components contribute to the logits, we'll be less misled by components that just add
        something to every logit.
        """
        state_dict["unembed.W_U"] = state_dict["unembed.W_U"] - state_dict["unembed.W_U"].mean(
            -1, keepdim=True
        )
        state_dict["unembed.b_U"] = state_dict["unembed.b_U"] - state_dict["unembed.b_U"].mean()
        return state_dict

    def fold_value_biases(self, state_dict: Dict[str, torch.Tensor]):
        """Fold the value biases into the output bias.

        Because attention patterns add up to 1, the value biases always have a constant effect on a
        head's output. Further, as the outputs of each head in a layer add together, each head's
        value bias has a constant effect on the *layer's* output, which can make it harder to
        interpret the effect of any given head, and it doesn't matter which head a bias is
        associated with. We can factor this all into a single output bias to the layer, and make it
        easier to interpret the head's output. Formally, we take b_O_new = b_O_original +
        sum_head(b_V_head @ W_O_head).
        """
        for layer in range(self.cfg.n_layers):
            # shape [head_index, d_head]
            if self.cfg.n_key_value_heads is None:
                b_V = state_dict[f"blocks.{layer}.attn.b_V"]
            else:
                b_V = state_dict[f"blocks.{layer}.attn._b_V"]
                b_V = torch.repeat_interleave(
                    b_V, dim=0, repeats=self.cfg.n_heads // self.cfg.n_key_value_heads
                )
            # [head_index, d_head, d_model]
            W_O = state_dict[f"blocks.{layer}.attn.W_O"]
            # [d_model]
            b_O_original = state_dict[f"blocks.{layer}.attn.b_O"]
            folded_b_O = b_O_original + (b_V[:, :, None] * W_O).sum([0, 1])

            state_dict[f"blocks.{layer}.attn.b_O"] = folded_b_O
            if self.cfg.n_key_value_heads is None:
                state_dict[f"blocks.{layer}.attn.b_V"] = torch.zeros_like(b_V)
            else:
                state_dict[f"blocks.{layer}.attn._b_V"] = torch.zeros_like(
                    state_dict[f"blocks.{layer}.attn._b_V"]
                )
        return state_dict

    def refactor_factored_attn_matrices(self, state_dict: Dict[str, torch.Tensor]):
        """Experimental method for managing queries, keys and values.

        As argued in [A Mathematical Framework for Transformer
        Circuits](https://transformer-circuits.pub/2021/framework/index.html), queries, keys and
        values are somewhat arbitrary intermediate terms when computing with the low rank factored
        matrices W_QK = W_Q @ W_K.T and W_OV = W_V @ W_O, and these matrices are the only thing
        determining head behaviour. But there are many ways to find a low rank factorization to a
        given matrix, and hopefully some of these are more interpretable than others! This method is
        one attempt, which makes all of the matrices have orthogonal rows or columns, W_O into a
        rotation and W_Q and W_K having the nth column in each having the same norm. The formula is
        $W_V = U @ S,W_O=Vh.T,W_Q=U@S.sqrt(),W_K=Vh@S.sqrt()$.

        More details:

        If W_OV = U @ S @ Vh.T in its singular value decomposition, (where S is in R^d_head not
        R^d_model, as W_OV is low rank), W_OV = (U @ S) @ (Vh.T) is an equivalent low rank
        factorisation, where rows/columns of each matrix are orthogonal! So setting $W_V=US$ and
        $W_O=Vh.T$ works just as well. I *think* this is a more interpretable setup, because now
        $W_O$ is just a rotation, and doesn't change the norm, so $z$ has the same norm as the
        result of the head.

        For $W_QK = W_Q @ W_K.T$ we use the refactor $W_Q = U @ S.sqrt()$ and $W_K = Vh @ S.sqrt()$,
        which is also equivalent ($S==S.sqrt() @ S.sqrt()$ as $S$ is diagonal). Here we keep the
        matrices as having the same norm, since there's not an obvious asymmetry between the keys
        and queries.

        Biases are more fiddly to deal with. For OV it's pretty easy - we just need (x @ W_V + b_V)
        @ W_O + b_O to be preserved, so we can set b_V' = 0. and b_O' = b_V @ W_O + b_O (note that
        b_V in R^{head_index x d_head} while b_O in R^{d_model}, so we need to sum b_V @ W_O along
        the head_index dimension too).

        For QK it's messy - we need to preserve the bilinear form of (x @ W_Q + b_Q) * (y @ W_K +
        b_K), which is fairly messy. To deal with the biases, we concatenate them to W_Q and W_K to
        simulate a d_model+1 dimensional input (whose final coordinate is always 1), do the SVD
        factorization on this effective matrix, then separate out into final weights and biases.
        """

        assert (
            self.cfg.positional_embedding_type != "rotary"
        ), "You can't refactor the QK circuit when using rotary embeddings (as the QK matrix depends on the position of the query and key)"

        for l in range(self.cfg.n_layers):
            # W_QK = W_Q @ W_K.T
            # Concatenate biases to make a d_model+1 input dimension
            W_Q_eff = torch.cat(
                [
                    state_dict[f"blocks.{l}.attn.W_Q"],
                    state_dict[f"blocks.{l}.attn.b_Q"][:, None, :],
                ],
                dim=1,
            )
            W_K_eff = torch.cat(
                [
                    state_dict[f"blocks.{l}.attn.W_K"],
                    state_dict[f"blocks.{l}.attn.b_K"][:, None, :],
                ],
                dim=1,
            )

            W_Q_eff_even, W_K_eff_even_T = (
                FactoredMatrix(W_Q_eff, W_K_eff.transpose(-1, -2)).make_even().pair
            )
            W_K_eff_even = W_K_eff_even_T.transpose(-1, -2)

            state_dict[f"blocks.{l}.attn.W_Q"] = W_Q_eff_even[:, :-1, :]
            state_dict[f"blocks.{l}.attn.b_Q"] = W_Q_eff_even[:, -1, :]
            state_dict[f"blocks.{l}.attn.W_K"] = W_K_eff_even[:, :-1, :]
            state_dict[f"blocks.{l}.attn.b_K"] = W_K_eff_even[:, -1, :]

            # W_OV = W_V @ W_O
            W_V = state_dict[f"blocks.{l}.attn.W_V"]
            W_O = state_dict[f"blocks.{l}.attn.W_O"]

            # Factors the bias to be consistent.
            b_V = state_dict[f"blocks.{l}.attn.b_V"]
            b_O = state_dict[f"blocks.{l}.attn.b_O"]

            # Add singleton dimension for broadcasting
            b_V_expanded = einops.rearrange(b_V, "head_index d_head -> head_index d_head 1")

            # Element-wise multiplication of b_V and W_O
            b_V_times_W_O = b_V_expanded * W_O

            # Sum over d_head and head_index dimensions
            b_V_contribution = b_V_times_W_O.sum(1).sum(0)

            effective_bias = b_O + b_V_contribution
            state_dict[f"blocks.{l}.attn.b_V"] = torch.zeros_like(b_V)
            state_dict[f"blocks.{l}.attn.b_O"] = effective_bias

            # Helper class to efficiently deal with low rank factored matrices.
            W_OV = FactoredMatrix(W_V, W_O)
            U, S, Vh = W_OV.svd()
            state_dict[f"blocks.{l}.attn.W_V"] = U @ S.diag_embed()
            state_dict[f"blocks.{l}.attn.W_O"] = utils.transpose(Vh)

        return state_dict

    def set_use_attn_result(self, use_attn_result: bool):
        """Toggle whether to explicitly calculate and expose the result for each attention head.

        Useful for interpretability but can easily burn through GPU memory.
        """
        self.cfg.use_attn_result = use_attn_result

    def set_use_split_qkv_input(self, use_split_qkv_input: bool):
        """
        Toggles whether to allow editing of inputs to each attention head.
        """
        self.cfg.use_split_qkv_input = use_split_qkv_input

    def set_use_hook_mlp_in(self, use_hook_mlp_in: bool):
        """Toggles whether to allow storing and editing inputs to each MLP layer."""

        assert not self.cfg.attn_only, "Can't use hook_mlp_in with attn_only model"
        self.cfg.use_hook_mlp_in = use_hook_mlp_in

    def set_use_attn_in(self, use_attn_in: bool):
        """
        Toggles whether to allow editing of inputs to each attention head.
        """
        assert (
            self.cfg.n_key_value_heads is None
        ), "Can't use attn_in with GroupedQueryAttention, please use split_qkv_input instead"
        self.cfg.use_attn_in = use_attn_in

    def set_ungroup_grouped_query_attention(self, ungroup_grouped_query_attention: bool):
        """
        Toggles whether to ungroup the grouped key and value heads in models with grouped query attention (GQA).
        """
        self.cfg.ungroup_grouped_query_attention = ungroup_grouped_query_attention

    def process_weights_(
        self,
        fold_ln: bool = True,
        center_writing_weights: bool = True,
        center_unembed: bool = True,
        refactor_factored_attn_matrices: bool = False,
    ):
        """Wrapper around `load_and_process_state_dict`.

        Wrapper around load_and_process_state_dict to allow for in-place processing of the weights.
        This is useful if using HookedTransformer for training, if we then want to analyse a cleaner
        version of the same model.
        """
        state_dict = self.state_dict()
        if fold_ln and self.cfg.num_experts and self.cfg.num_experts > 1:
            # If we're using MoE, we don't fold the layer norm weights, so we don't need to do any preprocessing
            # A warning is already issued in `load_and_process_state_dict`
            pass
        elif fold_ln and self.cfg.normalization_type == "LN":
            # If we're folding the LN into the weights, we need to replace all the layernorm layers
            # with LayerNormPres, which do not have learnable parameters. This is somewhat hacky,
            # but it's the easiest way to do it.
            self.cfg.normalization_type = "LNPre"
            self.ln_final = LayerNormPre(self.cfg)
            for layer in self.blocks:
                layer.ln1 = LayerNormPre(self.cfg)
                layer.ln2 = LayerNormPre(self.cfg)
                if self.cfg.is_layer_norm_activation():
                    layer.mlp.ln = LayerNormPre(self.cfg)
        elif fold_ln and self.cfg.normalization_type == "RMS":
            # We do the same for RMSNorm if used
            self.cfg.normalization_type = "RMSPre"
            self.ln_final = RMSNormPre(self.cfg)
            for layer in self.blocks:
                layer.ln1 = RMSNormPre(self.cfg)
                layer.ln2 = RMSNormPre(self.cfg)
                if self.cfg.is_layer_norm_activation():
                    layer.mlp.ln = RMSNormPre(self.cfg)

        self.load_and_process_state_dict(
            state_dict,
            fold_ln=fold_ln,
            center_writing_weights=center_writing_weights,
            center_unembed=center_unembed,
            refactor_factored_attn_matrices=refactor_factored_attn_matrices,
        )

    @torch.inference_mode()
    def generate(
        self,
        input: Union[
            str,
            List[str],
            Int[torch.Tensor, "batch pos"],
            Float[torch.Tensor, "batch pos hidden_size"],
        ] = "",
        max_new_tokens: int = 10,
        stop_at_eos: bool = True,
        eos_token_id: Optional[int] = None,
        do_sample: bool = True,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: float = 1.0,
        freq_penalty: float = 0.0,
        use_past_kv_cache: bool = True,
        prepend_bos: Optional[bool] = USE_DEFAULT_VALUE,
        padding_side: Optional[Literal["left", "right"]] = USE_DEFAULT_VALUE,
        return_type: Optional[str] = "input",
        verbose: bool = True,
    ) -> Union[
        str,
        List[str],
        Int[torch.Tensor, "batch pos_plus_new_tokens"],
        Float[torch.Tensor, "batch pos_plus_new_tokens hidden_size"],
    ]:
        """Sample Tokens from the Model.

        Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.

        To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish
        (by producing an EOT token), we keep running the model on the entire batch, but throw away
        the output for a finished sequence and just keep adding EOTs to pad.

        Args:
            input (Union[str, List[str], Int[torch.Tensor, "batch pos"], Float[torch.Tensor, "batch pos hidden_size"]]):
                A text string (this will be converted to a batch of tokens with batch
                size 1), a list of strings, batch of tokens or a tensor of precomputed embeddings of shape
                [batch, pos, hidden_size].
            max_new_tokens (int): Maximum number of tokens to generate.
            stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token.
            eos_token_id (Optional[Union[int, Sequence]]): The token ID to use for end
                of sentence. If None, use the tokenizer's eos_token_id - required if using
                stop_at_eos. It's also possible to provide a list of token IDs (not just the
                eos_token_id), in which case the generation will stop when any of them are output
                (useful e.g. for stable_lm).
            do_sample (bool): If True, sample from the model's output distribution. Otherwise, use
                greedy search (take the max logit each time).
            top_k (int): Number of tokens to sample from. If None, sample from all tokens.
            top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0,
                we take the top tokens with cumulative probability >= top_p.
            temperature (float): Temperature for sampling. Higher values will make the model more
                random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is
                sampling from a uniform distribution).
            freq_penalty (float): Frequency penalty for sampling - how much to penalise previous
                tokens. Higher values will make the model more random. Works only with str and tokens input.
            use_past_kv_cache (bool): If True, create and use cache to speed up generation.
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (applicable when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos (default is True unless specified
                otherwise). Pass True or False to override the default.
            padding_side (Union[Literal["left", "right"], None], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple
                strings of different lengths.
            return_type (Optional[str]): The type of the output to return - a string or a list of strings ('str'),
                a tensor of tokens ('tokens'), a tensor of output embeddings ('embeds') or whatever the format of the
                input was ('input').
            verbose (bool): If True, show tqdm progress bars for generation.

        Returns:
            outputs (str, List[str], Int[torch.Tensor, "batch pos_plus_new_tokens"], Float[torch.Tensor,
                "batch pos_plus_new_tokens hidden_size"]): generated sequence. Str, tokens or embeddings.
                If input is embeddings and return type is tokens or string, returns only new generated sequence.
                In other cases returns sequence including input sequence.
        """

        with utils.LocallyOverridenDefaults(
            self, prepend_bos=prepend_bos, padding_side=padding_side
        ):
            assert isinstance(input, (str, torch.Tensor, list)) and (
                isinstance(input, list)
                and all(isinstance(i, str) for i in input)
                or not isinstance(input, list)
            ), "Input must be either string, torch.Tensor, or List[str]"

            assert return_type in [
                "input",
                "str",
                "tokens",
                "embeds",
            ], "return_type must be one of ['input', 'str', 'tokens', 'embeds']"

            if return_type == "input":
                if isinstance(input, (str, list)):
                    return_type = "str"
                elif input.ndim == 2:
                    return_type = "tokens"
                else:
                    return_type = "embeds"

            if isinstance(input, (str, list)):
                input_type = "str"
                # If text, convert to tokens (batch_size=1)
                assert (
                    self.tokenizer is not None
                ), "Must provide a tokenizer if passing a string to the model"
                input = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)
            elif input.ndim == 2:
                input_type = "tokens"
            else:
                input_type = "embeds"

            input_tokens = input if input_type in ["str", "tokens"] else None
            batch_size, ctx_length = input.shape[0], input.shape[1]
            device = devices.get_device_for_block_index(0, self.cfg)
            input = input.to(device)
            if use_past_kv_cache:
                past_kv_cache = HookedTransformerKeyValueCache.init_cache(
                    self.cfg, self.cfg.device, batch_size
                )
            else:
                past_kv_cache = None

            shortformer_pos_embed = None
            embeds = input if input_type == "embeds" else self.embed(input)

            assert isinstance(embeds, torch.Tensor) and embeds.ndim == 3

            stop_tokens: List[int] = []
            eos_token_for_padding = 0
            assert self.tokenizer is not None
            if stop_at_eos:
                tokenizer_has_eos_token = (
                    self.tokenizer is not None and self.tokenizer.eos_token_id is not None
                )
                if eos_token_id is None:
                    assert (
                        tokenizer_has_eos_token
                    ), "Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id"

                    eos_token_id = self.tokenizer.eos_token_id

                if isinstance(eos_token_id, int):
                    stop_tokens = [eos_token_id]
                    eos_token_for_padding = eos_token_id
                else:
                    # eos_token_id is a Sequence (e.g. list or tuple)
                    stop_tokens = eos_token_id
                    eos_token_for_padding = (
                        self.tokenizer.eos_token_id if tokenizer_has_eos_token else eos_token_id[0]
                    )

            # An array to track which sequences in the batch have finished.
            finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.cfg.device)

            # Currently nothing in HookedTransformer changes with eval, but this is here in case
            # that changes in the future.
            self.eval()
            sampled_tokens_list: List[torch.Tensor] = []
            for index in tqdm.tqdm(range(max_new_tokens), disable=not verbose):
                pos_offset = self.get_pos_offset(past_kv_cache, batch_size)

                tokens = torch.zeros((embeds.size(0), embeds.size(1))).to(torch.int)
                attention_mask = utils.get_attention_mask(
                    self.tokenizer, tokens, False if prepend_bos is None else prepend_bos
                ).to(device)
                residual, shortformer_pos_embed = self.get_residual(
                    embeds,
                    pos_offset,
                    return_shortformer_pos_embed=True,
                    device=device,
                    attention_mask=attention_mask,
                )

                # While generating, we keep generating logits, throw away all but the final logits,
                # and then use those logits to sample from the distribution We keep adding the
                # sampled tokens to the end of tokens.
                start_at_layer = 0  # Make forward returns embeddings
                if use_past_kv_cache:
                    # We just take the final tokens, as a [batch, 1] tensor
                    if index > 0:
                        logits = self.forward(
                            residual[:, -1:],
                            return_type="logits",
                            prepend_bos=prepend_bos,
                            padding_side=padding_side,
                            past_kv_cache=past_kv_cache,
                            start_at_layer=start_at_layer,
                            shortformer_pos_embed=shortformer_pos_embed,
                        )
                    else:
                        logits = self.forward(
                            residual,
                            return_type="logits",
                            prepend_bos=prepend_bos,
                            padding_side=padding_side,
                            past_kv_cache=past_kv_cache,
                            start_at_layer=start_at_layer,
                            shortformer_pos_embed=shortformer_pos_embed,
                        )
                else:
                    # We input the entire sequence, as a [batch, pos] tensor, since we aren't using
                    # the cache.
                    logits = self.forward(
                        residual,
                        return_type="logits",
                        prepend_bos=prepend_bos,
                        padding_side=padding_side,
                        start_at_layer=start_at_layer,
                        shortformer_pos_embed=shortformer_pos_embed,
                    )
                final_logits = logits[:, -1, :]

                if do_sample:
                    if input_type in [
                        "str",
                        "tokens",
                    ]:  # Those types of inputs support frequency penalty
                        assert input_tokens is not None
                        sampled_tokens = utils.sample_logits(
                            final_logits,
                            top_k=top_k,
                            top_p=top_p,
                            temperature=temperature,
                            freq_penalty=freq_penalty,
                            tokens=torch.cat(
                                (input_tokens, torch.cat(sampled_tokens_list, dim=1)), dim=1
                            )
                            if "sampled_tokens" in locals()
                            else input_tokens,
                        ).to(devices.get_device_for_block_index(0, self.cfg))
                    else:
                        sampled_tokens = utils.sample_logits(
                            final_logits, top_k=top_k, top_p=top_p, temperature=temperature
                        ).to(devices.get_device_for_block_index(0, self.cfg))
                else:
                    sampled_tokens = final_logits.argmax(-1).to(
                        devices.get_device_for_block_index(0, self.cfg)
                    )
                sampled_tokens_list.append(sampled_tokens.unsqueeze(1))
                if stop_at_eos:
                    # For all unfinished sequences, add on the next token. If a sequence was
                    # finished, throw away the generated token and add eos_token_for_padding
                    # instead.
                    sampled_tokens[finished_sequences] = eos_token_for_padding
                    finished_sequences.logical_or_(
                        torch.isin(
                            sampled_tokens.to(self.cfg.device),
                            torch.tensor(stop_tokens).to(self.cfg.device),
                        )
                    )

                embeds = torch.hstack([embeds, self.embed(sampled_tokens.unsqueeze(-1))])

                if stop_at_eos and finished_sequences.all():
                    break

            sampled_tokens = torch.cat(sampled_tokens_list, dim=1)
            if input_type in ["str", "tokens"]:
                assert input_tokens is not None
                output_tokens = torch.cat((input_tokens, sampled_tokens), dim=1)
            else:
                output_tokens = sampled_tokens

            if return_type == "str":
                decoded_texts = [
                    self.tokenizer.decode(tokens, skip_special_tokens=True)
                    for tokens in output_tokens
                ]
                return decoded_texts[0] if len(decoded_texts) == 1 else decoded_texts
            elif return_type == "tokens":
                return output_tokens
            else:
                return embeds

    # Give access to all weights as properties.
    @property
    def W_U(self) -> Float[torch.Tensor, "d_model d_vocab"]:
        """Convenience to get the unembedding matrix.

        I.e. the linear map from the final residual stream to the output logits).
        """
        return self.unembed.W_U

    @property
    def b_U(self) -> Float[torch.Tensor, "d_vocab"]:
        return self.unembed.b_U

    @property
    def W_E(self) -> Float[torch.Tensor, "d_vocab d_model"]:
        """Convenience to get the embedding matrix."""
        return self.embed.W_E

    @property
    def W_pos(self) -> Float[torch.Tensor, "n_ctx d_model"]:
        """Convenience function to get the positional embedding.

        Only works on models with absolute positional embeddings!
        """
        return self.pos_embed.W_pos

    @property
    def W_E_pos(self) -> Float[torch.Tensor, "d_vocab+n_ctx d_model"]:
        """Concatenated W_E and W_pos.

        Used as a full (overcomplete) basis of the input space, useful for full QK and full OV
        circuits.
        """
        return torch.cat([self.W_E, self.W_pos], dim=0)

    # Layer-specific weights are stacked into one massive tensor and given as properties for
    # convenience and a cache is used to avoid repeated computation. Often a useful convenience when
    # we want to do analysis on weights across all layers. If GPU memory is a bottleneck, don't use
    # these properties!

    @property
    def W_K(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stack the key weights across all layers."""
        return torch.stack([block.attn.W_K for block in self.blocks], dim=0)

    @property
    def W_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stack the query weights across all layers."""
        return torch.stack([block.attn.W_Q for block in self.blocks], dim=0)

    @property
    def W_V(self) -> Float[torch.Tensor, "n_layers n_heads d_model d_head"]:
        """Stack the value weights across all layers."""
        return torch.stack([block.attn.W_V for block in self.blocks], dim=0)

    @property
    def W_O(self) -> Float[torch.Tensor, "n_layers n_heads d_head d_model"]:
        """Stack the attn output weights across all layers."""
        return torch.stack([block.attn.W_O for block in self.blocks], dim=0)

    @property
    def W_in(self) -> Float[torch.Tensor, "n_layers d_model d_mlp"]:
        """Stack the MLP input weights across all layers."""
        return torch.stack([block.mlp.W_in for block in self.blocks], dim=0)

    @property
    def W_gate(self) -> Union[Float[torch.Tensor, "n_layers d_model d_mlp"], None]:
        """Stack the MLP gate weights across all layers.

        Only works for models with gated MLPs.
        """
        if self.cfg.gated_mlp:
            return torch.stack([block.mlp.W_gate for block in self.blocks], dim=0)
        else:
            return None

    @property
    def W_out(self) -> Float[torch.Tensor, "n_layers d_mlp d_model"]:
        """Stack the MLP output weights across all layers."""
        return torch.stack([block.mlp.W_out for block in self.blocks], dim=0)

    @property
    def b_K(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stack the key biases across all layers."""
        return torch.stack([block.attn.b_K for block in self.blocks], dim=0)

    @property
    def b_Q(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stack the query biases across all layers."""
        return torch.stack([block.attn.b_Q for block in self.blocks], dim=0)

    @property
    def b_V(self) -> Float[torch.Tensor, "n_layers n_heads d_head"]:
        """Stack the value biases across all layers."""
        return torch.stack([block.attn.b_V for block in self.blocks], dim=0)

    @property
    def b_O(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stack the attn output biases across all layers."""
        return torch.stack([block.attn.b_O for block in self.blocks], dim=0)

    @property
    def b_in(self) -> Float[torch.Tensor, "n_layers d_mlp"]:
        """Stack the MLP input biases across all layers."""
        return torch.stack([block.mlp.b_in for block in self.blocks], dim=0)

    @property
    def b_out(self) -> Float[torch.Tensor, "n_layers d_model"]:
        """Stack the MLP output biases across all layers."""
        return torch.stack([block.mlp.b_out for block in self.blocks], dim=0)

    @property
    def QK(self):
        return FactoredMatrix(self.W_Q, self.W_K.transpose(-2, -1))

    @property
    def OV(self):
        return FactoredMatrix(self.W_V, self.W_O)

    # Various utility functions
    def accumulated_bias(
        self, layer: int, mlp_input: bool = False, include_mlp_biases=True
    ) -> Float[torch.Tensor, "d_model"]:
        """Accumulated Bias.

        Returns the accumulated bias from all layer outputs (ie the b_Os and b_outs), up to the
        input of layer L.

        Args:
            layer (int): Layer number, in [0, n_layers]. layer==0 means no layers, layer==n_layers
                means all layers.
            mlp_input (bool): If True, we take the bias up to the input of the MLP
                of layer L (ie we include the bias from the attention output of the current layer,
                otherwise just biases from previous layers)
            include_mlp_biases (bool): Whether to include the biases of MLP layers. Often useful to
                have as False if we're expanding attn_out into individual heads, but keeping mlp_out
                as is.

        Returns:
            bias (torch.Tensor): [d_model], accumulated bias
        """
        accumulated_bias = torch.zeros(self.cfg.d_model, device=self.cfg.device)

        for i in range(layer):
            block = cast(TransformerBlock, self.blocks[i])
            accumulated_bias += cast(torch.Tensor, block.attn.b_O)
            if include_mlp_biases:
                accumulated_bias += cast(torch.Tensor, block.mlp.b_out)
        if mlp_input:
            assert layer < self.cfg.n_layers, "Cannot include attn_bias from beyond the final layer"
            block = cast(TransformerBlock, self.blocks[layer])
            accumulated_bias += cast(torch.Tensor, block.attn.b_O)
        return accumulated_bias

    def all_composition_scores(
        self, mode
    ) -> Float[torch.Tensor, "n_layers n_heads n_layers n_heads"]:
        """All Composition Scores.

        Returns the Composition scores for all pairs of heads, as a L1, H1, L2, H2 tensor (which is
        upper triangular on the first and third axes).

        See
        https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition
        for three metrics used.

        Args:
            mode (str): One of ["Q", "K", "V"], the mode to use for the composition score.
        """
        left = self.OV
        if mode == "Q":
            right = self.QK
        elif mode == "K":
            right = self.QK.T
        elif mode == "V":
            right = self.OV
        else:
            raise ValueError(f"mode must be one of ['Q', 'K', 'V'] not {mode}")

        scores = utils.composition_scores(left, right, broadcast_dims=True)
        # Mask scores to be zero for all pairs with the right head in the same layer or earlier
        # layer than the left head.
        mask = (
            torch.arange(self.cfg.n_layers, device=self.cfg.device)[:, None, None, None]
            < torch.arange(self.cfg.n_layers, device=self.cfg.device)[None, None, :, None]
        )
        scores = torch.where(mask, scores, torch.zeros_like(scores))
        return scores

    def all_head_labels(self):
        """Returns a list of all head names in the model."""
        return [f"L{l}H{h}" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)]

    def load_sample_training_dataset(self, **kwargs):
        """Load Sample Training Dataset.

        Helper function to load in a 10K-20K dataset of elements from the model's training data
        distribution.

        Wrapper around utils.get_dataset, which identifies the appropriate dataset the pretrained
        models. Each dataset has a 'text' field, which contains the relevant info, some have several
        meta data fields.

        Kwargs will be passed to utils.get_dataset (e.g. cache_dir to set download location)

        Notes:

        - PT-2's training data is not open source. OpenWebText is a replication (links with
            >3 karma on Reddit)
        - OPT's training data is not open source, and is a mess of different things that is hard to
          replicate. I default to the Pile, which covers some of it, but imperfectly.

        (Some models will have actually been trained on the data supplied here, for some it's from
        the validation set).
        """
        model_dataset_map = {
            "neel": "c4_code",
            "neel-solu-old": "pile",
            "GPT2LMHeadModel": "openwebtext",
            "GPTNeoForCausalLM": "pile",
            "GPTNeoXForCausalLM": "pile",
            "GPTJForCausalLM": "pile",
            "OPTForCausalLM": "pile",
        }
        if self.cfg.original_architecture in model_dataset_map:
            self.dataset = utils.get_dataset(
                model_dataset_map[self.cfg.original_architecture], **kwargs
            )
        else:
            raise ValueError(
                f"We do not have an available dataset for the relevant model: {self.cfg.original_architecture}"
            )
        return self.dataset

    def sample_datapoint(
        self,
        tokenize: bool = False,
        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,
        padding_side: Optional[Literal["left", "right"]] = USE_DEFAULT_VALUE,
    ) -> Union[str, Float[torch.Tensor, "1 pos"]]:
        """Sample Data Point from Dataset.

        Helper function to randomly sample a data point from self.dataset, a small dataset from the
        data distribution the model was trained on.

        Implicitly calls self.load_sample_training_dataset if it hasn't already been called. Only
        works for pretrained models with an associated dataset. But you can manually replace
        self.dataset with a dataset of your choice if you want.

        Args:
            tokenize (bool): Whether to return tokens (instead of text). Defaults to False. Note
                that the returned tokens will be automatically truncated to the model's max context
                size.
            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend
                the BOS token to the input (applicable when input is a string). Defaults to None,
                implying usage of self.cfg.default_prepend_bos (default is True unless specified
                otherwise). Pass True or False to override the default.
            padding_side (Union[Literal["left", "right"], None], optional): Overrides
                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple
                strings of different lengths.
        """
        if self.dataset is None:
            self.load_sample_training_dataset()
        assert self.dataset is not None  # keep mypy happy
        sample_dataset_size = len(self.dataset)
        index = np.random.randint(0, sample_dataset_size)
        if not tokenize:
            return self.dataset[index]["text"]
        else:
            return self.to_tokens(
                self.dataset[index]["text"],
                prepend_bos=prepend_bos,
                padding_side=padding_side,
                truncate=True,
            )



---
File: /transformer_lens/HookedTransformerConfig.py
---

"""Hooked Transformer Config.

Module with a dataclass for storing the configuration of a
:class:`transformer_lens.HookedTransformer` model.
"""

from __future__ import annotations

import logging
import pprint
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch

from transformer_lens import utils
from transformer_lens.utilities.activation_functions import SUPPORTED_ACTIVATIONS


@dataclass
class HookedTransformerConfig:
    """
    Configuration class to store the configuration of a HookedTransformer model.

    See further_comments.md for more details on the more complex arguments.

    Args:
        d_model (int): The dimensionality of the embeddings.
        d_head (int): The dimensionality of each attention head.
        n_layers (int): The number of transformer blocks (one block = one attn layer AND one MLP layer).
        n_ctx (int): The maximum sequence length.
        n_heads (int): The number of attention heads. If not
            specified, will be set to d_model // d_head. (This is represented by a default value of -1)
        d_mlp (int, *optional*): The dimensionality of the feedforward mlp
            network. Defaults to 4 * d_model, and in an attn-only model is None.
        d_vocab (int): The size of the vocabulary. Defaults to -1, which means not set. If not set, will be
            automatically set from the tokenizer's vocab size.
        act_fn (str, *optional*): The activation function to use. Always
            lowercase. Supports ['relu', 'gelu', 'silu', 'gelu_new', 'solu_ln',
            'gelu_fast']. Must be set unless using an attn-only model.
        eps (float): The epsilon value to use for layer normalization. Defaults
            to 1e-5
        use_attn_result (bool): whether to explicitly calculate the amount
            each head adds to the residual stream (with a hook) and THEN add it
            up, vs just calculating the sum. This can be very memory intensive
            for large models, so defaults to False
        use_split_qkv_input (bool): whether to explicitly calculate the input of
            each head separately, with a hook. Defaults to false to save memory.
        use_hook_mlp_in (bool): whether to use a hook to get the input to the
            MLP layer. Defaults to false to save memory.
        use_attn_in (bool): whether to explicitly calculate the input of each
            attention head separately, with a hook. Defaults to false to save memory
        use_attn_scale (bool): whether to scale the attention weights by
            1/sqrt(d_head)
        ungroup_grouped_query_attention (bool): whether to ungroup key and value heads, for models that use
            grouped query attention.
        attn_scale (float): The amount to divide attention scores by (if applicable). Defaults to
            sqrt(d_head)
        model_name (str): the name of the model, used to load
            weights from HuggingFace or initialized to "custom" if not passed
        original_architecture (str, *optional*): the family of the model, used
        to help load
            weights from HuggingFace or initialized to "custom" if not passed
        from_checkpoint (bool): Whether the model weights were
            loaded from a checkpoint (only applies to pretrained models)
        checkpoint_index (int, *optional*): The index of the
            checkpoint loaded (only applies to pretrained models).
        checkpoint_label_type (str, *optional*): Whether
            checkpoints are labelled by the number of steps or number of tokens.
        checkpoint_value (int, *optional*): The value of the
            checkpoint label (whether of steps or tokens).
        tokenizer_name (str, *optional*): the full name of the model, passed into
            HuggingFace to access the tokenizer. Only used when passing in
            custom config, if loading from pretrained then this is not needed.
        use_local_attn (bool): whether to use local attention - ie each
            destination token can only attend to source tokens a certain distance back.
        window_size (int, *optional*): the size of the window for local
            attention
        attn_types (List[str], *optional*): the types of attention to use for
            local attention
        init_mode (str): the initialization mode to use for the
            weights. Only relevant for custom models, ignored for pre-trained.
            We now support 'gpt2', 'xavier_uniform', 'xavier_normal', 'kaiming_uniform',
            'kaiming_normal'. MuP support to come. Defaults to 'gpt2'.
        normalization_type (str, *optional*): the type of normalization to use.
            Options are None (no normalization), 'LN' (use LayerNorm, including weights
            & biases) and 'LNPre' (use LayerNorm, but no weights or biases), 'RMS'
            (use RMSNorm, including weights) and 'RMSPre' (use RMSNorm, but no weights or biases).
            Defaults to LN
        device(str): The device to use for the model. Defaults to 'cuda' if
            available, else 'cpu'. Must be 'cuda' if `n_devices` > 1.
        n_devices (int): The number of devices to use for the model. Defaults to 1. Layers are loaded
            to support "pipeline parallelism", where each device is responsible for a subset of the layers.
        attention_dir (str): Whether to use causal (aka unidirectional aka GPT-2
            style) or bidirectional attention. Options are 'causal' and
            'bidirectional'. Defaults to 'causal'
        attn_only (bool): Whether to only use attention layers, no feedforward
            layers. Defaults to False
        seed (int, *optional*): The seed to use for the model.
            Used to set sources of randomness (Python, PyTorch and NumPy) and to initialize weights.
            Defaults to None. We recommend setting a seed, so your experiments are reproducible.
        initializer_range (float): The standard deviation of the normal used to
            initialise the weights, initialized to 0.8 / sqrt(d_model). If init_mode is
            'xavier_uniform' or 'xavier_normal', this value is instead treated as the `gain` parameter for the weight
            initialisation (a constant factor to scale the weights by). Defaults to -1.0, which means not set.
        init_weights (bool): Whether to initialize the weights. Defaults to
            True. If False, does not initialize weights.
        scale_attn_by_inverse_layer_idx (bool): Whether to scale the attention
            weights by 1/(layer_id+1), used by Mistral (Stanford) models for numerical stability when
            training in FP16. Defaults to False.
        positional_embedding_type (str): The positional embedding used. Options
            are 'standard' (ie GPT-2 style, absolute, randomly initialized learned positional
            embeddings, directly added to the residual stream), 'rotary'
            (described here: https://blog.eleuther.ai/rotary-embeddings/ ) and
            'shortformer' (GPT-2 style absolute & learned, but rather than being
            added to the residual stream they're only added to the inputs to the
            keys and the queries (ie key = W_K(res_stream + pos_embed), but
            values and MLPs don't get any positional info)). Sinusoidal are not
            currently supported. Defaults to 'standard'.
        final_rms (bool): Whether to replace the final normalization (just
            before the unembed) with RMSNorm (ie no centering or bias, just
            scaling + weights). Only included because of a dumb bug in my
            original SoLU code. Defaults to False.
        d_vocab_out (int, *optional*): The size of the output vocabulary. Defaults to -1, which means not set. If not
            set, will be equal to d_vocab. Mainly useful for algorithmic tasks
            where the input and output vocabularies may be different.
        parallel_attn_mlp (bool): Whether to parallelize the attention and MLP
            layers - a weird cursed thing done by GPT-J. Means that
            mlp_out=MLP(ln1(resid_pre)) and resid_post=resid_pre+attn_out+mlp_out. Defaults to False.
        rotary_dim (int, *optional*): The dimensionality of the rotary
            embeddings, may be d_head in which case only the first rotary_dim
            dimensions of each head are rotated. Defaults to None, if
            positional_embedding_type=="rotary" post-init then sets it to d_head, i.e. "rotate all
            dimensions of the query and key".
        n_params (int, *optional*): The number of (hidden weight)
            parameters in the model. This is automatically calculated and not
            intended to be set by the user. (Non embedding parameters, because
            the [scaling laws paper](https://arxiv.org/pdf/2001.08361.pdf) found
            that that was a more meaningful number. Ignoring biases and layer
            norms, for convenience)
        use_hook_tokens (bool): Will add a hook point on the token input to
            HookedTransformer.forward, which lets you cache or intervene on the tokens.
            Defaults to False.
        default_prepend_bos (bool, optional): Default behavior of whether to prepend the BOS token when the
            methods of HookedTransformer process input text to tokenize (only when input is a string).
            Defaults to True - even for models not explicitly trained with this, heads often use the
            first position as a resting position and accordingly lose information from the first token,
            so this empirically seems to give better results. To change the default behavior to False, pass in
            default_prepend_bos=False. Note that you can also locally override the default behavior by passing
            in prepend_bos=True/False when you call a method that processes the input string.
        dtype (torch.dtype, *optional*): The model's dtype. Defaults to torch.float32.
        tokenizer_prepends_bos (bool, *optional*): This flag is set by set_tokenizer. It is set to True only
            when the tokenizer automatically prepends the BOS token if initialized with add_bos_token=True.
            We need this information to dynamically control bos prepending.
        load_in_4bit(bool): If this flag is set, then it's assumed that parameters are 4-bit quantized
            with bitsandbytes. Currently only supported for Llama.
        n_key_value_heads (int, *optional*): The number of groups of heads that use the same key and value matrix.
            Only for models that use Grouped Query Attention.
        post_embedding_ln (bool): Whether to apply layer normalization after embedding the tokens. Defaults
            to False.
        num_experts (int, *optional*): The number of experts to use in the MoE layer. If set, experts_per_token
            must also be set. Set to None if not using MoE.
        experts_per_token (int, *optional*): The number of experts to use for each pass in the MoE layer. If set,
            num_experts must also be set. Set to None if not using MoE.
        relative_attention_max_distance (int, *optional*): The maximum distance between tokens for relative
            attention. If set, relative_attention_num_buckets must also be set.Only used in EncoderDecoder models, like T5.
        relative_attention_num_buckets (int, *optional*): The number of buckets to use for relative attention.
            If set, relative_attention_max_distance must also be set.Only used in EncoderDecoder models, like T5.
        decoder_start_token_id (int, *optional*): The start token id for the decoder. Only used in EncoderDecoder models, like T5.
        tie_word_embeddings (bool): Whether to tie the word embeddings and the output layer weights. Defaults to False. Only used in EncoderDecoder (T5) by now.
        use_normalization_before_and_after (bool): Whether to apply normalization (LN/RMS/etc)
            to both the input of an attn/MLP block *and* the output (before adding back to the
            residual stream). Currently only used in Gemma-2. Defaults to False.
        attn_scores_soft_cap (float): An optional softcap for attention scores pre-softmax. If
            used, it will map attn_scores -> soft_cap * tanh(attn_scores / soft_cap). As tanh's
            output is in [-1, 1], this maps attn_scores to [-soft_cap, soft_cap], with little
            effect on small values, but squashing large values into that interval. Currently only
            used in Gemma-2. Defaults to -1.0, which means not set.
        output_logits_soft_cap (float): An optional softcap for output logits, currently only used
            in Gemma-2 (see attn_scores_soft_cap for details). Defaults to -1.0, which means not
            set.
        use_NTK_by_parts_rope (bool): Whether to apply the "NTK-by-parts" method when using Rotary
            Positional Embedding. This method adjusts the interpolation based on frequency factors
            for different parts of the hidden dimensions. See Section 3.2 in
            https://arxiv.org/pdf/2309.00071 for details. Defaults to False.
        NTK_by_parts_low_freq_factor (float): The threshold applied to low-frequency hidden
            dimensions during interpolation when using the "NTK-by-parts" method. Defaults to 1.0.
        NTK_by_parts_high_freq_factor (float): The threshold applied to high-frequency hidden
            dimensions during interpolation in the "NTK-by-parts" method. Defaults to 4.0.
        NTK_by_parts_factor (float): The overall factor used in the "NTK-by-parts" method that
            affects the rate of change between low and high-frequency interpolation strategies.
            Defaults to 8.0.


    """

    n_layers: int
    d_model: int
    n_ctx: int
    d_head: int
    model_name: str = "custom"
    n_heads: int = -1
    d_mlp: Optional[int] = None
    act_fn: Optional[str] = None
    d_vocab: int = -1
    eps: float = 1e-5
    use_attn_result: bool = False
    use_attn_scale: bool = True
    attn_scale: float = -1.0
    use_split_qkv_input: bool = False
    use_hook_mlp_in: bool = False
    use_attn_in: bool = False
    use_qk_norm: bool = False
    use_local_attn: bool = False
    ungroup_grouped_query_attention: bool = False
    original_architecture: Optional[str] = None
    from_checkpoint: bool = False
    checkpoint_index: Optional[int] = None
    checkpoint_label_type: Optional[str] = None
    checkpoint_value: Optional[int] = None
    tokenizer_name: Optional[str] = None
    window_size: Optional[int] = None
    attn_types: Optional[List] = None
    init_mode: str = "gpt2"
    normalization_type: Optional[str] = "LN"
    device: Optional[str] = None
    n_devices: int = 1
    attention_dir: str = "causal"
    attn_only: bool = False
    seed: Optional[int] = None
    initializer_range: float = -1.0
    init_weights: bool = True
    scale_attn_by_inverse_layer_idx: bool = False
    positional_embedding_type: str = "standard"
    final_rms: bool = False
    d_vocab_out: int = -1
    parallel_attn_mlp: bool = False
    rotary_dim: Optional[int] = None
    n_params: Optional[int] = None
    use_hook_tokens: bool = False
    gated_mlp: bool = False
    default_prepend_bos: bool = True
    dtype: torch.dtype = torch.float32
    tokenizer_prepends_bos: Optional[bool] = None
    n_key_value_heads: Optional[int] = None
    post_embedding_ln: bool = False
    rotary_base: int = 10000
    trust_remote_code: bool = False
    rotary_adjacent_pairs: bool = False
    load_in_4bit: bool = False
    num_experts: Optional[int] = None
    experts_per_token: Optional[int] = None
    relative_attention_max_distance: Optional[int] = None
    relative_attention_num_buckets: Optional[int] = None
    decoder_start_token_id: Optional[int] = None
    tie_word_embeddings: bool = False
    use_normalization_before_and_after: bool = False
    attn_scores_soft_cap: float = -1.0
    output_logits_soft_cap: float = -1.0
    use_NTK_by_parts_rope: bool = False
    NTK_by_parts_low_freq_factor: float = 1.0
    NTK_by_parts_high_freq_factor: float = 4.0
    NTK_by_parts_factor: float = 8.0
    NTK_original_ctx_len: int = 8192

    def __post_init__(self):
        if self.n_heads == -1:
            self.n_heads = self.d_model // self.d_head

            if not self.d_model % (self.d_head) == 0:
                logging.warning(
                    "d_model %d is not divisible by d_head %d."
                    "n_heads was inferred to be %d, rounding down the ratio.",
                    self.d_model,
                    self.d_head,
                    self.n_heads,
                )

        if self.seed is not None:
            self.set_seed_everywhere(self.seed)
        if self.use_local_attn:
            assert self.window_size is not None, "window_size must be specified for local attention"
            assert self.attn_types is not None, "attn_types must be specified for local attention"
        if not self.attn_only:
            if self.d_mlp is None:
                # For some reason everyone hard codes in this hyper-parameter!
                self.d_mlp: int = self.d_model * 4
            assert self.act_fn is not None, "act_fn must be specified for non-attn-only models"
            assert (
                self.act_fn in SUPPORTED_ACTIVATIONS
            ), f"act_fn={self.act_fn} must be one of {SUPPORTED_ACTIVATIONS}"
        if self.initializer_range < 0 and self.init_mode == "gpt2":
            # Roughly copy the GPT-2 value, but proportional to sqrt(1/d_model)
            self.initializer_range = 0.8 / np.sqrt(self.d_model)
        if self.initializer_range < 0 and self.init_mode != "gpt2":
            # This is the gain parameter for the weight initialisation
            self.initializer_range = 1.0

        if self.d_vocab_out == -1:
            # d_vocab_out defaults to d_vocab, unless there's an algorithmic task
            # If d_vocab is not set, it'll be inferred from tokenizer_name or from a tokenizer
            # explicitly passed to HookedTransformer initialisation.
            self.d_vocab_out = self.d_vocab

        if self.positional_embedding_type == "rotary" and self.rotary_dim is None:
            self.rotary_dim = self.d_head

        if self.num_experts is not None:
            assert (
                self.experts_per_token is not None
            ), "experts_per_token must be set if num_experts is set"
        if self.experts_per_token is not None:
            assert (
                self.num_experts is not None
            ), "num_experts must be set if experts_per_token is set"

        # The number of parameters in attention layers (ignoring biases and layer norm). 4 because W_Q, W_K, W_V and W_O
        self.n_params = self.n_layers * ((self.d_model * self.d_head * self.n_heads * 4))
        if not self.attn_only:
            assert self.d_mlp is not None  # mypy
            # Number of parameters in MLP layers (ignoring biases and layer norm). 2 because W_in and W_out
            mlp_params_per_layer = self.d_model * self.d_mlp * (2 + self.gated_mlp)

            if self.num_experts:
                # If we are using MoE, we multiply by num_experts, and add the expert gate parameters (d_model * num_experts)
                mlp_params_per_layer = (mlp_params_per_layer + self.d_model) * self.num_experts
            self.n_params += self.n_layers * mlp_params_per_layer

        if self.device is None:
            self.device = utils.get_device()

        if self.n_devices > 1:
            assert (
                torch.cuda.device_count() >= self.n_devices
            ), f"Not enough CUDA devices to support n_devices {self.n_devices}"

        if self.use_attn_scale and self.attn_scale == -1.0:
            self.attn_scale = np.sqrt(self.d_head)

        assert self.default_prepend_bos in [
            True,
            False,
        ], f"padding_side must be either True or False, but {self.default_prepend_bos} is given"

    @classmethod
    def unwrap(cls, config: Union[Dict, "HookedTransformerConfig"]) -> HookedTransformerConfig:
        """
        Convenience function to avoid duplicate code from a common way config is passed to various components
        """
        return HookedTransformerConfig.from_dict(config) if isinstance(config, Dict) else config

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> HookedTransformerConfig:
        """
        Instantiates a `HookedTransformerConfig` from a Python dictionary of
        parameters.
        """
        return cls(**config_dict)

    def to_dict(self):
        return self.__dict__

    def __repr__(self):
        return "HookedTransformerConfig:\n" + pprint.pformat(self.to_dict())

    def set_seed_everywhere(self, seed: int):
        torch.manual_seed(seed)
        random.seed(seed)
        np.random.seed(seed)

    def is_layer_norm_activation(self) -> bool:
        return self.act_fn is not None and self.act_fn.endswith("_ln")



---
File: /transformer_lens/loading_from_pretrained.py
---

from __future__ import annotations

"""Loading Pretrained Models Utilities.

This module contains functions for loading pretrained models from the Hugging Face Hub.
"""

import dataclasses
import logging
import os
import re
from pathlib import Path
from typing import Any, Optional, Union

import torch
from huggingface_hub import HfApi
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    BertForPreTraining,
    T5ForConditionalGeneration,
)

import transformer_lens.utils as utils
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.pretrained.weight_conversions import (
    convert_bert_weights,
    convert_bloom_weights,
    convert_coder_weights,
    convert_gemma_weights,
    convert_gpt2_weights,
    convert_gptj_weights,
    convert_llama_weights,
    convert_mingpt_weights,
    convert_mistral_weights,
    convert_mixtral_weights,
    convert_neel_solu_old_weights,
    convert_neo_weights,
    convert_neox_weights,
    convert_opt_weights,
    convert_phi3_weights,
    convert_phi_weights,
    convert_qwen2_weights,
    convert_qwen3_weights,
    convert_qwen_weights,
    convert_t5_weights,
)

OFFICIAL_MODEL_NAMES = [
    "gpt2",
    "gpt2-medium",
    "gpt2-large",
    "gpt2-xl",
    "distilgpt2",
    "facebook/opt-125m",
    "facebook/opt-1.3b",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "facebook/opt-13b",
    "facebook/opt-30b",
    "facebook/opt-66b",
    "EleutherAI/gpt-neo-125M",
    "EleutherAI/gpt-neo-1.3B",
    "EleutherAI/gpt-neo-2.7B",
    "EleutherAI/gpt-j-6B",
    "EleutherAI/gpt-neox-20b",
    "stanford-crfm/alias-gpt2-small-x21",
    "stanford-crfm/battlestar-gpt2-small-x49",
    "stanford-crfm/caprica-gpt2-small-x81",
    "stanford-crfm/darkmatter-gpt2-small-x343",
    "stanford-crfm/expanse-gpt2-small-x777",
    "stanford-crfm/arwen-gpt2-medium-x21",
    "stanford-crfm/beren-gpt2-medium-x49",
    "stanford-crfm/celebrimbor-gpt2-medium-x81",
    "stanford-crfm/durin-gpt2-medium-x343",
    "stanford-crfm/eowyn-gpt2-medium-x777",
    "EleutherAI/pythia-14m",
    "EleutherAI/pythia-31m",
    "EleutherAI/pythia-70m",
    "EleutherAI/pythia-160m",
    "EleutherAI/pythia-410m",
    "EleutherAI/pythia-1b",
    "EleutherAI/pythia-1.4b",
    "EleutherAI/pythia-2.8b",
    "EleutherAI/pythia-6.9b",
    "EleutherAI/pythia-12b",
    "EleutherAI/pythia-70m-deduped",
    "EleutherAI/pythia-160m-deduped",
    "EleutherAI/pythia-410m-deduped",
    "EleutherAI/pythia-1b-deduped",
    "EleutherAI/pythia-1.4b-deduped",
    "EleutherAI/pythia-2.8b-deduped",
    "EleutherAI/pythia-6.9b-deduped",
    "EleutherAI/pythia-12b-deduped",
    "EleutherAI/pythia-70m-v0",
    "EleutherAI/pythia-160m-v0",
    "EleutherAI/pythia-410m-v0",
    "EleutherAI/pythia-1b-v0",
    "EleutherAI/pythia-1.4b-v0",
    "EleutherAI/pythia-2.8b-v0",
    "EleutherAI/pythia-6.9b-v0",
    "EleutherAI/pythia-12b-v0",
    "EleutherAI/pythia-70m-deduped-v0",
    "EleutherAI/pythia-160m-deduped-v0",
    "EleutherAI/pythia-410m-deduped-v0",
    "EleutherAI/pythia-1b-deduped-v0",
    "EleutherAI/pythia-1.4b-deduped-v0",
    "EleutherAI/pythia-2.8b-deduped-v0",
    "EleutherAI/pythia-6.9b-deduped-v0",
    "EleutherAI/pythia-12b-deduped-v0",
    "EleutherAI/pythia-160m-seed1",
    "EleutherAI/pythia-160m-seed2",
    "EleutherAI/pythia-160m-seed3",
    "NeelNanda/SoLU_1L_v9_old",
    "NeelNanda/SoLU_2L_v10_old",
    "NeelNanda/SoLU_4L_v11_old",
    "NeelNanda/SoLU_6L_v13_old",
    "NeelNanda/SoLU_8L_v21_old",
    "NeelNanda/SoLU_10L_v22_old",
    "NeelNanda/SoLU_12L_v23_old",
    "NeelNanda/SoLU_1L512W_C4_Code",
    "NeelNanda/SoLU_2L512W_C4_Code",
    "NeelNanda/SoLU_3L512W_C4_Code",
    "NeelNanda/SoLU_4L512W_C4_Code",
    "NeelNanda/SoLU_6L768W_C4_Code",
    "NeelNanda/SoLU_8L1024W_C4_Code",
    "NeelNanda/SoLU_10L1280W_C4_Code",
    "NeelNanda/SoLU_12L1536W_C4_Code",
    "NeelNanda/GELU_1L512W_C4_Code",
    "NeelNanda/GELU_2L512W_C4_Code",
    "NeelNanda/GELU_3L512W_C4_Code",
    "NeelNanda/GELU_4L512W_C4_Code",
    "NeelNanda/Attn_Only_1L512W_C4_Code",
    "NeelNanda/Attn_Only_2L512W_C4_Code",
    "NeelNanda/Attn_Only_3L512W_C4_Code",
    "NeelNanda/Attn_Only_4L512W_C4_Code",
    "NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr",
    "NeelNanda/SoLU_1L512W_Wiki_Finetune",
    "NeelNanda/SoLU_4L512W_Wiki_Finetune",
    "ArthurConmy/redwood_attn_2l",
    "llama-7b-hf",
    "llama-13b-hf",
    "llama-30b-hf",
    "llama-65b-hf",
    "meta-llama/Llama-2-7b-hf",
    "meta-llama/Llama-2-7b-chat-hf",
    "meta-llama/Llama-2-13b-hf",
    "meta-llama/Llama-2-13b-chat-hf",
    "meta-llama/Llama-2-70b-chat-hf",
    "codellama/CodeLlama-7b-hf",
    "codellama/CodeLlama-7b-Python-hf",
    "codellama/CodeLlama-7b-Instruct-hf",
    "meta-llama/Meta-Llama-3-8B",
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "meta-llama/Meta-Llama-3-70B",
    "meta-llama/Meta-Llama-3-70B-Instruct",
    "meta-llama/Llama-3.1-70B",
    "meta-llama/Llama-3.1-8B",
    "meta-llama/Llama-3.1-8B-Instruct",
    "meta-llama/Llama-3.1-70B-Instruct",
    "meta-llama/Llama-3.2-1B",
    "meta-llama/Llama-3.2-3B",
    "meta-llama/Llama-3.2-1B-Instruct",
    "meta-llama/Llama-3.2-3B-Instruct",
    "meta-llama/Llama-3.3-70B-Instruct",
    "Baidicoot/Othello-GPT-Transformer-Lens",
    "google-bert/bert-base-cased",
    "google-bert/bert-base-uncased",
    "google-bert/bert-large-cased",
    "google-bert/bert-large-uncased",
    "roneneldan/TinyStories-1M",
    "roneneldan/TinyStories-3M",
    "roneneldan/TinyStories-8M",
    "roneneldan/TinyStories-28M",
    "roneneldan/TinyStories-33M",
    "roneneldan/TinyStories-Instruct-1M",
    "roneneldan/TinyStories-Instruct-3M",
    "roneneldan/TinyStories-Instruct-8M",
    "roneneldan/TinyStories-Instruct-28M",
    "roneneldan/TinyStories-Instruct-33M",
    "roneneldan/TinyStories-1Layer-21M",
    "roneneldan/TinyStories-2Layers-33M",
    "roneneldan/TinyStories-Instuct-1Layer-21M",
    "roneneldan/TinyStories-Instruct-2Layers-33M",
    "stabilityai/stablelm-base-alpha-3b",
    "stabilityai/stablelm-base-alpha-7b",
    "stabilityai/stablelm-tuned-alpha-3b",
    "stabilityai/stablelm-tuned-alpha-7b",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mistral-7B-Instruct-v0.1",
    "mistralai/Mistral-Small-24B-Base-2501",
    "mistralai/Mistral-Nemo-Base-2407",
    "mistralai/Mixtral-8x7B-v0.1",
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "bigscience/bloom-560m",
    "bigscience/bloom-1b1",
    "bigscience/bloom-1b7",
    "bigscience/bloom-3b",
    "bigscience/bloom-7b1",
    "bigcode/santacoder",
    "Qwen/Qwen-1_8B",
    "Qwen/Qwen-7B",
    "Qwen/Qwen-14B",
    "Qwen/Qwen-1_8B-Chat",
    "Qwen/Qwen-7B-Chat",
    "Qwen/Qwen-14B-Chat",
    "Qwen/Qwen1.5-0.5B",
    "Qwen/Qwen1.5-0.5B-Chat",
    "Qwen/Qwen1.5-1.8B",
    "Qwen/Qwen1.5-1.8B-Chat",
    "Qwen/Qwen1.5-4B",
    "Qwen/Qwen1.5-4B-Chat",
    "Qwen/Qwen1.5-7B",
    "Qwen/Qwen1.5-7B-Chat",
    "Qwen/Qwen1.5-14B",
    "Qwen/Qwen1.5-14B-Chat",
    "Qwen/Qwen2-0.5B",
    "Qwen/Qwen2-0.5B-Instruct",
    "Qwen/Qwen2-1.5B",
    "Qwen/Qwen2-1.5B-Instruct",
    "Qwen/Qwen2-7B",
    "Qwen/Qwen2-7B-Instruct",
    "Qwen/Qwen2.5-0.5B",
    "Qwen/Qwen2.5-0.5B-Instruct",
    "Qwen/Qwen2.5-1.5B",
    "Qwen/Qwen2.5-1.5B-Instruct",
    "Qwen/Qwen2.5-3B",
    "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-7B",
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen2.5-14B",
    "Qwen/Qwen2.5-14B-Instruct",
    "Qwen/Qwen2.5-32B",
    "Qwen/Qwen2.5-32B-Instruct",
    "Qwen/Qwen2.5-72B",
    "Qwen/Qwen2.5-72B-Instruct",
    "Qwen/QwQ-32B-Preview",
    "Qwen/Qwen3-0.6B",
    "Qwen/Qwen3-1.7B",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-8B",
    "Qwen/Qwen3-14B",
    "microsoft/phi-1",
    "microsoft/phi-1_5",
    "microsoft/phi-2",
    "microsoft/Phi-3-mini-4k-instruct",
    "microsoft/phi-4",
    "google/gemma-2b",
    "google/gemma-7b",
    "google/gemma-2b-it",
    "google/gemma-7b-it",
    "google/gemma-2-2b",
    "google/gemma-2-2b-it",
    "google/gemma-2-9b",
    "google/gemma-2-9b-it",
    "google/gemma-2-27b",
    "google/gemma-2-27b-it",
    "01-ai/Yi-6B",
    "01-ai/Yi-34B",
    "01-ai/Yi-6B-Chat",
    "01-ai/Yi-34B-Chat",
    "google-t5/t5-small",
    "google-t5/t5-base",
    "google-t5/t5-large",
    "ai-forever/mGPT",
]
"""Official model names for models on HuggingFace."""

# Model Aliases:
MODEL_ALIASES = {
    "NeelNanda/SoLU_1L_v9_old": ["solu-1l-pile", "solu-1l-old"],
    "NeelNanda/SoLU_2L_v10_old": ["solu-2l-pile", "solu-2l-old"],
    "NeelNanda/SoLU_4L_v11_old": ["solu-4l-pile", "solu-4l-old"],
    "NeelNanda/SoLU_6L_v13_old": ["solu-6l-pile", "solu-6l-old"],
    "NeelNanda/SoLU_8L_v21_old": ["solu-8l-pile", "solu-8l-old"],
    "NeelNanda/SoLU_10L_v22_old": ["solu-10l-pile", "solu-10l-old"],
    "NeelNanda/SoLU_12L_v23_old": ["solu-12l-pile", "solu-12l-old"],
    "NeelNanda/SoLU_1L512W_C4_Code": ["solu-1l", "solu-1l-new", "solu-1l-c4-code"],
    "NeelNanda/SoLU_2L512W_C4_Code": ["solu-2l", "solu-2l-new", "solu-2l-c4-code"],
    "NeelNanda/SoLU_3L512W_C4_Code": ["solu-3l", "solu-3l-new", "solu-3l-c4-code"],
    "NeelNanda/SoLU_4L512W_C4_Code": ["solu-4l", "solu-4l-new", "solu-4l-c4-code"],
    "NeelNanda/GELU_1L512W_C4_Code": ["gelu-1l", "gelu-1l-new", "gelu-1l-c4-code"],
    "NeelNanda/GELU_2L512W_C4_Code": ["gelu-2l", "gelu-2l-new", "gelu-2l-c4-code"],
    "NeelNanda/GELU_3L512W_C4_Code": ["gelu-3l", "gelu-3l-new", "gelu-3l-c4-code"],
    "NeelNanda/GELU_4L512W_C4_Code": ["gelu-4l", "gelu-4l-new", "gelu-4l-c4-code"],
    "NeelNanda/Attn_Only_1L512W_C4_Code": [
        "attn-only-1l",
        "attn-only-1l-new",
        "attn-only-1l-c4-code",
    ],
    "NeelNanda/Attn_Only_2L512W_C4_Code": [
        "attn-only-2l",
        "attn-only-2l-new",
        "attn-only-2l-c4-code",
    ],
    "NeelNanda/Attn_Only_3L512W_C4_Code": [
        "attn-only-3l",
        "attn-only-3l-new",
        "attn-only-3l-c4-code",
    ],
    "NeelNanda/Attn_Only_4L512W_C4_Code": [
        "attn-only-4l",
        "attn-only-4l-new",
        "attn-only-4l-c4-code",
    ],
    "NeelNanda/SoLU_6L768W_C4_Code": ["solu-6l", "solu-6l-new", "solu-6l-c4-code"],
    "NeelNanda/SoLU_8L1024W_C4_Code": ["solu-8l", "solu-8l-new", "solu-8l-c4-code"],
    "NeelNanda/SoLU_10L1280W_C4_Code": ["solu-10l", "solu-10l-new", "solu-10l-c4-code"],
    "NeelNanda/SoLU_12L1536W_C4_Code": ["solu-12l", "solu-12l-new", "solu-12l-c4-code"],
    "NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr": [
        "attn-only-2l-demo",
        "attn-only-2l-shortformer-6b-big-lr",
        "attn-only-2l-induction-demo",
        "attn-only-demo",
    ],
    "NeelNanda/SoLU_1L512W_Wiki_Finetune": [
        "solu-1l-wiki",
        "solu-1l-wiki-finetune",
        "solu-1l-finetune",
    ],
    "NeelNanda/SoLU_4L512W_Wiki_Finetune": [
        "solu-4l-wiki",
        "solu-4l-wiki-finetune",
        "solu-4l-finetune",
    ],
    "EleutherAI/pythia-14m": [
        "pythia-14m",
    ],
    "EleutherAI/pythia-31m": [
        "pythia-31m",
    ],
    "EleutherAI/pythia-70m": [
        "pythia-70m",
        "pythia",
        "EleutherAI/pythia-19m",
        "pythia-19m",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-160m": [
        "pythia-160m",
        "EleutherAI/pythia-125m",
        "pythia-125m",  # EleutherAI renamed this model"
    ],
    "EleutherAI/pythia-410m": [
        "pythia-410m",
        "EleutherAI/pythia-350m",
        "pythia-350m",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-1b": [
        "pythia-1b",
        "EleutherAI/pythia-800m",
        "pythia-800m",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-1.4b": [
        "pythia-1.4b",
        "EleutherAI/pythia-1.3b",
        "pythia-1.3b",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-2.8b": [
        "pythia-2.8b",
        "EleutherAI/pythia-2.7b",
        "pythia-2.7b",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-6.9b": [
        "pythia-6.9b",
        "EleutherAI/pythia-6.7b",
        "pythia-6.7b",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-12b": [
        "pythia-12b",
        "EleutherAI/pythia-13b",
        "pythia-13b",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-70m-deduped": [
        "pythia-70m-deduped",
        "EleutherAI/pythia-19m-deduped",  # EleutherAI renamed this model
        "pythia-19m-deduped",
    ],
    "EleutherAI/pythia-160m-deduped": [
        "pythia-160m-deduped",
        "EleutherAI/pythia-125m-deduped",  # EleutherAI renamed this model
        "pythia-125m-deduped",
    ],
    "EleutherAI/pythia-410m-deduped": [
        "pythia-410m-deduped",
        "EleutherAI/pythia-350m-deduped",  # EleutherAI renamed this model
        "pythia-350m-deduped",
    ],
    "EleutherAI/pythia-1b-deduped": [
        "pythia-1b-deduped",
        "EleutherAI/pythia-800m-deduped",  # EleutherAI renamed this model
        "pythia-800m-deduped",
    ],
    "EleutherAI/pythia-1.4b-deduped": [
        "pythia-1.4b-deduped",
        "EleutherAI/pythia-1.3b-deduped",  # EleutherAI renamed this model
        "pythia-1.3b-deduped",
    ],
    "EleutherAI/pythia-2.8b-deduped": [
        "pythia-2.8b-deduped",
        "EleutherAI/pythia-2.7b-deduped",  # EleutherAI renamed this model
        "pythia-2.7b-deduped",
    ],
    "EleutherAI/pythia-6.9b-deduped": [
        "pythia-6.9b-deduped",
        "EleutherAI/pythia-6.7b-deduped",  # EleutherAI renamed this model
        "pythia-6.7b-deduped",
    ],
    "EleutherAI/pythia-12b-deduped": [
        "pythia-12b-deduped",
        "EleutherAI/pythia-13b-deduped",  # EleutherAI renamed this model
        "pythia-13b-deduped",
    ],
    "EleutherAI/pythia-70m-v0": [
        "pythia-70m-v0",
        "pythia-v0",
        "EleutherAI/pythia-19m-v0",
        "pythia-19m-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-160m-v0": [
        "pythia-160m-v0",
        "EleutherAI/pythia-125m-v0",
        "pythia-125m-v0",  # EleutherAI renamed this model"
    ],
    "EleutherAI/pythia-410m-v0": [
        "pythia-410m-v0",
        "EleutherAI/pythia-350m-v0",
        "pythia-350m-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-1b-v0": [
        "pythia-1b-v0",
        "EleutherAI/pythia-800m-v0",
        "pythia-800m-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-1.4b-v0": [
        "pythia-1.4b-v0",
        "EleutherAI/pythia-1.3b-v0",
        "pythia-1.3b-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-2.8b-v0": [
        "pythia-2.8b-v0",
        "EleutherAI/pythia-2.7b-v0",
        "pythia-2.7b-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-6.9b-v0": [
        "pythia-6.9b-v0",
        "EleutherAI/pythia-6.7b-v0",
        "pythia-6.7b-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-12b-v0": [
        "pythia-12b-v0",
        "EleutherAI/pythia-13b-v0",
        "pythia-13b-v0",  # EleutherAI renamed this model
    ],
    "EleutherAI/pythia-70m-deduped-v0": [
        "pythia-70m-deduped-v0",
        "EleutherAI/pythia-19m-deduped-v0",  # EleutherAI renamed this model
        "pythia-19m-deduped-v0",
    ],
    "EleutherAI/pythia-160m-deduped-v0": [
        "pythia-160m-deduped-v0",
        "EleutherAI/pythia-125m-deduped-v0",  # EleutherAI renamed this model
        "pythia-125m-deduped-v0",
    ],
    "EleutherAI/pythia-410m-deduped-v0": [
        "pythia-410m-deduped-v0",
        "EleutherAI/pythia-350m-deduped-v0",  # EleutherAI renamed this model
        "pythia-350m-deduped-v0",
    ],
    "EleutherAI/pythia-1b-deduped-v0": [
        "pythia-1b-deduped-v0",
        "EleutherAI/pythia-800m-deduped-v0",  # EleutherAI renamed this model
        "pythia-800m-deduped-v0",
    ],
    "EleutherAI/pythia-1.4b-deduped-v0": [
        "pythia-1.4b-deduped-v0",
        "EleutherAI/pythia-1.3b-deduped-v0",  # EleutherAI renamed this model
        "pythia-1.3b-deduped-v0",
    ],
    "EleutherAI/pythia-2.8b-deduped-v0": [
        "pythia-2.8b-deduped-v0",
        "EleutherAI/pythia-2.7b-deduped-v0",  # EleutherAI renamed this model
        "pythia-2.7b-deduped-v0",
    ],
    "EleutherAI/pythia-6.9b-deduped-v0": [
        "pythia-6.9b-deduped-v0",
        "EleutherAI/pythia-6.7b-deduped-v0",  # EleutherAI renamed this model
        "pythia-6.7b-deduped-v0",
    ],
    "EleutherAI/pythia-12b-deduped-v0": [
        "pythia-12b-deduped-v0",
        "EleutherAI/pythia-13b-deduped-v0",  # EleutherAI renamed this model
        "pythia-13b-deduped-v0",
    ],
    "EleutherAI/pythia-160m-seed1": [
        "pythia-160m-seed1",
        "EleutherAI/pythia-125m-seed1",
        "pythia-125m-seed1",  # EleutherAI renamed this model"
    ],
    "EleutherAI/pythia-160m-seed2": [
        "pythia-160m-seed2",
        "EleutherAI/pythia-125m-seed2",
        "pythia-125m-seed2",  # EleutherAI renamed this model"
    ],
    "EleutherAI/pythia-160m-seed3": [
        "pythia-160m-seed3",
        "EleutherAI/pythia-125m-seed3",
        "pythia-125m-seed3",  # EleutherAI renamed this model"
    ],
    "gpt2": ["gpt2-small"],
    "distilgpt2": ["distillgpt2", "distill-gpt2", "distil-gpt2", "gpt2-xs"],
    "facebook/opt-125m": ["opt-125m", "opt-small", "opt"],
    "facebook/opt-1.3b": ["opt-1.3b", "opt-medium"],
    "facebook/opt-2.7b": ["opt-2.7b", "opt-large"],
    "facebook/opt-6.7b": ["opt-6.7b", "opt-xl"],
    "facebook/opt-13b": ["opt-13b", "opt-xxl"],
    "facebook/opt-30b": ["opt-30b", "opt-xxxl"],
    "facebook/opt-66b": ["opt-66b", "opt-xxxxl"],
    "EleutherAI/gpt-neo-125M": ["gpt-neo-125M", "gpt-neo-small", "neo-small", "neo"],
    "EleutherAI/gpt-neo-1.3B": ["gpt-neo-1.3B", "gpt-neo-medium", "neo-medium"],
    "EleutherAI/gpt-neo-2.7B": ["gpt-neo-2.7B", "gpt-neo-large", "neo-large"],
    "EleutherAI/gpt-j-6B": ["gpt-j-6B", "gpt-j", "gptj"],
    "EleutherAI/gpt-neox-20b": ["gpt-neox-20b", "gpt-neox", "neox"],
    "stanford-crfm/alias-gpt2-small-x21": [
        "stanford-gpt2-small-a",
        "alias-gpt2-small-x21",
        "gpt2-mistral-small-a",
        "gpt2-stanford-small-a",
    ],
    "stanford-crfm/battlestar-gpt2-small-x49": [
        "stanford-gpt2-small-b",
        "battlestar-gpt2-small-x49",
        "gpt2-mistral-small-b",
        "gpt2-mistral-small-b",
    ],
    "stanford-crfm/caprica-gpt2-small-x81": [
        "stanford-gpt2-small-c",
        "caprica-gpt2-small-x81",
        "gpt2-mistral-small-c",
        "gpt2-stanford-small-c",
    ],
    "stanford-crfm/darkmatter-gpt2-small-x343": [
        "stanford-gpt2-small-d",
        "darkmatter-gpt2-small-x343",
        "gpt2-mistral-small-d",
        "gpt2-mistral-small-d",
    ],
    "stanford-crfm/expanse-gpt2-small-x777": [
        "stanford-gpt2-small-e",
        "expanse-gpt2-small-x777",
        "gpt2-mistral-small-e",
        "gpt2-mistral-small-e",
    ],
    "stanford-crfm/arwen-gpt2-medium-x21": [
        "stanford-gpt2-medium-a",
        "arwen-gpt2-medium-x21",
        "gpt2-medium-small-a",
        "gpt2-stanford-medium-a",
    ],
    "stanford-crfm/beren-gpt2-medium-x49": [
        "stanford-gpt2-medium-b",
        "beren-gpt2-medium-x49",
        "gpt2-medium-small-b",
        "gpt2-stanford-medium-b",
    ],
    "stanford-crfm/celebrimbor-gpt2-medium-x81": [
        "stanford-gpt2-medium-c",
        "celebrimbor-gpt2-medium-x81",
        "gpt2-medium-small-c",
        "gpt2-medium-small-c",
    ],
    "stanford-crfm/durin-gpt2-medium-x343": [
        "stanford-gpt2-medium-d",
        "durin-gpt2-medium-x343",
        "gpt2-medium-small-d",
        "gpt2-stanford-medium-d",
    ],
    "stanford-crfm/eowyn-gpt2-medium-x777": [
        "stanford-gpt2-medium-e",
        "eowyn-gpt2-medium-x777",
        "gpt2-medium-small-e",
        "gpt2-stanford-medium-e",
    ],
    "ArthurConmy/redwood_attn_2l": ["redwood_attn_2l"],
    "llama-7b-hf": ["llama-7b"],
    "llama-13b-hf": ["llama-13b"],
    "llama-30b-hf": ["llama-30b"],
    "llama-65b-hf": ["llama-65b"],
    "meta-llama/Llama-2-7b-hf": ["Llama-2-7b", "meta-llama/Llama-2-7b-hf"],
    "meta-llama/Llama-2-7b-chat-hf": [
        "Llama-2-7b-chat",
        "meta-llama/Llama-2-7b-chat-hf",
    ],
    "meta-llama/Llama-2-13b-hf": ["Llama-2-13b", "meta-llama/Llama-2-13b-hf"],
    "meta-llama/Llama-2-13b-chat-hf": [
        "Llama-2-13b-chat",
        "meta-llama/Llama-2-13b-chat-hf",
    ],
    "meta-llama/Llama-2-70b-chat-hf": ["Llama-2-70b-chat", "meta-llama-2-70b-chat-hf"],
    "codellama/CodeLlama-7b-hf": ["CodeLlamallama-2-7b", "codellama/CodeLlama-7b-hf"],
    "codellama/CodeLlama-7b-Python-hf": [
        "CodeLlama-7b-python",
        "codellama/CodeLlama-7b-Python-hf",
    ],
    "codellama/CodeLlama-7b-Instruct-hf": [
        "CodeLlama-7b-instruct",
        "codellama/CodeLlama-7b-Instruct-hf",
    ],
    "Baidicoot/Othello-GPT-Transformer-Lens": ["othello-gpt"],
    "google-bert/bert-base-cased": ["bert-base-cased"],
    "google-bert/bert-base-uncased": ["bert-base-uncased"],
    "google-bert/bert-large-cased": ["bert-large-cased"],
    "google-bert/bert-large-uncased": ["bert-large-uncased"],
    "roneneldan/TinyStories-1M": ["tiny-stories-1M"],
    "roneneldan/TinyStories-3M": ["tiny-stories-3M"],
    "roneneldan/TinyStories-8M": ["tiny-stories-8M"],
    "roneneldan/TinyStories-28M": ["tiny-stories-28M"],
    "roneneldan/TinyStories-33M": ["tiny-stories-33M"],
    "roneneldan/TinyStories-Instruct-1M": ["tiny-stories-instruct-1M"],
    "roneneldan/TinyStories-Instruct-3M": ["tiny-stories-instruct-3M"],
    "roneneldan/TinyStories-Instruct-8M": ["tiny-stories-instruct-8M"],
    "roneneldan/TinyStories-Instruct-28M": ["tiny-stories-instruct-28M"],
    "roneneldan/TinyStories-Instruct-33M": ["tiny-stories-instruct-33M"],
    "roneneldan/TinyStories-1Layer-21M": ["tiny-stories-1L-21M"],
    "roneneldan/TinyStories-2Layers-33M": ["tiny-stories-2L-33M"],
    "roneneldan/TinyStories-Instuct-1Layer-21M": ["tiny-stories-instruct-1L-21M"],
    "roneneldan/TinyStories-Instruct-2Layers-33M": ["tiny-stories-instruct-2L-33M"],
    "stabilityai/stablelm-base-alpha-3b": [
        "stablelm-base-alpha-3b",
        "stablelm-base-3b",
    ],
    "stabilityai/stablelm-base-alpha-7b": [
        "stablelm-base-alpha-7b",
        "stablelm-base-7b",
    ],
    "stabilityai/stablelm-tuned-alpha-3b": [
        "stablelm-tuned-alpha-3b",
        "stablelm-tuned-3b",
    ],
    "stabilityai/stablelm-tuned-alpha-7b": [
        "stablelm-tuned-alpha-7b",
        "stablelm-tuned-7b",
    ],
    "mistralai/Mistral-7B-v0.1": ["mistral-7b"],
    "mistralai/Mistral-7B-Instruct-v0.1": ["mistral-7b-instruct"],
    "mistralai/Mistral-Nemo-Base-2407": ["mistral-nemo-base-2407"],
    "mistralai/Mixtral-8x7B-v0.1": ["mixtral", "mixtral-8x7b"],
    "mistralai/Mixtral-8x7B-Instruct-v0.1": [
        "mixtral-instruct",
        "mixtral-8x7b-instruct",
    ],
    "bigscience/bloom-560m": ["bloom-560m"],
    "bigscience/bloom-1b1": ["bloom-1b1"],
    "bigscience/bloom-1b7": ["bloom-1b7"],
    "bigscience/bloom-3b": ["bloom-3b"],
    "bigscience/bloom-7b1": ["bloom-7b1"],
    "bigcode/santacoder": ["santacoder"],
    "Qwen/Qwen-1_8B": ["qwen-1.8b"],
    "Qwen/Qwen-7B": ["qwen-7b"],
    "Qwen/Qwen-14B": ["qwen-14b"],
    "Qwen/Qwen-1_8B-Chat": ["qwen-1.8b-chat"],
    "Qwen/Qwen-7B-Chat": ["qwen-7b-chat"],
    "Qwen/Qwen-14B-Chat": ["qwen-14b-chat"],
    "Qwen/Qwen1.5-0.5B": ["qwen1.5-0.5b"],
    "Qwen/Qwen1.5-0.5B-Chat": ["qwen1.5-0.5b-chat"],
    "Qwen/Qwen1.5-1.8B": ["qwen1.5-1.8b"],
    "Qwen/Qwen1.5-1.8B-Chat": ["qwen1.5-1.8b-chat"],
    "Qwen/Qwen1.5-4B": ["qwen1.5-4b"],
    "Qwen/Qwen1.5-4B-Chat": ["qwen1.5-4b-chat"],
    "Qwen/Qwen1.5-7B": ["qwen1.5-7b"],
    "Qwen/Qwen1.5-7B-Chat": ["qwen1.5-7b-chat"],
    "Qwen/Qwen1.5-14B": ["qwen1.5-14b"],
    "Qwen/Qwen1.5-14B-Chat": ["qwen1.5-14b-chat"],
    "Qwen/Qwen2-0.5B": ["qwen2-0.5b"],
    "Qwen/Qwen2-0.5B-Instruct": ["qwen2-0.5b-instruct"],
    "Qwen/Qwen2-1.5B": ["qwen2-1.5b"],
    "Qwen/Qwen2-1.5B-Instruct": ["qwen2-1.5b-instruct"],
    "Qwen/Qwen2-7B": ["qwen2-7b"],
    "Qwen/Qwen2-7B-Instruct": ["qwen2-7b-instruct"],
    "Qwen/Qwen2.5-0.5B": ["qwen2.5-0.5b"],
    "Qwen/Qwen2.5-0.5B-Instruct": ["qwen2.5-0.5b-instruct"],
    "Qwen/Qwen2.5-1.5B": ["qwen2.5-1.5b"],
    "Qwen/Qwen2.5-1.5B-Instruct": ["qwen2.5-1.5b-instruct"],
    "Qwen/Qwen2.5-3B": ["qwen2.5-3b"],
    "Qwen/Qwen2.5-3B-Instruct": ["qwen2.5-3b-instruct"],
    "Qwen/Qwen2.5-7B": ["qwen2.5-7b"],
    "Qwen/Qwen2.5-7B-Instruct": ["qwen2.5-7b-instruct"],
    "Qwen/Qwen2.5-14B": ["qwen2.5-14b"],
    "Qwen/Qwen2.5-14B-Instruct": ["qwen2.5-14b-instruct"],
    "Qwen/Qwen2.5-32B": ["qwen2.5-32b"],
    "Qwen/Qwen2.5-32B-Instruct": ["qwen2.5-32b-instruct"],
    "Qwen/Qwen2.5-72B": ["qwen2.5-72b"],
    "Qwen/Qwen2.5-72B-Instruct": ["qwen2.5-72b-instruct"],
    "Qwen/QwQ-32B-Preview": ["qwen-32b-preview"],
    "Qwen/Qwen3-0.6B": ["qwen3-0.6b"],
    "Qwen/Qwen3-1.7B": ["qwen3-1.7b"],
    "Qwen/Qwen3-4B": ["qwen3-4b"],
    "Qwen/Qwen3-8B": ["qwen3-8b"],
    "Qwen/Qwen3-14B": ["qwen3-14b"],
    "microsoft/phi-1": ["phi-1"],
    "microsoft/phi-1_5": ["phi-1_5"],
    "microsoft/phi-2": ["phi-2"],
    "microsoft/Phi-3-mini-4k-instruct": ["phi-3"],
    "microsoft/phi-4": ["phi-4"],
    "google/gemma-2b": ["gemma-2b"],
    "google/gemma-7b": ["gemma-7b"],
    "google/gemma-2b-it": ["gemma-2b-it"],
    "google/gemma-7b-it": ["gemma-7b-it"],
    "google/gemma-2-2b": ["gemma-2-2b"],
    "google/gemma-2-2b-it": ["gemma-2-2b-it"],
    "google/gemma-2-9b": ["gemma-2-9b"],
    "google/gemma-2-9b-it": ["gemma-2-9b-it"],
    "google/gemma-2-27b": ["gemma-2-27b"],
    "google/gemma-2-27b-it": ["gemma-2-27b-it"],
    "01-ai/Yi-6B": ["yi-6b", "Yi-6B"],
    "01-ai/Yi-34B": ["yi-34b", "Yi-34B"],
    "01-ai/Yi-6B-Chat": ["yi-6b-chat", "Yi-6B-Chat"],
    "01-ai/Yi-34B-Chat": ["yi-34b-chat", "Yi-34B-Chat"],
    "google-t5/t5-small": ["t5-small"],
    "google-t5/t5-base": ["t5-base"],
    "google-t5/t5-large": ["t5-large"],
    "ai-forever/mGPT": ["mGPT"],
}
"""Model aliases for models on HuggingFace."""

NON_HF_HOSTED_MODEL_NAMES = [
    "llama-7b-hf",
    "llama-13b-hf",
    "llama-30b-hf",
    "llama-65b-hf",
]
"""Official model names for models not hosted on HuggingFace."""

# Sets a default model alias, by convention the first one in the model alias table, else the official name if it has no aliases
DEFAULT_MODEL_ALIASES = [
    MODEL_ALIASES[name][0] if name in MODEL_ALIASES else name for name in OFFICIAL_MODEL_NAMES
]

NEED_REMOTE_CODE_MODELS = (
    "bigcode/santacoder",
    "Qwen/Qwen-",
    "Qwen/Qwen3-",
    "microsoft/phi-2",
    "microsoft/Phi-3-mini-4k-instruct",
    "microsoft/phi-4",
)


def make_model_alias_map():
    """
    Converts OFFICIAL_MODEL_NAMES (the list of actual model names on
    HuggingFace) and MODEL_ALIASES (a dictionary mapping official model names to
    aliases) into a dictionary mapping all aliases to the official model name.
    """
    model_alias_map = {}
    for official_model_name in OFFICIAL_MODEL_NAMES:
        aliases = MODEL_ALIASES.get(official_model_name, [])
        for alias in aliases:
            model_alias_map[alias.lower()] = official_model_name
        model_alias_map[official_model_name.lower()] = official_model_name
    return model_alias_map


def get_official_model_name(model_name: str):
    """
    Returns the official model name for a given model name (or alias).
    """
    model_alias_map = make_model_alias_map()
    official_model_name = model_alias_map.get(model_name.lower(), None)
    if official_model_name is None:
        raise ValueError(
            f"{model_name} not found. Valid official model names (excl aliases): {OFFICIAL_MODEL_NAMES}"
        )
    return official_model_name


def convert_hf_model_config(model_name: str, **kwargs: Any):
    """
    Returns the model config for a HuggingFace model, converted to a dictionary
    in the HookedTransformerConfig format.

    Takes the official_model_name as an input.
    """
    # In case the user passed in an alias
    if (Path(model_name) / "config.json").exists():
        logging.info("Loading model config from local directory")
        official_model_name = model_name
    else:
        official_model_name = get_official_model_name(model_name)

    # Load HuggingFace model config
    if "llama" in official_model_name.lower():
        architecture = "LlamaForCausalLM"
    elif "gemma-2" in official_model_name.lower():
        architecture = "Gemma2ForCausalLM"
    elif "gemma" in official_model_name.lower():
        architecture = "GemmaForCausalLM"
    else:
        huggingface_token = os.environ.get("HF_TOKEN", "")
        hf_config = AutoConfig.from_pretrained(
            official_model_name,
            token=huggingface_token if len(huggingface_token) > 0 else None,
            **kwargs,
        )
        architecture = hf_config.architectures[0]

    cfg_dict: dict[str, Any]
    if official_model_name.startswith(
        ("llama-7b", "meta-llama/Llama-2-7b")
    ):  # same architecture for LLaMA and Llama-2
        cfg_dict = {
            "d_model": 4096,
            "d_head": 4096 // 32,
            "n_heads": 32,
            "d_mlp": 11008,
            "n_layers": 32,
            "n_ctx": 2048 if official_model_name.startswith("llama-7b") else 4096,
            "eps": 1e-6 if official_model_name.startswith("llama-7b") else 1e-5,
            "d_vocab": 32000,
            "act_fn": "silu",
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 4096 // 32,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif official_model_name.startswith("codellama"):  # same architecture CodeLlama and Llama-2
        cfg_dict = {
            "d_model": 4096,
            "d_head": 4096 // 32,
            "n_heads": 32,
            "d_mlp": 11008,
            "n_layers": 32,
            "n_ctx": 4096,
            "eps": 1e-5,
            "d_vocab": 32016,
            "act_fn": "silu",
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_dim": 4096 // 32,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 1000000,
        }
        if "python" in official_model_name.lower():
            # The vocab size of python version of CodeLlama-7b is 32000
            cfg_dict["d_vocab"] = 32000
    elif official_model_name.startswith(
        ("llama-13b", "meta-llama/Llama-2-13b")
    ):  # same architecture for LLaMA and Llama-2
        cfg_dict = {
            "d_model": 5120,
            "d_head": 5120 // 40,
            "n_heads": 40,
            "d_mlp": 13824,
            "n_layers": 40,
            "n_ctx": 2048 if official_model_name.startswith("llama-13b") else 4096,
            "eps": 1e-6 if official_model_name.startswith("llama-13b") else 1e-5,
            "d_vocab": 32000,
            "act_fn": "silu",
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 5120 // 40,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif "llama-30b" in official_model_name:
        cfg_dict = {
            "d_model": 6656,
            "d_head": 6656 // 52,
            "n_heads": 52,
            "d_mlp": 17920,
            "n_layers": 60,
            "n_ctx": 2048,
            "eps": 1e-6,
            "d_vocab": 32000,
            "act_fn": "silu",
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 6656 // 52,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif "llama-65b" in official_model_name:
        cfg_dict = {
            "d_model": 8192,
            "d_head": 8192 // 64,
            "n_heads": 64,
            "d_mlp": 22016,
            "n_layers": 80,
            "n_ctx": 2048,
            "eps": 1e-6,
            "d_vocab": 32000,
            "act_fn": "silu",
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_dim": 8192 // 64,
            "rotary_adjacent_pairs": False,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif "Llama-2-70b" in official_model_name:
        cfg_dict = {
            "d_model": 8192,
            "d_head": 128,
            "n_heads": 64,
            "d_mlp": 28672,
            "n_layers": 80,
            "n_ctx": 4096,
            "eps": 1e-5,
            "d_vocab": 32000,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif "Meta-Llama-3-8B" in official_model_name:
        cfg_dict = {
            "d_model": 4096,
            "d_head": 128,
            "n_heads": 32,
            "d_mlp": 14336,
            "n_layers": 32,
            "n_ctx": 8192,
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
        }
    elif "Meta-Llama-3-70B" in official_model_name:
        cfg_dict = {
            "d_model": 8192,
            "d_head": 128,
            "n_heads": 64,
            "d_mlp": 28672,
            "n_layers": 80,
            "n_ctx": 8192,
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
        }
    elif "Llama-3.2-1B" in official_model_name:
        cfg_dict = {
            "d_model": 2048,
            "d_head": 64,
            "n_heads": 32,
            "d_mlp": 8192,
            "n_layers": 16,
            "n_ctx": 2048,  # capped due to memory issues
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 64,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
            "use_NTK_by_parts_rope": True,
            "NTK_by_parts_low_freq_factor": 1.0,
            "NTK_by_parts_high_freq_factor": 4.0,
            "NTK_by_parts_factor": 32.0,
            "NTK_original_ctx_len": 8192,
        }
    elif "Llama-3.2-3B" in official_model_name:
        cfg_dict = {
            "d_model": 3072,
            "d_head": 128,
            "n_heads": 24,
            "d_mlp": 8192,
            "n_layers": 28,
            "n_ctx": 2048,  # capped due to memory issues
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
            "use_NTK_by_parts_rope": True,
            "NTK_by_parts_low_freq_factor": 1.0,
            "NTK_by_parts_high_freq_factor": 4.0,
            "NTK_by_parts_factor": 32.0,
            "NTK_original_ctx_len": 8192,
        }
    elif "Llama-3.3-70B" in official_model_name:
        cfg_dict = {
            "d_model": 8192,
            "d_head": 128,
            "n_heads": 64,
            "d_mlp": 28672,
            "n_layers": 80,
            "n_ctx": 2048,  # capped due to memory issues
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
            "use_NTK_by_parts_rope": True,
            "NTK_by_parts_low_freq_factor": 1.0,
            "NTK_by_parts_high_freq_factor": 4.0,
            "NTK_by_parts_factor": 8.0,
            "NTK_original_ctx_len": 8192,
        }
    elif "Llama-3.1-8B" in official_model_name:
        cfg_dict = {
            "d_model": 4096,
            "d_head": 128,
            "n_heads": 32,
            "d_mlp": 14336,
            "n_layers": 32,
            "n_ctx": 2048,  # capped due to memory issues
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
            "use_NTK_by_parts_rope": True,
            "NTK_by_parts_low_freq_factor": 1.0,
            "NTK_by_parts_high_freq_factor": 4.0,
            "NTK_by_parts_factor": 8.0,
            "NTK_original_ctx_len": 8192,
        }
    elif "Llama-3.1-70B" in official_model_name:
        cfg_dict = {
            "d_model": 8192,
            "d_head": 128,
            "n_heads": 64,
            "d_mlp": 28672,
            "n_layers": 80,
            "n_ctx": 2048,  # capped due to memory issues
            "eps": 1e-5,
            "d_vocab": 128256,
            "act_fn": "silu",
            "n_key_value_heads": 8,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": 128,
            "final_rms": True,
            "gated_mlp": True,
            "rotary_base": 500000.0,
            "use_NTK_by_parts_rope": True,
            "NTK_by_parts_low_freq_factor": 1.0,
            "NTK_by_parts_high_freq_factor": 4.0,
            "NTK_by_parts_factor": 8.0,
            "NTK_original_ctx_len": 8192,
        }
    elif architecture == "GPTNeoForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_heads,
            "n_heads": hf_config.num_heads,
            "d_mlp": hf_config.hidden_size * 4,
            "n_layers": hf_config.num_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.layer_norm_epsilon,
            "d_vocab": hf_config.vocab_size,
            "attn_types": hf_config.attention_layers,
            "act_fn": hf_config.activation_function,
            "use_attn_scale": False,
            "use_local_attn": True,
            "window_size": hf_config.window_size,
            "scale_attn_by_inverse_layer_idx": False,
            "normalization_type": "LN",
        }
    elif architecture == "GPT2LMHeadModel":
        cfg_dict = {
            "d_model": hf_config.n_embd,
            "d_head": hf_config.n_embd // hf_config.n_head,
            "n_heads": hf_config.n_head,
            "d_mlp": hf_config.n_embd * 4,
            "n_layers": hf_config.n_layer,
            "n_ctx": hf_config.n_ctx,
            "eps": hf_config.layer_norm_epsilon,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.activation_function,
            "use_attn_scale": True,
            "use_local_attn": False,
            "scale_attn_by_inverse_layer_idx": hf_config.scale_attn_by_inverse_layer_idx,
            "normalization_type": "LN",
        }
    elif architecture == "OPTForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.ffn_dim,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": 1e-5,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.activation_function,
            "use_attn_scale": True,
            "use_local_attn": False,
            "scale_attn_by_inverse_layer_idx": False,
            "normalization_type": "LN",
        }
    elif architecture == "GPTJForCausalLM":
        cfg_dict = {
            "d_model": hf_config.n_embd,
            "d_head": hf_config.n_embd // hf_config.n_head,
            "n_heads": hf_config.n_head,
            "d_mlp": 4 * hf_config.n_embd,
            "n_layers": hf_config.n_layer,
            "n_ctx": hf_config.n_positions,
            "eps": 1e-5,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.activation_function,
            "use_attn_scale": True,
            "use_local_attn": False,
            "scale_attn_by_inverse_layer_idx": False,
            "parallel_attn_mlp": True,
            "positional_embedding_type": "rotary",
            "rotary_dim": hf_config.rotary_dim,
            "rotary_adjacent_pairs": True,
            "normalization_type": "LN",
        }
    elif architecture == "GPTNeoXForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.layer_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "use_attn_scale": True,
            "use_local_attn": False,
            "scale_attn_by_inverse_layer_idx": False,
            "parallel_attn_mlp": True,
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "normalization_type": "LN",
        }
        rotary_pct = hf_config.rotary_pct
        cfg_dict["rotary_dim"] = round(rotary_pct * cfg_dict["d_head"])
    elif architecture == "BertForMaskedLM":
        # All supported Bert architectures have the same config,
        # so we can use the BertForMaskedLM config for all of them
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.layer_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": "gelu",
            "attention_dir": "bidirectional",
        }
    elif architecture == "MistralForCausalLM":
        use_local_attn = True if hf_config.sliding_window else False
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": (
                hf_config.head_dim
                if hasattr(hf_config, "head_dim")
                and hf_config.head_dim is not None
                and hf_config.head_dim > 0
                else hf_config.hidden_size // hf_config.num_attention_heads
            ),
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": 2048,  # Capped due to memory issues
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "window_size": hf_config.sliding_window,  # None if no sliding window was used
            "attn_types": ["local"] * hf_config.num_hidden_layers if use_local_attn else None,
            "eps": hf_config.rms_norm_eps,
            "rotary_base": hf_config.rope_theta,
            "n_key_value_heads": hf_config.num_key_value_heads,
            "use_local_attn": use_local_attn,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "gated_mlp": True,
        }
    elif architecture == "MixtralForCausalLM":
        cfg_dict = {
            "dtype": torch.bfloat16,
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,  # Capped due to memory issues
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_base": hf_config.rope_theta,
            "window_size": hf_config.sliding_window,  # This is None, as no sliding window was used
            "attn_types": ["global"] * 32,
            "eps": hf_config.rms_norm_eps,
            "n_key_value_heads": hf_config.num_key_value_heads,
            "gated_mlp": True,
            "use_local_attn": False,
            "rotary_dim": hf_config.hidden_size // hf_config.num_attention_heads,
            "num_experts": hf_config.num_local_experts,
            "experts_per_token": hf_config.num_experts_per_tok,
        }
    elif architecture == "BloomForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.n_head,
            "n_heads": hf_config.n_head,
            "d_mlp": hf_config.hidden_size * 4,
            "n_layers": hf_config.n_layer,
            "n_ctx": 2048,  # Capped due to HF Tokenizer Constraints
            "d_vocab": hf_config.vocab_size,
            "act_fn": "gelu_fast",
            "eps": hf_config.layer_norm_epsilon,
            "normalization_type": "LN",
            "post_embedding_ln": True,
            "positional_embedding_type": "alibi",
            "default_prepend_bos": False,
        }
    elif architecture == "GPT2LMHeadCustomModel":
        # santacoder
        cfg_dict = {
            "d_model": hf_config.n_embd,
            "d_head": hf_config.n_embd // hf_config.n_head,
            "n_heads": hf_config.n_head,
            "d_mlp": hf_config.n_embd * 4,
            "n_layers": hf_config.n_layer,
            "n_ctx": hf_config.n_positions,
            "eps": hf_config.layer_norm_epsilon,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.activation_function,
            "use_attn_scale": True,
            "use_local_attn": False,
            "trust_remote_code": "santacoder"
            in official_model_name,  # Only santacoder needs trust_remote_code
            "scale_attn_by_inverse_layer_idx": hf_config.scale_attn_by_inverse_layer_idx,
            "normalization_type": "LN",
        }
    elif architecture == "LlamaForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.rms_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "n_key_value_heads": (
                hf_config.num_key_value_heads
                if hf_config.num_key_value_heads != hf_config.num_attention_heads
                else None
            ),
            # This is done because the current implementation of GQA will use Grouped-Query Attention if
            # n_key_value_heads is not None, but hf_config.num_key_value_heads is sometimes specified as
            # the same as hf_config.num_attention_heads, in which case GQA should not be used.
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_adjacent_pairs": False,
            "rotary_dim": hf_config.hidden_size // hf_config.num_attention_heads,
            "final_rms": True,
            "gated_mlp": True,
        }
    elif architecture == "QWenLMHeadModel":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size // 2,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big
            "eps": hf_config.layer_norm_epsilon,
            "d_vocab": hf_config.vocab_size,
            "act_fn": "silu",
            "use_attn_scale": hf_config.scale_attn_weights,
            "initializer_range": hf_config.initializer_range,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_dim": hf_config.kv_channels,
            "rotary_adjacent_pairs": False,
            "tokenizer_prepends_bos": True,
            "trust_remote_code": True,
            "final_rms": True,
            "gated_mlp": True,
            "default_prepend_bos": False,
        }
    elif architecture == "Qwen2ForCausalLM":
        # Note that Qwen1.5 models have architecture type Qwen2ForCausalLM.
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "n_key_value_heads": hf_config.num_key_value_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big
            "eps": hf_config.rms_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "use_attn_scale": True,
            "initializer_range": hf_config.initializer_range,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_base": int(hf_config.rope_theta),
            "rotary_adjacent_pairs": False,
            "rotary_dim": hf_config.hidden_size // hf_config.num_attention_heads,
            "tokenizer_prepends_bos": True,
            "final_rms": True,
            "gated_mlp": True,
            "default_prepend_bos": False,
        }
    elif architecture == "Qwen3ForCausalLM":
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.head_dim
            if hasattr(hf_config, "head_dim")
            and hf_config.head_dim is not None
            and hf_config.head_dim > 0
            else hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "n_key_value_heads": (
                hf_config.num_key_value_heads
                if hf_config.num_key_value_heads != hf_config.num_attention_heads
                else None
            ),
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": 2048,
            "eps": hf_config.rms_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "use_attn_scale": True,
            "initializer_range": hf_config.initializer_range,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "rotary_base": int(hf_config.rope_theta),
            "rotary_adjacent_pairs": False,
            "rotary_dim": hf_config.head_dim
            if hasattr(hf_config, "head_dim") and hf_config.head_dim > 0
            else hf_config.hidden_size // hf_config.num_attention_heads,
            "tokenizer_prepends_bos": True,
            "final_rms": True,
            "gated_mlp": True,
            "default_prepend_bos": False,
            "use_qk_norm": True,
            "trust_remote_code": True,
        }
    elif architecture == "PhiForCausalLM":
        # Architecture for microsoft/phi models
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.layer_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "initializer_range": hf_config.initializer_range,
            "normalization_type": "LN",
            "positional_embedding_type": "rotary",
            "trust_remote_code": True,
            "rotary_base": hf_config.rope_theta,
            "use_attn_scale": True,
            "parallel_attn_mlp": True,
        }
        partial_rotary_factor = hf_config.partial_rotary_factor
        cfg_dict["rotary_dim"] = round(partial_rotary_factor * cfg_dict["d_head"])
    elif architecture == "Phi3ForCausalLM":
        # Architecture for microsoft/phi3 models
        cfg_dict = {
            "d_model": hf_config.hidden_size,
            "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
            "n_heads": hf_config.num_attention_heads,
            "d_mlp": hf_config.intermediate_size,
            "n_layers": hf_config.num_hidden_layers,
            "n_key_value_heads": (
                hf_config.num_key_value_heads
                if hf_config.num_key_value_heads != hf_config.num_attention_heads
                else None
            ),
            "n_ctx": hf_config.max_position_embeddings,
            "eps": hf_config.rms_norm_eps,
            "d_vocab": hf_config.vocab_size,
            "act_fn": hf_config.hidden_act,
            "initializer_range": hf_config.initializer_range,
            "normalization_type": "RMS",
            "positional_embedding_type": "rotary",
            "trust_remote_code": True,
            "rotary_base": hf_config.rope_theta,
            "use_attn_scale": True,
            "gated_mlp": True,
            "parallel_attn_mlp": False,
            "rotary_dim": hf_config.hidden_size // hf_config.num_attention_heads,
        }

    elif official_model_name.startswith("google/gemma-2b"):
        # Architecture for Gemma 2b and Gemma 2b Instruct models
        cfg_dict = {
            "d_model": 2048,
            "d_head": 256,
            "n_heads": 8,
            "d_mlp": 16384,
            "n_layers": 18,
            "n_ctx": 8192,
            "eps": 1e-06,
            "d_vocab": 256000,
            "act_fn": "gelu_new",
            "initializer_range": 0.02,
            "normalization_type": "RMS",
            "rotary_base": 10000,
            "rotary_dim": 256,
            "positional_embedding_type": "rotary",
            "use_attn_scale": True,
            "n_key_value_heads": 1,
            "gated_mlp": True,
            "final_rms": True,
        }
    elif official_model_name.startswith("google/gemma-7b"):
        # Architecture for Gemma 7b and Gemma 7b Instruct models
        cfg_dict = {
            "d_model": 3072,
            "d_head": 256,
            "n_heads": 16,
            "d_mlp": 24576,
            "n_layers": 28,
            "n_ctx": 8192,
            "eps": 1e-06,
            "d_vocab": 256000,
            "act_fn": "gelu_new",
            "initializer_range": 0.02,
            "normalization_type": "RMS",
            "rotary_base": 10000.0,
            "rotary_dim": 256,
            "positional_embedding_type": "rotary",
            "use_attn_scale": True,
            "n_key_value_heads": 16,
            "gated_mlp": True,
            "final_rms": True,
        }
    elif official_model_name.startswith("google/gemma-2-2b"):
        # Architecture for Gemma-2 2b and Gemma-2 2b Instruct models
        cfg_dict = {
            "d_model": 2304,
            "d_head": 256,
            "n_heads": 8,
            "d_mlp": 9216,
            "n_layers": 26,
            "n_ctx": 8192,
            "eps": 1e-06,
            "d_vocab": 256000,
            "act_fn": "gelu_pytorch_tanh",
            "initializer_range": 0.02,
            "normalization_type": "RMS",
            "rotary_base": 10000.0,
            "positional_embedding_type": "rotary",
            "use_attn_scale": True,
            "n_key_value_heads": 4,
            "window_size": 4096,
            "use_local_attn": True,
            "attn_types": ["global", "local"] * 21,  # Alternate global and local attn
            "attn_scores_soft_cap": 50.0,
            "output_logits_soft_cap": 30.0,
            "gated_mlp": True,
            "final_rms": True,
            "use_normalization_before_and_after": True,
        }
    elif official_model_name.startswith("google/gemma-2-9b"):
        # Architecture for Gemma-2 9b and Gemma-2 9b Instruct models
        cfg_dict = {
            "d_model": 3584,
            "d_head": 256,
            "n_heads": 16,
            "d_mlp": 14336,
            "n_layers": 42,
            "n_ctx": 8192,
            "eps": 1e-06,
            "d_vocab": 256000,
            "act_fn": "gelu_pytorch_tanh",
            "initializer_range": 0.02,
            "normalization_type": "RMS",
            "rotary_base": 10000.0,
            "positional_embedding_type": "rotary",
            "use_attn_scale": True,
            "n_key_value_heads": 8,
            "window_size": 4096,
            "use_local_attn": True,
            "attn_types": ["global", "local"] * 21,  # Alternate global and local attn
            "attn_scores_soft_cap": 50.0,
            "output_logits_soft_cap": 30.0,
            "gated_mlp": True,
            "final_rms": True,
            "use_normalization_before_and_after": True,
        }
    elif official_model_name.startswith("google/gemma-2-27b"):
        # Architecture for Gemma-2 27b and Gemma-2 27b Instruct models
        cfg_dict = {
            "d_model": 4608,
            "d_head": 128,
            "n_heads": 32,
            "d_mlp": 36864,
            "n_layers": 46,
            "n_ctx": 8192,
            "eps": 1e-06,
            "d_vocab": 256000,
            "act_fn": "gelu_pytorch_tanh",
            "initializer_range": 0.02,
            "normalization_type": "RMS",
            "rotary_base": 10000.0,
            "positional_embedding_type": "rotary",
            "use_attn_scale": True,
            "attn_scale": 12.0,
            "n_key_value_heads": 16,
            "window_size": 4096,
            "use_local_attn": True,
            "attn_types": ["global", "local"] * 23,  # Alternate global and local attn
            "attn_scores_soft_cap": 50.0,
            "output_logits_soft_cap": 30.0,
            "gated_mlp": True,
            "final_rms": True,
            "use_normalization_before_and_after": True,
        }
    elif architecture == "T5ForConditionalGeneration":
        cfg_dict = {
            "d_model": hf_config.d_model,
            "d_head": hf_config.d_kv,
            "n_heads": hf_config.num_heads,
            "d_mlp": hf_config.d_ff,
            "d_vocab": hf_config.vocab_size,
            "n_layers": hf_config.num_layers,
            "n_ctx": hf_config.max_length,
            "eps": hf_config.layer_norm_epsilon,
            "act_fn": hf_config.feed_forward_proj,
            "positional_embedding_type": "relative_positional_bias",
            "relative_attention_max_distance": hf_config.relative_attention_max_distance,
            "relative_attention_num_buckets": hf_config.relative_attention_num_buckets,
            "decoder_start_token_id": hf_config.decoder_start_token_id,
            "attention_dir": "bidirectional",
            "use_attn_scale": False,
            "tie_word_embeddings": hf_config.tie_word_embeddings,
        }
    else:
        raise NotImplementedError(f"{architecture} is not currently supported.")
    # All of these models use LayerNorm
    cfg_dict["original_architecture"] = architecture
    # The name such that AutoTokenizer.from_pretrained works
    cfg_dict["tokenizer_name"] = official_model_name
    if kwargs.get("trust_remote_code", False):
        cfg_dict["trust_remote_code"] = True
    return cfg_dict


def convert_neel_model_config(official_model_name: str, **kwargs: Any):
    """
    Loads the config for a model trained by me (NeelNanda), converted to a dictionary
    in the HookedTransformerConfig format.

    AutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.
    """
    official_model_name = get_official_model_name(official_model_name)
    cfg_json: dict = utils.download_file_from_hf(official_model_name, "config.json", **kwargs)
    cfg_arch = cfg_json.get(
        "architecture", "neel" if "_old" not in official_model_name else "neel-solu-old"
    )
    cfg_dict = {
        "d_model": cfg_json["d_model"],
        "n_layers": cfg_json["n_layers"],
        "d_mlp": cfg_json["d_mlp"],
        "d_head": cfg_json["d_head"],
        "n_heads": cfg_json["n_heads"],
        "n_ctx": cfg_json["n_ctx"],
        "d_vocab": cfg_json["d_vocab"],
        "tokenizer_name": cfg_json.get("tokenizer_name", None),
        "act_fn": cfg_json["act_fn"],
        "attn_only": cfg_json["attn_only"],
        "final_rms": cfg_json.get("final_rms", False),
        "original_architecture": cfg_arch,
    }
    if "normalization" in cfg_json:
        cfg_dict["normalization_type"] = cfg_json["normalization"]
    else:
        cfg_dict["normalization_type"] = cfg_json["normalization_type"]
    if "shortformer_pos" in cfg_json:
        cfg_dict["positional_embedding_type"] = (
            "shortformer" if cfg_json["shortformer_pos"] else "standard"
        )
    else:
        cfg_dict["positional_embedding_type"] = "standard"
    return cfg_dict


def get_pretrained_model_config(
    model_name: str,
    hf_cfg: Optional[dict] = None,
    checkpoint_index: Optional[int] = None,
    checkpoint_value: Optional[int] = None,
    fold_ln: bool = False,
    device: Optional[Union[str, torch.device]] = None,
    n_devices: int = 1,
    default_prepend_bos: Optional[bool] = None,
    dtype: torch.dtype = torch.float32,
    first_n_layers: Optional[int] = None,
    **kwargs: Any,
):
    """Returns the pretrained model config as an HookedTransformerConfig object.

    There are two types of pretrained models: HuggingFace models (where
    AutoModel and AutoConfig work), and models trained by me (NeelNanda) which
    aren't as integrated with HuggingFace infrastructure.

    Args:
        model_name: The name of the model. This can be either the official
            HuggingFace model name, or the name of a model trained by me
            (NeelNanda).
        hf_cfg (dict, optional): Config of a loaded pretrained HF model,
            converted to a dictionary.
        checkpoint_index (int, optional): If loading from a
            checkpoint, the index of the checkpoint to load. Defaults to None.
        checkpoint_value (int, optional): If loading from a checkpoint, the
        value of
            the checkpoint to load, ie the step or token number (each model has
            checkpoints labelled with exactly one of these). Defaults to None.
        fold_ln (bool, optional): Whether to fold the layer norm into the
            subsequent linear layers (see HookedTransformer.fold_layer_norm for
            details). Defaults to False.
        device (str, optional): The device to load the model onto. By
            default will load to CUDA if available, else CPU.
        n_devices (int, optional): The number of devices to split the model across. Defaults to 1.
        default_prepend_bos (bool, optional): Default behavior of whether to prepend the BOS token when the
            methods of HookedTransformer process input text to tokenize (only when input is a string).
            Resolution order for default_prepend_bos:
            1. If user passes value explicitly, use that value
            2. Model-specific default from cfg_dict if it exists (e.g. for bloom models it's False)
            3. Global default (True)

            Even for models not explicitly trained with the BOS token, heads often use the
            first position as a resting position and accordingly lose information from the first token,
            so this empirically seems to give better results. Note that you can also locally override the default behavior
            by passing in prepend_bos=True/False when you call a method that processes the input string.
        dtype (torch.dtype, optional): The dtype to load the TransformerLens model in.
        kwargs: Other optional arguments passed to HuggingFace's from_pretrained.
            Also given to other HuggingFace functions when compatible.

    """
    if Path(model_name).exists():
        # If the model_name is a path, it's a local model
        cfg_dict = convert_hf_model_config(model_name, **kwargs)
        official_model_name = model_name
    else:
        official_model_name = get_official_model_name(model_name)
    if (
        official_model_name.startswith("NeelNanda")
        or official_model_name.startswith("ArthurConmy")
        or official_model_name.startswith("Baidicoot")
    ):
        cfg_dict = convert_neel_model_config(official_model_name, **kwargs)
    else:
        if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(
            "trust_remote_code", False
        ):
            logging.warning(
                f"Loading model {official_model_name} requires setting trust_remote_code=True"
            )
            kwargs["trust_remote_code"] = True
        cfg_dict = convert_hf_model_config(official_model_name, **kwargs)
    # Processing common to both model types
    # Remove any prefix, saying the organization who made a model.
    cfg_dict["model_name"] = official_model_name.split("/")[-1]
    # Don't need to initialize weights, we're loading from pretrained
    cfg_dict["init_weights"] = False

    if (
        "positional_embedding_type" in cfg_dict
        and cfg_dict["positional_embedding_type"] == "shortformer"
        and fold_ln
    ):
        logging.warning(
            "You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_ln=False instead."
        )
        fold_ln = False

    if device is not None:
        cfg_dict["device"] = device

    cfg_dict["dtype"] = dtype

    if fold_ln:
        if cfg_dict["normalization_type"] in ["LN", "LNPre"]:
            cfg_dict["normalization_type"] = "LNPre"
        elif cfg_dict["normalization_type"] in ["RMS", "RMSPre"]:
            cfg_dict["normalization_type"] = "RMSPre"
        else:
            logging.warning("Cannot fold in layer norm, normalization_type is not LN.")

    if checkpoint_index is not None or checkpoint_value is not None:
        checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(
            official_model_name,
            **kwargs,
        )
        cfg_dict["from_checkpoint"] = True
        cfg_dict["checkpoint_label_type"] = checkpoint_label_type
        if checkpoint_index is not None:
            cfg_dict["checkpoint_index"] = checkpoint_index
            cfg_dict["checkpoint_value"] = checkpoint_labels[checkpoint_index]
        elif checkpoint_value is not None:
            assert (
                checkpoint_value in checkpoint_labels
            ), f"Checkpoint value {checkpoint_value} is not in list of available checkpoints"
            cfg_dict["checkpoint_value"] = checkpoint_value
            cfg_dict["checkpoint_index"] = checkpoint_labels.index(checkpoint_value)
    else:
        cfg_dict["from_checkpoint"] = False

    cfg_dict["device"] = device
    cfg_dict["n_devices"] = n_devices

    if default_prepend_bos is not None:
        # User explicitly set prepend_bos behavior, override config/default value
        cfg_dict["default_prepend_bos"] = default_prepend_bos
    elif "default_prepend_bos" not in cfg_dict:
        # No config value or user override, set default value (True)
        cfg_dict["default_prepend_bos"] = True

    if hf_cfg is not None:
        cfg_dict["load_in_4bit"] = hf_cfg.get("quantization_config", {}).get("load_in_4bit", False)
        cfg_dict["d_vocab"] = hf_cfg.get("vocab_size", cfg_dict["d_vocab"])
        if cfg_dict["original_architecture"] == "Qwen2ForCausalLM":
            cfg_dict["rotary_base"] = hf_cfg.get("rope_theta", cfg_dict["rotary_base"])
    if first_n_layers is not None:
        cfg_dict["n_layers"] = first_n_layers

    cfg = HookedTransformerConfig.from_dict(cfg_dict)
    return cfg


def get_num_params_of_pretrained(model_name: str):
    """
    Returns the number of parameters of a pretrained model, used to filter to only run code for sufficiently small models.
    """
    cfg = get_pretrained_model_config(model_name)
    return cfg.n_params


# %% Load checkpointed model state dicts
# The steps for which there are checkpoints in the stanford crfm models
STANFORD_CRFM_CHECKPOINTS = (
    list(range(0, 100, 10))
    + list(range(100, 2000, 50))
    + list(range(2000, 20000, 100))
    + list(range(20000, 400000 + 1, 1000))
)

# Linearly spaced checkpoints for Pythia models, taken every 1000 steps.
# Batch size 2,097,152 tokens, so checkpoints every 2.1B tokens
PYTHIA_CHECKPOINTS = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512] + list(
    range(1000, 143000 + 1, 1000)
)
# Pythia V1 has log-spaced early checkpoints (see line above), but V0 doesn't
PYTHIA_V0_CHECKPOINTS = list(range(1000, 143000 + 1, 1000))


def get_checkpoint_labels(model_name: str, **kwargs: Any):
    """Returns the checkpoint labels for a given model, and the label_type
    (step or token). Raises an error for models that are not checkpointed."""
    official_model_name = get_official_model_name(model_name)
    if official_model_name.startswith("stanford-crfm/"):
        return STANFORD_CRFM_CHECKPOINTS, "step"
    elif official_model_name.startswith("EleutherAI/pythia"):
        if "v0" in official_model_name:
            return PYTHIA_V0_CHECKPOINTS, "step"
        else:
            logging.warning(
                "Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models."
            )
            return PYTHIA_CHECKPOINTS, "step"
    elif official_model_name.startswith("NeelNanda/"):
        api = HfApi()
        files_list = api.list_repo_files(
            official_model_name,
            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),
        )
        labels = []
        for file_name in files_list:
            match = re.match(r"checkpoints/.*_(\d*)\.pth", file_name)
            if match:
                labels.append(int(match.group(1)))
        if labels[-1] > 1e9:
            label_type = "token"
        else:
            label_type = "step"
        return labels, label_type
    else:
        raise ValueError(f"Model {official_model_name} is not checkpointed.")


# %% Loading state dicts
def get_pretrained_state_dict(
    official_model_name: str,
    cfg: HookedTransformerConfig,
    hf_model: Optional[Any] = None,
    dtype: torch.dtype = torch.float32,
    **kwargs: Any,
) -> dict[str, torch.Tensor]:
    """
    Loads in the model weights for a pretrained model, and processes them to
    have the HookedTransformer parameter names and shapes. Supports checkpointed
    models (and expects the checkpoint info to be stored in the config object)

    hf_model: Optionally, a HuggingFace model object. If provided, we will use
        these weights rather than reloading the model.
    dtype: The dtype to load the HuggingFace model in.
    kwargs: Other optional arguments passed to HuggingFace's from_pretrained.
        Also given to other HuggingFace functions when compatible.
    """
    if "torch_dtype" in kwargs:
        dtype = kwargs["torch_dtype"]
        del kwargs["torch_dtype"]
    if Path(official_model_name).exists():
        official_model_name = str(Path(official_model_name).resolve())
        logging.info(f"Loading model from local path {official_model_name}")
    else:
        official_model_name = get_official_model_name(official_model_name)
    if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(
        "trust_remote_code", False
    ):
        logging.warning(
            f"Loading model {official_model_name} state dict requires setting trust_remote_code=True"
        )
        kwargs["trust_remote_code"] = True
    if (
        official_model_name.startswith("NeelNanda")
        or official_model_name.startswith("ArthurConmy")
        or official_model_name.startswith("Baidicoot")
    ):
        api = HfApi()
        repo_files = api.list_repo_files(
            official_model_name,
            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),
        )
        if cfg.from_checkpoint:
            file_name = list(
                filter(lambda x: x.endswith(f"{cfg.checkpoint_value}.pth"), repo_files)
            )[0]
        else:
            file_name = list(filter(lambda x: x.endswith("final.pth"), repo_files))[0]
        state_dict = utils.download_file_from_hf(official_model_name, file_name, **kwargs)

        # Convert to dtype
        state_dict = {k: v.to(dtype) for k, v in state_dict.items()}

        if cfg.original_architecture == "neel-solu-old":
            state_dict = convert_neel_solu_old_weights(state_dict, cfg)
        elif cfg.original_architecture == "mingpt":
            state_dict = convert_mingpt_weights(state_dict, cfg)
        return state_dict
    else:
        if cfg.from_checkpoint:
            huggingface_token = os.environ.get("HF_TOKEN", "")
            if official_model_name.startswith("stanford-crfm"):
                hf_model = AutoModelForCausalLM.from_pretrained(
                    official_model_name,
                    revision=f"checkpoint-{cfg.checkpoint_value}",
                    torch_dtype=dtype,
                    token=huggingface_token if len(huggingface_token) > 0 else None,
                    **kwargs,
                )
            elif official_model_name.startswith("EleutherAI/pythia"):
                hf_model = AutoModelForCausalLM.from_pretrained(
                    official_model_name,
                    revision=f"step{cfg.checkpoint_value}",
                    torch_dtype=dtype,
                    token=huggingface_token,
                    **kwargs,
                )
            else:
                raise ValueError(f"Checkpoints for model {official_model_name} are not supported")
        elif hf_model is None:
            huggingface_token = os.environ.get("HF_TOKEN", "")
            if official_model_name in NON_HF_HOSTED_MODEL_NAMES:
                raise NotImplementedError("Model not hosted on HuggingFace, must pass in hf_model")
            elif "bert" in official_model_name:
                hf_model = BertForPreTraining.from_pretrained(
                    official_model_name,
                    torch_dtype=dtype,
                    token=huggingface_token if len(huggingface_token) > 0 else None,
                    **kwargs,
                )
            elif "t5" in official_model_name:
                hf_model = T5ForConditionalGeneration.from_pretrained(
                    official_model_name,
                    torch_dtype=dtype,
                    token=huggingface_token if len(huggingface_token) > 0 else None,
                    **kwargs,
                )
            else:
                hf_model = AutoModelForCausalLM.from_pretrained(
                    official_model_name,
                    torch_dtype=dtype,
                    token=huggingface_token if len(huggingface_token) > 0 else None,
                    **kwargs,
                )

            # Load model weights, and fold in layer norm weights

        for param in hf_model.parameters():
            param.requires_grad = False

        if cfg.original_architecture == "GPT2LMHeadModel":
            state_dict = convert_gpt2_weights(hf_model, cfg)
        elif cfg.original_architecture == "GPTNeoForCausalLM":
            state_dict = convert_neo_weights(hf_model, cfg)
        elif cfg.original_architecture == "OPTForCausalLM":
            state_dict = convert_opt_weights(hf_model, cfg)
        elif cfg.original_architecture == "GPTJForCausalLM":
            state_dict = convert_gptj_weights(hf_model, cfg)
        elif cfg.original_architecture == "GPTNeoXForCausalLM":
            state_dict = convert_neox_weights(hf_model, cfg)
        elif cfg.original_architecture == "LlamaForCausalLM":
            state_dict = convert_llama_weights(hf_model, cfg)
        elif cfg.original_architecture == "BertForMaskedLM":
            state_dict = convert_bert_weights(hf_model, cfg)
        elif cfg.original_architecture == "T5ForConditionalGeneration":
            state_dict = convert_t5_weights(hf_model, cfg)
        elif cfg.original_architecture == "MistralForCausalLM":
            state_dict = convert_mistral_weights(hf_model, cfg)
        elif cfg.original_architecture == "MixtralForCausalLM":
            state_dict = convert_mixtral_weights(hf_model, cfg)
        elif cfg.original_architecture == "BloomForCausalLM":
            state_dict = convert_bloom_weights(hf_model, cfg)
        elif cfg.original_architecture == "GPT2LMHeadCustomModel":
            state_dict = convert_coder_weights(hf_model, cfg)
        elif cfg.original_architecture == "QWenLMHeadModel":
            state_dict = convert_qwen_weights(hf_model, cfg)
        elif cfg.original_architecture == "Qwen2ForCausalLM":
            state_dict = convert_qwen2_weights(hf_model, cfg)
        elif cfg.original_architecture == "Qwen3ForCausalLM":
            state_dict = convert_qwen3_weights(hf_model, cfg)
        elif cfg.original_architecture == "PhiForCausalLM":
            state_dict = convert_phi_weights(hf_model, cfg)
        elif cfg.original_architecture == "Phi3ForCausalLM":
            state_dict = convert_phi3_weights(hf_model, cfg)
        elif cfg.original_architecture == "GemmaForCausalLM":
            state_dict = convert_gemma_weights(hf_model, cfg)
        elif cfg.original_architecture == "Gemma2ForCausalLM":
            state_dict = convert_gemma_weights(hf_model, cfg)
        else:
            raise ValueError(
                f"Loading weights from the architecture is not currently supported: {cfg.original_architecture}, generated from model name {cfg.model_name}. Feel free to open an issue on GitHub to request this feature."
            )

        return state_dict


def fill_missing_keys(model: torch.nn.Module, state_dict: dict[str, torch.Tensor]):
    """Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.

    This function is assumed to be run before weights are initialized.

    Args:
        state_dict (dict): State dict from a pretrained model

    Returns:
        dict: State dict with missing keys filled in
    """
    # Get the default state dict
    default_state_dict = model.state_dict()
    # Get the keys that are missing from the pretrained model
    missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())
    # Fill in the missing keys with the default initialization
    for key in missing_keys:
        if "hf_model" in key:
            # Skip keys that are from the HuggingFace model, if loading from HF.
            continue
        if "W_" in key:
            logging.warning(
                "Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}".format(
                    key
                )
            )
        state_dict[key] = default_state_dict[key]
    return state_dict


@dataclasses.dataclass
class Config:
    d_model: int = 768
    debug: bool = True
    layer_norm_eps: float = 1e-5
    d_vocab: int = 50257
    init_range: float = 0.02
    n_ctx: int = 1024
    d_head: int = 64
    d_mlp: int = 3072
    n_heads: int = 12
    n_layers: int = 12


# Returns the configuration parameters of the model as a basic Config dataclass
def get_basic_config(model_name: str, **kwargs: Any) -> Config:
    return Config(
        **{
            k: v
            for k, v in get_pretrained_model_config(model_name, **kwargs).to_dict().items()
            if k
            in [
                "d_model",
                "debug",
                "layer_norm_eps",
                "d_vocab",
                "init_range",
                "n_ctx",
                "d_head",
                "d_mlp",
                "n_heads",
                "n_layers",
            ]
        }
    )



---
File: /transformer_lens/past_key_value_caching.py
---

"""Past Key Value Caching.

This module contains the HookedTransformerKeyValueCache and HookedTransformerKeyValueCacheEntry
classes, which are used to store past keys and values for the Transformer. This is important for
generating text - we can cache a lot of past computation and avoid repeating ourselves!
"""
from dataclasses import dataclass
from typing import List, Union

import torch
from jaxtyping import Float, Int

from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
from transformer_lens.utilities.devices import get_device_for_block_index


@dataclass
class HookedTransformerKeyValueCacheEntry:
    past_keys: Float[torch.Tensor, "batch pos_so_far n_heads d_head"]
    past_values: Float[torch.Tensor, "batch pos_so_far n_heads d_head"]
    frozen: bool = False

    @classmethod
    def init_cache_entry(
        cls,
        cfg: HookedTransformerConfig,
        device: Union[torch.device, str, None],
        batch_size: int = 1,
    ):
        n_heads = cfg.n_key_value_heads if cfg.n_key_value_heads is not None else cfg.n_heads
        return cls(
            past_keys=torch.empty(
                (batch_size, 0, n_heads, cfg.d_head), device=device, dtype=cfg.dtype
            ),
            past_values=torch.empty(
                (batch_size, 0, n_heads, cfg.d_head), device=device, dtype=cfg.dtype
            ),
        )

    def append(
        self,
        new_keys: Float[torch.Tensor, "batch new_tokens n_heads d_head"],
        new_values: Float[torch.Tensor, "batch new_tokens n_heads d_head"],
    ):
        updated_keys: Float[
            torch.Tensor, "batch pos_so_far_plus_new_tokens n_heads d_head"
        ] = torch.cat([self.past_keys, new_keys], dim=1)
        updated_values: Float[
            torch.Tensor, "batch pos_so_far_plus_new_tokens n_heads d_head"
        ] = torch.cat([self.past_values, new_values], dim=1)
        if not self.frozen:
            self.past_keys = updated_keys
            self.past_values = updated_values
        return updated_keys, updated_values


@dataclass
class HookedTransformerKeyValueCache:
    """
    A cache for storing past keys and values for the Transformer. This is important for generating text - we can cache a lot of past computation and avoid repeating ourselves!

    This cache is a list of HookedTransformerKeyValueCacheEntry objects, one for each layer in the Transformer. Each object stores a [batch, pos_so_far, n_heads, d_head] tensor for both keys and values, and each entry has an append method to add a single new key and value.

    The cache can be frozen so that it is not updated during the forward pass. This is useful when we want to run many inputs with the same prefix.
    """

    entries: List[HookedTransformerKeyValueCacheEntry]
    previous_attention_mask: Int[torch.Tensor, "batch pos_so_far"]
    frozen: bool = False

    @classmethod
    def init_cache(
        cls,
        cfg: HookedTransformerConfig,
        device: Union[torch.device, str, None],
        batch_size: int = 1,
    ):
        return cls(
            entries=[
                HookedTransformerKeyValueCacheEntry.init_cache_entry(
                    cfg,
                    get_device_for_block_index(i, cfg, device),
                    batch_size,
                )
                for i in range(cfg.n_layers)
            ],
            previous_attention_mask=torch.empty(
                # This may actually be an int64, but type promotion will handle it:
                # See: https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc
                # See: https://github.com/pytorch/pytorch/issues/35014
                (batch_size, 0),
                device=device,
                dtype=torch.int,
            ),
        )

    def freeze(self):
        self.frozen = True
        for entry in self.entries:
            entry.frozen = True

    def unfreeze(self):
        self.frozen = False
        for entry in self.entries:
            entry.frozen = False

    def append_attention_mask(self, attention_mask: Int[torch.Tensor, "batch new_tokens"]):
        attention_mask = attention_mask.to(self.previous_attention_mask.device)
        updated_attention_mask = torch.cat([self.previous_attention_mask, attention_mask], dim=-1)
        if not self.frozen:
            self.previous_attention_mask = updated_attention_mask
        return updated_attention_mask

    def __getitem__(self, idx):
        return self.entries[idx]



---
File: /transformer_lens/patching.py
---

"""Patching.

A module for patching activations in a transformer model, and measuring the effect of the patch on
the output. This implements the activation patching technique for a range of types of activation.
The structure is to have a single :func:`generic_activation_patch` function that does everything,
and to have a range of specialised functions for specific types of activation.

Context:

Activation Patching is technique introduced in the `ROME paper <http://rome.baulab.info/>`, which
uses a causal intervention to identify which activations in a model matter for producing some
output. It runs the model on input A, replaces (patches) an activation with that same activation on
input B, and sees how much that shifts the answer from A to B.

More details: The setup of activation patching is to take two runs of the model on two different
inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the
corrupted run does not. The key idea is that we give the model the corrupted input, but then
intervene on a specific activation and patch in the corresponding activation from the clean run (ie
replace the corrupted activation with the clean activation), and then continue the run. And we then
measure how much the output has updated towards the correct answer.

- We can then iterate over many
    possible activations and look at how much they affect the corrupted run. If patching in an
    activation significantly increases the probability of the correct answer, this allows us to
    localise which activations matter. 
- A key detail is that we move a single activation __from__ the clean run __to __the corrupted run.
    So if this changes the answer from incorrect to correct, we can be confident that the activation
    moved was important. 

Intuition:

The ability to **localise** is a key move in mechanistic interpretability - if the computation is
diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic
story for what's going on. But if we can identify precisely which parts of the model matter, we can
then zoom in and determine what they represent and how they connect up with each other, and
ultimately reverse engineer the underlying circuit that they represent. And, empirically, on at
least some tasks activation patching tends to find that computation is extremely localised:

- This technique helps us precisely identify which parts of the model matter for a certain
    part of a task. Eg, answering “The Eiffel Tower is in” with “Paris” requires figuring out that
    the Eiffel Tower is in Paris, and that it’s a factual recall task and that the output is a
    location. Patching to “The Colosseum is in” controls for everything other than the “Eiffel Tower
    is located in Paris” feature.
- It helps a lot if the corrupted prompt has the same number of tokens

This, unlike direct logit attribution, can identify meaningful parts of a circuit from anywhere
within the model, rather than just the end.
"""

from __future__ import annotations

import itertools
from functools import partial
from typing import Callable, Optional, Sequence, Tuple, Union, overload

import einops
import pandas as pd
import torch
from jaxtyping import Float, Int
from tqdm.auto import tqdm
from typing_extensions import Literal

import transformer_lens.utils as utils
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.HookedTransformer import HookedTransformer

# %%
Logits = torch.Tensor
AxisNames = Literal["layer", "pos", "head_index", "head", "src_pos", "dest_pos"]


# %%
from typing import Sequence


def make_df_from_ranges(
    column_max_ranges: Sequence[int], column_names: Sequence[str]
) -> pd.DataFrame:
    """
    Takes in a list of column names and max ranges for each column, and returns a dataframe with the cartesian product of the range for each column (ie iterating through all combinations from zero to column_max_range - 1, in order, incrementing the final column first)
    """
    rows = list(itertools.product(*[range(axis_max_range) for axis_max_range in column_max_ranges]))
    df = pd.DataFrame(rows, columns=column_names)
    return df


# %%
CorruptedActivation = torch.Tensor
PatchedActivation = torch.Tensor


@overload
def generic_activation_patch(
    model: HookedTransformer,
    corrupted_tokens: Int[torch.Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable[[Float[torch.Tensor, "batch pos d_vocab"]], Float[torch.Tensor, ""]],
    patch_setter: Callable[
        [CorruptedActivation, Sequence[int], ActivationCache], PatchedActivation
    ],
    activation_name: str,
    index_axis_names: Optional[Sequence[AxisNames]] = None,
    index_df: Optional[pd.DataFrame] = None,
    return_index_df: Literal[False] = False,
) -> torch.Tensor:
    ...


@overload
def generic_activation_patch(
    model: HookedTransformer,
    corrupted_tokens: Int[torch.Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable[[Float[torch.Tensor, "batch pos d_vocab"]], Float[torch.Tensor, ""]],
    patch_setter: Callable[
        [CorruptedActivation, Sequence[int], ActivationCache], PatchedActivation
    ],
    activation_name: str,
    index_axis_names: Optional[Sequence[AxisNames]],
    index_df: Optional[pd.DataFrame],
    return_index_df: Literal[True],
) -> Tuple[torch.Tensor, pd.DataFrame]:
    ...


def generic_activation_patch(
    model: HookedTransformer,
    corrupted_tokens: Int[torch.Tensor, "batch pos"],
    clean_cache: ActivationCache,
    patching_metric: Callable[[Float[torch.Tensor, "batch pos d_vocab"]], Float[torch.Tensor, ""]],
    patch_setter: Callable[
        [CorruptedActivation, Sequence[int], ActivationCache], PatchedActivation
    ],
    activation_name: str,
    index_axis_names: Optional[Sequence[AxisNames]] = None,
    index_df: Optional[pd.DataFrame] = None,
    return_index_df: bool = False,
) -> Union[torch.Tensor, Tuple[torch.Tensor, pd.DataFrame]]:
    """
    A generic function to do activation patching, will be specialised to specific use cases.

    Activation patching is about studying the counterfactual effect of a specific activation between a clean run and a corrupted run. The idea is have two inputs, clean and corrupted, which have two different outputs, and differ in some key detail. Eg "The Eiffel Tower is in" vs "The Colosseum is in". Then to take a cached set of activations from the "clean" run, and a set of corrupted.

    Internally, the key function comes from three things: A list of tuples of indices (eg (layer, position, head_index)), a index_to_act_name function which identifies the right activation for each index, a patch_setter function which takes the corrupted activation, the index and the clean cache, and a metric for how well the patched model has recovered.

    The indices can either be given explicitly as a pandas dataframe, or by listing the relevant axis names and having them inferred from the tokens and the model config. It is assumed that the first column is always layer.

    This function then iterates over every tuple of indices, does the relevant patch, and stores it

    Args:
        model: The relevant model
        corrupted_tokens: The input tokens for the corrupted run
        clean_cache: The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)
        patch_setter: A function which acts on (corrupted_activation, index, clean_cache) to edit the activation and patch in the relevant chunk of the clean activation
        activation_name: The name of the activation being patched
        index_axis_names: The names of the axes to (fully) iterate over, implicitly fills in index_df
        index_df: The dataframe of indices, columns are axis names and each row is a tuple of indices. Will be inferred from index_axis_names if not given. When this is input, the output will be a flattened tensor with an element per row of index_df
        return_index_df: A Boolean flag for whether to return the dataframe of indices too

    Returns:
        patched_output: The tensor of the patching metric for each patch. By default it has one dimension for each index dimension, via index_df set explicitly it is flattened with one element per row.
        index_df *optional*: The dataframe of indices
    """

    if index_df is None:
        assert index_axis_names is not None

        # Get the max range for all possible axes
        max_axis_range = {
            "layer": model.cfg.n_layers,
            "pos": corrupted_tokens.shape[-1],
            "head_index": model.cfg.n_heads,
        }
        max_axis_range["src_pos"] = max_axis_range["pos"]
        max_axis_range["dest_pos"] = max_axis_range["pos"]
        max_axis_range["head"] = max_axis_range["head_index"]

        # Get the max range for each axis we iterate over
        index_axis_max_range = [max_axis_range[axis_name] for axis_name in index_axis_names]

        # Get the dataframe where each row is a tuple of indices
        index_df = make_df_from_ranges(index_axis_max_range, index_axis_names)

        flattened_output = False
    else:
        # A dataframe of indices was provided. Verify that we did not *also* receive index_axis_names
        assert index_axis_names is None
        index_axis_max_range = index_df.max().to_list()

        flattened_output = True

    # Create an empty tensor to show the patched metric for each patch
    if flattened_output:
        patched_metric_output = torch.zeros(len(index_df), device=model.cfg.device)
    else:
        patched_metric_output = torch.zeros(index_axis_max_range, device=model.cfg.device)

    # A generic patching hook - for each index, it applies the patch_setter appropriately to patch the activation
    def patching_hook(corrupted_activation, hook, index, clean_activation):
        return patch_setter(corrupted_activation, index, clean_activation)

    # Iterate over every list of indices, and make the appropriate patch!
    for c, index_row in enumerate(tqdm((list(index_df.iterrows())))):
        index = index_row[1].to_list()

        # The current activation name is just the activation name plus the layer (assumed to be the first element of the input)
        current_activation_name = utils.get_act_name(activation_name, layer=index[0])

        # The hook function cannot receive additional inputs, so we use partial to include the specific index and the corresponding clean activation
        current_hook = partial(
            patching_hook,
            index=index,
            clean_activation=clean_cache[current_activation_name],
        )

        # Run the model with the patching hook and get the logits!
        patched_logits = model.run_with_hooks(
            corrupted_tokens, fwd_hooks=[(current_activation_name, current_hook)]
        )

        # Calculate the patching metric and store
        if flattened_output:
            patched_metric_output[c] = patching_metric(patched_logits).item()
        else:
            patched_metric_output[tuple(index)] = patching_metric(patched_logits).item()

    if return_index_df:
        return patched_metric_output, index_df
    else:
        return patched_metric_output


# %%
# Defining patch setters for various shapes of activations
def layer_pos_patch_setter(corrupted_activation, index, clean_activation):
    """
    Applies the activation patch where index = [layer, pos]

    Implicitly assumes that the activation axis order is [batch, pos, ...], which is true of everything that is not an attention pattern shaped tensor.
    """
    assert len(index) == 2
    layer, pos = index
    corrupted_activation[:, pos, ...] = clean_activation[:, pos, ...]
    return corrupted_activation


def layer_pos_head_vector_patch_setter(
    corrupted_activation,
    index,
    clean_activation,
):
    """
    Applies the activation patch where index = [layer, pos, head_index]

    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.
    """
    assert len(index) == 3
    layer, pos, head_index = index
    corrupted_activation[:, pos, head_index] = clean_activation[:, pos, head_index]
    return corrupted_activation


def layer_head_vector_patch_setter(
    corrupted_activation,
    index,
    clean_activation,
):
    """
    Applies the activation patch where index = [layer,  head_index]

    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.
    """
    assert len(index) == 2
    layer, head_index = index
    corrupted_activation[:, :, head_index] = clean_activation[:, :, head_index]

    return corrupted_activation


def layer_head_pattern_patch_setter(
    corrupted_activation,
    index,
    clean_activation,
):
    """
    Applies the activation patch where index = [layer,  head_index]

    Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.
    """
    assert len(index) == 2
    layer, head_index = index
    corrupted_activation[:, head_index, :, :] = clean_activation[:, head_index, :, :]

    return corrupted_activation


def layer_head_pos_pattern_patch_setter(
    corrupted_activation,
    index,
    clean_activation,
):
    """
    Applies the activation patch where index = [layer,  head_index, dest_pos]

    Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.
    """
    assert len(index) == 3
    layer, head_index, dest_pos = index
    corrupted_activation[:, head_index, dest_pos, :] = clean_activation[:, head_index, dest_pos, :]

    return corrupted_activation


def layer_head_dest_src_pos_pattern_patch_setter(
    corrupted_activation,
    index,
    clean_activation,
):
    """
    Applies the activation patch where index = [layer,  head_index, dest_pos, src_pos]

    Implicitly assumes that the activation axis order is [batch, head_index, dest_pos, src_pos], which is true of attention scores and patterns.
    """
    assert len(index) == 4
    layer, head_index, dest_pos, src_pos = index
    corrupted_activation[:, head_index, dest_pos, src_pos] = clean_activation[
        :, head_index, dest_pos, src_pos
    ]

    return corrupted_activation


# %%
# Defining activation patching functions for a range of common activation patches.
get_act_patch_resid_pre = partial(
    generic_activation_patch,
    patch_setter=layer_pos_patch_setter,
    activation_name="resid_pre",
    index_axis_names=("layer", "pos"),
)
get_act_patch_resid_pre.__doc__ = """
    Function to get activation patching results for the residual stream (at the start of each block) (by position). Returns a tensor of shape [n_layers, pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each resid_pre patch. Has shape [n_layers, pos]
    """

get_act_patch_resid_mid = partial(
    generic_activation_patch,
    patch_setter=layer_pos_patch_setter,
    activation_name="resid_mid",
    index_axis_names=("layer", "pos"),
)
get_act_patch_resid_mid.__doc__ = """
    Function to get activation patching results for the residual stream (between the attn and MLP layer of each block) (by position). Returns a tensor of shape [n_layers, pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos]
    """

get_act_patch_attn_out = partial(
    generic_activation_patch,
    patch_setter=layer_pos_patch_setter,
    activation_name="attn_out",
    index_axis_names=("layer", "pos"),
)
get_act_patch_attn_out.__doc__ = """
    Function to get activation patching results for the output of each Attention layer (by position). Returns a tensor of shape [n_layers, pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos]
    """

get_act_patch_mlp_out = partial(
    generic_activation_patch,
    patch_setter=layer_pos_patch_setter,
    activation_name="mlp_out",
    index_axis_names=("layer", "pos"),
)
get_act_patch_mlp_out.__doc__ = """
    Function to get activation patching results for the output of each MLP layer (by position). Returns a tensor of shape [n_layers, pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos]
    """
# %%
get_act_patch_attn_head_out_by_pos = partial(
    generic_activation_patch,
    patch_setter=layer_pos_head_vector_patch_setter,
    activation_name="z",
    index_axis_names=("layer", "pos", "head"),
)
get_act_patch_attn_head_out_by_pos.__doc__ = """
    Function to get activation patching results for the output of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]
    """

get_act_patch_attn_head_q_by_pos = partial(
    generic_activation_patch,
    patch_setter=layer_pos_head_vector_patch_setter,
    activation_name="q",
    index_axis_names=("layer", "pos", "head"),
)
get_act_patch_attn_head_q_by_pos.__doc__ = """
    Function to get activation patching results for the queries of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]
    """

get_act_patch_attn_head_k_by_pos = partial(
    generic_activation_patch,
    patch_setter=layer_pos_head_vector_patch_setter,
    activation_name="k",
    index_axis_names=("layer", "pos", "head"),
)
get_act_patch_attn_head_k_by_pos.__doc__ = """
    Function to get activation patching results for the keys of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]
    """

get_act_patch_attn_head_v_by_pos = partial(
    generic_activation_patch,
    patch_setter=layer_pos_head_vector_patch_setter,
    activation_name="v",
    index_axis_names=("layer", "pos", "head"),
)
get_act_patch_attn_head_v_by_pos.__doc__ = """
    Function to get activation patching results for the values of each Attention Head (by position). Returns a tensor of shape [n_layers, pos, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, pos, n_heads]
    """
# %%
get_act_patch_attn_head_pattern_by_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_pos_pattern_patch_setter,
    activation_name="pattern",
    index_axis_names=("layer", "head_index", "dest_pos"),
)
get_act_patch_attn_head_pattern_by_pos.__doc__ = """
    Function to get activation patching results for the attention pattern of each Attention Head (by destination position). Returns a tensor of shape [n_layers, n_heads, dest_pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads, dest_pos]
    """

get_act_patch_attn_head_pattern_dest_src_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_dest_src_pos_pattern_patch_setter,
    activation_name="pattern",
    index_axis_names=("layer", "head_index", "dest_pos", "src_pos"),
)
get_act_patch_attn_head_pattern_dest_src_pos.__doc__ = """
    Function to get activation patching results for each destination, source entry of the attention pattern for each Attention Head. Returns a tensor of shape [n_layers, n_heads, dest_pos, src_pos]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads, dest_pos, src_pos]
    """

# %%
get_act_patch_attn_head_out_all_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_vector_patch_setter,
    activation_name="z",
    index_axis_names=("layer", "head"),
)
get_act_patch_attn_head_out_all_pos.__doc__ = """
    Function to get activation patching results for the outputs of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]
    """

get_act_patch_attn_head_q_all_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_vector_patch_setter,
    activation_name="q",
    index_axis_names=("layer", "head"),
)
get_act_patch_attn_head_q_all_pos.__doc__ = """
    Function to get activation patching results for the queries of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]
    """

get_act_patch_attn_head_k_all_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_vector_patch_setter,
    activation_name="k",
    index_axis_names=("layer", "head"),
)
get_act_patch_attn_head_k_all_pos.__doc__ = """
    Function to get activation patching results for the keys of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]
    """

get_act_patch_attn_head_v_all_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_vector_patch_setter,
    activation_name="v",
    index_axis_names=("layer", "head"),
)
get_act_patch_attn_head_v_all_pos.__doc__ = """
    Function to get activation patching results for the values of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]
    """

get_act_patch_attn_head_pattern_all_pos = partial(
    generic_activation_patch,
    patch_setter=layer_head_pattern_patch_setter,
    activation_name="pattern",
    index_axis_names=("layer", "head_index"),
)
get_act_patch_attn_head_pattern_all_pos.__doc__ = """
    Function to get activation patching results for the attention pattern of each Attention Head (across all positions). Returns a tensor of shape [n_layers, n_heads]

    See generic_activation_patch for a more detailed explanation of activation patching 

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [n_layers, n_heads]
    """

# %%


def get_act_patch_attn_head_all_pos_every(
    model, corrupted_tokens, clean_cache, metric
) -> Float[torch.Tensor, "patch_type layer head"]:
    """Helper function to get activation patching results for every head (across all positions) for every act type (output, query, key, value, pattern). Wrapper around each's patching function, returns a stacked tensor of shape [5, n_layers, n_heads]

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [5, n_layers, n_heads]
    """
    act_patch_results: list[torch.Tensor] = []
    act_patch_results.append(
        get_act_patch_attn_head_out_all_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_q_all_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_k_all_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_v_all_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_pattern_all_pos(model, corrupted_tokens, clean_cache, metric)
    )
    return torch.stack(act_patch_results, dim=0)


def get_act_patch_attn_head_by_pos_every(
    model, corrupted_tokens, clean_cache, metric
) -> Float[torch.Tensor, "patch_type layer pos head"]:
    """Helper function to get activation patching results for every head (by position) for every act type (output, query, key, value, pattern). Wrapper around each's patching function, returns a stacked tensor of shape [5, n_layers, pos, n_heads]

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [5, n_layers, pos, n_heads]
    """
    act_patch_results = []
    act_patch_results.append(
        get_act_patch_attn_head_out_by_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_q_by_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_k_by_pos(model, corrupted_tokens, clean_cache, metric)
    )
    act_patch_results.append(
        get_act_patch_attn_head_v_by_pos(model, corrupted_tokens, clean_cache, metric)
    )

    # Reshape pattern to be compatible with the rest of the results
    pattern_results = get_act_patch_attn_head_pattern_by_pos(
        model, corrupted_tokens, clean_cache, metric
    )
    act_patch_results.append(einops.rearrange(pattern_results, "batch head pos -> batch pos head"))
    return torch.stack(act_patch_results, dim=0)


def get_act_patch_block_every(
    model, corrupted_tokens, clean_cache, metric
) -> Float[torch.Tensor, "patch_type layer pos"]:
    """Helper function to get activation patching results for the residual stream (at the start of each block), output of each Attention layer and output of each MLP layer. Wrapper around each's patching function, returns a stacked tensor of shape [3, n_layers, pos]

    Args:
        model: The relevant model
        corrupted_tokens (torch.Tensor): The input tokens for the corrupted run. Has shape [batch, pos]
        clean_cache (ActivationCache): The cached activations from the clean run
        metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)

    Returns:
        patched_output (torch.Tensor): The tensor of the patching metric for each patch. Has shape [3, n_layers, pos]
    """
    act_patch_results = []
    act_patch_results.append(get_act_patch_resid_pre(model, corrupted_tokens, clean_cache, metric))
    act_patch_results.append(get_act_patch_attn_out(model, corrupted_tokens, clean_cache, metric))
    act_patch_results.append(get_act_patch_mlp_out(model, corrupted_tokens, clean_cache, metric))
    return torch.stack(act_patch_results, dim=0)



---
File: /transformer_lens/SVDInterpreter.py
---

"""SVD Interpreter.

Module for getting the singular vectors of the OV, w_in, and w_out matrices of a
:class:`transformer_lens.HookedTransformer`.
"""

from typing import Optional, Union

import torch
from typeguard import typechecked
from typing_extensions import Literal

from transformer_lens.FactoredMatrix import FactoredMatrix
from transformer_lens.HookedTransformer import HookedTransformer

OUTPUT_EMBEDDING = "unembed.W_U"
VECTOR_TYPES = ["OV", "w_in", "w_out"]


class SVDInterpreter:
    def __init__(self, model: HookedTransformer):
        self.model = model
        self.cfg = model.cfg
        self.params = {name: param for name, param in model.named_parameters()}

    @typechecked
    def get_singular_vectors(
        self,
        vector_type: Union[Literal["OV"], Literal["w_in"], Literal["w_out"]],
        layer_index: int,
        num_vectors: int = 10,
        head_index: Optional[int] = None,
    ) -> torch.Tensor:
        """Gets the singular vectors for a given vector type, layer, and optionally head.

        This tensor can then be plotted using Neel's PySvelte, as demonstrated in the demo for this
        feature. The demo also points out some "gotchas" in this feature - numerical instability
        means inconsistency across devices, and the default HookedTransformer parameters don't
        replicate the original SVD post very well. So I'd recommend checking out the demo if you
        want to use this!

        Example:

        .. code-block:: python

            from transformer_lens import HookedTransformer, SVDInterpreter

            model = HookedTransformer.from_pretrained('gpt2-medium')
            svd_interpreter = SVDInterpreter(model)

            ov = svd_interpreter.get_singular_vectors('OV', layer_index=22, head_index=10)

            all_tokens = [model.to_str_tokens(np.array([i])) for i in range(model.cfg.d_vocab)]
            all_tokens = [all_tokens[i][0] for i in range(model.cfg.d_vocab)]

            def plot_matrix(matrix, tokens, k=10, filter="topk"):
                pysvelte.TopKTable(
                    tokens=all_tokens,
                    activations=matrix,
                    obj_type="SVD direction",
                    k=k,
                    filter=filter
                ).show()

            plot_matrix(ov, all_tokens)

        Args:
            vector_type: Type of the vector:
                - "OV": Singular vectors of the OV matrix for a particular layer and head.
                - "w_in": Singular vectors of the w_in matrix for a particular layer.
                - "w_out": Singular vectors of the w_out matrix for a particular layer.
            layer_index: The index of the layer.
            num_vectors: Number of vectors.
            head_index: Index of the head.
        """

        if head_index is None:
            assert vector_type in [
                "w_in",
                "w_out",
            ], f"Head index optional only for w_in and w_out, got {vector_type}"

        matrix: Union[FactoredMatrix, torch.Tensor]
        if vector_type == "OV":
            assert head_index is not None  # keep mypy happy
            matrix = self._get_OV_matrix(layer_index, head_index)
            V = matrix.Vh.T

        elif vector_type == "w_in":
            matrix = self._get_w_in_matrix(layer_index)
            _, _, V = torch.linalg.svd(matrix)

        elif vector_type == "w_out":
            matrix = self._get_w_out_matrix(layer_index)
            _, _, V = torch.linalg.svd(matrix)

        else:
            raise ValueError(f"Vector type must be in {VECTOR_TYPES}, instead got {vector_type}")

        return self._get_singular_vectors_from_matrix(V, self.params[OUTPUT_EMBEDDING], num_vectors)

    def _get_singular_vectors_from_matrix(
        self,
        V: Union[torch.Tensor, FactoredMatrix],
        embedding: torch.Tensor,
        num_vectors: int = 10,
    ) -> torch.Tensor:
        """Returns the top num_vectors singular vectors from a matrix."""

        vectors_list = []
        for i in range(num_vectors):
            activations = V[i, :].float() @ embedding  # type: ignore
            vectors_list.append(activations)

        vectors = torch.stack(vectors_list, dim=1).unsqueeze(1)
        assert vectors.shape == (
            self.cfg.d_vocab,
            1,
            num_vectors,
        ), f"Vectors shape should be {self.cfg.d_vocab, 1, num_vectors} but got {vectors.shape}"
        return vectors

    def _get_OV_matrix(self, layer_index: int, head_index: int) -> FactoredMatrix:
        """Gets the OV matrix for a particular layer and head."""

        assert (
            0 <= layer_index < self.cfg.n_layers
        ), f"Layer index must be between 0 and {self.cfg.n_layers-1} but got {layer_index}"
        assert (
            0 <= head_index < self.cfg.n_heads
        ), f"Head index must be between 0 and {self.cfg.n_heads-1} but got {head_index}"

        W_V: torch.Tensor = self.params[f"blocks.{layer_index}.attn.W_V"]
        W_O: torch.Tensor = self.params[f"blocks.{layer_index}.attn.W_O"]
        W_V, W_O = W_V[head_index, :, :], W_O[head_index, :, :]

        return FactoredMatrix(W_V, W_O)

    def _get_w_in_matrix(self, layer_index: int) -> torch.Tensor:
        """Gets the w_in matrix for a particular layer."""

        assert (
            0 <= layer_index < self.cfg.n_layers
        ), f"Layer index must be between 0 and {self.cfg.n_layers-1} but got {layer_index}"

        w_in = self.params[f"blocks.{layer_index}.mlp.W_in"].T

        if f"blocks.{layer_index}.ln2.w" in self.params:  # If fold_ln == False
            ln_2 = self.params[f"blocks.{layer_index}.ln2.w"]
            return w_in * ln_2

        return w_in

    def _get_w_out_matrix(self, layer_index: int) -> torch.Tensor:
        """Gets the w_out matrix for a particular layer."""

        assert (
            0 <= layer_index < self.cfg.n_layers
        ), f"Layer index must be between 0 and {self.cfg.n_layers-1} but got {layer_index}"

        return self.params[f"blocks.{layer_index}.mlp.W_out"]



---
File: /transformer_lens/train.py
---

"""Train.

Utilities for training :class:`transformer_lens.HookedTransformer` models on autoregressive language
modeling tasks.
"""

from dataclasses import dataclass
from typing import Optional

import torch
import torch.optim as optim
import wandb
from torch.optim import Optimizer
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

from transformer_lens import utils
from transformer_lens.HookedTransformer import HookedTransformer


@dataclass
class HookedTransformerTrainConfig:
    """
    Configuration class to store training hyperparameters for a training run of
    an HookedTransformer model.
    Args:
        num_epochs (int): Number of epochs to train for
        batch_size (int): Size of batches to use for training
        lr (float): Learning rate to use for training
        seed (int): Random seed to use for training
        momentum (float): Momentum to use for training
        max_grad_norm (float, *optional*): Maximum gradient norm to use for
        weight_decay (float, *optional*): Weight decay to use for training
        optimizer_name (str): The name of the optimizer to use
        device (str, *optional*): Device to use for training
        warmup_steps (int, *optional*): Number of warmup steps to use for training
        save_every (int, *optional*): After how many batches should a checkpoint be saved
        save_dir, (str, *optional*): Where to save checkpoints
        wandb (bool): Whether to use Weights and Biases for logging
        wandb_project (str, *optional*): Name of the Weights and Biases project to use
        print_every (int, *optional*): Print the loss every n steps
        max_steps (int, *optional*): Terminate the epoch after this many steps. Used for debugging.
    """

    num_epochs: int
    batch_size: int
    lr: float = 1e-3
    seed: int = 0
    momentum: float = 0.0
    max_grad_norm: Optional[float] = None
    weight_decay: Optional[float] = None
    optimizer_name: str = "Adam"
    device: Optional[str] = None
    warmup_steps: int = 0
    save_every: Optional[int] = None
    save_dir: Optional[str] = None
    wandb: bool = False
    wandb_project_name: Optional[str] = None
    print_every: Optional[int] = 50
    max_steps: Optional[int] = None


def train(
    model: HookedTransformer,
    config: HookedTransformerTrainConfig,
    dataset: Dataset,
) -> HookedTransformer:
    """
    Trains an HookedTransformer model on an autoregressive language modeling task.
    Args:
        model: The model to train
        config: The training configuration
        dataset: The dataset to train on - this function assumes the dataset is set up for autoregressive language modeling.
    Returns:
        The trained model
    """
    torch.manual_seed(config.seed)
    model.train()
    if config.wandb:
        if config.wandb_project_name is None:
            config.wandb_project_name = "easy-transformer"
        wandb.init(project=config.wandb_project_name, config=vars(config))

    if config.device is None:
        config.device = utils.get_device()

    optimizer: Optimizer
    if config.optimizer_name in ["Adam", "AdamW"]:
        # Weight decay in Adam is implemented badly, so use AdamW instead (see PyTorch AdamW docs)
        if config.weight_decay is not None:
            optimizer = optim.AdamW(
                model.parameters(),
                lr=config.lr,
                weight_decay=config.weight_decay,
            )
        else:
            optimizer = optim.Adam(
                model.parameters(),
                lr=config.lr,
            )
    elif config.optimizer_name == "SGD":
        optimizer = optim.SGD(
            model.parameters(),
            lr=config.lr,
            weight_decay=(config.weight_decay if config.weight_decay is not None else 0.0),
            momentum=config.momentum,
        )
    else:
        raise ValueError(f"Optimizer {config.optimizer_name} not supported")

    scheduler = None
    if config.warmup_steps > 0:
        scheduler = optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lambda step: min(1.0, step / config.warmup_steps),
        )

    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)

    model.to(config.device)

    for epoch in tqdm(range(1, config.num_epochs + 1)):
        samples = 0
        for step, batch in tqdm(enumerate(dataloader)):
            tokens = batch["tokens"].to(config.device)
            loss = model(tokens, return_type="loss")
            loss.backward()
            if config.max_grad_norm is not None:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
            if config.warmup_steps > 0:
                assert scheduler is not None
                scheduler.step()
            optimizer.zero_grad()

            samples += tokens.shape[0]

            if config.wandb:
                wandb.log({"train_loss": loss.item(), "samples": samples, "epoch": epoch})

            if config.print_every is not None and step % config.print_every == 0:
                print(f"Epoch {epoch} Samples {samples} Step {step} Loss {loss.item()}")

            if (
                config.save_every is not None
                and step % config.save_every == 0
                and config.save_dir is not None
            ):
                torch.save(model.state_dict(), f"{config.save_dir}/model_{step}.pt")

            if config.max_steps is not None and step >= config.max_steps:
                break

    return model



---
File: /transformer_lens/utils.py
---

"""Utils.

This module contains varied utility functions used throughout the library.
"""

from __future__ import annotations

import collections.abc
import inspect
import json
import os
import re
import shutil
from copy import deepcopy
from typing import Any, List, Optional, Tuple, Union, cast

import einops
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers
from datasets.arrow_dataset import Dataset
from datasets.load import load_dataset
from huggingface_hub import constants, hf_hub_download
from jaxtyping import Float, Int
from rich import print as rprint
from transformers import AutoTokenizer
from transformers.tokenization_utils_base import PreTrainedTokenizerBase

from transformer_lens.FactoredMatrix import FactoredMatrix

CACHE_DIR = constants.HUGGINGFACE_HUB_CACHE
USE_DEFAULT_VALUE = None


def select_compatible_kwargs(
    kwargs_dict: dict[str, Any], callable: collections.abc.Callable
) -> dict[str, Any]:
    """Return a dict with the elements kwargs_dict that are parameters of callable"""
    return {k: v for k, v in kwargs_dict.items() if k in inspect.getfullargspec(callable).args}


def download_file_from_hf(
    repo_name: str,
    file_name: str,
    subfolder: str = ".",
    cache_dir: Optional[str] = CACHE_DIR,
    force_is_torch: bool = False,
    **kwargs: Any,
):
    """
    Helper function to download files from the HuggingFace Hub, from subfolder/file_name in repo_name, saving locally to cache_dir and returning the loaded file (if a json or Torch object) and the file path otherwise.

    If it's a Torch file without the ".pth" extension, set force_is_torch=True to load it as a Torch object.
    """
    file_path = hf_hub_download(
        repo_id=repo_name,
        filename=file_name,
        subfolder=subfolder,
        cache_dir=cache_dir,
        **select_compatible_kwargs(kwargs, hf_hub_download),
    )

    if file_path.endswith(".pth") or force_is_torch:
        return torch.load(file_path, map_location="cpu", weights_only=False)
    elif file_path.endswith(".json"):
        return json.load(open(file_path, "r"))
    else:
        print("File type not supported:", file_path.split(".")[-1])
        return file_path


def clear_huggingface_cache():
    """
    Deletes the Hugging Face cache directory and all its contents.

    This function deletes the Hugging Face cache directory, which is used to store downloaded models and their associated files. Deleting the cache directory will remove all the downloaded models and their files, so you will need to download them again if you want to use them in your code.

    Parameters:
    None

    Returns:
    None
    """
    print("Deleting Hugging Face cache directory and all its contents.")
    shutil.rmtree(CACHE_DIR)


def print_gpu_mem(step_name: str = ""):
    print(f"{step_name} ~ {np.round(torch.cuda.memory_allocated()/2e30, 2)} GiB allocated on GPU.")


def get_corner(tensor: Any, n: int = 3):
    # Prints the top left corner of the tensor
    if isinstance(tensor, torch.Tensor):
        return tensor[tuple(slice(n) for _ in range(tensor.ndim))]
    elif isinstance(tensor, FactoredMatrix):
        return tensor[tuple(slice(n) for _ in range(tensor.ndim))].AB


def to_numpy(tensor: Any):
    """
    Helper function to convert a tensor to a numpy array. Also works on lists, tuples, and numpy arrays.
    """
    if isinstance(tensor, np.ndarray):
        return tensor
    elif isinstance(tensor, (list, tuple)):
        array = np.array(tensor)
        return array
    elif isinstance(tensor, (torch.Tensor, torch.nn.parameter.Parameter)):
        return tensor.detach().cpu().numpy()
    elif isinstance(tensor, (int, float, bool, str)):
        return np.array(tensor)
    else:
        raise ValueError(f"Input to to_numpy has invalid type: {type(tensor)}")


def lm_cross_entropy_loss(
    logits: Float[torch.Tensor, "batch pos d_vocab"],
    tokens: Int[torch.Tensor, "batch pos"],
    attention_mask: Optional[Int[torch.Tensor, "batch pos"]] = None,
    per_token: bool = False,
) -> Union[Float[torch.Tensor, ""], Float[torch.Tensor, "batch pos"]]:
    """Cross entropy loss for the language model, gives the loss for predicting the NEXT token.

    Args:
        logits (torch.Tensor): Logits. Shape [batch, pos, d_vocab]
        tokens (torch.Tensor[int64]): Input tokens. Shape [batch, pos]
        attention_mask (torch.Tensor[int64], optional): Attention mask. Shape [batch, pos]. Used to
            mask out padding tokens. Defaults to None.
        per_token (bool, optional): Whether to return the log probs predicted for the correct token, or the loss (ie mean of the predicted log probs). Note that the returned array has shape [batch, seq-1] as we cannot predict the first token (alternately, we ignore the final logit). Defaults to False.
    """
    log_probs = F.log_softmax(logits, dim=-1)
    # Use torch.gather to find the log probs of the correct tokens
    # Offsets needed because we're predicting the NEXT token (this means the final logit is meaningless)
    # None and [..., 0] needed because the tensor used in gather must have the same rank.
    predicted_log_probs = log_probs[..., :-1, :].gather(dim=-1, index=tokens[..., 1:, None])[..., 0]

    if attention_mask is not None:
        # Ignore token positions which are masked out or where the next token is masked out
        # (generally padding tokens)
        next_token_mask = torch.logical_and(attention_mask[:, :-1], attention_mask[:, 1:])
        predicted_log_probs *= next_token_mask
        n_tokens = next_token_mask.sum().item()
    else:
        n_tokens = predicted_log_probs.numel()

    if per_token:
        return -predicted_log_probs
    else:
        return -predicted_log_probs.sum() / n_tokens


def lm_accuracy(
    logits: Float[torch.Tensor, "batch pos d_vocab"],
    tokens: Int[torch.Tensor, "batch pos"],
    per_token: bool = False,
) -> Union[Float[torch.Tensor, ""], Float[torch.Tensor, "batch pos"]]:
    """Cross-Entropy Accuracy for Language Modelling. We measure the accuracy on the logits for predicting the NEXT token.

    If per_token is True, returns the boolean for top 1 accuracy for each token in the batch. Note that this has size [batch, seq_len-1], as we cannot predict the first token.
    """
    top_prediction = logits.argmax(dim=-1)
    correct_matches = top_prediction[:, :-1] == tokens[:, 1:]
    if per_token:
        return correct_matches
    else:
        return correct_matches.sum() / correct_matches.numel()


def gelu_new(
    input: Float[torch.Tensor, "batch pos d_mlp"]
) -> Float[torch.Tensor, "batch pos d_mlp"]:
    # Implementation of GeLU used by GPT2 - subtly different from PyTorch's
    return (
        0.5
        * input
        * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
    )


def gelu_fast(
    input: Float[torch.Tensor, "batch pos d_mlp"]
) -> Float[torch.Tensor, "batch pos d_mlp"]:
    return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))


def gelu_pytorch_tanh(input: torch.Tensor) -> torch.Tensor:
    """
    Approximation of the gelu activation function, used in some older models.
    """
    return F.gelu(input, approximate="tanh")


def solu(input: Float[torch.Tensor, "batch pos d_mlp"]) -> Float[torch.Tensor, "batch pos d_mlp"]:
    """
    SoLU activation function as described by
    https://transformer-circuits.pub/2022/solu/index.html.

    LayerNorm implemented by the MLP class.
    """
    return input * F.softmax(input, dim=-1)


ACTIVATION_FN_DICT = {
    "solu": solu,
    "solu_ln": solu,
    "gelu_new": gelu_new,
    "gelu_fast": gelu_fast,
    "silu": F.silu,
    "relu": F.relu,
    "gelu": F.gelu,
    "gelu_pytorch_tanh": gelu_pytorch_tanh,
}


def calc_fan_in_and_fan_out(tensor: torch.Tensor) -> tuple[int, int]:
    """
    Calculate the fan in and fan out of a tensor. We define it ourselves because Torch uses a
    different convention for weights (e.g. for an MLP they use d_out x d_in, and we use d_in x
    d_out, for attention they do (n_head d_head) x d_model, we do n_head x d_model x d_head).
    """
    shape = tensor.shape

    if len(shape) == 0:
        raise ValueError("Fan in and fan out can not be computed for scalars.")
    elif len(shape) == 1:
        fan_in = 1
        fan_out = shape[0]
    elif len(shape) == 2:  # Linear transform
        fan_in = shape[0]
        fan_out = shape[1]
    elif len(shape) == 3:  # Attention head weight, has shape n_head x d_model x d_head
        fan_in = shape[1]
        fan_out = shape[0] * shape[2]
    else:
        raise ValueError(f"Fan in and fan out can not be computed for shape {shape} tensors.")

    return fan_in, fan_out


def init_xavier_uniform_(param: torch.Tensor, gain: float = 1.0) -> torch.Tensor:
    """
    Initializes the input tensor using the Xavier initialization method.
    """
    fan_in, fan_out = calc_fan_in_and_fan_out(param)
    max = gain * np.sqrt(6.0 / (fan_in + fan_out))
    return nn.init.uniform_(param, -max, max)


def init_xavier_normal_(param: torch.Tensor, gain: float = 1.0) -> torch.Tensor:
    """
    Initializes the input tensor using the Xavier initialization method.
    """
    fan_in, fan_out = calc_fan_in_and_fan_out(param)
    std = gain * np.sqrt(2.0 / (fan_in + fan_out))
    return nn.init.normal_(param, mean=0.0, std=std)


def init_kaiming_uniform_(
    param: torch.Tensor,
    a: float = 0,
    nonlinearity: str = "relu",
    gain: float = 1.0,
    mode: str = "fan_in",
) -> torch.Tensor:
    """
    Initializes the input tensor using the Kaiming initialization method.

    Starting from a std 1 uniform distribution, we scale the weights by c / sqrt(fan_in), where c =
    sqrt(2) if the params were immediately preceded by a relu and 1 for everything else.

    As with torch, `a` is a hyperparameter for `nonlinearity`, if it takes one.
    """
    fan_in, fan_out = calc_fan_in_and_fan_out(param)
    fan = fan_in if mode == "fan_in" else fan_out
    gain *= nn.init.calculate_gain(nonlinearity, a)
    max = gain * np.sqrt(3.0 / fan)
    return nn.init.uniform_(param, -max, max)


def init_kaiming_normal_(
    param: torch.Tensor,
    a: float = 0,
    nonlinearity: str = "relu",
    gain: float = 1.0,
    mode: str = "fan_in",
) -> torch.Tensor:
    """
    Initializes the input tensor using the Kaiming initialization method.

    Starting from a std 1 normal distribution, we scale the weights by c / sqrt(fan_in), where c =
    sqrt(2) if the params were immediately preceded by a relu and 1 for everything else.

    As with torch, `a` is a hyperparameter for `nonlinearity`, if it takes one.
    """
    fan_in, fan_out = calc_fan_in_and_fan_out(param)
    fan = fan_in if mode == "fan_in" else fan_out
    gain *= nn.init.calculate_gain(nonlinearity, a)
    std = gain * np.sqrt(1.0 / fan)
    return nn.init.normal_(param, mean=0.0, std=std)


def keep_single_column(dataset: Dataset, col_name: str):
    """
    Acts on a HuggingFace dataset to delete all columns apart from a single column name - useful when we want to tokenize and mix together different strings
    """
    for key in dataset.features:
        if key != col_name:
            dataset = dataset.remove_columns(key)
    return dataset


def tokenize_and_concatenate(
    dataset: Dataset,
    tokenizer: PreTrainedTokenizerBase,
    streaming: bool = False,
    max_length: int = 1024,
    column_name: str = "text",
    add_bos_token: bool = True,
    num_proc: int = 10,
) -> Dataset:
    """Helper function to tokenizer and concatenate a dataset of text. This converts the text to tokens, concatenates them (separated by EOS tokens) and then reshapes them into a 2D array of shape (____, sequence_length), dropping the last batch. Tokenizers are much faster if parallelised, so we chop the string into 20, feed it into the tokenizer, in parallel with padding, then remove padding at the end.

    This tokenization is useful for training language models, as it allows us to efficiently train on a large corpus of text of varying lengths (without, eg, a lot of truncation or padding). Further, for models with absolute positional encodings, this avoids privileging early tokens (eg, news articles often begin with CNN, and models may learn to use early positional encodings to predict these)

    Args:
        dataset (Dataset): The dataset to tokenize, assumed to be a HuggingFace text dataset.
        tokenizer (transformers.PreTrainedTokenizerBase): The tokenizer. Assumed to have a bos_token_id and an eos_token_id.
        streaming (bool, optional): Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.
        max_length (int, optional): The length of the context window of the sequence. Defaults to 1024.
        column_name (str, optional): The name of the text column in the dataset. Defaults to 'text'.
        add_bos_token (bool, optional): . Defaults to True.

    Returns:
        Dataset: Returns the tokenized dataset, as a dataset of tensors, with a single column called "tokens"
    """
    dataset = keep_single_column(dataset, column_name)
    if tokenizer.pad_token is None:
        # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.
        tokenizer.add_special_tokens({"pad_token": "<PAD>"})
    # Define the length to chop things up into - leaving space for a bos_token if required
    if add_bos_token:
        seq_len = max_length - 1
    else:
        seq_len = max_length

    def tokenize_function(examples: dict[str, list[str]]) -> dict[str, np.ndarray]:
        text = examples[column_name]
        # Concatenate it all into an enormous string, separated by eos_tokens
        assert tokenizer.eos_token is not None, "Tokenizer must have an EOS token."
        full_text = tokenizer.eos_token.join(text)

        # Handle the case when full_text is empty
        if not full_text.strip():
            return {"tokens": np.array([], dtype=np.int64)}

        # Divide into 20 chunks of ~ equal length
        num_chunks = 20
        chunk_length = (len(full_text) - 1) // num_chunks + 1
        chunks = [full_text[i * chunk_length : (i + 1) * chunk_length] for i in range(num_chunks)]
        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned
        tokens = tokenizer(chunks, return_tensors="np", padding=True)["input_ids"].flatten()
        # Drop padding tokens
        tokens = tokens[tokens != tokenizer.pad_token_id]
        num_tokens = len(tokens)

        # Handle cases where num_tokens is less than seq_len
        if num_tokens < seq_len:
            num_batches = 1
            # Pad tokens if necessary
            tokens = tokens[:seq_len]
            if len(tokens) < seq_len:
                padding_length = seq_len - len(tokens)
                padding = np.full(padding_length, tokenizer.pad_token_id)
                tokens = np.concatenate([tokens, padding], axis=0)
        else:
            num_batches = num_tokens // seq_len
            # Drop the final tokens if not enough to make a full sequence
            tokens = tokens[: seq_len * num_batches]

        tokens = einops.rearrange(
            tokens, "(batch seq) -> batch seq", batch=num_batches, seq=seq_len
        )
        if add_bos_token:
            prefix = np.full((num_batches, 1), tokenizer.bos_token_id)
            tokens = np.concatenate([prefix, tokens], axis=1)
        return {"tokens": tokens}

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=(num_proc if not streaming else None),
        remove_columns=[column_name],
    )
    tokenized_dataset.set_format(type="torch", columns=["tokens"])
    return tokenized_dataset


def sample_logits(
    final_logits: Float[torch.Tensor, "batch d_vocab"],
    top_k: Optional[int] = None,
    top_p: Optional[float] = None,
    temperature: float = 1.0,
    freq_penalty: float = 0.0,
    tokens: Optional[Int[torch.Tensor, "batch pos"]] = None,
) -> Int[torch.Tensor, "batch"]:
    """
    Sample from the logits, in order to generate text

    final_logits has shape [batch, vocab_size]
    We divide the logits by temperature before softmaxing and sampling - high temperature = more uniform, low = more argmaxy. Temp = 0.0 is greedy sampling
    We apply top_k and top_p filtering to the logits, to encourage diversity. top_k = 10 means we only sample from the 10 most likely tokens. top_p = 0.9 means we only sample from the top 90% of tokens, and then renormalise the distribution. top_k and top_p are mutually exclusive. By default we apply neither and just sample from the full distribution.

    Frequency penalty is a penalty on the probability of a token, proportional to the number of times it has been generated so far. This encourages the model to generate new tokens, rather than repeating itself. It is a hyperparameter, and should be tuned. It is applied to the logits before sampling. If this is non-zero it is required to input the input_tokens

    #! TODO: Finish testing all the edge cases here. Useful testing code:
    logits = torch.randn(4)
    print(logits)
    np.unique(np.array([sample_logits(logits, top_k=2).item() for i in range(1000)]), return_counts=True)
    """
    if temperature == 0.0:
        # Greedy sampling
        return final_logits.argmax(dim=-1)
    else:
        # Sample from the distribution

        final_logits = final_logits / temperature
        if freq_penalty > 0:
            assert tokens is not None, "Must provide input_tokens if applying a frequency penalty"
            assert (
                len(tokens.shape) == 2
            ), "Frequency penalty do not support input in the form of embeddings"
            for batch_index in range(final_logits.shape[0]):
                # torch.bincount returns a tensor of length d_vocab, with the number of occurences of each token in the tokens.
                final_logits[batch_index] = final_logits[
                    batch_index
                ] - freq_penalty * torch.bincount(
                    tokens[batch_index], minlength=final_logits.shape[-1]
                )
        if top_k is not None:
            assert top_k > 0, "top_k has to be greater than 0"
            top_logits, _ = final_logits.topk(top_k, dim=-1)
            indices_to_remove = final_logits < top_logits[..., -1].unsqueeze(-1)
            final_logits = final_logits.masked_fill(indices_to_remove, -float("inf"))
        elif top_p is not None:
            assert 1.0 >= top_p > 0.0, "top_p has to be in (0, 1]"
            sorted_logits, sorted_indices = torch.sort(final_logits, descending=True)
            cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
            # We round up - we want prob >= top_p not <top_p
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices_to_remove.scatter(
                -1, sorted_indices, sorted_indices_to_remove
            )
            final_logits = final_logits.masked_fill(indices_to_remove, -float("inf"))

        final_logits = final_logits.to(torch.float32)
        return torch.distributions.categorical.Categorical(logits=final_logits).sample()


# Type alias
SliceInput = Optional[
    Union[
        int,
        Tuple[int,],
        Tuple[int, int],
        Tuple[int, int, int],
        List[int],
        torch.Tensor,
        np.ndarray,
    ]
]
"""An object that represents a slice input. It can be a tuple of integers or a slice object.

An optional type alias for a slice input used in the `ActivationCache` module.

A `SliceInput` can be one of the following types:
    - `int`: an integer representing a single position
    - `Tuple[int, int]`: a tuple of two integers representing a range of positions
    - `Tuple[int, int, int]`: a tuple of three integers representing a range of positions with a step size
    - `List[int]`: a list of integers representing multiple positions
    - `torch.Tensor`: a tensor containing a boolean mask or a list of indices to be selected from the input tensor.

`SliceInput` is used in the `apply_ln_to_stack` method in the `ActivationCache` module.
"""


class Slice:
    """An object that represents a slice input. It can be a tuple of integers or a slice object.

    We use a custom slice syntax because Python/Torch's don't let us reduce the number of dimensions:

    Note that slicing with input_slice=None means do nothing, NOT add an extra dimension (use unsqueeze for that)

    There are several modes:
    int - just index with that integer (decreases number of dimensions)
    slice - Input is a tuple converted to a slice ((k,) means :k, (k, m) means m:k, (k, m, n) means m:k:n)
    array - Input is a list or tensor or numpy array, converted to a numpy array, and we take the stack of values at those indices
    identity - Input is None, leave it unchanged.

    Examples for dim=0:
    if input_slice=0, tensor -> tensor[0]
    elif input_slice = (1, 5), tensor -> tensor[1:5]
    elif input_slice = (1, 5, 2), tensor -> tensor[1:5:2] (ie indexing with [1, 3])
    elif input_slice = [1, 4, 5], tensor -> tensor[[1, 4, 5]] (ie changing the first axis to have length 3, and taking the indices 1, 4, 5 out).
    elif input_slice is a Tensor, same as list - Tensor is assumed to be a 1D list of indices.
    """

    slice: Union[int, slice, np.ndarray]

    def __init__(
        self,
        input_slice: SliceInput = None,
    ):
        """
        Modular component for slicing tensors. Can be used to slice a tensor along a given dimension, or to index into a tensor along a given dimension.

        Args:
            input_slice (SliceInput): The slice to apply. Can be an int, a tuple, a list, a torch.Tensor, or None. If None, do nothing.

        Raises:
            ValueError: If the input_slice is not one of the above types.
        """
        if isinstance(input_slice, tuple):
            self.slice = slice(*input_slice)
            self.mode = "slice"
        elif isinstance(input_slice, int):
            self.slice = input_slice
            self.mode = "int"
        elif isinstance(input_slice, slice):
            self.slice = input_slice
            self.mode = "slice"
        elif type(input_slice) in [list, torch.Tensor, np.ndarray]:
            self.slice = to_numpy(input_slice)
            self.mode = "array"
        elif input_slice is None:
            self.slice = slice(None)
            self.mode = "identity"
        else:
            raise ValueError(f"Invalid input_slice {input_slice}")

    def apply(
        self,
        tensor: torch.Tensor,
        dim: int = 0,
    ) -> torch.Tensor:
        """
        Takes in a tensor and a slice, and applies the slice to the given dimension (supports positive and negative dimension syntax). Returns the sliced tensor.

        Args:
            tensor (torch.Tensor): The tensor to slice.
            dim (int, optional): The dimension to slice along. Supports positive and negative dimension syntax.

        Returns:
            torch.Tensor: The sliced tensor.
        """
        ndim = tensor.ndim
        slices = [slice(None)] * ndim
        slices[dim] = self.slice  # type: ignore
        return tensor[tuple(slices)]

    def indices(
        self,
        max_ctx: Optional[int] = None,
    ) -> Union[np.ndarray, np.int32, np.int64]:
        """
        Returns the indices of the slice, as a numpy array or an int.
        If max_ctx is given, slices relative to the end (e.g. slice(-5, None)) are converted to absolute indices.

        Args:
            max_ctx (int, optional): The size of the axis to slice. Only used if the slice is not an integer.

        Returns:
            Union[np.ndarray, np.int32, np.int64]: The indices that this slice will select.

        Raises:
            ValueError: If the slice is not an integer and max_ctx is not specified.
        """
        if self.mode == "int":
            return np.array([self.slice], dtype=np.int64)
        if max_ctx is None:
            raise ValueError("max_ctx must be specified if slice is not an integer")
        return np.arange(max_ctx, dtype=np.int64)[self.slice]

    def __repr__(
        self,
    ) -> str:
        return f"Slice: {self.slice} Mode: {self.mode} "

    @classmethod
    def unwrap(
        cls,
        slice_input: Union["Slice", SliceInput],
    ) -> "Slice":
        """
        Takes a Slice-like input and converts it into a Slice, if it is not already.

        Args:
            slice_input (Union[Slice, SliceInput]): The input to turn into a Slice.

        Returns:
            Slice: A Slice object.
        """
        if not isinstance(slice_input, Slice):
            if isinstance(
                slice_input, int
            ):  # slicing with an int collapses the dimension so this stops the pos dimension from collapsing
                slice_input = [slice_input]
            slice_input = Slice(slice_input)
        return slice_input


def get_act_name(
    name: str,
    layer: Optional[Union[int, str]] = None,
    layer_type: Optional[str] = None,
):
    """
    Helper function to convert shorthand to an activation name. Pretty hacky, intended to be useful for short feedback
    loop hacking stuff together, more so than writing good, readable code. But it is deterministic!

    Returns a name corresponding to an activation point in a TransformerLens model.

    Args:
         name (str): Takes in the name of the activation. This can be used to specify any activation name by itself.
            The code assumes the first sequence of digits passed to it (if any) is the layer number, and anything after
            that is the layer type.

            Given only a word and number, it leaves layer_type as is.
            Given only a word, it leaves layer and layer_type as is.

         layer (int, optional): Takes in the layer number. Used for activations that appear in every block.

         layer_type (string, optional): Used to distinguish between activations that appear multiple times in one block.

    Examples::

        get_act_name('k', 6, 'a')=='blocks.6.attn.hook_k'
        get_act_name('pre', 2)=='blocks.2.mlp.hook_pre'
        get_act_name('embed')=='hook_embed'
        get_act_name('normalized', 27, 'ln2')=='blocks.27.ln2.hook_normalized'
        get_act_name('k6')=='blocks.6.attn.hook_k'
        get_act_name('scale4ln1')=='blocks.4.ln1.hook_scale'
        get_act_name('pre5')=='blocks.5.mlp.hook_pre'
    """
    if ("." in name or name.startswith("hook_")) and layer is None and layer_type is None:
        # If this was called on a full name, just return it
        return name
    match = re.match(r"([a-z]+)(\d+)([a-z]?.*)", name)
    if match is not None:
        name, layer, layer_type = match.groups(0)  # type: ignore

    layer_type_alias = {
        "a": "attn",
        "m": "mlp",
        "b": "",
        "block": "",
        "blocks": "",
        "attention": "attn",
    }

    act_name_alias = {
        "attn": "pattern",
        "attn_logits": "attn_scores",
        "key": "k",
        "query": "q",
        "value": "v",
        "mlp_pre": "pre",
        "mlp_mid": "mid",
        "mlp_post": "post",
    }

    layer_norm_names = ["scale", "normalized"]

    if name in act_name_alias:
        name = act_name_alias[name]

    full_act_name = ""
    if layer is not None:
        full_act_name += f"blocks.{layer}."
    if name in [
        "k",
        "v",
        "q",
        "z",
        "rot_k",
        "rot_q",
        "result",
        "pattern",
        "attn_scores",
    ]:
        layer_type = "attn"
    elif name in ["pre", "post", "mid", "pre_linear"]:
        layer_type = "mlp"
    elif layer_type in layer_type_alias:
        layer_type = layer_type_alias[layer_type]

    if layer_type:
        full_act_name += f"{layer_type}."
    full_act_name += f"hook_{name}"

    if name in layer_norm_names and layer is None:
        full_act_name = f"ln_final.{full_act_name}"
    return full_act_name


def remove_batch_dim(tensor: Float[torch.Tensor, "1 ..."]) -> Float[torch.Tensor, "..."]:
    """
    Removes the first dimension of a tensor if it is size 1, otherwise returns the tensor unchanged
    """
    if tensor.shape[0] == 1:
        return tensor.squeeze(0)
    else:
        return tensor


def test_prompt(
    prompt: str,
    answer: Union[str, list[str]],
    model,  # Can't give type hint due to circular imports
    prepend_space_to_answer: bool = True,
    print_details: bool = True,
    prepend_bos: Optional[bool] = USE_DEFAULT_VALUE,
    top_k: int = 10,
) -> None:
    """Test if the Model Can Give the Correct Answer to a Prompt.

    Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),
    as well as the top k tokens. Works for multi-token prompts and multi-token answers.

    Warning:

    This will print the results (it does not return them).

    Examples:

    >>> from transformer_lens import HookedTransformer, utils
    >>> model = HookedTransformer.from_pretrained("tiny-stories-1M")
    Loaded pretrained model tiny-stories-1M into HookedTransformer

    >>> prompt = "Why did the elephant cross the"
    >>> answer = "road"
    >>> utils.test_prompt(prompt, answer, model)
    Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']
    Tokenized answer: [' road']
    Performance on answer token:
    Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|
    Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|
    Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|
    Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|
    Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|
    Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|
    Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|
    Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|
    Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|
    Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|
    Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|
    Ranks of the answer tokens: [(' road', 2)]

    Args:
        prompt:
            The prompt string, e.g. "Why did the elephant cross the".
        answer:
            The answer, e.g. "road". Note that if you set prepend_space_to_answer to False, you need
            to think about if you have a space before the answer here (as e.g. in this example the
            answer may really be " road" if the prompt ends without a trailing space). If this is a
            list of strings, then we only look at the next-token completion, and we compare them all
            as possible model answers.
        model:
            The model.
        prepend_space_to_answer:
            Whether or not to prepend a space to the answer. Note this will only ever prepend a
            space if the answer doesn't already start with one.
        print_details:
            Print the prompt (as a string but broken up by token), answer and top k tokens (all
            with logit, rank and probability).
        prepend_bos:
            Overrides self.cfg.default_prepend_bos if set. Whether to prepend
            the BOS token to the input (applicable when input is a string). Models generally learn
            to use the BOS token as a resting place for attention heads (i.e. a way for them to be
            "turned off"). This therefore often improves performance slightly.
        top_k:
            Top k tokens to print details of (when print_details is set to True).

    Returns:
        None (just prints the results directly).
    """
    answers = [answer] if isinstance(answer, str) else answer
    n_answers = len(answers)
    using_multiple_answers = n_answers > 1

    if prepend_space_to_answer:
        answers = [answer if answer.startswith(" ") else " " + answer for answer in answers]

    # GPT-2 often treats the first token weirdly, so lets give it a resting position
    prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)
    answer_tokens = model.to_tokens(answers, prepend_bos=False)

    # If we have multiple answers, we're only allowed a single token generation
    if using_multiple_answers:
        answer_tokens = answer_tokens[:, :1]

    # Deal with case where answers is a list of strings
    prompt_tokens = prompt_tokens.repeat(answer_tokens.shape[0], 1)
    tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)

    prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)
    answer_str_tokens_list = [model.to_str_tokens(answer, prepend_bos=False) for answer in answers]
    prompt_length = len(prompt_str_tokens)
    answer_length = 1 if using_multiple_answers else len(answer_str_tokens_list[0])
    if print_details:
        print("Tokenized prompt:", prompt_str_tokens)
        if using_multiple_answers:
            print("Tokenized answers:", answer_str_tokens_list)
        else:
            print("Tokenized answer:", answer_str_tokens_list[0])
    logits = model(tokens)
    probs = logits.softmax(dim=-1)
    answer_ranks = []

    for index in range(prompt_length, prompt_length + answer_length):
        # Get answer tokens for this sequence position
        answer_tokens = tokens[:, index]
        answer_str_tokens = [a[index - prompt_length] for a in answer_str_tokens_list]
        # Offset by 1 because models predict the NEXT token
        token_probs = probs[:, index - 1]
        sorted_token_probs, sorted_token_positions = token_probs.sort(descending=True)
        answer_token_ranks = sorted_token_positions.argsort(-1)[
            range(n_answers), answer_tokens.cpu()
        ].tolist()
        answer_ranks.append(
            [
                (answer_str_token, answer_token_rank)
                for answer_str_token, answer_token_rank in zip(
                    answer_str_tokens, answer_token_ranks
                )
            ]
        )
        if print_details:
            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.
            # rprint gives rich text printing
            rprint(
                f"Performance on answer token{'s' if n_answers > 1 else ''}:\n"
                + "\n".join(
                    [
                        f"[b]Rank: {answer_token_ranks[i]: <8} Logit: {logits[i, index-1, answer_tokens[i]].item():5.2f} Prob: {token_probs[i, answer_tokens[i]].item():6.2%} Token: |{answer_str_tokens[i]}|[/b]"
                        for i in range(n_answers)
                    ]
                )
            )
            for i in range(top_k):
                print(
                    f"Top {i}th token. Logit: {logits[0, index-1, sorted_token_positions[0, i]].item():5.2f} Prob: {sorted_token_probs[0, i].item():6.2%} Token: |{model.to_string(sorted_token_positions[0, i])}|"
                )

    # If n_answers = 1 then unwrap answer ranks, so printed output matches original version of function
    if not using_multiple_answers:
        single_answer_ranks = [r[0] for r in answer_ranks]
        rprint(f"[b]Ranks of the answer tokens:[/b] {single_answer_ranks}")
    else:
        rprint(f"[b]Ranks of the answer tokens:[/b] {answer_ranks}")


def transpose(tensor: Float[torch.Tensor, "... a b"]) -> Float[torch.Tensor, "... b a"]:
    """
    Utility to swap the last two dimensions of a tensor, regardless of the number of leading dimensions
    """
    return tensor.transpose(-1, -2)


def composition_scores(
    left: "FactoredMatrix", right: "FactoredMatrix", broadcast_dims=True
) -> Union[
    Float[torch.Tensor, "*leading_dims"], Float[torch.Tensor, "*leading_dims_left_and_right"]
]:
    """
    See `HookedTransformer.all_composition_scores` for documentation.
    """
    if broadcast_dims:
        r_leading = right.ndim - 2
        l_leading = left.ndim - 2
        for i in range(l_leading):
            right = right.unsqueeze(i)
        for i in range(r_leading):
            left = left.unsqueeze(i + l_leading)
    assert (
        left.rdim == right.ldim
    ), f"Composition scores require left.rdim==right.ldim, shapes were left: {left.shape}, right:{right.shape}"

    new_right = right.collapse_r()
    new_left = left.collapse_l()
    r_norms = new_right.norm(dim=[-2, -1])
    l_norms = new_left.norm(dim=[-2, -1])
    comp_norms = (new_left @ new_right).norm(dim=[-2, -1])
    return comp_norms / r_norms / l_norms


def get_dataset(dataset_name: str, **kwargs) -> Dataset:
    """
    Returns a small HuggingFace dataset, for easy testing and exploration. Accesses several convenience datasets with 10,000 elements (dealing with the enormous 100GB - 2TB datasets is a lot of effort!). Note that it returns a dataset (ie a dictionary containing all the data), *not* a DataLoader (iterator over the data + some fancy features). But you can easily convert it to a DataLoader.

    Each dataset has a 'text' field, which contains the relevant info, some also have several meta data fields

    Kwargs will be passed to the huggingface dataset loading function, e.g. "data_dir"

    Possible inputs:
    * openwebtext (approx the GPT-2 training data https://huggingface.co/datasets/openwebtext)
    * pile (The Pile, a big mess of tons of diverse data https://pile.eleuther.ai/)
    * c4 (Colossal, Cleaned, Common Crawl - basically openwebtext but bigger https://huggingface.co/datasets/c4)
    * code (Codeparrot Clean, a Python code dataset https://huggingface.co/datasets/codeparrot/codeparrot-clean )
    * c4_code (c4 + code - the 20K data points from c4-10k and code-10k. This is the mix of datasets used to train my interpretability-friendly models, though note that they are *not* in the correct ratio! There's 10K texts for each, but about 22M tokens of code and 5M tokens of C4)
    * wiki (Wikipedia, generated from the 20220301.en split of https://huggingface.co/datasets/wikipedia )
    """
    dataset_aliases = {
        "openwebtext": "stas/openwebtext-10k",
        "owt": "stas/openwebtext-10k",
        "pile": "NeelNanda/pile-10k",
        "c4": "NeelNanda/c4-10k",
        "code": "NeelNanda/code-10k",
        "python": "NeelNanda/code-10k",
        "c4_code": "NeelNanda/c4-code-20k",
        "c4-code": "NeelNanda/c4-code-20k",
        "wiki": "NeelNanda/wiki-10k",
    }
    if dataset_name in dataset_aliases:
        dataset = load_dataset(dataset_aliases[dataset_name], split="train", **kwargs)
    else:
        raise ValueError(f"Dataset {dataset_name} not supported")
    return dataset


def is_square(x: torch.Tensor) -> bool:
    """Checks if `x` is a square matrix."""
    return x.ndim == 2 and x.shape[0] == x.shape[1]


def is_lower_triangular(x: torch.Tensor) -> bool:
    """Checks if `x` is a lower triangular matrix."""
    if not is_square(x):
        return False
    return x.equal(x.tril())


def check_structure(t1: torch.Tensor, t2: torch.Tensor, *, verbose: bool = False) -> None:
    """Validate that the two square tensors have the same structure, i.e.,
    that the directionality of comparisons points in the same directions both
    row-wise and column-wise.

    This function is not used anywhere in the code right now, just for debugging tests.
    """
    assert t1.ndim == 2
    assert t1.shape == t2.shape
    n_rows, n_cols = cast(Tuple[int, int], t1.shape)

    if verbose:
        print("Checking rows")
    row_mismatch = []
    for row_i in range(n_rows - 1):
        t1_result = t1[row_i].ge(t1[row_i + 1])
        t2_result = t2[row_i].ge(t2[row_i + 1])
        if any(t1_result != t2_result):
            row_mismatch.append(row_i)
            if verbose:
                print(f"\trows {row_i}:{row_i + 1}")
                print(f"\tt1: {t1_result.tolist()}")
                print(f"\tt2: {t2_result.tolist()}")

    if verbose:
        print("Checking columns")
    col_mismatch = []
    for col_i in range(n_cols - 1):
        t1_result = t1[:, col_i].ge(t1[:, col_i + 1])
        t2_result = t2[:, col_i].ge(t2[:, col_i + 1])
        if any(t1_result != t2_result):
            col_mismatch.append(col_i)
            if verbose:
                print(f"\trows {col_i}:{col_i + 1}")
                print(f"\tt1: {t1_result.tolist()}")
                print(f"\tt2: {t2_result.tolist()}")
    if not row_mismatch and not col_mismatch:
        print("PASSED")
    elif row_mismatch:
        print(f"row mismatch: {row_mismatch}")
    elif col_mismatch:
        print(f"column mismatch: {col_mismatch}")


def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        # Parse the PyTorch version to check if it's below version 2.0
        major_version = int(torch.__version__.split(".")[0])
        if major_version >= 2:
            return torch.device("mps")

    return torch.device("cpu")


def override_or_use_default_value(
    default_flag: Any,
    override: Optional[Any] = None,
) -> Any:
    """
    Determines which flag to return based on whether an overriding flag is provided.
    If a not-None overriding flag is provided, it is returned.
    Otherwise, the global flag is returned.
    """
    return override if override is not None else default_flag


def get_offset_position_ids(
    past_kv_pos_offset: int,
    attention_mask: Int[torch.Tensor, "batch offset_pos"],
) -> Int[torch.Tensor, "batch pos"]:
    """
    Returns the indices of non-padded tokens, offset by the position of the first attended token.
    """
    # shift the position ids so that the id at the the first attended token position becomes zero.
    # The position ids of the prepending pad tokens are shifted to -1.
    shifted_position_ids = attention_mask.cumsum(dim=1) - 1  # [batch, tokens_length]

    # Set the position ids of all prepending pad tokens to an arbitrary number (zero here)
    # just to avoid indexing errors.
    position_ids = shifted_position_ids.masked_fill(shifted_position_ids < 0, 0)
    return position_ids[:, past_kv_pos_offset:]  # [pos, batch]


def get_cumsum_along_dim(tensor, dim, reverse=False):
    """
    Returns the cumulative sum of a tensor along a given dimension.
    """
    if reverse:
        tensor = tensor.flip(dims=(dim,))
    cumsum = tensor.cumsum(dim=dim)
    if reverse:
        cumsum = cumsum.flip(dims=(dim,))
    return cumsum


def get_attention_mask(
    tokenizer: transformers.PreTrainedTokenizerBase,
    tokens: torch.Tensor,
    prepend_bos: bool,
) -> torch.Tensor:
    """
    Computes the attention mask for the tokenized input.
    NOTE: Only the leftmost leading pads (when `padding_side == left`)
    or rightmost trailing pads (when `padding_side == right`) are
    considered as real pad tokens that should not be attended.

    Args:
        tokenizer (transformers.PreTrainedTokenizerBase): The tokenizer used for tokenization.
        tokens (torch.Tensor): The tokenized input.
        prepend_bos (bool): If True, a BOS token is prepended to the input.

    Returns:
        torch.Tensor: The attention mask for the input.
    """

    # Initialize the attention mask with ones (indicating all tokens should be attended to)
    attention_mask = torch.ones_like(tokens)
    if tokenizer is None:
        return attention_mask
    is_not_pad_token = tokens.ne(tokenizer.pad_token_id)

    if tokenizer.padding_side == "right":
        # Zero-out the rightmost trailing pad tokens
        is_trailing_pad = get_cumsum_along_dim(is_not_pad_token, -1, reverse=True) == 0
        attention_mask[is_trailing_pad] = 0
    else:
        # Zero-out the leftmost leading pad tokens
        is_leading_pad = get_cumsum_along_dim(is_not_pad_token, -1, reverse=False) == 0
        attention_mask[is_leading_pad] = 0

        # If the bos token is the same as the pad token,
        # the last token of the leftmost leading pad tokens is the bos token.
        # We need to set the attention mask for the bos token to 1.
        if prepend_bos and tokenizer.bos_token_id == tokenizer.pad_token_id:
            pad_bos_positions = is_leading_pad.sum(-1) - 1
            attention_mask[torch.arange(attention_mask.shape[0]), pad_bos_positions] = 1

    return attention_mask


def repeat_along_head_dimension(
    tensor: Float[torch.Tensor, "batch pos d_model"],
    n_heads: int,
    clone_tensor=True,
    # `einops.repeat` uses a view in torch, so we generally clone the tensor to avoid using shared storage for each head entry
):
    repeated_tensor = einops.repeat(
        tensor,
        "batch pos d_model -> batch pos n_heads d_model",
        n_heads=n_heads,
    )
    if clone_tensor:
        return repeated_tensor.clone()
    else:
        return repeated_tensor


def get_nested_attr(obj, attr_str):
    """
    Retrieves a nested attribute from an object based on a dot-separated string.

    For example, if `attr_str` is "a.b.c", this function will return `obj.a.b.c`.

    Args:
        obj (Any): The object from which to retrieve the attribute.
        attr_str (str): A dot-separated string representing the attribute hierarchy.

    Returns:
        Any: The value of the nested attribute.
    """
    attrs = attr_str.split(".")
    for attr in attrs:
        obj = getattr(obj, attr)
    return obj


def set_nested_attr(obj, attr_str, value):
    """
    Sets a nested attribute of an object based on a dot-separated string.

    For example, if `attr_str` is "a.b.c", this function will set the value of `obj.a.b.c` to `value`.

    Args:
        obj (Any): The object on which to set the attribute.
        attr_str (str): A dot-separated string representing the attribute hierarchy.
        value (Any): The value to set for the nested attribute.
    """
    attrs = attr_str.split(".")

    # Navigate to the deepest object containing the attribute to be set
    for attr in attrs[:-1]:
        obj = getattr(obj, attr)

    # Set the nested attribute's value
    setattr(obj, attrs[-1], value)


class LocallyOverridenDefaults:
    """
    Context manager that allows temporary overriding of default values within a model.
    Once the context is exited, the default values are restored.

    WARNING: This context manager must be used for any function/method that directly accesses
    default values which may be overridden by the user using the function/method's arguments,
    e.g., `model.cfg.default_prepend_bos` and `model.tokenizer.padding_side` which can be
    overriden by `prepend_bos` and `padding_side` arguments, respectively, in the `to_tokens`.
    """

    def __init__(self, model, **overrides):
        """
        Initializes the context manager.

        Args:
            model (HookedTransformer): The model whose default values will be overridden.
            overrides (dict): Key-value pairs of properties to override and their new values.
        """
        self.model = model
        self.overrides = overrides

        # Dictionary defining valid defaults, valid values, and locations to find and store them
        self.values_with_defaults = {
            "prepend_bos": {
                "default_location": "model.cfg.default_prepend_bos",
                "valid_values": [USE_DEFAULT_VALUE, True, False],
                "skip_overriding": False,
                "default_value_to_restore": None,  # Will be set later
            },
            "padding_side": {
                "default_location": "model.tokenizer.padding_side",
                "valid_values": [USE_DEFAULT_VALUE, "left", "right"],
                "skip_overriding": model.tokenizer is None,  # Do not override if tokenizer is None
                "default_value_to_restore": None,  # Will be set later
            },
        }

        # Ensure provided overrides are defined in the dictionary above
        for override in overrides:
            assert override in self.values_with_defaults, (
                f"{override} is not a valid parameter to override. "
                f"Valid parameters are {self.values_with_defaults.keys()}."
            )

    def __enter__(self):
        """
        Override default values upon entering the context.
        """
        for property, override in self.overrides.items():
            info = self.values_with_defaults[property]
            if info["skip_overriding"]:
                continue  # Skip if overriding for this property is disabled

            # Ensure the override is a valid value
            valid_values = info["valid_values"]
            assert (
                override in valid_values  # type: ignore
            ), f"{property} must be one of {valid_values}, but got {override}."

            # Fetch current default and store it to restore later
            default_location = info["default_location"]
            default_value = get_nested_attr(self, default_location)
            info["default_value_to_restore"] = deepcopy(default_value)

            # Override the default value
            locally_overriden_value = override_or_use_default_value(default_value, override)
            set_nested_attr(self, default_location, locally_overriden_value)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Restore default values upon exiting the context.
        """
        for property in self.overrides:
            info = self.values_with_defaults[property]
            if info["skip_overriding"]:
                continue

            # Restore the default value from before the context was entered
            default_location = info["default_location"]
            default_value = info["default_value_to_restore"]
            set_nested_attr(self, default_location, default_value)


def get_tokenizer_with_bos(
    tokenizer: transformers.PreTrainedTokenizerBase,
) -> transformers.PreTrainedTokenizerBase:
    """
    Returns the tokenizer initialized with add_bos_token=True.
    Such a tokenizer should be set as the default tokenizer because the tokenization of some
    tokenizers like LlamaTokenizer are different when bos token is automatically/manually
    prepended.

    Args:
        tokenizer (transformers.PreTrainedTokenizerBase): The tokenizer to initialize with add_bos_token=True.

    Returns:
        transformers.PreTrainedTokenizerBase: The tokenizer initialized with add_bos_token=True.
    """
    init_kwargs = deepcopy(tokenizer.init_kwargs)
    pretrained_model_name_or_path = init_kwargs.pop("name_or_path")
    add_bos_token = init_kwargs.pop("add_bos_token", None)
    if add_bos_token is None:
        add_bos_token = getattr(tokenizer, "add_bos_token", False)

    if add_bos_token:
        tokenizer_with_bos = tokenizer
    else:
        huggingface_token = os.environ.get("HF_TOKEN", "")
        tokenizer_with_bos = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            add_bos_token=True,
            token=huggingface_token if len(huggingface_token) > 0 else None,
            **init_kwargs,
        )

    return tokenizer_with_bos


def get_input_with_manually_prepended_bos(
    tokenizer: transformers.PreTrainedTokenizerBase, input: Union[str, list[str]]
):
    """
    Prepends a BOS token to the input, in a way that is compatible with the model's tokenizer.

    Args:
        tokenizer (transformers.PreTrainedTokenizerBase): The tokenizer to use for prepending the bos token.
        input (Union[str, list[str]]): The input to prepend the bos token to.

    Returns:
        Union[str, list[str]]: The input with the bos token manually prepended.
    """
    if isinstance(input, str):
        input = tokenizer.bos_token + input
    else:
        input = [tokenizer.bos_token + string for string in input]
    return input


def get_tokens_with_bos_removed(
    tokenizer: transformers.PreTrainedTokenizerBase,
    tokens: Int[torch.Tensor, "batch pos"],
):
    """
    Removes the bos token from the beginning of each sequence in `tokens`.
    The last dimension of `tokens` must be the sequence length.

    Args:
        tokenizer (transformers.PreTrainedTokenizerBase): The tokenizer used to tokenize the input.
        tokens (torch.Tensor): The tokenized input.

    Returns:
        torch.Tensor: The tokenized input with the bos token removed.
    """
    if tokenizer.padding_side == "right":
        return tokens[..., 1:]

    else:
        bos_removed_shape = list(tokens.shape)
        bos_removed_shape[-1] -= 1

        if tokenizer.bos_token_id == tokenizer.pad_token_id:
            is_not_pad_token = tokens.ne(tokenizer.pad_token_id)
            is_leading_pad = get_cumsum_along_dim(is_not_pad_token, -1, reverse=False) == 0
            real_bos_positions = is_leading_pad.sum(-1) - 1
        else:
            real_bos_positions = (tokens == tokenizer.bos_token_id).int().argmax(-1)

        tokens = tokens.scatter(dim=1, index=real_bos_positions.unsqueeze(-1), value=-100)
        return tokens[tokens != -100].view(*bos_removed_shape)


try:
    import pytest

    # Note: Docstring won't be tested with PyTest (it's ignored), as it thinks this is a regular unit
    # test (because its name is prefixed `test_`).
    pytest.mark.skip(test_prompt)
except ModuleNotFoundError:
    pass  # disregard if pytest not in env



---
File: /further_comments.md
---

# Further Details on Config Options
## Shortformer Attention (`positional_embeddings_type == "shortformer"`)
Shortformer style models are a variant on GPT-2 style positional embeddings, which do not add positional embeddings into the residual stream but instead add it in to the queries and keys immediately before multiplying by W_Q and W_K, and NOT having it around for the values or MLPs. It's otherwise the same - the positional embeddings are absolute, and are learned. The positional embeddings are NOT added to the residual stream in the standard way, and instead the queries and keys are calculated as W_Q(res_stream + pos_embed) and W_K(res_stream + pos_embed). The values and MLPs are calculated as W_V(res_stream) and W_MLP(res_stream) and so don't have access to positional information. This is otherwise the same as GPT-2 style positional embeddings. This is a variant on the Shortformer model from the paper [Shortformer: The Benefits of Shorter Sequences in Language Modeling](https://arxiv.org/abs/2012.15832). It's morally similar to rotary, which also only gives keys & queries access to positional info

The original intention was to use this to do more efficient caching: caching is hard with absolute positional embeddings, since you can't translate the context window without recomputing the entire thing, but easier if the prior values and residual stream terms are the same. I've mostly implemented it because it makes it easier for models to form induction heads. I'm not entirely sure why, though hypothesise that it's because there's two ways for induction heads to form with positional embeddings in the residual stream and only one with shortformer style positional embeddings.
    
# Weight Processing
## What is LayerNorm Folding? (`fold_ln`)
[LayerNorm](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1) is a common regularisation technique used in transformers. Annoyingly, unlike eg BatchNorm, it can't be turned off at inference time, it's a meaningful change to the mathematical function implemented by the transformer. From an interpretability perspective, this is a headache! And it's easy to shoot yourself in the foot by naively ignoring it - eg, making the mistake of saying neuron_pre = resid_mid @ W_in, rather than LayerNorm(resid_mid) @ W_in. This mistake is an OK approximation, but by folding in the LayerNorm we can do much better!

TLDR: If we have LayerNorm (weights w_ln and b_ln) followed by a linear layer (W+b), we can reduce the LayerNorm to LayerNormPre (just centering & normalising) and follow it by a linear layer with `W_eff = w[:, None] * W` (element-wise multiplication) and `b_eff = b + b_ln @ W`. This is computationally equivalent, and it never makes sense to think of W and w_ln as separate objects, so HookedTransformer handles it for you when loading pre-trained weights - set fold_ln = False when loading a state dict if you want to turn this off
        
Mathematically, LayerNorm is the following:
```
x1 = x0 - x0.mean()
x2 = x1 / ((x1**2).mean()).sqrt()
x3 = x2 * w
x4 = x3 + b
```
        
Apart from dividing by the norm, these are all pretty straightforwards operations from a linear algebra perspective. And from an interpretability perspective, if anything is linear, it's really easy and you can mostly ignore it (everything breaks up into sums, you can freely change basis, don't need to track interference between terms, etc) - the hard part is engaging with non-linearities!
        
A key thing to bear in mind is that EVERY time we read from the residual stream, we apply a LayerNorm - this gives us a lot of leverage to reason about it!
        
So let's translate this into linear algebra notation.
        `x0` is a vector in `R^n`

```
x1 = x0 - x0.mean()
   = x0 - (x0.mean()) * ones (broadcasting, ones=torch.ones(n))
   = x0 - (x0 @ ones/sqrt(n)) * ones/sqrt(n).
```

ones has norm sqrt(n), so ones/sqrt(n) is the unit vector in the diagonal direction. We're just projecting x0 onto this (fixed) vector and subtracting that value off. Alternately, we're projecting onto the n-1 dimensional subspace orthogonal to ones.
            
Since LayerNorm is applied EVERY time we read from the stream, the model just never uses the ones direction of the residual stream, so it's essentially just decreasing d_model by one. We can simulate this by just centering all matrices writing to the residual stream.

Why is removing this dimension useful? I have no idea! I'm not convinced it is...
        
```
x2 = x1 / ((x1**2).mean()).sqrt() (Ignoring eps)
   = (x1 / x1.norm()) * sqrt(n)
```

This is a projection onto the unit sphere (well, sphere of radius sqrt(n) - the norm of ones). This is fundamentally non-linear, eg doubling the input keeps the output exactly the same.

This is by far the most irritating part of LayerNorm. I THINK it's mostly useful for numerical stability reasons and not used to do useful computation by the model, but I could easily be wrong! And interpreting a circuit containing LayerNorm sounds like a nightmare...

In practice, you can mostly get aware with ignore this and treating the scaling factor as a constant, since it does apply across the entire residual stream for each token - this makes it a "global" property of the model's calculation, so for any specific question it hopefully doesn't matter that much. But when you're considering a sufficiently important circuit that it's a good fraction of the norm of the residual stream, it's probably worth thinking about.

```
x3 = x2 * w
   = x2 @ W_ln
```

(`W_ln` is a diagonal matrix with the weights of the LayerNorm - this is equivalent to element-wise multiplication)
This is really easy to deal with - we're about to be input to a linear layer, and can say `(x2 @ W_ln) @ W = x2 @ (W_ln @ W) = x2 @ W_eff` - we can just fold the LayerNorm weights into the linear layer weights.
        
`x4 = x3 + b` is similarly easy - `x4 @ W + B = x2 @ W_eff + B_eff`, where `W_eff = W_ln @ W` and `B_eff = B + b @ W`
        
This function is calculating `W_eff` and `B_eff` for each layer reading from the residual stream and replacing W and B with those.

A final optimisation we can make is to **center the reading weights**. x2 has mean 0, which means it's orthogonal to the vector of all ones (`x2 @ ones = x2.sum() = len(x2) * x2.mean()`). This means that the component of `W_eff` that's parallel to `ones` is irrelevant, and we can set that to zero. In code, this means `W_eff -= W_eff.mean(dim=0, keepdim=True)`. This doesn't change the computation but makes things a bit simpler.
        
See this for more: https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization

## Centering Writing Weights (`center_writing_weight`)

A related idea to folding layernorm - *every* component reading an input from the residual stream is preceded by a LayerNorm, which means that the mean of a residual stream vector (ie the component in the direction of all ones) never matters. This means we can remove the all ones component of weights and biases whose output *writes* to the residual stream. Mathematically, `W_writing -= W_writing.mean(dim=1, keepdim=True)`

## Centering Unembed (`center_unembed`)

The logits are fed into a softmax. Softmax is translation invariant (eg, adding 1 to every logit doesn't change the output), so we can simplify things by setting the mean of the logits to be zero. This is equivalent to setting the mean of every output vector of `W_U` to zero. In code, `W_U -= W_U.mean(dim=-1, keepdim=True)`

## Fold Value Biases (`fold_value_biases`)

Each attention head has a value bias. Values are averaged to create mixed values (`z`), weighted by the attention pattern, but as the bias is constant, its contribution to `z` is exactly the same. The output of a head is `z @ W_O`, and so the value bias just linearly adds to the output of the head. This means that the value bias of a head has *nothing to do with the head*, and is just a constant added to the attention layer outputs. We can take the sum across these and `b_O` to get an "effective bias" for the layer. In code, we set `b_V=0.` and `b_O = (b_V @ W_O).sum(dim=0) + b_O`

<details><summary>Technical derivation</summary>

`v = residual @ W_V[h] + broadcast_b_V[h]` for each head `h` (where `b_V` is broadcast up from shape `d_head` to shape `[position, d_head]`). And `z = pattern[h] @ v = pattern[h] @ residual @ W_V[h] + pattern[h] @ broadcast_b_V[h]`. Because `pattern[h]` is `[destination_position, source_position]` and `broadcast_b_V` is *constant* along the `(source_)position` dimension, we're basically just multiplying it by the sum of the pattern across the `source_position` dimension, which is just 1. So it remains exactly the same, and so is just brodcast across the destination positions. 
</details>



---
File: /README.md
---

# TransformerLens

<!-- Status Icons -->
[![Pypi](https://img.shields.io/pypi/v/transformer-lens?color=blue)](https://pypi.org/project/transformer-lens/)
![Pypi Total Downloads](https://img.shields.io/pepy/dt/transformer_lens?color=blue) ![PyPI -
License](https://img.shields.io/pypi/l/transformer_lens?color=blue) [![Release
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml)
[![Tests
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml)
[![Docs
CD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment)

A Library for Mechanistic Interpretability of Generative Language Models. Maintained by [Bryce Meyer](https://github.com/bryce13950) and created by [Neel Nanda](https://neelnanda.io/about)

[![Read the Docs
Here](https://img.shields.io/badge/-Read%20the%20Docs%20Here-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white&link=https://TransformerLensOrg.github.io/TransformerLens/)](https://TransformerLensOrg.github.io/TransformerLens/)

This is a library for doing [mechanistic
interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models. The
goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms
the model learned during training from its weights.

TransformerLens lets you load in 50+ different open source language models, and exposes the internal
activations of the model to you. You can cache any internal activation in the model, and add in
functions to edit, remove or replace these activations as the model runs.

## Quick Start

### Install

```shell
pip install transformer_lens
```

### Use

```python
import transformer_lens

# Load a model (eg GPT-2 Small)
model = transformer_lens.HookedTransformer.from_pretrained("gpt2-small")

# Run the model and get logits and activations
logits, activations = model.run_with_cache("Hello World")
```

## Key Tutorials

* [Introduction to the Library and Mech
  Interp](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)
* [Demo of Main TransformerLens Features](https://neelnanda.io/transformer-lens-demo)

## Gallery

Research done involving TransformerLens:

<!-- If you change this also change docs/source/content/gallery.md -->
* [Progress Measures for Grokking via Mechanistic
  Interpretability](https://arxiv.org/abs/2301.05217) (ICLR Spotlight, 2023) by Neel Nanda, Lawrence
  Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt
* [Finding Neurons in a Haystack: Case Studies with Sparse
  Probing](https://arxiv.org/abs/2305.01610) by Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine
  Harvey, Dmitrii Troitskii, Dimitris Bertsimas
* [Towards Automated Circuit Discovery for Mechanistic
  Interpretability](https://arxiv.org/abs/2304.14997) by Arthur Conmy, Augustine N. Mavor-Parker,
  Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso
* [Actually, Othello-GPT Has A Linear Emergent World Representation](https://neelnanda.io/othello)
  by Neel Nanda
* [A circuit for Python docstrings in a 4-layer attention-only
  transformer](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)
  by Stefan Heimersheim and Jett Janiak
* [A Toy Model of Universality](https://arxiv.org/abs/2302.03025) (ICML, 2023) by Bilal Chughtai,
  Lawrence Chan, Neel Nanda
* [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language
  Models](https://openreview.net/forum?id=ZB6bK6MTYq) (2023, ICLR Workshop RTML) by Alex Foote, Neel
  Nanda, Esben Kran, Ioannis Konstas, Fazl Barez
* [Eliciting Latent Predictions from Transformers with the Tuned
  Lens](https://arxiv.org/abs/2303.08112) by Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
  Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt

User contributed examples of the library being used in action:

* [Induction Heads Phase Change
  Replication](https://colab.research.google.com/github/ckkissane/induction-heads-transformer-lens/blob/main/Induction_Heads_Phase_Change.ipynb):
  A partial replication of [In-Context Learning and Induction
  Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
  from Connor Kissane
* [Decision Transformer
  Interpretability](https://github.com/jbloomAus/DecisionTransformerInterpretability): A set of
  scripts for training decision transformers which uses transformer lens to view intermediate
  activations, perform attribution and ablations. A write up of the initial work can be found
  [here](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability).

Check out [our demos folder](https://github.com/TransformerLensOrg/TransformerLens/tree/main/demos) for
more examples of TransformerLens in practice

## Getting Started in Mechanistic Interpretability

Mechanistic interpretability is a very young and small field, and there are a _lot_ of open
problems. This means there's both a lot of low-hanging fruit, and that the bar for entry is low - if
you would like to help, please try working on one! The standard answer to "why has no one done this
yet" is just that there aren't enough people! Key resources:

* [A Guide to Getting Started in Mechanistic Interpretability](https://neelnanda.io/getting-started)
* [ARENA Mechanistic Interpretability Tutorials](https://arena3-chapter1-transformer-interp.streamlit.app/) from
  Callum McDougall. A comprehensive practical introduction to mech interp, written in
  TransformerLens - full of snippets to copy and they come with exercises and solutions! Notable
  tutorials:
  * [Coding GPT-2 from
    scratch](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch), with
    accompanying video tutorial from me ([1](https://neelnanda.io/transformer-tutorial)
    [2](https://neelnanda.io/transformer-tutorial-2)) - a good introduction to transformers
  * [Introduction to Mech Interp and
    TransformerLens](https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp): An
    introduction to TransformerLens and mech interp via studying induction heads. Covers the
    foundational concepts of the library
  * [Indirect Object
    Identification](https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification):
    a replication of interpretability in the wild, that covers standard techniques in mech interp
    such as [direct logit
    attribution](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ),
    [activation patching and path
    patching](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)
* [Mech Interp Paper Reading List](https://neelnanda.io/paper-list)
* [200 Concrete Open Problems in Mechanistic
  Interpretability](https://neelnanda.io/concrete-open-problems)
* [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): To look
  up all the jargon and unfamiliar terms you're going to come across!
* [Neel Nanda's Youtube channel](https://www.youtube.com/channel/UCBMJ0D-omcRay8dh4QT0doQ): A range
  of mech interp video content, including [paper
  walkthroughs](https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hpsJYYhlt1WbHHgdfRLM2eY&index=1),
  and [walkthroughs of doing
  research](https://www.youtube.com/watch?v=yo4QvDn-vsU&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T)

## Support & Community

[![Contributing
Guide](https://img.shields.io/badge/-Contributing%20Guide-blue?style=for-the-badge&logo=GitHub&logoColor=white)](https://TransformerLensOrg.github.io/TransformerLens/content/contributing.html)

If you have issues, questions, feature requests or bug reports, please search the issues to check if
it's already been answered, and if not please raise an issue!

You're also welcome to join the open source mech interp community on
[Slack](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2n26nfoh1-TzMHrzyW6HiOsmCESxXtyw).
Please use issues for concrete discussions about the package, and Slack for higher bandwidth
discussions about eg supporting important new use cases, or if you want to make substantial
contributions to the library and want a maintainer's opinion. We'd also love for you to come and
share your projects on the Slack!

| :exclamation:  HookedSAETransformer Removed   |
|-----------------------------------------------|

Hooked SAE has been removed from TransformerLens in version 2.0. The functionality is being moved to
[SAELens](http://github.com/jbloomAus/SAELens). For more information on this release, please see the
accompanying
[announcement](https://transformerlensorg.github.io/TransformerLens/content/news/release-2.0.html)
for details on what's new, and the future of TransformerLens.

## Credits

This library was created by **[Neel Nanda](https://neelnanda.io)** and is maintained by **[Bryce Meyer](https://github.com/bryce13950)**.

The core features of TransformerLens were heavily inspired by the interface to [Anthropic's
excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson
Elhage and Chris Olah for building Garcon and showing the value of good infrastructure for enabling
exploratory research!

### Creator's Note (Neel Nanda)

I (Neel Nanda) used to work for the [Anthropic interpretability team](transformer-circuits.pub), and
I wrote this library because after I left and tried doing independent research, I got extremely
frustrated by the state of open source tooling. There's a lot of excellent infrastructure like
HuggingFace and DeepSpeed to _use_ or _train_ models, but very little to dig into their internals
and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get
into the field even if you don't work at an industry org with real infrastructure! One of the great
things about mechanistic interpretability is that you don't need large models or tons of compute.
There are lots of important open problems that can be solved with a small model in a Colab notebook!

### Citation

Please cite this library as:

```BibTeX
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}
```

# ARENA_Content.ipynb

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
IN_GITHUB = True
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False

if not IN_GITHUB and not IN_COLAB:
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_GITHUB or IN_COLAB:
    %pip install torch
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git@dev

from transformer_lens import HookedTransformer, HookedTransformerConfig
import torch as t

device = t.device("cuda" if t.cuda.is_available() else "cpu")
```

```python
# NBVAL_IGNORE_OUTPUT

reference_gpt2 = HookedTransformer.from_pretrained(
    "gpt2-small",
    fold_ln=False,
    center_unembed=False,
    center_writing_weights=False,
    device=device,
)
```

```python

# [1.1] Transformer From Scratch
# 1️⃣ UNDERSTANDING INPUTS & OUTPUTS OF A TRANSFORMER

sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])
first_vocab = sorted_vocab[0]
assert isinstance(first_vocab, tuple)
assert isinstance(first_vocab[0], str)
first_vocab[1]
```

```python
reference_gpt2.to_str_tokens("Ralph")
```

```python
reference_gpt2.to_str_tokens(" Ralph")
```

```python

reference_gpt2.to_str_tokens(" ralph")

```

```python
reference_gpt2.to_str_tokens("ralph")
```

```python

reference_text = "I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!"
tokens = reference_gpt2.to_tokens(reference_text)
tokens.shape

```

```python

logits, cache = reference_gpt2.run_with_cache(tokens, device=device)
logits.shape

```

```python

most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])
most_likely_next_tokens[-1]

```

```python
# 2️⃣ CLEAN TRANSFORMER IMPLEMENTATION

layer_0_hooks = [
    (name, tuple(tensor.shape)) for name, tensor in cache.items() if ".0." in name
]
non_layer_hooks = [
    (name, tuple(tensor.shape)) for name, tensor in cache.items() if "blocks" not in name
]

sorted(non_layer_hooks, key=lambda x: x[0])

```

```python

sorted(layer_0_hooks, key=lambda x: x[0])
```

```python
# NBVAL_IGNORE_OUTPUT
# [1.2] Intro to mech interp
# 2️⃣ FINDING INDUCTION HEADS

cfg = HookedTransformerConfig(
    d_model=768,
    d_head=64,
    n_heads=12,
    n_layers=2,
    n_ctx=2048,
    d_vocab=50278,
    attention_dir="causal",
    attn_only=True, # defaults to False
    tokenizer_name="EleutherAI/gpt-neox-20b",
    seed=398,
    use_attn_result=True,
    normalization_type=None, # defaults to "LN", i.e. layernorm with weights & biases
    positional_embedding_type="shortformer"
)
model = HookedTransformer(cfg)
```

```python

text = "We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this."

logits, cache = model.run_with_cache(text, remove_batch_dim=True)

logits.shape
```

```python
cache["embed"].ndim
```

---

# Activation_Patching_in_TL_Demo.ipynb


---

# Attribution_Patching_Demo.ipynb


---

# BERT.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/BERT.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# BERT in TransformerLens
This demo shows how to use BERT in TransformerLens for the Masked Language Modelling and Next Sentence Prediction task.

# Setup
(No need to read)

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False

if not IN_GITHUB and not IN_COLAB:
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB:
    %pip install transformer_lens
    %pip install circuitsvis
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
import circuitsvis as cv

# Testing that the library works
cv.examples.hello("Neel")
```

```python
# Import stuff
import torch

from transformers import AutoTokenizer

from transformer_lens import HookedEncoder, BertNextSentencePrediction
```

```python
torch.set_grad_enabled(False)
```

# BERT

In this section, we will load a pretrained BERT model and use it for the Masked Language Modelling and Next Sentence Prediction task

```python
# NBVAL_IGNORE_OUTPUT
bert = HookedEncoder.from_pretrained("bert-base-cased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

## Masked Language Modelling
Use the "[MASK]" token to mask any tokens which you would like the model to predict.
When specifying return_type="predictions" the prediction of the model is returned, alternatively (and by default) the function returns logits.
You can also specify None as return type for which nothing is returned

```python
prompt = "The [MASK] is bright today."

prediction = bert(prompt, return_type="predictions")

print(f"Prompt: {prompt}")
print(f'Prediction: "{prediction}"')
```

You can also input a list of prompts:

```python
prompts = ["The [MASK] is bright today.", "She [MASK] to the store.", "The dog [MASK] the ball."]

predictions = bert(prompts, return_type="predictions")

print(f"Prompt: {prompts}")
print(f'Prediction: "{predictions}"')
```

## Next Sentence Prediction
To carry out Next Sentence Prediction, you have to use the class BertNextSentencePrediction, and pass a HookedEncoder in its constructor.
Then, create a list with the two sentences you want to perform NSP on as elements and use that as input to the forward function.
The model will then predict the probability of the sentence at position 1 following (i.e. being the next sentence) to the sentence at position 0.

```python
nsp = BertNextSentencePrediction(bert)
sentence_a = "A man walked into a grocery store."
sentence_b = "He bought an apple."

input = [sentence_a, sentence_b]

predictions = nsp(input, return_type="predictions")

print(f"Sentence A: {sentence_a}")
print(f"Sentence B: {sentence_b}")
print(f'Prediction: "{predictions}"')
```

# Inputting tokens directly
You can also input tokens instead of a string or a list of strings into the model, which could look something like this

```python
prompt = "The [MASK] is bright today."

tokens = tokenizer(prompt, return_tensors="pt")["input_ids"]
logits = bert(tokens) # Since we are not specifying return_type, we get the logits
logprobs = logits[tokens == tokenizer.mask_token_id].log_softmax(dim=-1)
prediction = tokenizer.decode(logprobs.argmax(dim=-1).item())

print(f"Prompt: {prompt}")
print(f'Prediction: "{prediction}"')
```

Well done, BERT!

---

# Colab_Compatibility.ipynb

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"

try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    # %pip install sentencepiece # Llama tokenizer requires sentencepiece
    %pip install transformers>=4.31.0 # Llama requires transformers>=4.31.0 and transformers in turn requires Python 3.8
    %pip install torch
    %pip install tiktoken
    # %pip install transformer_lens
    %pip install transformers_stream_generator
    # !huggingface-cli login --token NEEL'S TOKEN
```

```python
import torch
from transformer_lens import HookedTransformer, HookedEncoderDecoder, HookedEncoder, BertNextSentencePrediction, loading
from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer
from typing import List
import gc

untested_models = []
untested_models.extend(loading.OFFICIAL_MODEL_NAMES)

print("TransformerLens currently supports " + str(len(untested_models)) + " models out of the box.")

GENERATE = True
# Fill this in if you have llama weights uploaded, and you with to test those models
LLAMA_MODEL_PATH = ""
```

```python
def mark_models_as_tested(model_set: List[str]) -> None:
    for model in model_set:
        untested_models.remove(model)

def run_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tl_model = HookedTransformer.from_pretrained_no_processing(model, device=device)
        if GENERATE:
            print(tl_model.generate("Hello my name is"))
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_llama_set(model_set: List[str], weight_root: str, device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        # to run this, make sure weight root is the root that contains all models with the
        # sub directories sharing the same name as the model in the list of models
        tokenizer = LlamaTokenizer.from_pretrained(weight_root + model)
        hf_model = LlamaForCausalLM.from_pretrained(weight_root + model, low_cpu_mem_usage=True)
        tl_model = HookedTransformer.from_pretrained_no_processing(
            model,
            hf_model=hf_model,
            device=device,
            fold_ln=False,
            center_writing_weights=False,
            center_unembed=False,
            tokenizer=tokenizer,
        )
        if GENERATE:
            print(tl_model.generate("Hello my name is"))
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_encoder_decoder_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tokenizer = AutoTokenizer.from_pretrained(model)
        tl_model = HookedEncoderDecoder.from_pretrained(model, device=device)
        if GENERATE:
            # Originally from the t5 demo
            prompt = "Hello, how are you? "
            inputs = tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"]
            attention_mask = inputs["attention_mask"]
            decoder_input_ids = torch.tensor([[tl_model.cfg.decoder_start_token_id]]).to(input_ids.device)

            while True:
                logits = tl_model.forward(input=input_ids, one_zero_attention_mask=attention_mask, decoder_input=decoder_input_ids)
                # logits.shape == (batch_size (1), predicted_pos, vocab_size)

                token_idx = torch.argmax(logits[0, -1, :]).item()
                print("generated token: \"", tokenizer.decode(token_idx), "\", token id: ", token_idx, sep="")

                # append token to decoder_input_ids
                decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[token_idx]]).to(input_ids.device)], dim=-1)

                # break if End-Of-Sequence token generated
                if token_idx == tokenizer.eos_token_id:
                    break
        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*

def run_encoder_only_set(model_set: List[str], device="cuda") -> None:
    for model in model_set:
        print("Testing " + model)
        tl_model = HookedEncoder.from_pretrained(model, device=device)
        tl_model_nsp = NextSentencePrediction.from_pretrained(model, device=device)

        if GENERATE:
            print("Testing Masked Language Modelling:")
            # Slightly adapted version of the BERT demo
            prompt = "The capital of France is [MASK]."

            prediction = tl_model(prompt, return_type="predictions")

            print(f"Prompt: {prompt}")
            print(f'Prediction: "{prediction}"')

            print("Testing Next Sentence Prediction:")
            sentence_a = "She went to the grocery store."
            sentence_b = "She bought some milk."

            prediction = tl_model_nsp([sentence_a, sentence_b], return_type="predictions")

            print(f"Sentence A: {sentence_a}")
            print(f"Sentence B: {sentence_b}")
            print(f"Prediction: {prediction}")

        del tl_model
        gc.collect()
        if IN_COLAB:
            %rm -rf /root/.cache/huggingface/hub/models*
```

```python
# The following models can run in the T4 free environment
free_compatible = [
    "ai-forever/mGPT",
    "ArthurConmy/redwood_attn_2l",
    "bigcode/santacoder",
    "bigscience/bloom-1b1",
    "bigscience/bloom-560m",
    "distilgpt2",
    "EleutherAI/gpt-neo-1.3B",
    "EleutherAI/gpt-neo-125M",
    "EleutherAI/gpt-neo-2.7B",
    "EleutherAI/pythia-1.4b",
    "EleutherAI/pythia-1.4b-deduped",
    "EleutherAI/pythia-1.4b-deduped-v0",
    "EleutherAI/pythia-1.4b-v0",
    "EleutherAI/pythia-14m",
    "EleutherAI/pythia-160m",
    "EleutherAI/pythia-160m-deduped",
    "EleutherAI/pythia-160m-deduped-v0",
    "EleutherAI/pythia-160m-seed1",
    "EleutherAI/pythia-160m-seed2",
    "EleutherAI/pythia-160m-seed3",
    "EleutherAI/pythia-160m-v0",
    "EleutherAI/pythia-1b",
    "EleutherAI/pythia-1b-deduped",
    "EleutherAI/pythia-1b-deduped-v0",
    "EleutherAI/pythia-1b-v0",
    "EleutherAI/pythia-31m",
    "EleutherAI/pythia-410m",
    "EleutherAI/pythia-410m-deduped",
    "EleutherAI/pythia-410m-deduped-v0",
    "EleutherAI/pythia-410m-v0",
    "EleutherAI/pythia-70m",
    "EleutherAI/pythia-70m-deduped",
    "EleutherAI/pythia-70m-deduped-v0",
    "EleutherAI/pythia-70m-v0",
    "facebook/opt-1.3b",
    "facebook/opt-125m",
    "gpt2",
    "gpt2-large",
    "gpt2-medium",
    "gpt2-xl",
    "meta-llama/Llama-3.2-1B",
    "meta-llama/Llama-3.2-1B-Instruct",
    "microsoft/phi-1",
    "microsoft/phi-1_5",
    "NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr",
    "NeelNanda/Attn_Only_1L512W_C4_Code",
    "NeelNanda/Attn_Only_2L512W_C4_Code",
    "NeelNanda/Attn_Only_3L512W_C4_Code",
    "NeelNanda/Attn_Only_4L512W_C4_Code",
    "NeelNanda/GELU_1L512W_C4_Code",
    "NeelNanda/GELU_2L512W_C4_Code",
    "NeelNanda/GELU_3L512W_C4_Code",
    "NeelNanda/GELU_4L512W_C4_Code",
    "NeelNanda/SoLU_10L1280W_C4_Code",
    "NeelNanda/SoLU_10L_v22_old",
    "NeelNanda/SoLU_12L1536W_C4_Code",
    "NeelNanda/SoLU_12L_v23_old",
    "NeelNanda/SoLU_1L512W_C4_Code",
    "NeelNanda/SoLU_1L512W_Wiki_Finetune",
    "NeelNanda/SoLU_1L_v9_old",
    "NeelNanda/SoLU_2L512W_C4_Code",
    "NeelNanda/SoLU_2L_v10_old",
    "NeelNanda/SoLU_3L512W_C4_Code",
    "NeelNanda/SoLU_4L512W_C4_Code",
    "NeelNanda/SoLU_4L512W_Wiki_Finetune",
    "NeelNanda/SoLU_4L_v11_old",
    "NeelNanda/SoLU_6L768W_C4_Code",
    "NeelNanda/SoLU_6L_v13_old",
    "NeelNanda/SoLU_8L1024W_C4_Code",
    "NeelNanda/SoLU_8L_v21_old",
    "Qwen/Qwen-1_8B",
    "Qwen/Qwen-1_8B-Chat",
    "Qwen/Qwen1.5-0.5B",
    "Qwen/Qwen1.5-0.5B-Chat",
    "Qwen/Qwen1.5-1.8B",
    "Qwen/Qwen1.5-1.8B-Chat",
    "Qwen/Qwen2-0.5B",
    "Qwen/Qwen2-0.5B-Instruct",
    "Qwen/Qwen2-1.5B",
    "Qwen/Qwen2-1.5B-Instruct",
    "Qwen/Qwen2.5-0.5B",
    "Qwen/Qwen2.5-0.5B-Instruct",
    "Qwen/Qwen2.5-1.5B",
    "Qwen/Qwen2.5-1.5B-Instruct",
    "Qwen/Qwen3-0.6B",
    "Qwen/Qwen3-1.7B",
    "roneneldan/TinyStories-1Layer-21M",
    "roneneldan/TinyStories-1M",
    "roneneldan/TinyStories-28M",
    "roneneldan/TinyStories-2Layers-33M",
    "roneneldan/TinyStories-33M",
    "roneneldan/TinyStories-3M",
    "roneneldan/TinyStories-8M",
    "roneneldan/TinyStories-Instruct-1M",
    "roneneldan/TinyStories-Instruct-28M",
    "roneneldan/TinyStories-Instruct-2Layers-33M",
    "roneneldan/TinyStories-Instruct-33M",
    "roneneldan/TinyStories-Instruct-3M",
    "roneneldan/TinyStories-Instruct-8M",
    "roneneldan/TinyStories-Instuct-1Layer-21M",
    "stanford-crfm/alias-gpt2-small-x21",
    "stanford-crfm/arwen-gpt2-medium-x21",
    "stanford-crfm/battlestar-gpt2-small-x49",
    "stanford-crfm/beren-gpt2-medium-x49",
    "stanford-crfm/caprica-gpt2-small-x81",
    "stanford-crfm/celebrimbor-gpt2-medium-x81",
    "stanford-crfm/darkmatter-gpt2-small-x343",
    "stanford-crfm/durin-gpt2-medium-x343",
    "stanford-crfm/eowyn-gpt2-medium-x777",
    "stanford-crfm/expanse-gpt2-small-x777",
]

if IN_COLAB:
    run_set(free_compatible)

mark_models_as_tested(free_compatible)
```

```python
paid_gpu_models = [
    "01-ai/Yi-6B",
    "01-ai/Yi-6B-Chat",
    "bigscience/bloom-1b7",
    "bigscience/bloom-3b",
    "bigscience/bloom-7b1",
    "codellama/CodeLlama-7b-hf",
    "codellama/CodeLlama-7b-Instruct-hf",
    "codellama/CodeLlama-7b-Python-hf",
    "EleutherAI/pythia-2.8b",
    "EleutherAI/pythia-2.8b-deduped",
    "EleutherAI/pythia-2.8b-deduped-v0",
    "EleutherAI/pythia-2.8b-v0",
    "EleutherAI/pythia-6.9b",
    "EleutherAI/pythia-6.9b-deduped",
    "EleutherAI/pythia-6.9b-deduped-v0",
    "EleutherAI/pythia-6.9b-v0",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "google/gemma-2-2b",
    "google/gemma-2-2b-it",
    "google/gemma-2b",
    "google/gemma-2b-it",
    "google/gemma-7b",
    "google/gemma-7b-it",
    "meta-llama/Llama-2-7b-chat-hf",
    "meta-llama/Llama-2-7b-hf",
    "meta-llama/Llama-3.1-8B",
    "meta-llama/Llama-3.1-8B-Instruct",
    "meta-llama/Llama-3.2-3B",
    "meta-llama/Llama-3.2-3B-Instruct",
    "meta-llama/Meta-Llama-3-8B",
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "microsoft/phi-2",
    "microsoft/Phi-3-mini-4k-instruct",
    "mistralai/Mistral-7B-Instruct-v0.1",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mistral-Nemo-Base-2407",
    "mistralai/Mistral-Small-24B-Base-2501",
    "Qwen/Qwen-7B",
    "Qwen/Qwen-7B-Chat",
    "Qwen/Qwen1.5-4B",
    "Qwen/Qwen1.5-4B-Chat",
    "Qwen/Qwen1.5-7B",
    "Qwen/Qwen1.5-7B-Chat",
    "Qwen/Qwen2-7B",
    "Qwen/Qwen2-7B-Instruct",
    "Qwen/Qwen2.5-3B",
    "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-7B",
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-8B",
    "stabilityai/stablelm-base-alpha-3b",
    "stabilityai/stablelm-base-alpha-7b",
    "stabilityai/stablelm-tuned-alpha-3b",
    "stabilityai/stablelm-tuned-alpha-7b",
]

if IN_COLAB:
    run_set(paid_gpu_models)

mark_models_as_tested(paid_gpu_models)
```

```python
paid_cpu_models = [
    "EleutherAI/gpt-j-6B",
    "EleutherAI/gpt-neox-20b",
    "EleutherAI/pythia-12b",
    "EleutherAI/pythia-12b-deduped",
    "EleutherAI/pythia-12b-deduped-v0",
    "EleutherAI/pythia-12b-v0",
    "facebook/opt-13b",
    "google/gemma-2-9b",
    "google/gemma-2-9b-it",
    "meta-llama/Llama-2-13b-chat-hf",
    "meta-llama/Llama-2-13b-hf",
    "microsoft/phi-4",
    "Qwen/Qwen-14B",
    "Qwen/Qwen-14B-Chat",
    "Qwen/Qwen1.5-14B",
    "Qwen/Qwen1.5-14B-Chat",
    "Qwen/Qwen2.5-14B",
    "Qwen/Qwen2.5-14B-Instruct",
]

if IN_COLAB:
    run_set(paid_cpu_models, "cpu")

mark_models_as_tested(paid_cpu_models)
```

```python
incompatible_models = [
    "01-ai/Yi-34B",
    "01-ai/Yi-34B-Chat",
    "facebook/opt-30b",
    "facebook/opt-66b",
    "google/gemma-2-27b",
    "google/gemma-2-27b-it",
    "meta-llama/Llama-2-70b-chat-hf",
    "meta-llama/Llama-3.1-70B",
    "meta-llama/Llama-3.1-70B-Instruct",
    "meta-llama/Llama-3.3-70B-Instruct",
    "meta-llama/Meta-Llama-3-70B",
    "meta-llama/Meta-Llama-3-70B-Instruct",
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "mistralai/Mixtral-8x7B-v0.1",
    "Qwen/Qwen2.5-32B",
    "Qwen/Qwen2.5-32B-Instruct",
    "Qwen/Qwen2.5-72B",
    "Qwen/Qwen2.5-72B-Instruct",
    "Qwen/Qwen3-14B",
    "Qwen/QwQ-32B-Preview",
]

mark_models_as_tested(incompatible_models)
```

```python
# The following models take a few extra steps to function. Check the official demo for more
# information on how to use. 7b and 13b will work in the paid environment. 30b and 65b will not work
# in Colab
not_hosted_models = [
    "llama-7b-hf",
    "llama-13b-hf",
    "llama-30b-hf",
    "llama-65b-hf",
]

if LLAMA_MODEL_PATH:
    run_llama_set(not_hosted_models, LLAMA_MODEL_PATH)

mark_models_as_tested(not_hosted_models)
```

```python
# These all work on the free version of Colab
encoder_decoders = [
    "google-t5/t5-base",
    "google-t5/t5-large",
    "google-t5/t5-small",
]
if IN_COLAB:
    run_encoder_decoder_set(encoder_decoders)

mark_models_as_tested(encoder_decoders)
```

```python
# This model works on the free version of Colab
encoder_only_models = [
    "google-bert/bert-base-cased",
    "google-bert/bert-base-uncased",
    "google-bert/bert-large-cased",
    "google-bert/bert-large-uncased",
]

if IN_COLAB:
    run_encoder_only_set(encoder_only_models)

mark_models_as_tested(encoder_only_models)
```

```python
broken_models = [
    "Baidicoot/Othello-GPT-Transformer-Lens",
]
```

```python
# Any models listed in the cell below have not been tested. This should always remain blank. If your
# PR fails due to this notebook, most likely you need to check any new model changes to ensure that
# this notebook is up to date.
print(*untested_models, sep="\n")
```

---

# Config_Overhaul.ipynb

# Overview

The current way configuration is designed in TransformerLens has a lot of limitations. It does not
allow for outside people to pass through configurations that are not officially supported, and it
is very bug prone with something as simple as typo potentially giving you a massive headache. There
are also a number of hidden rules that are not clearly documented, which can go hidden until
different pieces of TransformerLens are activated. Allowing to pass in an optional object of configuration
with no further changes does solve a couple of these problems, but it does not solve the bigger
issues. It also introduces new problems with users potentially passing in architectures that are not
supported without having a clear way to inform the user what isn't supported.

My proposal for how all of these problems can be resolved is to fundamentally revamp the
configuration to allow for something that I like to call configuration composition. From a technical
perspective, this involves creating a centralized class that describes all supported configurations
by TransformerLens. This class would then be used to construct specific configurations for all models
that are currently supported, and it would then allow anyone to easily see in a single place all
configuration features supported by TransformerLens while also being able to read the code to
understand how they can create their own configurations for the purpose of either submitting new
models into TransformerLens, or configuring an unofficially supported model by TransformerLens,
when TransformerLens already happens to support all of the architectural pieces separately.

This could simple be an overhaul of the existing HookedTransformerConfig. Everything I am
describing here could be made compatible with that class to give it a more usable interface that is
then directly interacted with by the end user. At the moment, that class is not really built to be
interacted with, and is instead used as a wrapper around spreading configured anonymous objects.
Overhauling this class to do what I am about to describe is a viable path, but keeping it as it is,
and making a new class as something meant to be used by the end user would be a way to maintain
compatibility, avoid refactors, and keep model configuration only focused on putting together
configuration for models, as opposed to configuring full settings needed by HookedTransformer, which
includes checking the available environment.

A very unscientific basic example of how this would look in code by the end user can be seen
immediately below. I will delve into details of each piece in this document.

```python
config = ModelConfig(
    d_model=4096,
    d_head=8192 // 64,
    n_heads=64,
    act_fn="silu"
    # Other universally required properties across all models go here in the constructor
)
# Enabling specific features not universal among all models
config.enabled_gated_mlp()
# Customizing optional attributes
config.set_positional_embedding_type("alibi")

# and so on, until the full configuration is set

```

## The constructor

The first piece of this I want to talk about is what will be injected into the constructor. It
should basically take everything absolutely required by all models. This keeps the code easy for
someone to understand, without adding too much clutter. All fields should be required, and if there
is ever an idea that a field should be in the constructor as an option, then that is probably an
indication that there is a good case to add a function to configure that variable in a different
point in the class. An example of what this would look like can be seen below...

```python
# make it easy for someone to see what activation functions are supported, this would be moved from
# HookedTransformerConfig
ActivationFunction = "silu" | "gelu"

class ModelConfig:
    def __init__(
        self,
        d_model: int,
        eps: int,
        act_fn: ActivationFunction,
        remaining_required_attributes,
    ):
        self.d_model = d_model
        self.eps = eps
        self.act_fn = act_fn
        # Set defaults for any remaining supported attributes that are not required here
        self.gated_mlp = False

```

## Boolean Variables

Within TransformerLens config, anything that is a boolean variable is essentially a feature flag.
This means that all features at the time of construction would have default values, most likely set
to false. They then get toggled on with an `enable_feature` function call on the config object.
Having these functions will make very clear for someone less familiar with TransformerLens what
features are available. It also allows us to decorate these calls, which is very important. There
are some instances where if a boolean is true, a different one cannot be true, but this requirement
is not clear anywhere without analyzing code. Decorating these functions allows us to make sure
these sort of bugs are not possible. I will use `gated_mlp` as an example here, but it is not
meant to be a real implementation.

```python
def enabled_gated_mlp(self: ModelConfig) -> ModelConfig:
    self.gated_mlp = True
    # Configure any side effects caused by enabling of a feature
    self.another_feature = False
    # Returning self allows someone to chain together config calls
    return self

ModelConfig.enabled_gated_mlp = enabled_gated_mlp
```

## Additional Options

Any other options would similarly have their own functions to configure. This allows for similar
decoration as with feature flags, and it also in a way documents the architectural capabilities of
TransformerLens in a single place. If there are groups of options that are also always required
together, this then gives us a way to require all of those options as opposed to having them all be
configured at the root level. This also allows us to make changes to other attributes that may be
affected as a side affect of having some values set, which again makes it both harder for people to
introduce bugs, and also creates code that documents itself. Another off the cuff example of
something like this can be seen below.

```python
def set_rotary_dim(self: ModelConfig, rotary_dim: int) -> ModelConfig:
    self.rotary_dim = rotary_dim
    # Additional settings that seem to be present whenever rotary_dim is set
    self.positional_embedding_type = "rotary"
    self.rotary_adjacent_pairs = False
    return self

ModelConfig.set_rotary_dim = set_rotary_dim
```

## Config Final Thoughts

The best way to describe this idea is configuration composition. The reason being is that the user is
essentially composing a model configuration by setting the base, and then combining various options
from predefined functions. Doing it like this has a lot of advantages. One of those advantages being
that there would need to be a lot less memorization on how architectures should be combined. e.g.
maybe it's not that hard to remember that `rotary_adjacent_pairs` should be False when `rotary_dim`
is set, but these sorts of combinations accumulate. Having it interfaced out gives everyone a
place to look to see how parts of configuration work in isolation without the need to memorize a
large amount of rules.

This would also allow us to more easily mock out fake configurations and enable specific features in
order to test that functionality in isolation. This also should make it easier for someone to at a
glance understand all model compatibilities with TransformerLens, since there would be a single file
where they would all be listed out and documented. It will also allow for people to see
compatibility limitations at a glance.

As for compatibility, this change would be 100% compatible with the existing structure. The objects
I am suggesting are abstractions of the existing configuration dictionaries for the purpose of
communication and ease of use. This means that they can be passed around just like the current
anonymous dictionaries.

## Further Changes

With this, there are a number of changes that I would like to make to the actual
`loading_from_pretrained` file in order to revise it to be ready for the possibility of rapidly
supporting new models. The biggest change in this respect would be to break out what is now a
configuration dictionary for every model into having its own module where one of these configuration
objects would be constructed. That object would then be exposed, so that it can be imported into
`loading_from_pretrained`. We would then create a dictionary where the official name of the
model would have the configuration object as its value, thus completely eliminating that big giant
if else statement, and replacing it with a simple return from the dictionary. The configurations
themselves would then live in a directory structure like so...

config/ <- where the ModelConfig file lives
config/meta-llama/ <- directory for all models from the group
config/meta-llama/Llama-2-13b.py <- name matching hugging face to make it really easy to find the
                                    configuration

## Impact on Testing

This change, would allow us to directly interact with these configuration objects to allow us to
more easily assert that configurations are set properly, and to also allow us to more easily access
these configurations in tests for the purposes of writing better unit tests.

## Summary

This change should solve a lot of problems. It may be a big change at first from what currently
exists, but in time I think most people will find it more elegant, and easier to understand.

```python

```

---

# Exploratory_Analysis_Demo.ipynb

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb)

# Exploratory Analysis Demo

This notebook demonstrates how to use the
[TransformerLens](https://github.com/TransformerLensOrg/TransformerLens/) library to perform exploratory
analysis. The notebook tries to replicate the analysis of the Indirect Object Identification circuit
in the [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper.

## Tips for Reading This

* If running in Google Colab, go to Runtime > Change Runtime Type and select GPU as the hardware
accelerator.
* Look up unfamiliar terms in [the mech interp explainer](https://neelnanda.io/glossary)
* You can run all this code for yourself
* The graphs are interactive
* Use the table of contents pane in the sidebar to navigate (in Colab) or VSCode's "Outline" in the
  explorer tab.
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar (with Colab) not CTRL+F

## Setup

### Environment Setup (ignore)

**You can ignore this part:** It's just for use internally to setup the tutorial in different
environments. You can delete this section if using in your own repo.

```python

# Detect if we're running in Google Colab
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False

# Install if in Colab
if IN_COLAB:
    %pip install transformer_lens
    %pip install circuitsvis
    # Install a faster Node version
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa

# Hot reload in development mode & not running on the CD
if not IN_COLAB:
    from IPython import get_ipython
    ip = get_ipython()
    if not ip.extension_manager.loaded:
        ip.extension_manager.load('autoreload')
        %autoreload 2

```

### Imports

```python
from functools import partial
from typing import List, Optional, Union

import einops
import numpy as np
import plotly.express as px
import plotly.io as pio
import torch
from circuitsvis.attention import attention_heads
from fancy_einsum import einsum
from IPython.display import HTML, IFrame
from jaxtyping import Float

import transformer_lens.utils as utils
from transformer_lens import ActivationCache, HookedTransformer
```

### PyTorch Setup

We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training.

```python
torch.set_grad_enabled(False)
print("Disabled automatic differentiation")
```

### Plotting Helper Functions (ignore)

Some plotting helper functions are included here (for simplicity).

```python
def imshow(tensor, **kwargs):
    px.imshow(
        utils.to_numpy(tensor),
        color_continuous_midpoint=0.0,
        color_continuous_scale="RdBu",
        **kwargs,
    ).show()

def line(tensor, **kwargs):
    px.line(
        y=utils.to_numpy(tensor),
        **kwargs,
    ).show()

def scatter(x, y, xaxis="", yaxis="", caxis="", **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(
        y=y,
        x=x,
        labels={"x": xaxis, "y": yaxis, "color": caxis},
        **kwargs,
    ).show()
```

## Introduction

This is a demo notebook for [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens), a library for mechanistic interpretability of GPT-2 style transformer language models. A core design principle of the library is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state.

The goal of this notebook is to demonstrate what exploratory analysis looks like in practice with the library. I use my standard toolkit of basic mechanistic interpretability techniques to try interpreting a real circuit in GPT-2 small. Check out [the main demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) for an introduction to the library and how to use it.

Stylistically, I will go fairly slowly and explain in detail what I'm doing and why, aiming to help convey how to do this kind of research yourself! But the code itself is written to be simple and generic, and easy to copy and paste into your own projects for different tasks and models.

Details tags contain asides, flavour + interpretability intuitions. These are more in the weeds and you don't need to read them or understand them, but they're helpful if you want to learn how to do mechanistic interpretability yourself! I star the ones I think are most important.
<details><summary>(*) Example details tag</summary>Example aside!</details>

### Indirect Object Identification

The first step when trying to reverse engineer a circuit in a model is to identify *what* capability
I want to reverse engineer. Indirect Object Identification is a task studied in Redwood Research's
excellent [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper (see [my interview
with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo) or [Kevin Wang's Twitter
thread](https://threadreaderapp.com/thread/1587601532639494146.html) for an overview). The task is
to complete sentences like "After John and Mary went to the shops, John gave a bottle of milk to"
with " Mary" rather than " John".

In the paper they rigorously reverse engineer a 26 head circuit, with 7 separate categories of heads
used to perform this capability. Their rigorous methods are fairly involved, so in this notebook,
I'm going to skimp on rigour and instead try to speed run the process of finding suggestive evidence
for this circuit!

The circuit they found roughly breaks down into three parts:
1. Identify what names are in the sentence
2. Identify which names are duplicated
3. Predict the name that is *not* duplicated

The first step is to load in our model, GPT-2 Small, a 12 layer and 80M parameter transformer with `HookedTransformer.from_pretrained`. The various flags are simplifications that preserve the model's output but simplify its internals.

```python
# NBVAL_IGNORE_OUTPUT
model = HookedTransformer.from_pretrained(
    "gpt2-small",
    center_unembed=True,
    center_writing_weights=True,
    fold_ln=True,
    refactor_factored_attn_matrices=True,
)

# Get the default device used
device: torch.device = utils.get_device()
```

The next step is to verify that the model can *actually* do the task! Here we use `utils.test_prompt`, and see that the model is significantly better at predicting Mary than John!

<details><summary>Asides:</summary>

Note: If we were being careful, we'd want to run the model on a range of prompts and find the average performance

`prepend_bos` is a flag to add a BOS (beginning of sequence) to the start of the prompt. GPT-2 was not trained with this, but I find that it often makes model behaviour more stable, as the first token is treated weirdly.
</details>

```python
example_prompt = "After John and Mary went to the store, John gave a bottle of milk to"
example_answer = " Mary"
utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)
```

We now want to find a reference prompt to run the model on. Even though our ultimate goal is to reverse engineer how this behaviour is done in general, often the best way to start out in mechanistic interpretability is by zooming in on a concrete example and understanding it in detail, and only *then* zooming out and verifying that our analysis generalises.

We'll run the model on 4 instances of this task, each prompt given twice - one with the first name as the indirect object, one with the second name. To make our lives easier, we'll carefully choose prompts with single token names and the corresponding names in the same token positions.

<details> <summary>(*) <b>Aside on tokenization</b></summary>

We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency!

Tokens are a *massive* headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`

**Exercise:** I recommend using `model.to_str_tokens` to explore how the model tokenizes different strings. In particular, try adding or removing spaces at the start, or changing capitalization - these change tokenization!</details>

```python
prompt_format = [
    "When John and Mary went to the shops,{} gave the bag to",
    "When Tom and James went to the park,{} gave the ball to",
    "When Dan and Sid went to the shops,{} gave an apple to",
    "After Martin and Amy went to the park,{} gave a drink to",
]
names = [
    (" Mary", " John"),
    (" Tom", " James"),
    (" Dan", " Sid"),
    (" Martin", " Amy"),
]
# List of prompts
prompts = []
# List of answers, in the format (correct, incorrect)
answers = []
# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)
answer_tokens = []
for i in range(len(prompt_format)):
    for j in range(2):
        answers.append((names[i][j], names[i][1 - j]))
        answer_tokens.append(
            (
                model.to_single_token(answers[-1][0]),
                model.to_single_token(answers[-1][1]),
            )
        )
        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.
        prompts.append(prompt_format[i].format(answers[-1][1]))
answer_tokens = torch.tensor(answer_tokens).to(device)
print(prompts)
print(answers)
```

**Gotcha**: It's important that all of your prompts have the same number of tokens. If they're different lengths, then the position of the "final" logit where you can check logit difference will differ between prompts, and this will break the below code. The easiest solution is just to choose your prompts carefully to have the same number of tokens (you can eg add filler words like The, or newlines to start).

There's a range of other ways of solving this, eg you can index more intelligently to get the final logit. A better way is to just use left padding by setting `model.tokenizer.padding_side = 'left'` before tokenizing the inputs and running the model; this way, you can use something like `logits[:, -1, :]` to easily access the final token outputs without complicated indexing. TransformerLens checks the value of `padding_side` of the tokenizer internally, and if the flag is set to be `'left'`, it adjusts the calculation of absolute position embedding and causal masking accordingly.

In this demo, though, we stick to using the prompts of the same number of tokens because we want to show some visualisations aggregated along the batch dimension later in the demo.

```python
for prompt in prompts:
    str_tokens = model.to_str_tokens(prompt)
    print("Prompt length:", len(str_tokens))
    print("Prompt as tokens:", str_tokens)
```

We now run the model on these prompts and use `run_with_cache` to get both the logits and a cache of all internal activations for later analysis

```python
tokens = model.to_tokens(prompts, prepend_bos=True)

# Run the model and cache all activations
original_logits, cache = model.run_with_cache(tokens)
```

We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the indirect object's name and the subject's name (eg, `logit(Mary)-logit(John)`).

```python
def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):
    # Only the final logits are relevant for the answer
    final_logits = logits[:, -1, :]
    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)
    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]
    if per_prompt:
        return answer_logit_diff
    else:
        return answer_logit_diff.mean()

print(
    "Per prompt logit difference:",
    logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)
    .detach()
    .cpu()
    .round(decimals=3),
)
original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)
print(
    "Average logit difference:",
    round(logits_to_ave_logit_diff(original_logits, answer_tokens).item(), 3),
)
```

We see that the average logit difference is 3.5 - for context, this represents putting an $e^{3.5}\approx 33\times$ higher probability on the correct answer.

## Brainstorm What's Actually Going On (Optional)

Before diving into running experiments, it's often useful to spend some time actually reasoning about how the behaviour in question could be implemented in the transformer. **This is optional, and you'll likely get the most out of engaging with this section if you have a decent understanding already of what a transformer is and how it works!**

You don't have to do this and forming hypotheses after exploration is also reasonable, but I think it's often easier to explore and interpret results with some grounding in what you might find. In this particular case, I'm cheating somewhat, since I know the answer, but I'm trying to simulate the process of reasoning about it!

Note that often your hypothesis will be wrong in some ways and often be completely off. We're doing science here, and the goal is to understand how the model *actually* works, and to form true beliefs! There are two separate traps here at two extremes that it's worth tracking:
* Confusion: Having no hypotheses at all, getting a lot of data and not knowing what to do with it, and just floundering around
* Dogmatism: Being overconfident in an incorrect hypothesis and being unwilling to let go of it when reality contradicts you, or flinching away from running the experiments that might disconfirm it.

**Exercise:** Spend some time thinking through how you might imagine this behaviour being implemented in a transformer. Try to think through this for yourself before reading through my thoughts!

<details> <summary>(*) <b>My reasoning</b></summary>

<h3>Brainstorming:</h3>

So, what's hard about the task? Let's focus on the concrete example of the first prompt, "When John and Mary went to the shops, John gave the bag to" -> " Mary".

A good starting point is thinking though whether a tiny model could do this, eg a <a href="https://transformer-circuits.pub/2021/framework/index.html">1L Attn-Only model</a>. I'm pretty sure the answer is no! Attention is really good at the primitive operations of looking nearby, or copying information. I can believe a tiny model could figure out that at `to` it should look for names and predict that those names came next (eg the skip trigram " John...to -> John"). But it's much harder to tell how <i>many</i> of each previous name there are - attending 0.3 to each copy of John will look exactly the same as attending 0.6 to a single John token. So this will be pretty hard to figure out on the " to" token!

The natural place to break this symmetry is on the second " John" token - telling whether there is an earlier copy of the <i>current</i> token should be a much easier task. So I might expect there to be a head which detects duplicate tokens on the second " John" token, and then another head which moves that information from the second " John" token to the " to" token.

The model then needs to learn to predict " Mary" and <i>not</i> " John". I can see two natural ways to do this:
1. Detect all preceding names and move this information to " to" and then delete the any name corresponding to the duplicate token feature. This feels easier done with a non-linearity, since precisely cancelling out vectors is hard, so I'd imagine an MLP layer deletes the " John" direction of the residual stream
2. Have a head which attends to all previous names, but where the duplicate token features <i>inhibit</i> it from attending to specific names. So this only attends to Mary. And then the output of this head maps to the logits.

(Spoiler: It's the second one).

<h3>Experiment Ideas</h3>

A test that could distinguish these two is to look at which components of the model add directly to the logits - if it's mostly attention heads which attend to " Mary" and to neither " John" it's probably hypothesis 2, if it's mostly MLPs it's probably hypothesis 1.

And we should be able to identify duplicate token heads by finding ones which attend from " John" to " John", and whose outputs are then moved to the " to" token by V-Composition with another head (Spoiler: It's more complicated than that!)

Note that all of the above reasoning is very simplistic and could easily break in a real model! There'll be significant parts of the model that figure out whether to use this circuit at all (we don't want to inhibit duplicated names when, eg, figuring out what goes at the start of the <i>next</i> sentence), and may be parts towards the end of the model that do "post-processing" just before the final output. But it's a good starting point for thinking about what's going on.

## Direct Logit Attribution

*Look up unfamiliar terms in the [mech interp explainer](https://neelnanda.io/glossary)*

Further, the easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards. The main technique used to do this is called **direct logit attribution**

**Background:** The central object of a transformer is the **residual stream**. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).

The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called **direct logit attribution**. Here we look at the direct attribution to the logit difference!

<details> <summary>(*) <b>Background and motivation of the logit difference</b></summary>

Logit difference is actually a *really* nice and elegant metric and is a particularly nice aspect of the setup of Indirect Object Identification. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities).

The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged.

But `log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)`, and so `log_probs(" Mary") - log_probs(" John") = logits(" Mary") - logits(" John")` - the ability to add an arbitrary constant cancels out!

Further, the metric helps us isolate the precise capability we care about - figuring out *which* name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.

Our metric is further refined, because each prompt is repeated twice, for each possible indirect object. This controls for irrelevant behaviour such as the model learning that John is a more frequent token than Mary (this actually happens! The final layernorm bias increases the John logit by 1 relative to the Mary logit)

</details>

<details> <summary>Ignoring LayerNorm</summary>

LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is *almost* a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts).

But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of *all* components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the `cache` to apply the global layernorm scaling factor to each constant. See [my clean GPT-2 implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Clean_Transformer_Implementation) for more on LayerNorm.
</details>

Getting an output logit is equivalent to projecting onto a direction in the residual stream. We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch

```python
answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)
print("Answer residual directions shape:", answer_residual_directions.shape)
logit_diff_directions = (
    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]
)
print("Logit difference directions shape:", logit_diff_directions.shape)
```

To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer.

<details> <summary>Technical details</summary>

`logits = Unembed(LayerNorm(final_residual_stream))`, so we technically need to account for the centering, and then learned translation and scaling of the layernorm, not just the variance 1 scaling.

The centering is accounted for with the preprocessing flag `center_writing_weights` which ensures that every weight matrix writing to the residual stream has mean zero.

The learned scaling is folded into the unembedding weights `model.unembed.W_U` via `W_U_fold = layer_norm.weights[:, None] * unembed.W_U`

The learned translation is folded to `model.unembed.b_U`, a bias added to the logits (note that GPT-2 is not trained with an existing `b_U`). This roughly represents unigram statistics. But we can ignore this because each prompt occurs twice with names in the opposite order, so this perfectly cancels out.

Note that rather than using layernorm scaling we could just study cache["ln_final.hook_normalised"]

</details>

```python
# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].
final_residual_stream = cache["resid_post", -1]
print("Final residual stream shape:", final_residual_stream.shape)
final_token_residual_stream = final_residual_stream[:, -1, :]
# Apply LayerNorm scaling
# pos_slice is the subset of the positions we take - here the final token of each prompt
scaled_final_token_residual_stream = cache.apply_ln_to_stack(
    final_token_residual_stream, layer=-1, pos_slice=-1
)

average_logit_diff = einsum(
    "batch d_model, batch d_model -> ",
    scaled_final_token_residual_stream,
    logit_diff_directions,
) / len(prompts)
print("Calculated average logit diff:", round(average_logit_diff.item(), 3))
print("Original logit difference:", round(original_average_logit_diff.item(), 3))
```

### Logit Lens

We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers.

```python
def residual_stack_to_logit_diff(
    residual_stack: Float[torch.Tensor, "components batch d_model"],
    cache: ActivationCache,
) -> float:
    scaled_residual_stack = cache.apply_ln_to_stack(
        residual_stack, layer=-1, pos_slice=-1
    )
    return einsum(
        "... batch d_model, batch d_model -> ...",
        scaled_residual_stack,
        logit_diff_directions,
    ) / len(prompts)
```

Fascinatingly, we see that the model is utterly unable to do the task until layer 7, almost all performance comes from attention layer 9, and performance actually *decreases* from there.

**Note:** Hover over each data point to see what residual stream position it's from!

<details> <summary>Details on `accumulated_resid`</summary>
**Key:** `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)

* `layer` is the layer for which we input the residual stream (this is used to identify *which* layer norm scaling factor we want)
* `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention & before MLP
* `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.
* return_labels is whether to return the labels for each component returned (useful for plotting)
</details>

```python
accumulated_residual, labels = cache.accumulated_resid(
    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True
)
logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)
line(
    logit_lens_logit_diffs,
    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,
    hover_name=labels,
    title="Logit Difference From Accumulate Residual Stream",
)
```

### Layer Attribution

We can repeat the above analysis but for each layer (this is equivalent to the differences between adjacent residual streams)

Note: Annoying terminology overload - layer k of a transformer means the kth **transformer block**, but each block consists of an **attention layer** (to move information around) *and* an **MLP layer** (to process information).

We see that only attention layers matter, which makes sense! The IOI task is about moving information around (ie moving the correct name and not the incorrect name), and less about processing it. And again we note that attention layer 9 improves things a lot, while attention 10 and attention 11 *decrease* performance

```python
per_layer_residual, labels = cache.decompose_resid(
    layer=-1, pos_slice=-1, return_labels=True
)
per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)
line(per_layer_logit_diffs, hover_name=labels, title="Logit Difference From Each Layer")
```

## Head Attribution

We can further break down the output of each attention layer into the sum of the outputs of each attention head. Each attention layer consists of 12 heads, which each act independently and additively.

<details> <summary>Decomposing attention output into sums of heads</summary>
The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer)
</details>

We see that only a few heads really matter - heads L9H6 and L9H9 contribute a lot positively (explaining why attention layer 9 is so important), while heads L10H7 and L11H10 contribute a lot negatively (explaining why attention layer 10 and layer 11 are actively harmful). These correspond to (some of) the name movers and negative name movers discussed in the paper. There are also several heads that matter positively or negatively but less strongly (other name movers and backup name movers)

There are a few meta observations worth making here - our model has 144 heads, yet we could localise this behaviour to a handful of specific heads, using straightforward, general techniques. This supports the claim in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) that attention heads are the right level of abstraction to understand attention. It also really surprising that there are *negative* heads - eg L10H7 makes the incorrect logit 7x *more* likely. I'm not sure what's going on there, though the paper discusses some possibilities.

```python
per_head_residual, labels = cache.stack_head_results(
    layer=-1, pos_slice=-1, return_labels=True
)
per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)
per_head_logit_diffs = einops.rearrange(
    per_head_logit_diffs,
    "(layer head_index) -> layer head_index",
    layer=model.cfg.n_layers,
    head_index=model.cfg.n_heads,
)
imshow(
    per_head_logit_diffs,
    labels={"x": "Head", "y": "Layer"},
    title="Logit Difference From Each Head",
)
```

## Attention Analysis

Attention heads are particularly easy to study because we can look directly at their attention patterns and study from what positions they move information from and two. This is particularly easy here as we're looking at the direct effect on the logits so we need only look at the attention patterns from the final token.

We use Alan Cooney's circuitsvis library to visualize the attention patterns! We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt (as an illustration).

<details> <summary>Interpreting Attention Patterns</summary>
An easy mistake to make when looking at attention patterns is thinking that they must convey information about the <i>token</i> looked at (maybe accounting for the context of the token). But actually, all we can confidently say is that it moves information from the *residual stream position* corresponding to that input token. Especially later on in the model, there may be components in the residual stream that are nothing to do with the input token! Eg the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in ".", "!" or "?"
</details>

```python
def visualize_attention_patterns(
    heads: Union[List[int], int, Float[torch.Tensor, "heads"]],
    local_cache: ActivationCache,
    local_tokens: torch.Tensor,
    title: Optional[str] = "",
    max_width: Optional[int] = 700,
) -> str:
    # If a single head is given, convert to a list
    if isinstance(heads, int):
        heads = [heads]

    # Create the plotting data
    labels: List[str] = []
    patterns: List[Float[torch.Tensor, "dest_pos src_pos"]] = []

    # Assume we have a single batch item
    batch_index = 0

    for head in heads:
        # Set the label
        layer = head // model.cfg.n_heads
        head_index = head % model.cfg.n_heads
        labels.append(f"L{layer}H{head_index}")

        # Get the attention patterns for the head
        # Attention patterns have shape [batch, head_index, query_pos, key_pos]
        patterns.append(local_cache["attn", layer][batch_index, head_index])

    # Convert the tokens to strings (for the axis labels)
    str_tokens = model.to_str_tokens(local_tokens)

    # Combine the patterns into a single tensor
    patterns: Float[torch.Tensor, "head_index dest_pos src_pos"] = torch.stack(
        patterns, dim=0
    )

    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)
    plot = attention_heads(
        attention=patterns, tokens=str_tokens, attention_head_names=labels
    ).show_code()

    # Display the title
    title_html = f"<h2>{title}</h2><br/>"

    # Return the visualisation as raw code
    return f"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>"
```

Inspecting the patterns, we can see that both types of name movers attend to the indirect object - this suggests they're simply copying the name attended to (with the OV circuit) and that the interesting part is the circuit behind the attention pattern that calculates *where* to move information from (the QK circuit)

```python
top_k = 3

top_positive_logit_attr_heads = torch.topk(
    per_head_logit_diffs.flatten(), k=top_k
).indices

positive_html = visualize_attention_patterns(
    top_positive_logit_attr_heads,
    cache,
    tokens[0],
    f"Top {top_k} Positive Logit Attribution Heads",
)

top_negative_logit_attr_heads = torch.topk(
    -per_head_logit_diffs.flatten(), k=top_k
).indices

negative_html = visualize_attention_patterns(
    top_negative_logit_attr_heads,
    cache,
    tokens[0],
    title=f"Top {top_k} Negative Logit Attribution Heads",
)

HTML(positive_html + negative_html)
```

## Activation Patching

**This section explains how to do activation patching conceptually by implementing it from scratch. To use it in practice with TransformerLens, see [this demonstration instead](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Activation_Patching_in_TL_Demo.ipynb)**.

The obvious limitation to the techniques used above is that they only look at the very end of the circuit - the parts that directly affect the logits. Clearly this is not sufficient to understand the circuit! We want to understand how things compose together to produce this final output, and ideally to produce an end-to-end circuit fully explaining this behaviour.

The technique we'll use to investigate this is called **activation patching**. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.

The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.

We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to *localise* which activations matter.

The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.

Here's an animation from the ROME paper demonstrating this technique (they studied factual recall, and use stars to represent corruption applied to the subject of the sentence, but the same principles apply):

![CT Animation](https://rome.baulab.info/images/small-ct-animation.gif)

See also [the explanation in a mech interp explainer](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx) and [this piece](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching#how-to-think-about-activation-patching) describing how to think about patching on a conceptual level

The above was all fairly abstract, so let's zoom in and lay out a concrete example to understand Indirect Object Identification.

Here our clean input will be eg "After John and Mary went to the store, **John** gave a bottle of milk to" and our corrupted input will be eg "After John and Mary went to the store, **Mary** gave a bottle of milk to". These prompts are identical except for the name of the indirect object, and so patching is a causal intervention which will allow us to understand precisely which parts of the network are identifying the indirect object.

One natural thing to patch in is the residual stream at a specific layer and specific position. For example, the model is likely initially doing some processing on the second subject token to realise that it's a duplicate, but then uses attention to move that information to the " to" token. So patching in the residual stream at the " to" token will likely matter a lot in later layers but not at all in early layers.

We can zoom in much further and patch in specific activations from specific layers. For example, we think that the output of head L9H9 on the final token is significant for directly connecting to the logits

We can patch in specific activations, and can zoom in as far as seems reasonable. For example, if we patch in the output of head L9H9 on the final token, we would predict that it will significantly affect performance.

Note that this technique does *not* tell us how the components of the circuit connect up, just what they are.

<details> <summary>Technical details</summary>
The choice of clean and corrupted prompt has both pros and cons. By carefully setting up the counterfactual, that <i>only</i> differs in the second subject, we avoid detecting the parts of the model doing irrelevant computation like detecting that the indirect object task is relevant at all or that it should be outputting a name rather than an article or pronoun. Or even context like that John and Mary are names at all.

However, it *also* bakes in some details that *are* relevant to the task. Such as finding the location of the second subject, and of the names in the first clause. Or that the name mover heads have learned to copy whatever they look at.

Some of these could be patched by also changing up the order of the names in the original sentence - patching in "After <b>John and Mary</b> went to the store, John gave a bottle of milk to" vs "After <b>Mary and John</b> went to the store, John gave a bottle of milk to".

In the ROME paper they take a different tack. Rather than carefully setting up counterfactuals between two different but related inputs, they **corrupt** the clean input by adding Gaussian noise to the token embedding for the subject. This is in some ways much lower effort (you don't need to set up a similar but different prompt) but can also introduce some issues, such as ways this noise might break things. In practice, you should take care about how you choose your counterfactuals and try out several. Try to reason beforehand about what they will and will not tell you, and compare the results between different counterfactuals.

I discuss some of these limitations and how the author's solved them with much more refined usage of these techniques <a href="https://www.youtube.com/watch?v=gzwj0jWbvbo">in our interview</a>
</details>

## Residual Stream

Lets begin by patching in the residual stream at the start of each layer and for each token position.

We first create a set of corrupted tokens - where we swap each pair of prompts to have the opposite answer.

```python
corrupted_prompts = []
for i in range(0, len(prompts), 2):
    corrupted_prompts.append(prompts[i + 1])
    corrupted_prompts.append(prompts[i])
corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)
corrupted_logits, corrupted_cache = model.run_with_cache(
    corrupted_tokens, return_type="logits"
)
corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)
print("Corrupted Average Logit Diff", round(corrupted_average_logit_diff.item(), 2))
print("Clean Average Logit Diff", round(original_average_logit_diff.item(), 2))
```

```python
model.to_string(corrupted_tokens)
```

We now intervene on the corrupted run and patch in the clean residual stream at a specific layer and position.

We do the intervention using TransformerLens's `HookPoint` feature. We can design a hook function that takes in a specific activation and returns an edited copy, and temporarily add it in with `model.run_with_hooks`.

```python
def patch_residual_component(
    corrupted_residual_component: Float[torch.Tensor, "batch pos d_model"],
    hook,
    pos,
    clean_cache,
):
    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]
    return corrupted_residual_component

def normalize_patched_logit_diff(patched_logit_diff):
    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise
    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance
    return (patched_logit_diff - corrupted_average_logit_diff) / (
        original_average_logit_diff - corrupted_average_logit_diff
    )

patched_residual_stream_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for position in range(tokens.shape[1]):
        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("resid_pre", layer), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can immediately see that, exactly as predicted, originally all relevant computation happens on the second subject token, and at layers 7 and 8, the information is moved to the final token. Moving the residual stream at the correct position near *exactly* recovers performance!

For reference, tokens and their index from the first prompt are on the x-axis. In an abuse of notation, note that the difference here is averaged over *all* 8 prompts, while the labels only come from the *first* prompt.

To be easier to interpret, we normalise the logit difference, by subtracting the corrupted logit difference, and dividing by the total improvement from clean to corrupted to normalise
0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance

```python
prompt_position_labels = [
    f"{tok}_{i}" for i, tok in enumerate(model.to_str_tokens(tokens[0]))
]
imshow(
    patched_residual_stream_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched Residual Stream",
    labels={"x": "Position", "y": "Layer"},
)
```

## Layers

We can apply exactly the same idea, but this time patching in attention or MLP layers. These are also residual components with identical shapes to the residual stream terms, so we can reuse the same hooks.

```python
patched_attn_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
patched_mlp_diff = torch.zeros(
    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for position in range(tokens.shape[1]):
        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)
        patched_attn_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("attn_out", layer), hook_fn)],
            return_type="logits",
        )
        patched_attn_logit_diff = logits_to_ave_logit_diff(
            patched_attn_logits, answer_tokens
        )
        patched_mlp_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("mlp_out", layer), hook_fn)],
            return_type="logits",
        )
        patched_mlp_logit_diff = logits_to_ave_logit_diff(
            patched_mlp_logits, answer_tokens
        )

        patched_attn_diff[layer, position] = normalize_patched_logit_diff(
            patched_attn_logit_diff
        )
        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(
            patched_mlp_logit_diff
        )
```

We see that several attention layers are significant but that, matching the residual stream results, early layers matter on the second subject token, and later layers matter on the final token, and layers essentially don't matter on any other token. Extremely localised! As with direct logit attribution, layer 9 is positive and layers 10 and 11 are not, suggesting that the late layers only matter for direct logit effects, but we also see that layers 7 and 8 matter significantly. Presumably these are the heads that move information about which name is duplicated from the second subject token to the final token.

```python
imshow(
    patched_attn_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched Attention Layer",
    labels={"x": "Position", "y": "Layer"},
)
```

In contrast, the MLP layers do not matter much. This makes sense, since this is more a task about moving information than about processing it, and the MLP layers specialise in processing information.

The one exception is MLP 0, which matters a lot, but I think this is misleading and just a generally true statement about MLP 0 rather than being about the circuit on this task.

<details> <summary>My takes on MLP0</summary>
It's often observed on GPT-2 Small that MLP0 matters a lot, and that ablating it utterly destroys performance. My current best guess is that the first MLP layer is essentially acting as an extension of the embedding (for whatever reason) and that when later layers want to access the input tokens they mostly read in the output of the first MLP layer, rather than the token embeddings. Within this frame, the first attention layer doesn't do much.

In this framing, it makes sense that MLP0 matters on the second subject token, because that's the one position with a different input token!

I'm not entirely sure why this happens, but I would guess that it's because the embedding and unembedding matrices in GPT-2 Small are the same. This is pretty unprincipled, as the tasks of embedding and unembedding tokens are <i>not</i> inverses, but this is common practice, and plausibly models want to dedicate some parameters to overcoming this.

I only have suggestive evidence of this, and would love to see someone look into this properly!
</details>

```python
imshow(
    patched_mlp_diff,
    x=prompt_position_labels,
    title="Logit Difference From Patched MLP Layer",
    labels={"x": "Position", "y": "Layer"},
)
```

## Heads

We can refine the above analysis by patching in individual heads! This is somewhat more annoying, because there are now three dimensions (head_index, position and layer), so for now lets patch in a head's output across all positions.

The easiest way to do this is to patch in the activation `z`, the "mixed value" of the attention head. That is, the average of all previous values weighted by the attention pattern, ie the activation that is then multiplied by `W_O`, the output weights.

```python
def patch_head_vector(
    corrupted_head_vector: Float[torch.Tensor, "batch pos head_index d_head"],
    hook,
    head_index,
    clean_cache,
):
    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][
        :, :, head_index, :
    ]
    return corrupted_head_vector

patched_head_z_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("z", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can now see that, in addition to the name mover heads identified before, in mid-late layers the heads L8H6, L8H10, L7H9 matter and are presumably responsible for moving information from the second subject to the final token. And heads L5H5, L6H9, L3H0 also matter a lot, and are presumably involved in detecting duplicated tokens.

```python
imshow(
    patched_head_z_diff,
    title="Logit Difference From Patched Head Output",
    labels={"x": "Head", "y": "Layer"},
)
```

## Decomposing Heads

Decomposing attention layers into patching in individual heads has already helped us localise the behaviour a lot. But we can understand it further by decomposing heads. An attention head consists of two semi-independent operations - calculating *where* to move information from and to (represented by the attention pattern and implemented via the QK-circuit) and calculating *what* information to move (represented by the value vectors and implemented by the OV circuit). We can disentangle which of these is important by patching in just the attention pattern *or* the value vectors. (See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [my walkthrough video](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition. If you're not familiar with the details of how attention is implemented, I recommend checking out [my clean transformer implementation](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=3Pb0NYbZ900e) to see how the code works))

First let's patch in the value vectors, to measure when figuring out what to move is important. . This has the same shape as z ([batch, pos, head_index, d_head]) so we can reuse the same hook.

```python
patched_head_v_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("v", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

We can plot this as a heatmap and it's initially hard to interpret.

```python
imshow(
    patched_head_v_diff,
    title="Logit Difference From Patched Head Value",
    labels={"x": "Head", "y": "Layer"},
)
```

But it's very easy to interpret if we plot a scatter plot against patching head outputs. Here we see that the earlier heads (L5H5, L6H9, L3H0) and late name movers (L9H9, L10H7, L11H10) don't matter at all now, while the mid-late heads (L8H6, L8H10, L7H9) do.

Meta lesson: Plot things early, often and in diverse ways as you explore a model's internals!

```python
head_labels = [
    f"L{l}H{h}" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)
]
scatter(
    x=utils.to_numpy(patched_head_v_diff.flatten()),
    y=utils.to_numpy(patched_head_z_diff.flatten()),
    xaxis="Value Patch",
    yaxis="Output Patch",
    caxis="Layer",
    hover_name=head_labels,
    color=einops.repeat(
        np.arange(model.cfg.n_layers), "layer -> (layer head)", head=model.cfg.n_heads
    ),
    range_x=(-0.5, 0.5),
    range_y=(-0.5, 0.5),
    title="Scatter plot of output patching vs value patching",
)
```

When we patch in attention patterns, we see the opposite effect - early and late heads matter a lot, middle heads don't. (In fact, the sum of value patching and pattern patching is approx the same as output patching)

```python
def patch_head_pattern(
    corrupted_head_pattern: Float[torch.Tensor, "batch head_index query_pos d_head"],
    hook,
    head_index,
    clean_cache,
):
    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][
        :, head_index, :, :
    ]
    return corrupted_head_pattern

patched_head_attn_diff = torch.zeros(
    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32
)
for layer in range(model.cfg.n_layers):
    for head_index in range(model.cfg.n_heads):
        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)
        patched_logits = model.run_with_hooks(
            corrupted_tokens,
            fwd_hooks=[(utils.get_act_name("attn", layer, "attn"), hook_fn)],
            return_type="logits",
        )
        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)

        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(
            patched_logit_diff
        )
```

```python
imshow(
    patched_head_attn_diff,
    title="Logit Difference From Patched Head Pattern",
    labels={"x": "Head", "y": "Layer"},
)
head_labels = [
    f"L{l}H{h}" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)
]
scatter(
    x=utils.to_numpy(patched_head_attn_diff.flatten()),
    y=utils.to_numpy(patched_head_z_diff.flatten()),
    hover_name=head_labels,
    xaxis="Attention Patch",
    yaxis="Output Patch",
    title="Scatter plot of output patching vs attention patching",
)
```

## Consolidating Understanding

OK, let's zoom out and reconsolidate. At a high-level, we find that all the action is on the second subject token until layer 7 and then transitions to the final token. And that attention layers matter a lot, MLP layers not so much (apart from MLP0, likely as an extended embedding).

We've further localised important behaviour to several categories of heads. We've found 3 categories of heads that matter a lot - early heads (L5H5, L6H9, L3H0) whose output matters on the second subject and whose behaviour is determined by their attention patterns, mid-late heads (L8H6, L8H10, L7H9, L7H3) whose output matters on the final token and whose behaviour is determined by their value vectors, and late heads (L9H9, L10H7, L11H10) whose output matters on the final token and whose behaviour is determined by their attention patterns.

A natural speculation is that early heads detect both that the second subject is a repeated token and *which* is repeated (ie the " John" token is repeated), middle heads compose with this and move this duplicated token information from the second subject token to the final token, and the late heads compose with this to *inhibit* their attention to the duplicated token, and then attend to the correct indirect object name and copy that directly to the logits.

### Visualizing Attention Patterns

We can validate this by looking at the attention patterns of these heads! Let's take the top 10 heads by output patching (in absolute value) and split it into early, middle and late.

We see that middle heads attend from the final token to the second subject, and late heads attend from the final token to the indirect object, which is completely consistent with the above speculation! But weirdly, while *one* early head attends from the second subject to its first copy, the other two mysteriously attend to the word *after* the first copy.

```python
top_k = 10
top_heads_by_output_patch = torch.topk(
    patched_head_z_diff.abs().flatten(), k=top_k
).indices
first_mid_layer = 7
first_late_layer = 9
early_heads = top_heads_by_output_patch[
    top_heads_by_output_patch < model.cfg.n_heads * first_mid_layer
]
mid_heads = top_heads_by_output_patch[
    torch.logical_and(
        model.cfg.n_heads * first_mid_layer <= top_heads_by_output_patch,
        top_heads_by_output_patch < model.cfg.n_heads * first_late_layer,
    )
]
late_heads = top_heads_by_output_patch[
    model.cfg.n_heads * first_late_layer <= top_heads_by_output_patch
]

early = visualize_attention_patterns(
    early_heads, cache, tokens[0], title=f"Top Early Heads"
)
mid = visualize_attention_patterns(
    mid_heads, cache, tokens[0], title=f"Top Middle Heads"
)
late = visualize_attention_patterns(
    late_heads, cache, tokens[0], title=f"Top Late Heads"
)

HTML(early + mid + late)
```

### Comparing to the Paper

We can now refer to the (far, far more rigorous and detailed) analysis in the paper to compare our results! Here's the diagram they give of their results.

![IOI1](https://pbs.twimg.com/media/FghGkTAWAAAmkhm.jpg)

(Head 1.2 in their notation is L1H2 in my notation etc. And note - in the [latest version of the paper](https://arxiv.org/pdf/2211.00593.pdf) they add 9.0 as a backup name mover, and remove 11.3)

The heads form three categories corresponding to the early, middle and late categories we found and we did fairly well! Definitely not perfect, but with some fairly generic techniques and some a priori reasoning, we found the broad strokes of the circuit and what it looks like. We focused on the most important heads, so we didn't find all relevant heads in each category (especially not the heads in brackets, which are more minor), but this serves as a good base for doing more rigorous and involved analysis, especially for finding the *complete* circuit (ie all of the parts of the model which participate in this behaviour) rather than just a partial and suggestive circuit. Go check out [their paper](https://arxiv.org/abs/2211.00593) or [our interview](https://www.youtube.com/watch?v=gzwj0jWbvbo) to learn more about what they did and what they found!

Breaking down their categories:

* Early: The duplicate token heads, previous token heads and induction heads. These serve the purpose of detecting that the second subject is duplicated and which earlier name is the duplicate.
    * We found a direct duplicate token head which behaves exactly as expected, L3H0. Heads L5H0 and L6H9 are induction heads, which explains why they don't attend directly to the earlier copy of John!
    * Note that the duplicate token heads and induction heads do not compose with each other - both directly add to the S-Inhibition heads. The diagram is somewhat misleading.
* Middle: They call these S-Inhibition heads - they copy the information about the duplicate token from the second subject to the to token, and their output is used to *inhibit* the attention paid from the name movers to the first subject copy. We found all these heads, and had a decent guess for what they did.
    * In either case they attend to the second subject, so the patch that mattered was their value vectors!
* Late: They call these name movers, and we found some of them. They attend from the final token to the indirect object name and copy that to the logits, using the S-Inhibition heads to inhibit attention to the first copy of the subject token.
    * We did find their surprising result of *negative* name movers - name movers that inhibit the correct answer!
    * They have an entire category of heads we missed called backup name movers - we'll get to these later.

So, now, let's dig into the two anomalies we missed - induction heads and backup name mover heads

## Bonus: Exploring Anomalies

### Early Heads are Induction Heads(?!)

A really weird observation is that some of the early heads detecting duplicated tokens are induction heads, not just direct duplicate token heads. This is very weird! What's up with that?

First off, what's an induction head? An induction head is an important type of attention head that can detect and continue repeated sequences. It is the second head in a two head induction circuit, which looks for previous copies of the current token and attends to the token *after* it, and then copies that to the current position and predicts that it will come next. They're enough of a big deal that [we wrote a whole paper on them](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).

![Move image demo](https://pbs.twimg.com/media/FNWAzXjVEAEOGRe.jpg)

Second, why is it surprising that they come up here? It's surprising because it feels like overkill. The model doesn't care about *what* token comes after the first copy of the subject, just that it's duplicated. And it already has simpler duplicate token heads. My best guess is that it just already had induction heads around and that, in addition to their main function, they *also* only activate on duplicated tokens. So it was useful to repurpose this existing machinery.

This suggests that as we look for circuits in larger models life may get more and more complicated, as components in simpler circuits get repurposed and built upon.

We can verify that these are induction heads by running the model on repeated text and plotting the heads.

```python
example_text = "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components."
example_repeated_text = example_text + example_text
example_repeated_tokens = model.to_tokens(example_repeated_text, prepend_bos=True)
example_repeated_logits, example_repeated_cache = model.run_with_cache(
    example_repeated_tokens
)
induction_head_labels = [81, 65]
```

```python
code = visualize_attention_patterns(
    induction_head_labels,
    example_repeated_cache,
    example_repeated_tokens,
    title="Induction Heads",
    max_width=800,
)
HTML(code)
```

#### Implications

One implication of this is that it's useful to categories heads according to whether they occur in
simpler circuits, so that as we look for more complex circuits we can easily look for them. This is
easy to do here! An interesting fact about induction heads is that they work on a sequence of
repeated random tokens - notable for being wildly off distribution from the natural language GPT-2
was trained on. Being able to predict a model's behaviour off distribution is a good mark of success
for mechanistic interpretability! This is a good sanity check for whether a head is an induction
head or not.

We can characterise an induction head by just giving a sequence of random tokens repeated once, and
measuring the average attention paid from the second copy of a token to the token after the first
copy. At the same time, we can also measure the average attention paid from the second copy of a
token to the first copy of the token, which is the attention that the induction head would pay if it
were a duplicate token head, and the average attention paid to the previous token to find previous
token heads.

Note that this is a superficial study of whether something is an induction head - we totally ignore
the question of whether it actually does boost the correct token or whether it composes with a
single previous head and how. In particular, we sometimes get anti-induction heads which suppress
the induction-y token (no clue why!), and this technique will find those too . But given the
previous rigorous analysis, we can be pretty confident that this picks up on some true signal about
induction heads.

<details> <summary>Technical Implementation Details</summary>
We can do this again by using hooks, this time just to access the attention patterns rather than to intervene on them.

Our hook function acts on the attention pattern activation. This has the name
"blocks.{layer}.{layer_type}.hook_{activation_name}" in general, here it's
"blocks.{layer}.attn.hook_attn". And it has shape [batch, head_index, query_pos, token_pos]. Our
hook function takes in the attention pattern activation, calculates the score for the relevant type
of head, and write it to an external cache.

We add in hooks using `model.run_with_hooks(tokens, fwd_hooks=[(names_filter, hook_fn)])` to
temporarily add in the hooks and run the model, getting the resulting output. Previously
names_filter was the name of the activation, but here it's a boolean function mapping activation
names to whether we want to hook them or not. Here it's just whether the name ends with hook_attn.
hook_fn must take in the two inputs activation (the activation tensor) and hook (the HookPoint
object, which contains the name of the activation and some metadata such as the current layer).

Internally our hooks use the function `tensor.diagonal`, this takes the diagonal between two
dimensions, and allows an arbitrary offset - offset by 1 to get previous tokens, seq_len to get
duplicate tokens (the distance to earlier copies) and seq_len-1 to get induction heads (the distance
to the token *after* earlier copies). Different offsets give a different length of output tensor,
and we can now just average to get a score in [0, 1] for each head
</details>

```python
seq_len = 100
batch_size = 2

prev_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device)

def prev_token_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=1, dim1=-1, dim2=-2)
    # print(diagonal)
    # print(pattern)
    prev_token_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

duplicate_token_scores = torch.zeros(
    (model.cfg.n_layers, model.cfg.n_heads), device=device
)

def duplicate_token_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=seq_len, dim1=-1, dim2=-2)
    duplicate_token_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

induction_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device)

def induction_hook(pattern, hook):
    layer = hook.layer()
    diagonal = pattern.diagonal(offset=seq_len - 1, dim1=-1, dim2=-2)
    induction_scores[layer] = einops.reduce(
        diagonal, "batch head_index diagonal -> head_index", "mean"
    )

torch.manual_seed(0)
original_tokens = torch.randint(
    100, 20000, size=(batch_size, seq_len), device="cpu"
).to(device)
repeated_tokens = einops.repeat(
    original_tokens, "batch seq_len -> batch (2 seq_len)"
).to(device)

pattern_filter = lambda act_name: act_name.endswith("hook_pattern")

loss = model.run_with_hooks(
    repeated_tokens,
    return_type="loss",
    fwd_hooks=[
        (pattern_filter, prev_token_hook),
        (pattern_filter, duplicate_token_hook),
        (pattern_filter, induction_hook),
    ],
)
print(torch.round(utils.get_corner(prev_token_scores).detach().cpu(), decimals=3))
print(torch.round(utils.get_corner(duplicate_token_scores).detach().cpu(), decimals=3))
print(torch.round(utils.get_corner(induction_scores).detach().cpu(), decimals=3))
```

We can now plot the head scores, and instantly see that the relevant early heads are induction heads or duplicate token heads (though also that there's a lot of induction heads that are *not* use - I have no idea why!).

```python
imshow(
    prev_token_scores, labels={"x": "Head", "y": "Layer"}, title="Previous Token Scores"
)
imshow(
    duplicate_token_scores,
    labels={"x": "Head", "y": "Layer"},
    title="Duplicate Token Scores",
)
imshow(
    induction_scores, labels={"x": "Head", "y": "Layer"}, title="Induction Head Scores"
)
```

The above suggests that it would be a useful bit of infrastructure to have a "wiki" for the heads of a model, giving their scores according to some metrics re head functions, like the ones we've seen here. TransformerLens makes this easy to make, as just changing the name input to `HookedTransformer.from_pretrained` gives a different model but in the same architecture, so the same code should work. If you want to make this, I'd love to see it!

As a proof of concept, [I made a mosaic of all induction heads across the 40 models then in TransformerLens](https://www.neelnanda.io/mosaic).

![induction scores as proof of concept](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNeelNanda%2F5vtuFmdzt_.png?alt=media&token=4d613de4-9d14-48d6-ba9d-e591c562d429)

### Backup Name Mover Heads

Another fascinating anomaly is that of the **backup name mover heads**. A standard technique to apply when interpreting model internals is ablations, or knock-out. If we run the model but intervene to set a specific head to zero, what happens? If the model is robust to this intervention, then naively we can be confident that the head is not doing anything important, and conversely if the model is much worse at the task this suggests that head was important. There are several conceptual flaws with this approach, making the evidence only suggestive, eg that the average output of the head may be far from zero and so the knockout may send it far from expected activations, breaking internals on *any* task. But it's still an easy technique to apply to give some data.

But a wild finding in the paper is that models have **built in redundancy**. If we knock out one of the name movers, then there are some backup name movers in later layers that *change their behaviour* and do (some of) the job of the original name mover head. This means that naive knock-out will significantly underestimate the importance of the name movers.

Let's test this! Let's ablate the most important name mover (head L9H9) on just the final token using a custom ablation hook and then cache all new activations and compared performance. We focus on the final position because we want to specifically ablate the direct logit effect. When we do this, we see that naively, removing the top name mover should reduce the logit diff massively, from 3.55 to 0.57. **But actually, it only goes down to 2.99!**

<details> <summary>Implementation Details</summary>
Ablating heads is really easy in TransformerLens! We can just define a hook on the z activation in the relevant attention layer (recall, z is the mixed values, and comes immediately before multiplying by the output weights $W_O$). z has a head_index axis, so we can set the component for the relevant head and for position -1 to zero, and return it. (Technically we could just edit in place without returning it, but by convention we always return an edited activation).

We now want to compare all internal activations with a hook, which is hard to do with the nice `run_with_hooks` API. So we can directly access the hook on the z activation with `model.blocks[layer].attn.hook_z` and call its `add_hook` method. This adds in the hook to the *global state* of the model. We can now use run_with_cache, and don't need to care about the global state, because run_with_cache internally adds a bunch of caching hooks, and then removes all hooks after the run, *including* the previously added ablation hook. This can be disabled with the reset_hooks_end flag, but here it's useful!
</details>

```python
top_name_mover = per_head_logit_diffs.flatten().argmax().item()
top_name_mover_layer = top_name_mover // model.cfg.n_heads
top_name_mover_head = top_name_mover % model.cfg.n_heads
print(f"Top Name Mover to ablate: L{top_name_mover_layer}H{top_name_mover_head}")

def ablate_top_head_hook(z: Float[torch.Tensor, "batch pos head_index d_head"], hook):
    z[:, -1, top_name_mover_head, :] = 0
    return z

# Adds a hook into global model state
model.blocks[top_name_mover_layer].attn.hook_z.add_hook(ablate_top_head_hook)
# Runs the model, temporarily adds caching hooks and then removes *all* hooks after running, including the ablation hook.
ablated_logits, ablated_cache = model.run_with_cache(tokens)
print(f"Original logit diff: {original_average_logit_diff:.2f}")
print(
    f"Post ablation logit diff: {logits_to_ave_logit_diff(ablated_logits, answer_tokens).item():.2f}"
)
print(
    f"Direct Logit Attribution of top name mover head: {per_head_logit_diffs.flatten()[top_name_mover].item():.2f}"
)
print(
    f"Naive prediction of post ablation logit diff: {original_average_logit_diff - per_head_logit_diffs.flatten()[top_name_mover].item():.2f}"
)
```

So what's up with this? As before, we can look at the direct logit attribution of each head to see what's going on. It's easiest to interpret if plotted as a scatter plot against the initial per head logit difference.

And we can see a *really* big difference in a few heads! (Hover to see labels) In particular the negative name mover L10H7 decreases its negative effect a lot, adding +1 to the logit diff, and the backup name mover L10H10 adjusts its effect to be more positive, adding +0.8 to the logit diff (with several other marginal changes). (And obviously the ablated head has gone down to zero!)

```python
per_head_ablated_residual, labels = ablated_cache.stack_head_results(
    layer=-1, pos_slice=-1, return_labels=True
)
per_head_ablated_logit_diffs = residual_stack_to_logit_diff(
    per_head_ablated_residual, ablated_cache
)
per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(
    model.cfg.n_layers, model.cfg.n_heads
)
imshow(per_head_ablated_logit_diffs, labels={"x": "Head", "y": "Layer"})
scatter(
    y=per_head_logit_diffs.flatten(),
    x=per_head_ablated_logit_diffs.flatten(),
    hover_name=head_labels,
    range_x=(-3, 3),
    range_y=(-3, 3),
    xaxis="Ablated",
    yaxis="Original",
    title="Original vs Post-Ablation Direct Logit Attribution of Heads",
)
```

One natural hypothesis is that this is because the final LayerNorm scaling has changed, which can scale up or down the final residual stream. This is slightly true, and we can see that the typical head is a bit off from the x=y line. But the average LN scaling ratio is 1.04, and this should uniformly change *all* heads by the same factor, so this can't be sufficient

```python
print(
    "Average LN scaling ratio:",
    round(
        (
            cache["ln_final.hook_scale"][:, -1]
            / ablated_cache["ln_final.hook_scale"][:, -1]
        )
        .mean()
        .item(),
        3,
    ),
)
print(
    "Ablation LN scale",
    ablated_cache["ln_final.hook_scale"][:, -1].detach().cpu().round(decimals=2),
)
print(
    "Original LN scale",
    cache["ln_final.hook_scale"][:, -1].detach().cpu().round(decimals=2),
)
```

**Exercise to the reader:** Can you finish off this analysis? What's going on here? Why are the backup name movers changing their behaviour? Why is one negative name mover becoming significantly less important?

---

# Grokking_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Grokking Demo Notebook

<b style="color: red">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>

# Setup
(No need to read)

```python
TRAIN_MODEL = True
```

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install circuitsvis
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio
if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
pio.templates['plotly'].layout.xaxis.title.font.size = 20
pio.templates['plotly'].layout.yaxis.title.font.size = 20
pio.templates['plotly'].layout.title.font.size = 30
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import os
import tqdm.auto as tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from typing import List, Union, Optional
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
```

```python
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache

device = "cuda" if torch.cuda.is_available() else "cpu"
```

Plotting helper functions:

```python
def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(y=y, x=x, labels={"x":xaxis, "y":yaxis, "color":caxis}, **kwargs).show(renderer)
```

```python
# Define the location to save the model, using a relative path
PTH_LOCATION = "workspace/_scratch/grokking_demo.pth"

# Create the directory if it does not exist
os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)
```

# Model Training

## Config

```python
p = 113
frac_train = 0.3

# Optimizer config
lr = 1e-3
wd = 1.
betas = (0.9, 0.98)

num_epochs = 25000
checkpoint_every = 100

DATA_SEED = 598
```

## Define Task
* Define modular addition
* Define the dataset & labels

Input format:
|a|b|=|

```python
a_vector = einops.repeat(torch.arange(p), "i -> (i j)", j=p)
b_vector = einops.repeat(torch.arange(p), "j -> (i j)", i=p)
equals_vector = einops.repeat(torch.tensor(113), " -> (i j)", i=p, j=p)

```

```python
dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)
print(dataset[:5])
print(dataset.shape)
```

```python
labels = (dataset[:, 0] + dataset[:, 1]) % p
print(labels.shape)
print(labels[:5])
```

Convert this to a train + test set - 30% in the training set

```python
torch.manual_seed(DATA_SEED)
indices = torch.randperm(p*p)
cutoff = int(p*p*frac_train)
train_indices = indices[:cutoff]
test_indices = indices[cutoff:]

train_data = dataset[train_indices]
train_labels = labels[train_indices]
test_data = dataset[test_indices]
test_labels = labels[test_indices]
print(train_data[:5])
print(train_labels[:5])
print(train_data.shape)
print(test_data[:5])
print(test_labels[:5])
print(test_data.shape)
```

## Define Model

```python

cfg = HookedTransformerConfig(
    n_layers = 1,
    n_heads = 4,
    d_model = 128,
    d_head = 32,
    d_mlp = 512,
    act_fn = "relu",
    normalization_type=None,
    d_vocab=p+1,
    d_vocab_out=p,
    n_ctx=3,
    init_weights=True,
    device=device,
    seed = 999,
)
```

```python
model = HookedTransformer(cfg)
```

Disable the biases, as we don't need them for this task and it makes things easier to interpret.

```python
for name, param in model.named_parameters():
    if "b_" in name:
        param.requires_grad = False

```

## Define Optimizer + Loss

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)
```

```python
def loss_fn(logits, labels):
    if len(logits.shape)==3:
        logits = logits[:, -1]
    logits = logits.to(torch.float64)
    log_probs = logits.log_softmax(dim=-1)
    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]
    return -correct_log_probs.mean()
train_logits = model(train_data)
train_loss = loss_fn(train_logits, train_labels)
print(train_loss)
test_logits = model(test_data)
test_loss = loss_fn(test_logits, test_labels)
print(test_loss)
```

```python
print("Uniform loss:")
print(np.log(p))
```

## Actually Train

**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots.

```python
train_losses = []
test_losses = []
model_checkpoints = []
checkpoint_epochs = []
if TRAIN_MODEL:
    for epoch in tqdm.tqdm(range(num_epochs)):
        train_logits = model(train_data)
        train_loss = loss_fn(train_logits, train_labels)
        train_loss.backward()
        train_losses.append(train_loss.item())

        optimizer.step()
        optimizer.zero_grad()

        with torch.inference_mode():
            test_logits = model(test_data)
            test_loss = loss_fn(test_logits, test_labels)
            test_losses.append(test_loss.item())

        if ((epoch+1)%checkpoint_every)==0:
            checkpoint_epochs.append(epoch)
            model_checkpoints.append(copy.deepcopy(model.state_dict()))
            print(f"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}")
```

```python
torch.save(
    {
        "model":model.state_dict(),
        "config": model.cfg,
        "checkpoints": model_checkpoints,
        "checkpoint_epochs": checkpoint_epochs,
        "test_losses": test_losses,
        "train_losses": train_losses,
        "train_indices": train_indices,
        "test_indices": test_indices,
    },
    PTH_LOCATION)
```

```python
if not TRAIN_MODEL:
    cached_data = torch.load(PTH_LOCATION)
    model.load_state_dict(cached_data['model'])
    model_checkpoints = cached_data["checkpoints"]
    checkpoint_epochs = cached_data["checkpoint_epochs"]
    test_losses = cached_data['test_losses']
    train_losses = cached_data['train_losses']
    train_indices = cached_data["train_indices"]
    test_indices = cached_data["test_indices"]
```

## Show Model Training Statistics, Check that it groks!

```python
%pip install git+https://github.com/neelnanda-io/neel-plotly.git
from neel_plotly.plot import line
line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Training Curve for Modular Addition", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)
```

# Analysing the Model

## Standard Things to Try

```python
original_logits, cache = model.run_with_cache(dataset)
print(original_logits.numel())
```

Get key weight matrices:

```python
W_E = model.embed.W_E[:-1]
print("W_E", W_E.shape)
W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in
print("W_neur", W_neur.shape)
W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U
print("W_logit", W_logit.shape)
```

```python
original_loss = loss_fn(original_logits, labels).item()
print("Original Loss:", original_loss)
```

### Looking at Activations

Helper variable:

```python
pattern_a = cache["pattern", 0, "attn"][:, :, -1, 0]
pattern_b = cache["pattern", 0, "attn"][:, :, -1, 1]
neuron_acts = cache["post", 0, "mlp"][:, -1, :]
neuron_pre_acts = cache["pre", 0, "mlp"][:, -1, :]
```

Get all shapes:

```python
for param_name, param in cache.items():
    print(param_name, param.shape)
```

```python
imshow(cache["pattern", 0].mean(dim=0)[:, -1, :], title="Average Attention Pattern per Head", xaxis="Source", yaxis="Head", x=['a', 'b', '='])
```

```python
imshow(cache["pattern", 0][5][:, -1, :], title="Average Attention Pattern per Head", xaxis="Source", yaxis="Head", x=['a', 'b', '='])
```

```python
dataset[:4]
```

```python
imshow(cache["pattern", 0][:, 0, -1, 0].reshape(p, p), title="Attention for Head 0 from a -> =", xaxis="b", yaxis="a")
```

```python
imshow(
    einops.rearrange(cache["pattern", 0][:, :, -1, 0], "(a b) head -> head a b", a=p, b=p),
    title="Attention for Head 0 from a -> =", xaxis="b", yaxis="a", facet_col=0)
```

Plotting neuron activations

```python
cache["post", 0, "mlp"].shape
```

```python
imshow(
    einops.rearrange(neuron_acts[:, :5], "(a b) neuron -> neuron a b", a=p, b=p),
    title="First 5 neuron acts", xaxis="b", yaxis="a", facet_col=0)
```

### Singular Value Decomposition

```python
W_E.shape
```

```python
U, S, Vh = torch.svd(W_E)
line(S, title="Singular Values")
imshow(U, title="Principal Components on the Input")
```

```python
# Control - random Gaussian matrix
U, S, Vh = torch.svd(torch.randn_like(W_E))
line(S, title="Singular Values Random")
imshow(U, title="Principal Components Random")
```

## Explaining Algorithm

### Analyse the Embedding - It's a Lookup Table!

```python
U, S, Vh = torch.svd(W_E)
line(U[:, :8].T, title="Principal Components of the embedding", xaxis="Input Vocabulary")
```

```python
fourier_basis = []
fourier_basis_names = []
fourier_basis.append(torch.ones(p))
fourier_basis_names.append("Constant")
for freq in range(1, p//2+1):
    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))
    fourier_basis_names.append(f"Sin {freq}")
    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))
    fourier_basis_names.append(f"Cos {freq}")
fourier_basis = torch.stack(fourier_basis, dim=0).to(device)
fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)
imshow(fourier_basis, xaxis="Input", yaxis="Component", y=fourier_basis_names)
```

```python
line(fourier_basis[:8], xaxis="Input", line_labels=fourier_basis_names[:8], title="First 8 Fourier Components")
line(fourier_basis[25:29], xaxis="Input", line_labels=fourier_basis_names[25:29], title="Middle Fourier Components")
```

```python
imshow(fourier_basis @ fourier_basis.T, title="All Fourier Vectors are Orthogonal")
```

### Analyse the Embedding

```python
imshow(fourier_basis @ W_E, yaxis="Fourier Component", xaxis="Residual Stream", y=fourier_basis_names, title="Embedding in Fourier Basis")
```

```python
line((fourier_basis @ W_E).norm(dim=-1), xaxis="Fourier Component", x=fourier_basis_names, title="Norms of Embedding in Fourier Basis")
```

```python
key_freqs = [17, 25, 32, 47]
key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]
fourier_embed = fourier_basis @ W_E
key_fourier_embed = fourier_embed[key_freq_indices]
print("key_fourier_embed", key_fourier_embed.shape)
imshow(key_fourier_embed @ key_fourier_embed.T, title="Dot Product of embedding of key Fourier Terms")
```

### Key Frequencies

```python
line(fourier_basis[[34, 50, 64, 94]], title="Cos of key freqs", line_labels=[34, 50, 64, 94])
```

```python
line(fourier_basis[[34, 50, 64, 94]].mean(0), title="Constructive Interference")
```

## Analyse Neurons

```python
imshow(
    einops.rearrange(neuron_acts[:, :5], "(a b) neuron -> neuron a b", a=p, b=p),
    title="First 5 neuron acts", xaxis="b", yaxis="a", facet_col=0)
```

```python
imshow(
    einops.rearrange(neuron_acts[:, 0], "(a b) -> a b", a=p, b=p),
    title="First neuron act", xaxis="b", yaxis="a",)
```

```python
imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title="Cos 47a * cos 47b")
```

```python
imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title="Cos 47a * const")
```

```python
imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of neuron 0", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

```python
imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of neuron 5", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

```python
imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title="2D Fourier Transformer of RANDOM", xaxis="b", yaxis="a", x=fourier_basis_names, y=fourier_basis_names)
```

### Neuron Clusters

```python
fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, "(a b) neuron -> neuron a b", a=p, b=p) @ fourier_basis.T
# Center these by removing the mean - doesn't matter!
fourier_neuron_acts[:, 0, 0] = 0.
print("fourier_neuron_acts", fourier_neuron_acts.shape)
```

```python
neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)
for freq in range(0, p//2):
    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:
        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:
            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2
neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]
imshow(neuron_freq_norm, xaxis="Neuron", yaxis="Freq", y=torch.arange(1, p//2+1), title="Neuron Frac Explained by Freq")
```

```python
line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis="Neuron", title="Max Neuron Frac Explained over Freqs")
```

## Read Off the Neuron-Logit Weights to Interpret

```python
W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U
print("W_logit", W_logit.shape)
```

```python
line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title="W_logit in the Fourier Basis")
```

```python
neurons_17 = neuron_freq_norm[17-1]>0.85
neurons_17.shape
```

```python
neurons_17.sum()
```

```python
line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title="W_logit for freq 17 neurons in the Fourier Basis")
```

Study sin 17

```python
freq = 17
W_logit_fourier = W_logit @ fourier_basis
neurons_sin_17 = W_logit_fourier[:, 2*freq-1]
line(neurons_sin_17)
```

```python
neuron_acts.shape
```

```python
inputs_sin_17c = neuron_acts @ neurons_sin_17
imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title="Fourier Heatmap over inputs for sin17c", x=fourier_basis_names, y=fourier_basis_names)
```

# Black Box Methods + Progress Measures

## Setup Code

Code to plot embedding freqs

```python
def embed_to_cos_sin(fourier_embed):
    if len(fourier_embed.shape) == 1:
        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])
    else:
        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)

from neel_plotly.plot import melt

def plot_embed_bars(
    fourier_embed,
    title="Norm of embedding of each Fourier Component",
    return_fig=False,
    **kwargs
):
    cos_sin_embed = embed_to_cos_sin(fourier_embed)
    df = melt(cos_sin_embed)
    # display(df)
    group_labels = {0: "sin", 1: "cos"}
    df["Trig"] = df["0"].map(lambda x: group_labels[x])
    fig = px.bar(
        df,
        barmode="group",
        color="Trig",
        x="1",
        y="value",
        labels={"1": "$w_k$", "value": "Norm"},
        title=title,
        **kwargs
    )
    fig.update_layout(dict(legend_title=""))

    if return_fig:
        return fig
    else:
        fig.show()
```

Code to test a tensor of edited logits

```python
def test_logits(logits, bias_correction=False, original_logits=None, mode="all"):
    # Calculates cross entropy loss of logits representing a batch of all p^2
    # possible inputs
    # Batch dimension is assumed to be first
    if logits.shape[1] == p * p:
        logits = logits.T
    if logits.shape == torch.Size([p * p, p + 1]):
        logits = logits[:, :-1]
    logits = logits.reshape(p * p, p)
    if bias_correction:
        # Applies bias correction - we correct for any missing bias terms,
        # independent of the input, by centering the new logits along the batch
        # dimension, and then adding the average original logits across all inputs
        logits = (
            einops.reduce(original_logits - logits, "batch ... -> ...", "mean") + logits
        )
    if mode == "train":
        return loss_fn(logits[train_indices], labels[train_indices])
    elif mode == "test":
        return loss_fn(logits[test_indices], labels[test_indices])
    elif mode == "all":
        return loss_fn(logits, labels)
```

Code to run a metric over every checkpoint

```python
metric_cache = {}
```

```python
def get_metrics(model, metric_cache, metric_fn, name, reset=False):
    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):
        metric_cache[name] = []
        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):
            model.reset_hooks()
            model.load_state_dict(sd)
            out = metric_fn(model)
            if type(out) == torch.Tensor:
                out = utils.to_numpy(out)
            metric_cache[name].append(out)
        model.load_state_dict(model_checkpoints[-1])
        try:
            metric_cache[name] = torch.tensor(metric_cache[name])
        except:
            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))

```

## Defining Progress Measures

### Loss Curves

```python
memorization_end_epoch = 1500
circuit_formation_end_epoch = 13300
cleanup_end_epoch = 16600
```

```python
def add_lines(figure):
    figure.add_vline(memorization_end_epoch, line_dash="dash", opacity=0.7)
    figure.add_vline(circuit_formation_end_epoch, line_dash="dash", opacity=0.7)
    figure.add_vline(cleanup_end_epoch, line_dash="dash", opacity=0.7)
    return figure
```

```python
fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Training Curve for Modular Addition", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)
add_lines(fig)
```

### Logit Periodicity

```python
all_logits = original_logits[:, -1, :]
print(all_logits.shape)
all_logits = einops.rearrange(all_logits, "(a b) c -> a b c", a=p, b=p)
print(all_logits.shape)
```

```python
coses = {}
for freq in key_freqs:
    print("Freq:", freq)
    a = torch.arange(p)[:, None, None]
    b = torch.arange(p)[None, :, None]
    c = torch.arange(p)[None, None, :]
    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)
    cube_predicted_logits /= cube_predicted_logits.norm()
    coses[freq] = cube_predicted_logits
```

```python
approximated_logits = torch.zeros_like(all_logits)
for freq in key_freqs:
    print("Freq:", freq)
    coeff = (all_logits * coses[freq]).sum()
    print("Coeff:", coeff)
    cosine_sim = coeff / all_logits.norm()
    print("Cosine Sim:", cosine_sim)
    approximated_logits += coeff * coses[freq]
residual = all_logits - approximated_logits
print("Residual size:", residual.norm())
print("Residual fraction of norm:", residual.norm()/all_logits.norm())
```

```python
random_logit_cube = torch.randn_like(all_logits)
print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())
```

```python
test_logits(all_logits)
```

```python
test_logits(approximated_logits)
```

#### Look During Training

```python
cos_cube = []
for freq in range(1, p//2 + 1):
    a = torch.arange(p)[:, None, None]
    b = torch.arange(p)[None, :, None]
    c = torch.arange(p)[None, None, :]
    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)
    cube_predicted_logits /= cube_predicted_logits.norm()
    cos_cube.append(cube_predicted_logits)
cos_cube = torch.stack(cos_cube, dim=0)
print(cos_cube.shape)
```

```python
def get_cos_coeffs(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    return vals

get_metrics(model, metric_cache, get_cos_coeffs, "cos_coeffs")
print(metric_cache["cos_coeffs"].shape)
```

```python
fig = line(metric_cache["cos_coeffs"].T, line_labels=[f"Freq {i}" for i in range(1, p//2+1)], title="Coefficients with Predicted Logits", xaxis="Epoch", x=checkpoint_epochs, yaxis="Coefficient", return_fig=True)
add_lines(fig)
```

```python
def get_cos_sim(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    return vals / logits.norm()

get_metrics(model, metric_cache, get_cos_sim, "cos_sim") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!
print(metric_cache["cos_sim"].shape)

fig = line(metric_cache["cos_sim"].T, line_labels=[f"Freq {i}" for i in range(1, p//2+1)], title="Cosine Sim with Predicted Logits", xaxis="Epoch", x=checkpoint_epochs, yaxis="Cosine Sim", return_fig=True)
add_lines(fig)
```

```python
def get_residual_cos_sim(model):
    logits = model(dataset)[:, -1]
    logits = einops.rearrange(logits, "(a b) c -> a b c", a=p, b=p)
    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])
    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)
    return residual.norm() / logits.norm()

get_metrics(model, metric_cache, get_residual_cos_sim, "residual_cos_sim")
print(metric_cache["residual_cos_sim"].shape)

fig = line([metric_cache["cos_sim"][:, i] for i in range(p//2)]+[metric_cache["residual_cos_sim"]], line_labels=[f"Freq {i}" for i in range(1, p//2+1)]+["residual"], title="Cosine Sim with Predicted Logits + Residual", xaxis="Epoch", x=checkpoint_epochs, yaxis="Cosine Sim", return_fig=True)
add_lines(fig)
```

## Restricted Loss

```python
neuron_acts.shape
```

```python
neuron_acts_square = einops.rearrange(neuron_acts, "(a b) neur -> a b neur", a=p, b=p).clone()
# Center it
neuron_acts_square -= einops.reduce(neuron_acts_square, "a b neur -> 1 1 neur", "mean")
neuron_acts_square_fourier = einsum("a b neur, fa a, fb b -> fa fb neur", neuron_acts_square, fourier_basis, fourier_basis)
imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis="Fourier Component b", yaxis="Fourier Component a", title="Norms of neuron activations by Fourier Component", x=fourier_basis_names, y=fourier_basis_names)
```

```python
original_logits, cache = model.run_with_cache(dataset)
print(original_logits.numel())
neuron_acts = cache["post", 0, "mlp"][:, -1, :]
```

```python
approx_neuron_acts = torch.zeros_like(neuron_acts)
approx_neuron_acts += neuron_acts.mean(dim=0)
a = torch.arange(p)[:, None]
b = torch.arange(p)[None, :]
for freq in key_freqs:
    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
    cos_apb_vec /= cos_apb_vec.norm()
    cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
    sin_apb_vec /= sin_apb_vec.norm()
    sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
restricted_logits = approx_neuron_acts @ W_logit
print(loss_fn(restricted_logits[test_indices], test_labels))
```

```python
print(loss_fn(all_logits, labels)) # This bugged on models not fully trained
```

### Look During Training

```python
def get_restricted_loss(model):
    logits, cache = model.run_with_cache(dataset)
    logits = logits[:, -1, :]
    neuron_acts = cache["post", 0, "mlp"][:, -1, :]
    approx_neuron_acts = torch.zeros_like(neuron_acts)
    approx_neuron_acts += neuron_acts.mean(dim=0)
    a = torch.arange(p)[:, None]
    b = torch.arange(p)[None, :]
    for freq in key_freqs:
        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
        cos_apb_vec /= cos_apb_vec.norm()
        cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
        sin_apb_vec /= sin_apb_vec.norm()
        sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U
    # Add bias term
    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)
    return loss_fn(restricted_logits[test_indices], test_labels)
get_restricted_loss(model)
```

```python
get_metrics(model, metric_cache, get_restricted_loss, "restricted_loss", reset=True)
print(metric_cache["restricted_loss"].shape)
```

```python
fig = line([train_losses[::100], test_losses[::100], metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Restricted Loss Curve", line_labels=['train', 'test', "restricted_loss"], toggle_x=True, toggle_y=True, return_fig=True)
add_lines(fig)
```

```python
fig = line([torch.tensor(test_losses[::100])/metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Restricted Loss to Test Loss Ratio", toggle_x=True, toggle_y=True, return_fig=True)
# WARNING: bugged when cancelling training half way thr ough
add_lines(fig)
```

## Excluded Loss

```python
approx_neuron_acts = torch.zeros_like(neuron_acts)
# approx_neuron_acts += neuron_acts.mean(dim=0)
a = torch.arange(p)[:, None]
b = torch.arange(p)[None, :]
for freq in key_freqs:
    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
    cos_apb_vec /= cos_apb_vec.norm()
    cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
    sin_apb_vec /= sin_apb_vec.norm()
    sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
excluded_neuron_acts = neuron_acts - approx_neuron_acts
excluded_logits = excluded_neuron_acts @ W_logit
print(loss_fn(excluded_logits[train_indices], train_labels))
```

```python
def get_excluded_loss(model):
    logits, cache = model.run_with_cache(dataset)
    logits = logits[:, -1, :]
    neuron_acts = cache["post", 0, "mlp"][:, -1, :]
    approx_neuron_acts = torch.zeros_like(neuron_acts)
    # approx_neuron_acts += neuron_acts.mean(dim=0)
    a = torch.arange(p)[:, None]
    b = torch.arange(p)[None, :]
    for freq in key_freqs:
        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)
        cos_apb_vec /= cos_apb_vec.norm()
        cos_apb_vec = einops.rearrange(cos_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec
        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)
        sin_apb_vec /= sin_apb_vec.norm()
        sin_apb_vec = einops.rearrange(sin_apb_vec, "a b -> (a b) 1")
        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec
    excluded_neuron_acts = neuron_acts - approx_neuron_acts
    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache["resid_mid", 0][:, -1, :]
    excluded_logits = residual_stream_final @ model.unembed.W_U
    return loss_fn(excluded_logits[train_indices], train_labels)
get_excluded_loss(model)
```

```python
get_metrics(model, metric_cache, get_excluded_loss, "excluded_loss", reset=True)
print(metric_cache["excluded_loss"].shape)
```

```python
fig = line([train_losses[::100], test_losses[::100], metric_cache["excluded_loss"], metric_cache["restricted_loss"]], x=np.arange(0, len(train_losses), 100), xaxis="Epoch", yaxis="Loss", log_y=True, title="Excluded and Restricted Loss Curve", line_labels=['train', 'test', "excluded_loss", "restricted_loss"], toggle_x=True, toggle_y=True, return_fig=True)

add_lines(fig)
```

---

# Head_Detector_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# TransformerLens Head Detector Demo

A common technique in mechanistic interpretability of transformer-based neural networks is identification of specialized attention heads, based on the attention patterns elicited by one or more prompts. The most basic examples of such heads are: previous token head, duplicate token head, or induction head ([more info](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_Jzi6YHRHKP1JziwdE02qdYZ)). Usually, such heads are identified manually, by through visualizations of attention patterns layer by layer, head by head, and trying to recognize the patterns by eye.

The purpose of the `TransformerLens.head_detector` feature is to automate a part of that workflow. The pattern characterizing a head of particular type/function is specified as a `Tensor` being a `seq_len x seq_len` [lower triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix). It can be either passed to the `detect_head` function directly or by giving a string identifying of several pre-defined detection patterns.

## How to use this notebook

Go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.

Tips for reading this Colab:

* You can run all this code for yourself!
* The graphs are interactive!
* Use the table of contents pane in the sidebar to navigate
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar, not CTRL+F

## Setup (Ignore)

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git
    # Install Neel's personal plotting utils
    %pip install git+https://github.com/neelnanda-io/neel-plotly.git
    # Install another version of node that makes PySvelte work way faster
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    %pip install git+https://github.com/neelnanda-io/PySvelte.git
    # Needed for PySvelte to work, v3 came out and broke things...
    %pip install typeguard==2.13.3
    %pip install typing-extensions
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEBUG_MODE:
    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "png"
```

```python
import torch
import einops
import pysvelte
from tqdm import tqdm

import transformer_lens
from transformer_lens import HookedTransformer, ActivationCache
from neel_plotly import line, imshow, scatter
```

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device = }")
```

### Some plotting utils

```python
# Util for plotting head detection scores

def plot_head_detection_scores(
    scores: torch.Tensor,
    zmin: float = -1,
    zmax: float = 1,
    xaxis: str = "Head",
    yaxis: str = "Layer",
    title: str = "Head Matches"
) -> None:
    imshow(scores, zmin=zmin, zmax=zmax, xaxis=xaxis, yaxis=yaxis, title=title)

def plot_attn_pattern_from_cache(cache: ActivationCache, layer_i: int):
    attention_pattern = cache["pattern", layer_i, "attn"].squeeze(0)
    attention_pattern = einops.rearrange(attention_pattern, "heads seq1 seq2 -> seq1 seq2 heads")
    print(f"Layer {layer_i} Attention Heads:")
    return pysvelte.AttentionMulti(tokens=model.to_str_tokens(prompt), attention=attention_pattern)
```

## Head detector

Utils: these will be in `transformer_lens.utils` after merging the fork to the main repo

```python
def is_square(x: torch.Tensor) -> bool:
    """Checks if `x` is a square matrix."""
    return x.ndim == 2 and x.shape[0] == x.shape[1]

def is_lower_triangular(x: torch.Tensor) -> bool:
    """Checks if `x` is a lower triangular matrix."""
    if not is_square(x):
        return False
    return x.equal(x.tril())
```

The code below is copy-pasted from the expanded (not yet merged) version of `transformer_lens.head_detector`.

After merging the code below can be replaced with simply

```py
from transformer_lens.head_detector import *
```

(but please don't use star-imports in production ;))

```python
from collections import defaultdict
import logging
from typing import cast, Dict, List, Optional, Tuple, Union
from typing_extensions import get_args, Literal

import numpy as np
import torch

from transformer_lens import HookedTransformer, ActivationCache
# from transformer_lens.utils import is_lower_triangular, is_square

HeadName = Literal["previous_token_head", "duplicate_token_head", "induction_head"]
HEAD_NAMES = cast(List[HeadName], get_args(HeadName))
ErrorMeasure = Literal["abs", "mul"]

LayerHeadTuple = Tuple[int, int]
LayerToHead = Dict[int, List[int]]

INVALID_HEAD_NAME_ERR = (
    f"detection_pattern must be a Tensor or one of head names: {HEAD_NAMES}; got %s"
)

SEQ_LEN_ERR = (
    "The sequence must be non-empty and must fit within the model's context window."
)

DET_PAT_NOT_SQUARE_ERR = "The detection pattern must be a lower triangular matrix of shape (sequence_length, sequence_length); sequence_length=%d; got detection patern of shape %s"

def detect_head(
    model: HookedTransformer,
    seq: Union[str, List[str]],
    detection_pattern: Union[torch.Tensor, HeadName],
    heads: Optional[Union[List[LayerHeadTuple], LayerToHead]] = None,
    cache: Optional[ActivationCache] = None,
    *,
    exclude_bos: bool = False,
    exclude_current_token: bool = False,
    error_measure: ErrorMeasure = "mul",
) -> torch.Tensor:
    """Searches the model (or a set of specific heads, for circuit analysis) for a particular type of attention head.
    This head is specified by a detection pattern, a (sequence_length, sequence_length) tensor representing the attention pattern we expect that type of attention head to show.
    The detection pattern can be also passed not as a tensor, but as a name of one of pre-specified types of attention head (see `HeadName` for available patterns), in which case the tensor is computed within the function itself.

    There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern.

    1. `"mul"` (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.
    Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score:
    how big fraction of this head's attention is allocated to these specific query-key pairs?
    Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).
    2. `"abs"` calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.
    The "raw result" ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval,
    with 1 being perfect match and -1 perfect mismatch.

    **Which one should you use?** `"abs"` is likely better for quick or exploratory investigations. For precise examinations where you're trying to
    reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to `"abs"`.

    The advantage of `"abs"` is that you can make more precise predictions, and have that measured in the score.
    You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer.
    The "mul" metric does not allow this, you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.

    Args:
    ----------
        model: Model being used.
        seq: String or list of strings being fed to the model.
        head_name: Name of an existing head in HEAD_NAMES we want to check. Must pass either a head_name or a detection_pattern, but not both!
        detection_pattern: (sequence_length, sequence_length) Tensor representing what attention pattern corresponds to the head we're looking for **or** the name of a pre-specified head. Currently available heads are: `["previous_token_head", "duplicate_token_head", "induction_head"]`.
        heads: If specific attention heads is given here, all other heads' score is set to -1. Useful for IOI-style circuit analysis. Heads can be spacified as a list tuples (layer, head) or a dictionary mapping a layer to heads within that layer that we want to analyze.
        cache: Include the cache to save time if you want.
        exclude_bos: Exclude attention paid to the beginning of sequence token.
        exclude_current_token: Exclude attention paid to the current token.
        error_measure: `"mul"` for using element-wise multiplication (default). `"abs"` for using absolute values of element-wise differences as the error measure.

    Returns:
    ----------
    A (n_layers, n_heads) Tensor representing the score for each attention head.

    Example:
    --------
    .. code-block:: python

        >>> from transformer_lens import HookedTransformer,  utils
        >>> from transformer_lens.head_detector import detect_head
        >>> import plotly.express as px

        >>> def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
        >>>     px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

        >>> model = HookedTransformer.from_pretrained("gpt2-small")
        >>> sequence = "This is a test sequence. This is a test sequence."

        >>> attention_score = detect_head(model, sequence, "previous_token_head")
        >>> imshow(attention_score, zmin=-1, zmax=1, xaxis="Head", yaxis="Layer", title="Previous Head Matches")
    """

    cfg = model.cfg
    tokens = model.to_tokens(seq).to(cfg.device)
    seq_len = tokens.shape[-1]

    # Validate error_measure

    assert error_measure in get_args(ErrorMeasure), f"Invalid {error_measure=}; valid values are {get_args(ErrorMeasure)}"

    # Validate detection pattern if it's a string
    if isinstance(detection_pattern, str):
        assert detection_pattern in HEAD_NAMES, (
            INVALID_HEAD_NAME_ERR % detection_pattern
        )
        if isinstance(seq, list):
            batch_scores = [detect_head(model, seq, detection_pattern) for seq in seq]
            return torch.stack(batch_scores).mean(0)
        detection_pattern = cast(
            torch.Tensor,
            eval(f"get_{detection_pattern}_detection_pattern(tokens.cpu())"),
        ).to(cfg.device)

    # if we're using "mul", detection_pattern should consist of zeros and ones
    if error_measure == "mul" and not set(detection_pattern.unique().tolist()).issubset(
        {0, 1}
    ):
        logging.warning(
            "Using detection pattern with values other than 0 or 1 with error_measure 'mul'"
        )

    # Validate inputs and detection pattern shape
    assert 1 < tokens.shape[-1] < cfg.n_ctx, SEQ_LEN_ERR
    assert (
        is_lower_triangular(detection_pattern) and seq_len == detection_pattern.shape[0]
    ), DET_PAT_NOT_SQUARE_ERR % (seq_len, detection_pattern.shape)

    if cache is None:
        _, cache = model.run_with_cache(tokens, remove_batch_dim=True)

    if heads is None:
        layer2heads = {
            layer_i: list(range(cfg.n_heads)) for layer_i in range(cfg.n_layers)
        }
    elif isinstance(heads, list):
        layer2heads = defaultdict(list)
        for layer, head in heads:
            layer2heads[layer].append(head)
    else:
        layer2heads = heads

    matches = -torch.ones(cfg.n_layers, cfg.n_heads)

    for layer, layer_heads in layer2heads.items():
        # [n_heads q_pos k_pos]
        layer_attention_patterns = cache["pattern", layer, "attn"]
        for head in layer_heads:
            head_attention_pattern = layer_attention_patterns[head, :, :]
            head_score = compute_head_attention_similarity_score(
                head_attention_pattern,
                detection_pattern=detection_pattern,
                exclude_bos=exclude_bos,
                exclude_current_token=exclude_current_token,
                error_measure=error_measure,
            )
            matches[layer, head] = head_score
    return matches

# Previous token head
def get_previous_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [previous token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc).

    Args:
      tokens: Tokens being fed to the model.
    """
    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])
    # Adds a diagonal of 1's below the main diagonal.
    detection_pattern[1:, :-1] = torch.eye(tokens.shape[-1] - 1)
    return torch.tril(detection_pattern)

# Duplicate token head
def get_duplicate_token_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [duplicate token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo).

    Args:
      sequence: String being fed to the model.
    """
    # [pos x pos]
    token_pattern = tokens.repeat(tokens.shape[-1], 1).numpy()

    # If token_pattern[i][j] matches its transpose, then token j and token i are duplicates.
    eq_mask = np.equal(token_pattern, token_pattern.T).astype(int)

    np.fill_diagonal(
        eq_mask, 0
    )  # Current token is always a duplicate of itself. Ignore that.
    detection_pattern = eq_mask.astype(int)
    return torch.tril(torch.as_tensor(detection_pattern).float())

# Induction head
def get_induction_head_detection_pattern(
    tokens: torch.Tensor,  # [batch (1) x pos]
) -> torch.Tensor:
    """Outputs a detection score for [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY).

    Args:
      sequence: String being fed to the model.
    """
    duplicate_pattern = get_duplicate_token_head_detection_pattern(tokens)

    # Shift all items one to the right
    shifted_tensor = torch.roll(duplicate_pattern, shifts=1, dims=1)

    # Replace first column with 0's
    # we don't care about bos but shifting to the right moves the last column to the first,
    # and the last column might contain non-zero values.
    zeros_column = torch.zeros(duplicate_pattern.shape[0], 1)
    result_tensor = torch.cat((zeros_column, shifted_tensor[:, 1:]), dim=1)
    return torch.tril(result_tensor)

def get_supported_heads() -> None:
    """Returns a list of supported heads."""
    print(f"Supported heads: {HEAD_NAMES}")

def compute_head_attention_similarity_score(
    attention_pattern: torch.Tensor,  # [q_pos k_pos]
    detection_pattern: torch.Tensor,  # [seq_len seq_len] (seq_len == q_pos == k_pos)
    *,
    exclude_bos: bool,
    exclude_current_token: bool,
    error_measure: ErrorMeasure,
) -> float:
    """Compute the similarity between `attention_pattern` and `detection_pattern`.

    Args:
      attention_pattern: Lower triangular matrix (Tensor) representing the attention pattern of a particular attention head.
      detection_pattern: Lower triangular matrix (Tensor) representing the attention pattern we are looking for.
      exclude_bos: `True` if the beginning-of-sentence (BOS) token should be omitted from comparison. `False` otherwise.
      exclude_bcurrent_token: `True` if the current token at each position should be omitted from comparison. `False` otherwise.
      error_measure: "abs" for using absolute values of element-wise differences as the error measure. "mul" for using element-wise multiplication (legacy code).
    """
    assert is_square(
        attention_pattern
    ), f"Attention pattern is not square; got shape {attention_pattern.shape}"

    # mul

    if error_measure == "mul":
        if exclude_bos:
            attention_pattern[:, 0] = 0
        if exclude_current_token:
            attention_pattern.fill_diagonal_(0)
        score = attention_pattern * detection_pattern
        return (score.sum() / attention_pattern.sum()).item()

    # abs

    abs_diff = (attention_pattern - detection_pattern).abs()
    assert (abs_diff - torch.tril(abs_diff).to(abs_diff.device)).sum() == 0

    size = len(abs_diff)
    if exclude_bos:
        abs_diff[:, 0] = 0
    if exclude_current_token:
        abs_diff.fill_diagonal_(0)

    return 1 - round((abs_diff.mean() * size).item(), 3)

```

## Using Head Detector For Premade Heads

Load the model

```python
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
```

See what heads are supported out of the box

```python
get_supported_heads()
```

Let's test detecting previous token head in the following prompt.

```python
prompt = "The head detector feature for TransformerLens allows users to check for various common heads automatically, reducing the cost of discovery."
head_scores = detect_head(model, prompt, "previous_token_head")
plot_head_detection_scores(head_scores, title="Previous Head Matches")
```

We can see both L2H2 and L4H11 are doing a fair bit of previous token detection. Let's take a look and see if that pans out.

```python
_, cache = model.run_with_cache(prompt)
```

```python
plot_attn_pattern_from_cache(cache, 2)
```

```python
plot_attn_pattern_from_cache(cache, 4)
```

As we expected, L2H2 is doing a lot of previous token detection, but doesn't appear to be a sharp previous token detection head. L4H11, on the other hand, is pretty much perfect. In fact, the only place it seems to be putting any other attention is the very first token, where it pays attention to the BOS (*beginning-of-sentence*) token.

Mechanistic interpretability is still a very new field, and we don't know the best ways to measure things yet. Ignoring attention paid to BOS allows us to solve problems like the above, but may also give us artifically high results for a head like L4H10, which doesn't appear to be doing much of anything, but does have a bit of previous token attention going on if you squint carefully.

As such, the head detector supports both an `exclude_bos` and `exclude_current_token` argument, which ignores all BOS attention and all current token attention respectively. By default these are `False`, but this is a pretty arbitrary decision, so feel free to try things out! You don't need a good reason to change these arguments - pick whatever best helps you find out useful things!

```python
head_scores = detect_head(model, prompt, "previous_token_head", exclude_bos=True, exclude_current_token=True)
plot_head_detection_scores(head_scores, title="Previous Head Matches")
```

Now we have a lot more detection, including L0H3 and L5H6 which were unremarkable before. Let's check them out!

```python
plot_attn_pattern_from_cache(cache, 5)
```

```python
plot_attn_pattern_from_cache(cache, 0)
```

Here, we see some interesting results. L5H6 does very little, but happens to react quite strongly to the first token of "Trans|former". (Capital letters? Current word detection? We don't know)

L0H3 reacts almost entirely to the current token, but what little it does outside of this pays attention to the previous token. Again, it seems to be caring about the first token of "Trans|former".

In order to more fully automate these heads, we'll need to discover more principled ways of expressing these scores. For now, you can see how while scores may be misleading, different scores lead us to interesting results.

## Using Head Detector for Custom Heads

These heads are great, but sometimes there are more than three things going on in Transformers. [citation needed] As a result, we may want to use our head detector for things that aren't pre-included in TransformerLens. Fortunately, the head detector provides support for this, via **detection patterns**.

A detection pattern is simply a matrix of the same size as our attention pattern, which specifies the attention pattern exhibited by the kind of head we're looking for.

There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern. You can choose it by passing the right value to the `error_measure` argument.

### 1. `"mul"` (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.

Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score: how big fraction of this head's attention is allocated to these specific query-key pairs? Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).

<br>

$$
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0.5 & 0.5 & 0 & 0 \\
0.2 & 0.3 & 0.5 & 0 \\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
\odot
\begin{pmatrix}
0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 & 0 & 0 \\
0.5 & 0 & 0 & 0 \\
0 & 0.3 & 0 & 0 \\
0 & 0 & 0.5 & 0
\end{pmatrix}
$$

<br>

0.5, 0.3, and 0.5 all get multiplied by 1, so they get kept. All the others go to 0 and are removed. (Note: You can use values other than 0 or 1 when creating your own heads)

Our total score would then be 1.3 / 4, or 0.325. If we ignore bos and current token, it would be 0.8 / 0.95 instead, or ~0.842. (This is a large difference, but the difference generally gets smaller as the matrices get bigger)

This is how the head detector works under the hood - each existing head just has its own detection pattern. Thus, we can pass in our own detection pattern using the `detection_pattern` argument.

### 2. `"abs"` calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.

The "raw result" ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval, with 1 being perfect match and -1 perfect mismatch.

We take the attention pattern and compute its absolute element-wise difference with our detection pattern. Since every number in any of the two patterns has a value between -1 and 1, the maximum absolute difference of any pair is 2 and the minimum is 0:

$$|-1-1|=|1-(-1)|=2$$

$$|x-x|=0$$

That number tells us how much our expectation and the real attention pattern diverge, i.e., the error.

$$
M_{diff}=
\left|
\begin{pmatrix}
1 & 0 & 0 & 0
\\
0.5 & 0.5 & 0 & 0
\\
0.2 & 0.3 & 0.5 & 0
\\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
-
\begin{pmatrix}
0 & 0 & 0 & 0
\\
1 & 0 & 0 & 0
\\
0 & 1 & 0 & 0
\\
0 & 0 & 1 & 0
\end{pmatrix}
\right|
=
\begin{pmatrix}
1 & 0 & 0 & 0
\\
0.5 & 0.5 & 0 & 0
\\
0.2 & 0.7 & 0.5 & 0
\\
0.1 & 0.15 & 0.5 & 0.25
\end{pmatrix}
$$

We take the mean and multiply it by the number of rows.

We subtract the result from 1 in order to map the (0, 2) interval where lower is better to the (-1, 1) interval where higher is better.

$$1 - \text{n_rows} \times \text{mean}(M_{diff}) = 1 - 4 \times 0.275 = 1 - 1.1 = -.1$$

Our final score would then be -1. If we ignore `BOS` and current token, it would be 0.6625. (This is a large difference, but the difference generally gets smaller as the matrices get bigger.)

This is how the head detector works under the hood - each existing head just has its own detection pattern. Thus, we can pass in our own detection pattern using the `detection_pattern` argument.

I'm curious what's going on with this L0H3 result, where we mostly focus on the current token but occasionally focus on the "Trans" token in "Trans|former". Let's make a **current word head** detection pattern, which returns 1 for previous tokens that are part of the current word being looked at, and 0 for everything else.

### **Which one should you use?**

`"abs"` is likely better for quick or exploratory investigations. For precise examinations where you're trying to reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to `"abs"`.

The advantage of `"abs"` is that you can make more precise predictions, and have that measured in the score. You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer. The "mul" metric does not allow this, you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.

Below we show how different scores these two measures can give on the same prompt. After that, we will proceed with using `"abs"` and will get back to `"mul"` at the end of the notebook.

```python
prompt = "The following lexical sequence has been optimised for the maximisation of loquaciously multitoken letter combinations."
tokens = model.to_str_tokens(prompt)
print(len(tokens), tokens)
detection_pattern = []
for i in range(2):
  detection_pattern.append([0 for t in tokens]) # Ignore BOS token and first token.
for i in range(2, len(tokens)):
    current_token = i
    previous_tokens_in_word = 0
    while not tokens[current_token].startswith(' '): # If the current token does not start with a space (and is not the first token) it's part of a word.
      previous_tokens_in_word += 1
      current_token -= 1
    # Hacky code that adds in some 1's where needed, and fills the rest of the row with 0's.
    detection_pattern.append([0 for j in range(i - previous_tokens_in_word)] + [1 for j in range(previous_tokens_in_word)] + [0 for j in range(i+1, len(tokens)+1)])
detection_pattern = torch.as_tensor(detection_pattern).to(device)
detection_pattern.shape
```

```python
_, cache = model.run_with_cache(prompt)
```

`"mul"`

```python
head_scores = detect_head(
    model,
    prompt,
    detection_pattern=detection_pattern,
    exclude_bos=False,
    exclude_current_token=True,
    error_measure="mul"
)
plot_head_detection_scores(head_scores, title="Current Word Head Matches (mul)")
```

`"abs"`

```python
head_scores = detect_head(
    model,
    prompt,
    detection_pattern=detection_pattern,
    exclude_bos=False,
    exclude_current_token=True,
    error_measure="abs"
)
plot_head_detection_scores(head_scores, title="Current Word Head Matches (abs)")
```

75% match for L0H3 - only 16% for L5H6. Let's check them out with our new sequence!

```python
plot_attn_pattern_from_cache(cache, 5)
```

```python
plot_attn_pattern_from_cache(cache, 0)
```

As we can see, L5H6 appears to be doing something totally different than we expected, whereas L0H3 is mostly doing what we expected - by our original hypothesis, we would expect "lo|qu|aciously" to have a lot of attention paid to, and "combinations|." the same, which didn't happen. However, our two-token words were exactly as we expected. Could this be a two-token detector (that doesn't work on punctuation)? A "current word" detector that just doesn't understand an obscure word like "loquaciously"? The field is full of such problems, just waiting to be answered!

So, why do this at all? For just a couple of sentences, it's easier to just look at the attention patterns directly and see what we get. But as we can see, heads react differently to different sentences. What we might want to do is give an entire dataset or distribution of sentences to our attention head and see that it consistently does what we want - that's something that would be much harder without this feature!

So what if we gave it a whole distribution? Rather than actually create one, which is not the point of this demo, we're just going to repeat our last sentence a thousand times.

```python
scores = []
for i in tqdm(range(100)):
    scores.append(detect_head(model, prompt, detection_pattern=detection_pattern, exclude_bos=False, exclude_current_token=True, error_measure="abs"))
scores = torch.stack(scores).mean(dim=0)
plot_head_detection_scores(scores, title="Current Word Head Matches")
```

## Processing Many Prompts

`detect_head` can also take more than one prompt. The resulting attention score is the mean of scores for each prompt.

```python
prompts = [
    "This is the first the test prompt.",
    "This is another test prompt, being just a sequence of tokens.",
    "If you're interested in mechanistic interpretability, this is how the sausage REALLY is made."
]
```

```python
head_scores = detect_head(model, prompts, "previous_token_head", error_measure="abs")
plot_head_detection_scores(head_scores, title="Previous token head; average across 3 prompts")
```

L4H11 emerges again as the dominant head, exactly as expected.

What about duplicate token heads?

```python
head_scores = detect_head(model, prompts, "duplicate_token_head", error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

Nothing but this should be expected, in hindsight, since our prompts don't contain too many duplicate tokens. Let's try three other prompts that do.

```python
prompts = [
    "one two three one two three one two three",
    "1 2 3 4 5 1 2 3 4 1 2 3 1 2 3 4 5 6 7",
    "green ideas sleep furiously; green ideas don't sleep furiously"
]
```

```python
head_scores = detect_head(model, prompts, "duplicate_token_head", exclude_bos=False, exclude_current_token=False, error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

3 or 4 heads seem to do something that we would expected from a duplicate token head but the signal is not very strong. You can tweak the `exclude_bos` and `exclude_current_token` flags if you want, but it doesn't change much.

Let's hunt for induction heads now!

```python
head_scores = detect_head(model, prompts, "induction_head", exclude_bos=False, exclude_current_token=False, error_measure="abs")
plot_head_detection_scores(head_scores, title="Duplicate token head; average across 3 prompts")
```

Similarly, at least on average.

Try running the script on different prompts and see if you can get high values for duplicate token or induction heads.

## Why not element-wise multiplication - robustness against [Goodharting](https://en.wikipedia.org/wiki/Goodhart%27s_law)

Initially, the error measure was not the mean element-wise absolute value error (normalized to the number of rows) but the mean [element-wise product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)). However, it had its problems, such as susceptibility to Goodharting. You can specify a pattern consisting of all ones and in this way achieve a perfect match for all layers and heads in the model.

More generally, using element-wise product causes the score to go down when we narrow our hypothesis. We can get a maximum score by just predicting 1 for everything.

```python
prompt = "The head detector feature for TransformerLens allows users to check for various common heads automatically, reducing the cost of discovery."
seq_len = len(model.to_str_tokens(prompt))
# torch.tril to make the pattern lower triangular
ones_detection_pattern = torch.tril(torch.ones(seq_len, seq_len).to(device))
```

```python
ones_head_scores = detect_head(
    model,
    prompt,
    ones_detection_pattern,
    exclude_bos=True,
    exclude_current_token=True,
)
plot_head_detection_scores(ones_head_scores, title="Transformers Have Now Been Solved, We Can All Go Home")
```

The new error measure also achieves uniform score but this time its uniformly extremely negative because **not a single head in the model matches this pattern**.

*(It's true that the scores descend below -9 whereas in theory they should remain within the (-1, 1) range. It's not yet clear if that matters for real-world uses.)*

An alternative would be to demand that *predictions add up to 1 for each row* but that seems unnecessarily nitpicky considering that your score will get reduced in general for not doing that anyway.

Mean squared errors have also bean tried before converging on the absolute ones. The problem with MSE is that the scores get lower as attention gets more diffuse. Error value of 1 would become 1, 0.5 would become 0.25 etc.

```python
ones_head_scores = detect_head(
    model,
    prompt,
    ones_detection_pattern,
    exclude_bos=True,
    exclude_current_token=True,
    error_measure="abs" # we specify the error measure here
)
plot_head_detection_scores(ones_head_scores, title="Transformers Have Not Been Solved Yet, Get Back To Work!")
```

## Further improvements

**Performance for large distributions** isn't as good as it could be. The head detector could be rewritten to support taking in a list of sequences and performing these computations in parallel, but 1000 sequences per minute is certainly adequate for most use cases. If having this be faster would help your research, please write up an issue on TransformerLens, mention it on the Open Source Mechanistic Interpretability Slack, or e-mail jaybaileycs@gmail.com.

### Other

- Extending to few-shot learning/translation heads
- More pre-specified heads?
- For inspiration, see [this post from Neel](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/btasQF7wiCYPsr5qw)

---

# Interactive_Neuroscope.ipynb

<a target="_blank" href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Interactive_Neuroscope.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Interactive Neuroscope

*This is an interactive accompaniment to [neuroscope.io](https://neuroscope.io) and to the [studying learned language features post](https://www.alignmentforum.org/posts/Qup9gorqpd9qKAEav/200-cop-in-mi-studying-learned-features-in-language-models) in [200 Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems)*

There's a surprisingly rich ecosystem of easy ways to create interactive graphics, especially for ML systems. If you're trying to do mechanistic interpretability, the ability to do web dev and to both visualize data and interact with it seems high value!

This is a demo of how you can combine HookedTransformer and [Gradio](https://gradio.app/) to create an interactive Neuroscope - a visualization of a neuron's activations on text that will dynamically update as you edit the text. I don't particularly claim that this code is any *good*, but the goal is to illustrate what quickly hacking together a custom visualisation (while knowing fuck all about web dev, like me) can look like! (And as such, I try to explain the basic web dev concepts I use)

Note that you'll need to run the code yourself to get the interactive interface, so the cell at the bottom will be blank at first!

To emphasise - the point of this notebook is to be a rough proof of concept that just about works, *not* to be the well executed ideal of interactively studying neurons! You are highly encouraged to write your own (and ideally, to [make a pull request](https://github.com/neelnanda-io/TransformerLens/pulls) with improvements!)

## Setup

```python
# NBVAL_IGNORE_OUTPUT
# Janky code to do different setup when run in a Colab notebook vs VSCode
import os

DEVELOPMENT_MODE = True
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install gradio
    %pip install datasets==2.19.1
```

```python
import gradio as gr
from transformer_lens import HookedTransformer
from transformer_lens.utils import to_numpy
from IPython.display import HTML
```

## Extracting Model Activations

We first write some code using HookedTransformer's cache to extract the neuron activations on a given layer and neuron, for a given text

```python
# NBVAL_IGNORE_OUTPUT
model_name = "gpt2-small"
model = HookedTransformer.from_pretrained(model_name)
```

```python
def get_neuron_acts(text, layer, neuron_index):
    # Hacky way to get out state from a single hook - we have a single element list and edit that list within the hook.
    cache = {}

    def caching_hook(act, hook):
        cache["activation"] = act[0, :, neuron_index]

    model.run_with_hooks(
        text, fwd_hooks=[(f"blocks.{layer}.mlp.hook_post", caching_hook)]
    )
    return to_numpy(cache["activation"])
```

We can run this function and verify that it gives vaguely sensible outputs

```python
default_layer = 9
default_neuron_index = 652
default_text = "The following is a list of powers of 10: 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000"
print(model.to_str_tokens(default_text))
```

```python
# NBVAL_IGNORE_OUTPUT
print(get_neuron_acts(default_text, default_layer, default_neuron_index))
```

## Visualizing Model Activations

We now write some code to visualize the neuron activations on some text - we're going to hack something together which just does some string processing to make an HTML string, with each token element colored according to the intensity neuron activation. We normalize the neuron activations so they all lie in [0, 1]. You can do much better, but this is a useful proof of concept of what "just hack stuff together" can look like!

I'll be keeping neuron 562 in layer 9 as a running example, as it seems to activate strongly on powers of 10.

Note that this visualization is very sensitive to `max_val` and `min_val`! You can tune those to whatever seems reasonable for the distribution of neuron activations you care about - I generally default to `min_val=0` and `max_val` as the max activation across the dataset.

```python
# This is some CSS (tells us what style )to give each token a thin gray border, to make it easy to see token separation
style_string = """<style>
    span.token {
        border: 1px solid rgb(123, 123, 123)
        }
    </style>"""

def calculate_color(val, max_val, min_val):
    # Hacky code that takes in a value val in range [min_val, max_val], normalizes it to [0, 1] and returns a color which interpolates between slightly off-white and red (0 = white, 1 = red)
    # We return a string of the form "rgb(240, 240, 240)" which is a color CSS knows
    normalized_val = (val - min_val) / max_val
    return f"rgb(240, {240*(1-normalized_val)}, {240*(1-normalized_val)})"

def basic_neuron_vis(text, layer, neuron_index, max_val=None, min_val=None):
    """
    text: The text to visualize
    layer: The layer index
    neuron_index: The neuron index
    max_val: The top end of our activation range, defaults to the maximum activation
    min_val: The top end of our activation range, defaults to the minimum activation

    Returns a string of HTML that displays the text with each token colored according to its activation

    Note: It's useful to be able to input a fixed max_val and min_val, because otherwise the colors will change as you edit the text, which is annoying.
    """
    if layer is None:
        return "Please select a Layer"
    if neuron_index is None:
        return "Please select a Neuron"
    acts = get_neuron_acts(text, layer, neuron_index)
    act_max = acts.max()
    act_min = acts.min()
    # Defaults to the max and min of the activations
    if max_val is None:
        max_val = act_max
    if min_val is None:
        min_val = act_min
    # We want to make a list of HTML strings to concatenate into our final HTML string
    # We first add the style to make each token element have a nice border
    htmls = [style_string]
    # We then add some text to tell us what layer and neuron we're looking at - we're just dealing with strings and can use f-strings as normal
    # h4 means "small heading"
    htmls.append(f"<h4>Layer: <b>{layer}</b>. Neuron Index: <b>{neuron_index}</b></h4>")
    # We then add a line telling us the limits of our range
    htmls.append(
        f"<h4>Max Range: <b>{max_val:.4f}</b>. Min Range: <b>{min_val:.4f}</b></h4>"
    )
    # If we added a custom range, print a line telling us the range of our activations too.
    if act_max != max_val or act_min != min_val:
        htmls.append(
            f"<h4>Custom Range Set. Max Act: <b>{act_max:.4f}</b>. Min Act: <b>{act_min:.4f}</b></h4>"
        )
    # Convert the text to a list of tokens
    str_tokens = model.to_str_tokens(text)
    for tok, act in zip(str_tokens, acts):
        # A span is an HTML element that lets us style a part of a string (and remains on the same line by default)
        # We set the background color of the span to be the color we calculated from the activation
        # We set the contents of the span to be the token
        htmls.append(
            f"<span class='token' style='background-color:{calculate_color(act, max_val, min_val)}' >{tok}</span>"
        )

    return "".join(htmls)
```

```python
# NBVAL_IGNORE_OUTPUT
# The function outputs a string of HTML
default_max_val = 4.0
default_min_val = 0.0
default_html_string = basic_neuron_vis(
    default_text,
    default_layer,
    default_neuron_index,
    max_val=default_max_val,
    min_val=default_min_val,
)

# IPython lets us display HTML
print("Displayed HTML")
display(HTML(default_html_string))

# We can also print the string directly
print("HTML String - it's just raw HTML code!")
print(default_html_string)
```

## Create Interactive UI

We now put all these together to create an interactive visualization in Gradio!

The internal format is that there's a bunch of elements - Textboxes, Numbers, etc which the user can interact with and which return strings and numbers. And we can also define output elements that just display things - in this case, one which takes in an arbitrary HTML string. We call `input.change(update_function, inputs, output)` - this says "if that input element changes, run the update function on the value of each of the elements in `inputs` and set the value of `output` to the output of the function". As a bonus, this gives us live interactivity!

This is also more complex than a typical Gradio intro example - I wanted to use custom HTML to display the nice colours, which made things much messier! Normally you could just make `out` into another Textbox and pass it a string.

```python
# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components
with gr.Blocks() as demo:
    gr.HTML(value=f"Hacky Interactive Neuroscope for {model_name}")
    # The input elements
    with gr.Row():
        with gr.Column():
            text = gr.Textbox(label="Text", value=default_text)
            # Precision=0 makes it an int, otherwise it's a float
            # Value sets the initial default value
            layer = gr.Number(label="Layer", value=default_layer, precision=0)
            neuron_index = gr.Number(
                label="Neuron Index", value=default_neuron_index, precision=0
            )
            # If empty, these two map to None
            max_val = gr.Number(label="Max Value", value=default_max_val)
            min_val = gr.Number(label="Min Value", value=default_min_val)
            inputs = [text, layer, neuron_index, max_val, min_val]
        with gr.Column():
            # The output element
            out = gr.HTML(label="Neuron Acts", value=default_html_string)
    for inp in inputs:
        inp.change(basic_neuron_vis, inputs, out)
```

We can now launch our demo element, and we're done! The setting share=True even gives you a public link to the demo (though it just redirects to the backend run by this notebook, and will go away once you turn the notebook off!) Sharing makes it much slower, and can be turned off if you aren't in a colab.

**Exercise:** Explore where this neuron does and does not activate. Is it just powers of ten? Just comma separated numbers? Numbers in any particular sequence?

```python
# NBVAL_IGNORE_OUTPUT
demo.launch(share=True, height=1000)
```

---

# LLaMA.ipynb


---

# LLaMA2_GPU_Quantized.ipynb


---

# LLaVA.ipynb

### LLaVA use case demonstration

At that notebook you can see simple example of how to use TransformerLens for LLaVA interpretability. More specifically you can pass united image patch embeddings and textual embedding to LLaVA language model (Vicuna) with TransformerLens and get logits and cache that contains activations for next analysis. Here we consider the simplest example of LLaVA and TransformerLens sharing.

```python
# import staff
import sys

# Uncomment if use clonned version of TransformerLens
# currently forked version https://github.com/zazamrykh/TransformerLens supports
TL_path = r"../"
if TL_path not in sys.path:
	sys.path.insert(0, TL_path)
	sys.path.insert(0, TL_path + r"/transformer_lens")

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration  # Should update transformer to latest version

# For image loading
from PIL import Image
import requests
from io import BytesIO

device = 'cuda' if torch.cuda.is_available() else 'cpu'

import matplotlib.pyplot as plt
%matplotlib inline

from transformer_lens import HookedTransformer
import circuitsvis as cv

_ = torch.set_grad_enabled(False)
```

Load llava model from hugging face. Load some revision because at this moment newest one is not working.

```python
model_id = "llava-hf/llava-1.5-7b-hf"

llava = LlavaForConditionalGeneration.from_pretrained(
	model_id,
	torch_dtype=torch.float16,
	load_in_4bit=False,
	low_cpu_mem_usage=True,
	revision="a272c74",
	device_map="cpu"
)

for param in llava.parameters():  # At this demo we don't need grads
	param.requires_grad = False

processor = AutoProcessor.from_pretrained(model_id, revision="a272c74")
tokenizer = processor.tokenizer

# Taking model apart
language_model = llava.language_model.eval()
config = language_model.config
print("Base language model:", config._name_or_path)

vision_tower = llava.vision_tower.to(device).eval()
projector = llava.multi_modal_projector.to(device).eval()
```

```python
# You can write your own version of getting language model's input embeddings similar way
# This function will not be working with old transformers library version. Should update transformers library.
def get_llm_input_embeddings(llava, processor, image: Image, text: str, device='cuda'):
    """ Extract features from image, project them to LLM's space and insert them to text embedding sequence.
    Returns:
    	inputs_embeds, attention_mask, labels, position_ids - input for language model of LLaVA
    """
    conversation = [
      {
        "role": "user",
        "content": [
            {"type": "text", "text": text},
            {"type": "image"},
          ],
      },
    	]
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)
    llava.vision_tower.to(device)
    llava.multi_modal_projector.to(device)

    clip_output = llava.vision_tower(inputs['pixel_values'])
    projector_output = llava.multi_modal_projector(clip_output.last_hidden_state)

    before_device = llava.language_model.model.embed_tokens.weight.device
    llava.language_model.model.embed_tokens.to(device)
    text_embeddings = llava.language_model.model.embed_tokens(inputs['input_ids'])
    llava.language_model.model.embed_tokens.to(before_device)

    full_sequence = torch.hstack([projector_output, text_embeddings])

    attention_mask = torch.ones(full_sequence.shape[:-1], device=full_sequence.device, dtype=int)
    inputs_embeds, attention_mask, labels, position_ids = llava._merge_input_ids_with_image_features(
		projector_output, text_embeddings, inputs['input_ids'], attention_mask, labels=None
	)  # Access to private member... Well, but what can i do :-)

    return inputs_embeds, attention_mask, labels, position_ids
```

Okay, now create HookedTransformer model

```python
hooked_llm = HookedTransformer.from_pretrained(
	"llama-7b-hf",  # Use config of llama
	center_unembed=False,
	fold_ln=False,
	fold_value_biases=False,
	device='cuda',
	hf_model=language_model,  # Use Vicuna's weights
	tokenizer=tokenizer,
	center_writing_weights=False,
	dtype=torch.float16,
	vocab_size=language_model.config.vocab_size  # New argument. llama and vicuna have different vocab size, so we pass it here
)

for param in hooked_llm.parameters():
	param.requires_grad = False
```

Now try if hooked model is working

```python
image_url = "https://github.com/zazamrykh/PicFinder/blob/main/images/doge.jpg?raw=true"
response = requests.get(image_url)
image = Image.open(BytesIO(response.content))
plt.axis('off')
_ = plt.imshow(image)
```

```python
question = "What do you see on photo?"
inputs_embeds, attention_mask, labels, position_ids = get_llm_input_embeddings(llava, processor, image, question, device=device)

# Return tokens
outputs = hooked_llm.generate(
	inputs_embeds,
	max_new_tokens=30,
	do_sample=True,
    return_type='tokens'
)
generated_text = processor.decode(outputs[0], skip_special_tokens=True)
print('Generated text:', generated_text)
```

```python
# Now return embeddings and then project them on vocab space
outputs = hooked_llm.generate(
	inputs_embeds,
	max_new_tokens=30,
	do_sample=True,
)

logits = outputs[:,-30:,:].to(device) @ language_model.model.embed_tokens.weight.T.to(device)
generated_text = processor.decode(logits.argmax(-1)[0], skip_special_tokens=True)
print('Generated text:', generated_text)
```

As we can see everything is working. Now try visualize attention patterns in generated output.

```python
# Here we visualize attention for the last 30 tokens.
logits, cache = hooked_llm.run_with_cache(inputs_embeds, start_at_layer=0, remove_batch_dim=True)

layer_to_visualize = 16
tokens_to_show = 30
attention_pattern = cache["pattern", layer_to_visualize, "attn"]

product = inputs_embeds @ language_model.model.embed_tokens.weight.T.to(device)  # Project embeddings to vocab
llama_str_tokens = hooked_llm.to_str_tokens(product.argmax(dim=-1)[0])

print(f"Layer {layer_to_visualize} Head Attention Patterns:")
display(cv.attention.attention_patterns(tokens=llama_str_tokens[-tokens_to_show:],
										attention=attention_pattern[:, -tokens_to_show:, -tokens_to_show:]))
```

As we can see image tokens also appears and can be used for multimodal attention exploration.

---

# Main_Demo.ipynb


---

# No_Position_Experiment.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Introduction

The accompanying notebook to my [real-time research](https://www.youtube.com/watch?v=yo4QvDn-vsU) video. Trains a model with no positional embeddings to predict the previous token, and makes a start at analysing what's going on there!

EDIT: The loss spikes were due to the learning rate being max(step/100, 1.0) not min! Thanks to MadHatter for catching that.

# Setup

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")

if IN_COLAB or IN_GITHUB:
    %pip install einops
    %pip install transformer_lens@v1.15.0

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git

from transformer_lens import HookedTransformer, HookedTransformerConfig
import torch
import numpy as np
import plotly.express as px
import plotly.io as pio

pio.renderers.default = "colab"
import tqdm.auto as tqdm
import einops
from transformer_lens.utils import to_numpy

device = "cuda" if torch.cuda.is_available() else "cpu"
```

Some plotting code. Wrappers around Plotly, not important to understand.

```python
def line(tensor, line_labels=None, yaxis="", xaxis="", **kwargs):
    tensor = to_numpy(tensor)
    labels = {"y": yaxis, "x": xaxis}
    fig = px.line(tensor, labels=labels, **kwargs)
    if line_labels:
        for c, label in enumerate(line_labels):
            fig.data[c].name = label
    fig.show()

def imshow(tensor, yaxis="", xaxis="", **kwargs):
    tensor = to_numpy(tensor)
    plot_kwargs = {
        "color_continuous_scale": "RdBu",
        "color_continuous_midpoint": 0.0,
        "labels": {"x": xaxis, "y": yaxis},
    }
    plot_kwargs.update(kwargs)
    px.imshow(tensor, **plot_kwargs).show()
```

# Model Training

## Setup

### Defining the Model

```python
cfg = HookedTransformerConfig(
    n_layers=2,
    d_model=64,
    d_head=64,
    n_heads=1,
    d_mlp=256,
    d_vocab=300,
    n_ctx=50,
    act_fn="relu",
    normalization_type="LN",
    device=device,
)
model = HookedTransformer(cfg)
```

```python
def deactivate_position(model):
    model.pos_embed.W_pos.data[:] = 0.0
    model.pos_embed.W_pos.requires_grad = False

deactivate_position(model)
```

```python
print(model)
```

### Define data + Loss function

```python
def make_data_generator(cfg, batch_size, seed=123, incl_bos_token=True):
    torch.manual_seed(seed)
    while True:
        x = torch.randint(1, cfg.d_vocab, (batch_size, cfg.n_ctx))
        if incl_bos_token:
            x[:, 0] = 0
        yield x

data_generator = make_data_generator(cfg, 2)
print(next(data_generator))
```

```python
def loss_fn(logits, tokens, per_token=False):
    # logit shape: [batch, pos, vocab]
    # token shape: [batch, pos]
    logits = logits[:, 1:]
    tokens = tokens[:, :-1]
    log_probs = logits.log_softmax(-1)
    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]
    if per_token:
        return -correct_log_probs
    else:
        return -correct_log_probs.mean()
```

```python
# Test the loss function works
test_tokens = torch.arange(5)[None, :]
test_logits = torch.randn(1, 5, 10)
test_logits[:, 1, 0] = 10.0
test_logits[:, 2, 1] = 10.0
test_logits[:, 3, 2] = 10.0
test_logits[:, 4, 3] = 10.0
print(loss_fn(test_logits, test_tokens, per_token=True))
print(loss_fn(test_logits, test_tokens, per_token=False))
```

### Setup Optimizer

```python
batch_size = 256
num_epochs = 4000
lr = 1e-4
betas = (0.9, 0.95)
max_grad_norm = 1.0
wd = 0.1
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=wd)
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: min(i / 100, 1.0))

data_loader = make_data_generator(cfg, batch_size)
```

## Model Training

```python
losses = []
for epoch in tqdm.tqdm(range(num_epochs)):
    tokens = next(data_loader)
    tokens = tokens.to(device)
    logits = model(tokens)
    loss = loss_fn(logits, tokens)
    loss.backward()
    if max_grad_norm is not None:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
    optimizer.step()
    optimizer.zero_grad()
    scheduler.step()
    losses.append(loss.item())
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: {loss.item()}")
px.line(losses, labels={"x": "Epoch", "y": "Loss"})
```

```python
# torch.save(model.state_dict(), "no_pos_experiment_state_dict_v0.pth")
```

# Model Interpretability

```python
model.pos_embed.W_pos.norm()
```

## Look at attention patterns

```python
big_data_loader = make_data_generator(cfg, 4000)
big_tokens = next(big_data_loader)
big_tokens = big_tokens.to(device)
logits, cache = model.run_with_cache(big_tokens)
print("Loss:", loss_fn(logits, big_tokens).item())
```

```python
print(cache)
```

```python
cache["blocks.0.attn.hook_pattern"].shape
```

```python
batch_index = 0
tokens = big_tokens[batch_index]
imshow(
    to_numpy(cache["attn", 0].mean([0, 1])),
    title="Layer 0 Attention Pattern",
    height=500,
    width=500,
)
imshow(
    to_numpy(cache["attn", 1].mean([0, 1])),
    title="Layer 1 Attention Pattern",
    height=500,
    width=500,
)
```

## Look at how different bits of the model directly contribute to the logits

```python
resid_components = [
    cache["embed"],
    cache["attn_out", 0],
    cache["mlp_out", 0],
    cache["attn_out", 1],
    cache["mlp_out", 1],
]
labels = ["embed", "A0", "M0", "A1", "M2"]
resid_stack = torch.stack(resid_components, 0)
resid_stack = resid_stack - resid_stack.mean(-1, keepdim=True)
print(resid_stack.shape)
```

```python
fold_W_U = model.ln_final.w[:, None] * model.unembed.W_U
logit_components = resid_stack[:, batch_index] @ fold_W_U / cache["scale"][batch_index]
print(logit_components.shape)
```

```python
logit_components = logit_components - logit_components.mean(-1, keepdim=True)
line(
    logit_components[:, torch.arange(1, model.cfg.n_ctx).to(device), tokens[:-1]].T,
    line_labels=labels,
)
```

## Folding In LayerNorm

```python
analysis_cfg = HookedTransformerConfig(
    n_layers=2,
    d_model=64,
    d_head=64,
    n_heads=1,
    d_mlp=256,
    d_vocab=300,
    n_ctx=50,
    act_fn="relu",
    normalization_type="LNPre",
    init_weights=False,
)
analysis_model = HookedTransformer(analysis_cfg)
state_dict = model.state_dict()
analysis_model.load_and_process_state_dict(
    state_dict, fold_ln=True, center_writing_weights=True, center_unembed=True
)
deactivate_position(analysis_model)
```

```python
# analysis_model()
```

## Understand Attn 0

```python
QK = model.W_E @ model.W_Q[0, 0] @ model.W_K[0, 0].T @ model.W_E.T
imshow(QK, yaxis="Query", xaxis="Key")
```

```python
OV = model.W_E @ model.W_V[0, 0] @ model.W_O[0, 0] @ model.W_in[0]
imshow(OV, yaxis="Input Vocab", xaxis="Neuron")
```

```python
line(OV[:, torch.randint(0, 256, (5,))])
```

## Understand MLP 0

```python
imshow(cache["post", 0][batch_index], yaxis="Pos", xaxis="Neuron")
imshow(cache["post", 0].mean(0), yaxis="Pos", xaxis="Neuron")
imshow((cache["post", 0] > 0).float()[batch_index], yaxis="Pos", xaxis="Neuron")
imshow((cache["post", 0] > 0).float().mean(0), yaxis="Pos", xaxis="Neuron")
```

## Understand Attn 1

## Understand MLP 1

```python

```

# Experiment

```python
new_token_batch = next(big_data_loader).to(device)
baseline_loss = loss_fn(model(new_token_batch), new_token_batch).item()
print("Baseline loss:", baseline_loss)
```

```python
hook_list = list(model.hook_dict.keys())
losses = []
loss_labels = []
for hook_name in hook_list:
    if (
        hook_name in cache
        and hook_name != "hook_pos_embed"
        and "result" not in hook_name
    ):
        average_act = cache[hook_name].mean(0)

        def replacing_with_average_act(activation, hook):
            activation[:] = einops.repeat(
                average_act, "... -> batch ...", batch=new_token_batch.size(0)
            )
            return activation

        logits = model.run_with_hooks(
            new_token_batch, fwd_hooks=[(hook_name, replacing_with_average_act)]
        )
        loss = loss_fn(logits, new_token_batch)
        print(hook_name, loss.item())
        losses.append(loss.item())
        loss_labels.append(hook_name)
```

```python
line(losses, hover_name=loss_labels)
```

```python
cache.cache_dict.keys()
```

---

# Othello_GPT.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Othello_GPT.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

This is a demo notebook porting the weights of the Othello-GPT Model from the excellent [Emergent World Representations](https://arxiv.org/pdf/2210.13382.pdf) paper to my TransformerLens library. Check out the paper's [blog post](https://thegradient.pub/othello/), [paper](https://arxiv.org/pdf/2210.13382.pdf), and [github](https://github.com/likenneth/othello_world/)

I think this is a super interesting paper, and I want to better enable work trying to reverse-engineer this model! I'm particularly curious about:
* Why non-linear probes work much better than linear probes?
    * Is the model internally representing the board in a usable yet non-linear way?
    * Is there a representation of simpler concepts (eg diagonal lines in the board, number of black pieces, whether a cell is blank)) that the non-linear probe uses to compute board positions, but where the model internally reasons in this simpler representation?
* What's going up with the model editing?
    * The paper edits across many layers at once. What's the minimal edit that works?
        * Can we edit just before the final layer?
        * Can we do a single edit rather than across many layers?
    * If we contrast model activations pre and post edit, what changes?
        * Which components shift their output and how does this affect the logits?
        * Is there significant depth of composition, or does it just affect the output logits?
* Can we find any non-trivial circuits in the model?
    * Start with [exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), like direct logit attribution, or just looking at head attention patterns, and try to get traction
    * Pick a simple sub-task, eg figuring out whether a cell is blank, and try to interpret that.

I uploaded pre-converted checkpoints to HuggingFace, which can be automatically downloaded, and there's a code snippet to do this after the setup.

If you want to use the author's code, I wrote a script to load and convert checkpoints from the author's code, given below this.

To get started, check out the transformer lens [main tutorial](https://neelnanda.io/transformer-lens-demo) and [tutorial on exploratory techniques](https://neelnanda.io/exploratory-analysis-demo), and the author's [excellent Github](https://github.com/likenneth/othello_world/) (Ot**hello world**) for various notebooks demonstrating their code, showing how to load inputs, etc. And check out my [concrete open problems in mechanistic interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj) sequence, especially the algorithmic problems post, for tips on this style of research.

# Setup (Skip)

```python
# NBVAL_IGNORE_OUTPUT
import os

# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
IN_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"
try:
    import google.colab

    IN_COLAB = True
    print("Running as a Colab notebook")

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")

if IN_COLAB or IN_GITHUB:
    %pip install transformer_lens
    %pip install circuitsvis
    %pip install torchtyping
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
import circuitsvis as cv

# Testing that the library works
cv.examples.hello("Neel")
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import tqdm.auto as tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from torchtyping import TensorType as TT
from typing import List, Union, Optional
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
```

```python
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import (
    HookedTransformer,
    HookedTransformerConfig,
    FactoredMatrix,
    ActivationCache,
)
```

We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training.

```python
torch.set_grad_enabled(False)
```

Plotting helper functions:

```python
def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(
        utils.to_numpy(tensor),
        color_continuous_midpoint=0.0,
        color_continuous_scale="RdBu",
        labels={"x": xaxis, "y": yaxis},
        **kwargs,
    ).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x": xaxis, "y": yaxis}, **kwargs).show(
        renderer
    )

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(
        y=y, x=x, labels={"x": xaxis, "y": yaxis, "color": caxis}, **kwargs
    ).show(renderer)
```

# Othello GPT

```python
LOAD_AND_CONVERT_CHECKPOINT = False
```

```python
import transformer_lens.utils as utils

cfg = HookedTransformerConfig(
    n_layers=8,
    d_model=512,
    d_head=64,
    n_heads=8,
    d_mlp=2048,
    d_vocab=61,
    n_ctx=59,
    act_fn="gelu",
    normalization_type="LNPre",
)
model = HookedTransformer(cfg)
```

```python
# NBVAL_IGNORE_OUTPUT
sd = utils.download_file_from_hf(
    "NeelNanda/Othello-GPT-Transformer-Lens", "synthetic_model.pth"
)
# champion_ship_sd = utils.download_file_from_hf("NeelNanda/Othello-GPT-Transformer-Lens", "championship_model.pth")
model.load_state_dict(sd)
```

Code to load and convert one of the author's checkpoints to TransformerLens:

```python
def convert_to_transformer_lens_format(in_sd, n_layers=8, n_heads=8):
    out_sd = {}
    out_sd["pos_embed.W_pos"] = in_sd["pos_emb"].squeeze(0)
    out_sd["embed.W_E"] = in_sd["tok_emb.weight"]

    out_sd["ln_final.w"] = in_sd["ln_f.weight"]
    out_sd["ln_final.b"] = in_sd["ln_f.bias"]
    out_sd["unembed.W_U"] = in_sd["head.weight"].T

    for layer in range(n_layers):
        out_sd[f"blocks.{layer}.ln1.w"] = in_sd[f"blocks.{layer}.ln1.weight"]
        out_sd[f"blocks.{layer}.ln1.b"] = in_sd[f"blocks.{layer}.ln1.bias"]
        out_sd[f"blocks.{layer}.ln2.w"] = in_sd[f"blocks.{layer}.ln2.weight"]
        out_sd[f"blocks.{layer}.ln2.b"] = in_sd[f"blocks.{layer}.ln2.bias"]

        out_sd[f"blocks.{layer}.attn.W_Q"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.query.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_Q"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.query.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_K"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.key.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_K"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.key.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_V"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.value.weight"],
            "(head d_head) d_model -> head d_model d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_V"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.value.bias"],
            "(head d_head) -> head d_head",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.W_O"] = einops.rearrange(
            in_sd[f"blocks.{layer}.attn.proj.weight"],
            "d_model (head d_head) -> head d_head d_model",
            head=n_heads,
        )
        out_sd[f"blocks.{layer}.attn.b_O"] = in_sd[f"blocks.{layer}.attn.proj.bias"]

        out_sd[f"blocks.{layer}.mlp.b_in"] = in_sd[f"blocks.{layer}.mlp.0.bias"]
        out_sd[f"blocks.{layer}.mlp.W_in"] = in_sd[f"blocks.{layer}.mlp.0.weight"].T
        out_sd[f"blocks.{layer}.mlp.b_out"] = in_sd[f"blocks.{layer}.mlp.2.bias"]
        out_sd[f"blocks.{layer}.mlp.W_out"] = in_sd[f"blocks.{layer}.mlp.2.weight"].T

    return out_sd

if LOAD_AND_CONVERT_CHECKPOINT:
    synthetic_checkpoint = torch.load("/workspace/othello_world/gpt_synthetic.ckpt")
    for name, param in synthetic_checkpoint.items():
        if name.startswith("blocks.0") or not name.startswith("blocks"):
            print(name, param.shape)

    cfg = HookedTransformerConfig(
        n_layers=8,
        d_model=512,
        d_head=64,
        n_heads=8,
        d_mlp=2048,
        d_vocab=61,
        n_ctx=59,
        act_fn="gelu",
        normalization_type="LNPre",
    )
    model = HookedTransformer(cfg)

    model.load_and_process_state_dict(
        convert_to_transformer_lens_format(synthetic_checkpoint)
    )
```

Testing code for the synthetic checkpoint giving the correct outputs

```python
# An example input
sample_input = torch.tensor(
    [
        [
            20,
            19,
            18,
            10,
            2,
            1,
            27,
            3,
            41,
            42,
            34,
            12,
            4,
            40,
            11,
            29,
            43,
            13,
            48,
            56,
            33,
            39,
            22,
            44,
            24,
            5,
            46,
            6,
            32,
            36,
            51,
            58,
            52,
            60,
            21,
            53,
            26,
            31,
            37,
            9,
            25,
            38,
            23,
            50,
            45,
            17,
            47,
            28,
            35,
            30,
            54,
            16,
            59,
            49,
            57,
            14,
            15,
            55,
            7,
        ]
    ]
)
# The argmax of the output (ie the most likely next move from each position)
sample_output = torch.tensor(
    [
        [
            21,
            41,
            40,
            34,
            40,
            41,
            3,
            11,
            21,
            43,
            40,
            21,
            28,
            50,
            33,
            50,
            33,
            5,
            33,
            5,
            52,
            46,
            14,
            46,
            14,
            47,
            38,
            57,
            36,
            50,
            38,
            15,
            28,
            26,
            28,
            59,
            50,
            28,
            14,
            28,
            28,
            28,
            28,
            45,
            28,
            35,
            15,
            14,
            30,
            59,
            49,
            59,
            15,
            15,
            14,
            15,
            8,
            7,
            8,
        ]
    ]
)
model(sample_input).argmax(dim=-1)
```

---

# Patchscopes_Generation_Demo.ipynb


---

# Qwen.ipynb

```python
%pip install transformers_stream_generator plotly circuitsvis huggingface_hub einops tiktoken datasets
```

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git
    %pip install circuitsvis

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio
if IN_COLAB or not DEVELOPMENT_MODE:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "notebook_connected"
print(f"Using renderer: {pio.renderers.default}")
```

```python
%cd ~/TransformerLens
import torch
torch.set_grad_enabled(False)

from transformers import AutoTokenizer
from transformer_lens import HookedTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

from functools import partial
```

```python
def assert_hf_and_tl_model_are_close(
    hf_model,
    tl_model,
    tokenizer,
    prompt="This is a prompt to test out",
    atol=1e-3,
):
    prompt_toks = tokenizer(prompt, return_tensors="pt").input_ids

    hf_logits = hf_model(prompt_toks.to(hf_model.device)).logits
    tl_logits = tl_model(prompt_toks).to(hf_logits)

    assert torch.allclose(torch.softmax(hf_logits, dim=-1), torch.softmax(tl_logits, dim=-1), atol=atol)
```

## Qwen, first generation

```python
model_path = "Qwen/Qwen-1_8B-Chat"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

hf_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=device,
    fp32=True,
    use_logn_attn=False,
    use_dynamic_ntk = False,
    scale_attn_weights = False,
    trust_remote_code = True
).eval()

tl_model = HookedTransformer.from_pretrained_no_processing(
    model_path,
    device=device,
    fp32=True,
    dtype=torch.float32,
).to(device)

assert_hf_and_tl_model_are_close(hf_model, tl_model, tokenizer)
```

## Qwen, new generation

```python
model_path = "Qwen/Qwen1.5-1.8B-Chat"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
)

hf_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=device,
).eval()

tl_model = HookedTransformer.from_pretrained_no_processing(
    model_path,
    device=device,
    dtype=torch.float32,
).to(device)

assert_hf_and_tl_model_are_close(hf_model, tl_model, tokenizer)
```

```python

```

---

# SVD_Interpreter_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/SVD_Interpreter_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## TransformerLens SVD Interpreter Demo

A few months ago, a Conjecture post came out about how the singular value decompositions of transformer matrices were [surprisingly interpretable](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight#Directly_editing_SVD_representations), leading to recognisable semantic clusters. This seemed like good functionality to add to TransformerLens, which is what the SVD Interpreter feature does. You simply need to pass it a model, the type of matrix you want, and the size of the results you want, then you can plot it using PySvelte. This demo will show you how it's done.

How to use this notebook:

**Go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.**

Tips for reading this Colab:

* You can run all this code for yourself!
* The graphs are interactive!
* Use the table of contents pane in the sidebar to navigate
* Collapse irrelevant sections with the dropdown arrows
* Search the page using the search in the sidebar, not CTRL+F

## Setup (Can be ignored)

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEBUG_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/JayBaileyCS/TransformerLens.git # TODO: Change!
    # Install Neel's personal plotting utils
    %pip install git+https://github.com/neelnanda-io/neel-plotly.git
    # Install another version of node that makes PySvelte work way faster
    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    %pip install git+https://github.com/neelnanda-io/PySvelte.git
    # Needed for PySvelte to work, v3 came out and broke things...
    %pip install typeguard==2.13.3
    %pip install typing-extensions
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
import plotly.io as pio

if IN_COLAB or not DEBUG_MODE:
    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "png"
```

```python
import torch
import pysvelte
import numpy as np
import transformer_lens
import transformer_lens.utils as utils
from transformer_lens import HookedTransformer, SVDInterpreter
```

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device = }")
```

## SVD Interpretation

The SVD Interpreter supports interpretation for three types of Transformer matrix:

* OV - The [output-value circuit](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS) of the matrix. (d_model x d_model) in size.
* w_in - Weights passed into the MLP block of the matrix. (d_model x (4 x d_model)) in size.
* w_out - Weights that come out of the MLP block of the matrix. ((4 x d_model) x d_model) in size.

The SVD interpreter handles everything behind the scenes, so you only need to pass in the model and the type of matrix you want. Let's give it a go!

We'll be passing in **fold_ln = False, center_writing_weights+false, and center_unembed=False** here to mimic the existing post as closely as possible in order to demonstrate that this works (and the numerical instability that makes it not *completely* work). You can do interpretability on the default model without these parameters, but you won't be able to replicate the same results. I haven't checked much to see how it affects their quality, though w_out seemed to decay greatly when center_unembed was True - this would be worth testing properly!

Replication with this type of analysis is inherently difficult, because linear dependence is numerically unstable. Very minor numerical changes (Like floating-point discrepancies) can alter the results slightly. (See [this comment](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight?commentId=4e8534hbyWCpZFgFD)) So don't worry if you don't get exactly the same results on different devices - this is, unfortunately, expected. Try to stick to the same device for all your experiments and be sure to point out which one you used when writing them up. (And if anyone has a more stable way to get these results, [let us know](https://github.com/TransformerLensOrg/TransformerLens/issues)!)

```python
model = HookedTransformer.from_pretrained("gpt2-medium", fold_ln=False, center_writing_weights=False, center_unembed=False)
```

```python
all_tokens = [model.to_str_tokens(np.array([i])) for i in range(model.cfg.d_vocab)]
all_tokens = [all_tokens[i][0] for i in range(model.cfg.d_vocab)]

# Utility function to plot values in the same style as the Conjecture post.
def plot_matrix(matrix, tokens, k=10, filter="topk"):
  pysvelte.TopKTable(tokens=all_tokens, activations=matrix, obj_type="SVD direction", k=k, filter=filter).show()
```

```python
svd_interpreter = SVDInterpreter(model)

ov = svd_interpreter.get_singular_vectors('OV', layer_index=22, head_index=10)
w_in = svd_interpreter.get_singular_vectors('w_in', layer_index=20)
w_out = svd_interpreter.get_singular_vectors('w_out', layer_index=16)

plot_matrix(ov, all_tokens)
plot_matrix(w_in, all_tokens)
plot_matrix(w_out, all_tokens)
```

Currently, this is the extent of our support for SVD interpretability. However, this is a very new idea, and we're excited to see how people use it! If you find an interesting use for this type of research that we don't cover, feel free to [open a ticket](https://github.com/TransformerLensOrg/TransformerLens/issues) or contact the code's author at jaybaileycs@gmail.com.

One thing I'd love to see that basically anyone who followed this demo could get started with (I'd consider it an **A-level problem** from Neel's [Concrete Open Problems sequence](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj)) is to try different combinations of model parameters (fold_ln, center_writing_weights, center_unembed) and see which ones lead to big changes in the interpretability of the SVD matrices.

Are these changes positive, or negative? Can you pick any set of parameters you want? Are different parameters more or less interpretable in general, or does it vary by head and layer? Can you get two different interpretations of the same head with different parameters? What else can you find? This is very low-hanging fruit that would be immediately tractable and immediately useful!

---

# Santa_Coder.ipynb

```python
# Janky code to do different setup when run in a Colab notebook vs VSCode
DEVELOPMENT_MODE = False
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git``
    %pip install circuitsvis
    %pip install torchtyping

    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
    # # Install another version of node that makes PySvelte work way faster
    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
    # %pip install git+https://github.com/neelnanda-io/PySvelte.git
except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    from IPython import get_ipython

    ipython = get_ipython()
    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    ipython.magic("load_ext autoreload")
    ipython.magic("autoreload 2")
```

```python
# Import stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import einops
from fancy_einsum import einsum
import tqdm.auto as tqdm
from tqdm import tqdm
import random
from pathlib import Path
import plotly.express as px
from torch.utils.data import DataLoader

from torchtyping import TensorType as TT
from typing import List, Union, Optional
from jaxtyping import Float, Int
from functools import partial
import copy

import itertools
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import dataclasses
import datasets
from IPython.display import HTML
# import circuitsvis as cv

import transformer_lens
import transformer_lens.utils as utils
from transformer_lens.hook_points import (
    HookedRootModule,
    HookPoint,
)  # Hooking utilities
from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache

torch.set_grad_enabled(False)

def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
    px.line(utils.to_numpy(tensor), labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
    x = utils.to_numpy(x)
    y = utils.to_numpy(y)
    px.scatter(y=y, x=x, labels={"x":xaxis, "y":yaxis, "color":caxis}, **kwargs).show(renderer)
```

```python
# load hf model
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m")
```

```python
# Disable folding norms and folding norms and biases so that intermediate value
# in between transformer blocks can be compared
bloom = HookedTransformer.from_pretrained("bloom-560m",fold_ln=False, fold_value_biases=False, center_writing_weights=False)
```

```python
text = '''
TransformerLens lets you load in 50+ different open source language models,
and exposes the internal activations of the model to you. You can cache
any internal activation in the model, and add in functions to edit, remove
or replace these activations as the model runs.
'''
input_ids = tokenizer(text, return_tensors='pt')['input_ids']
gt_logits = model(input_ids)['logits'] # ground truth logits from hf
my_logits = bloom(input_ids)
centered_gt_logits = gt_logits - gt_logits.mean(-1, keepdim=True)
mean_diff = (my_logits.cpu() - centered_gt_logits).mean()
print("avg logits difference:", mean_diff.item())
max_diff = (my_logits.cpu() - centered_gt_logits).abs().max()
print("max logits difference:", max_diff.item())
```

```python
gt_cache = model(input_ids, output_hidden_states=True)['hidden_states']
_, my_cache = bloom.run_with_cache(input_ids)
use_loose_bound = False
pass_loose_bound = True
print("*"*5, "Matching hf and T-Lens residual stream in between transformer blocks", "*"*5)
for i in range(24):
    try:
        torch.testing.assert_close(my_cache['resid_pre',i], gt_cache[i].cuda())
    except:
        max_diff = (my_cache['resid_pre',i] - gt_cache[i].cuda()).abs().max()
        print(f"layer {i} \t not close, max difference: {max_diff}")
        use_loose_bound = True

if use_loose_bound:
    atol = rtol = 1e-3
    print("*"*5, f"\ttesting with atol={atol} and rtol={rtol}\t","*"*5)
    for i in range(24):
        try:
            torch.testing.assert_close(my_cache['resid_pre',i], gt_cache[i].cuda(), atol=atol, rtol=rtol)
        except:
            max_diff = (my_cache['resid_pre',i] - gt_cache[i].cuda()).abs().max()
            print(f"layer {i} \t not close, max difference: {max_diff}")
            pass_loose_bound = False

    if pass_loose_bound:
        print(f"All layers match with atol={atol} rtol={rtol}")
else:
    print("All layers match")
```

```python
my_loss = bloom(input_ids, return_type='loss')
print("T-Lens next token loss:", my_loss.item())
gt_outputs = model(input_ids, labels=input_ids)
gt_loss = gt_outputs.loss
print("HF next token loss:", gt_loss.item())
print("diff in loss (abs):", (gt_loss-my_loss).abs().item())
```

---

# T5.ipynb


---

# Tracr_to_Transformer_Lens_Demo.ipynb

<a target="_blank" href="https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Tracr_to_Transformer_Lens_Demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

# Tracr to TransformerLens Converter
[Tracr](https://github.com/deepmind/tracr) is a cool new DeepMind tool that compiles a written program in RASP to transformer weights. TransformerLens is a library I've written to easily do mechanistic interpretability on a transformer and to poke around at its internals. This is a (hacky!) script to convert Tracr weights from the JAX form to a TransformerLens HookedTransformer in PyTorch.

See [the TransformerLens tutorial](https://neelnanda.io/transformer-lens-demo) to get started

Python version must be >=3.8 (my fork of Tracr is a bit more backwards compatible, original library is at least 3.9)

```python
!python --version
```

```python
try:
    import google.colab
    IN_COLAB = True
    print("Running as a Colab notebook")
    %pip install transformer_lens
    # Fork of Tracr that's backward compatible with Python 3.8
    %pip install git+https://github.com/neelnanda-io/Tracr

except:
    IN_COLAB = False
    print("Running as a Jupyter notebook - intended for development only!")
    # from IPython import get_ipython

    # ipython = get_ipython()
    # # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
    # ipython.magic("load_ext autoreload")
    # ipython.magic("autoreload 2")
```

```python
from transformer_lens import HookedTransformer, HookedTransformerConfig
import einops
import torch
import numpy as np

from tracr.rasp import rasp
from tracr.compiler import compiling
```

Loads an example RASP program model. This program reverses lists. The model takes as input a list of pre-tokenization elements (here `["BOS", 1, 2, 3]`), these are tokenized (`[3, 0, 1, 2]`), the transformer is applied, and then an argmax is taken over the output and it is detokenized - this can be seen on the `out.decoded` attribute of the output

```python

def make_length():
  all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)
  return rasp.SelectorWidth(all_true_selector)

length = make_length()  # `length` is not a primitive in our implementation.
opp_index = length - rasp.indices - 1
flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)
reverse = rasp.Aggregate(flip, rasp.tokens)

bos = "BOS"
model = compiling.compile_rasp_to_model(
    reverse,
    vocab={1, 2, 3},
    max_seq_len=5,
    compiler_bos=bos,
)

out = model.apply([bos, 1, 2, 3])
```

Extract the model config from the Tracr model, and create a blank HookedTransformer object

```python

# %%

n_heads = model.model_config.num_heads
n_layers = model.model_config.num_layers
d_head = model.model_config.key_size
d_mlp = model.model_config.mlp_hidden_size
act_fn = "relu"
normalization_type = "LN"  if model.model_config.layer_norm else None
attention_type = "causal"  if model.model_config.causal else "bidirectional"

n_ctx = model.params["pos_embed"]['embeddings'].shape[0]
# Equivalent to length of vocab, with BOS and PAD at the end
d_vocab = model.params["token_embed"]['embeddings'].shape[0]
# Residual stream width, I don't know of an easy way to infer it from the above config.
d_model = model.params["token_embed"]['embeddings'].shape[1]

# Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never care about these outputs
# In practice, we always feed the logits into an argmax
d_vocab_out = model.params["token_embed"]['embeddings'].shape[0] - 2

cfg = HookedTransformerConfig(
    n_layers=n_layers,
    d_model=d_model,
    d_head=d_head,
    n_ctx=n_ctx,
    d_vocab=d_vocab,
    d_vocab_out=d_vocab_out,
    d_mlp=d_mlp,
    n_heads=n_heads,
    act_fn=act_fn,
    attention_dir=attention_type,
    normalization_type=normalization_type,
)
tl_model = HookedTransformer(cfg)
```

Extract the state dict, and do some reshaping so that everything has a n_heads dimension

```python

# %%
sd = {}
sd["pos_embed.W_pos"] = model.params["pos_embed"]['embeddings']
sd["embed.W_E"] = model.params["token_embed"]['embeddings']
# Equivalent to max_seq_len plus one, for the BOS

# The unembed is just a projection onto the first few elements of the residual stream, these store output tokens
# This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.
sd["unembed.W_U"] = np.eye(d_model, d_vocab_out)

for l in range(n_layers):
    sd[f"blocks.{l}.attn.W_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_K"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/key"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_Q"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/query"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["w"],
        "d_model (n_heads d_head) -> n_heads d_model d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_V"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/value"]["b"],
        "(n_heads d_head) -> n_heads d_head",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.W_O"] = einops.rearrange(
        model.params[f"transformer/layer_{l}/attn/linear"]["w"],
        "(n_heads d_head) d_model -> n_heads d_head d_model",
        d_head = d_head,
        n_heads = n_heads
    )
    sd[f"blocks.{l}.attn.b_O"] = model.params[f"transformer/layer_{l}/attn/linear"]["b"]

    sd[f"blocks.{l}.mlp.W_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["w"]
    sd[f"blocks.{l}.mlp.b_in"] = model.params[f"transformer/layer_{l}/mlp/linear_1"]["b"]
    sd[f"blocks.{l}.mlp.W_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["w"]
    sd[f"blocks.{l}.mlp.b_out"] = model.params[f"transformer/layer_{l}/mlp/linear_2"]["b"]
print(sd.keys())

```

Convert weights to tensors and load into the tl_model

```python

for k, v in sd.items():
    # I cannot figure out a neater way to go from a Jax array to a numpy array lol
    sd[k] = torch.tensor(np.array(v))

tl_model.load_state_dict(sd, strict=False)

```

Create helper functions to do the tokenization and de-tokenization

```python

# %%
INPUT_ENCODER = model.input_encoder
OUTPUT_ENCODER = model.output_encoder

def create_model_input(input, input_encoder=INPUT_ENCODER):
    encoding = input_encoder.encode(input)
    return torch.tensor(encoding).unsqueeze(dim=0)

def decode_model_output(logits, output_encoder=OUTPUT_ENCODER, bos_token=INPUT_ENCODER.bos_token):
    max_output_indices = logits.squeeze(dim=0).argmax(dim=-1)
    decoded_output = output_encoder.decode(max_output_indices.tolist())
    decoded_output_with_bos = [bos_token] + decoded_output[1:]
    return decoded_output_with_bos

```

We can now run the model!

```python

input = [bos, 1, 2, 3]
out = model.apply(input)
print("Original Decoding:", out.decoded)

input_tokens_tensor = create_model_input(input)
logits = tl_model(input_tokens_tensor)
decoded_output = decode_model_output(logits)
print("TransformerLens Replicated Decoding:", decoded_output)
# %%

```

Lets cache all intermediate activations in the model, and check that they're the same:

```python
logits, cache = tl_model.run_with_cache(input_tokens_tensor)

for layer in range(tl_model.cfg.n_layers):
    print(f"Layer {layer} Attn Out Equality Check:", np.isclose(cache["attn_out", layer].detach().cpu().numpy(), np.array(out.layer_outputs[2*layer])).all())
    print(f"Layer {layer} MLP Out Equality Check:", np.isclose(cache["mlp_out", layer].detach().cpu().numpy(), np.array(out.layer_outputs[2*layer+1])).all())
```

Look how pretty and ordered the final residual stream is!

(The logits are the first 3 dimensions of the residual stream, and we can see that they're flipped!)

```python
import plotly.express as px
px.imshow(cache["resid_post", -1].detach().cpu().numpy()[0],
color_continuous_scale="Blues", labels={"x":"Residual Stream", "y":"Position"}, y=[str(i) for i in input]).show("colab" if IN_COLAB else "")
```

---

# comparing-to-huggingface.ipynb

Compare the TransformerLens implementation of a model to the Huggingface implementation. This script was originally use in https://github.com/TransformerLensOrg/TransformerLens/issues/570 to debug Mixtral.

## setup

```python
%pip install transformers matplotlib
```

```python
# Everything can be configured here
model_id = ""
text = "Hello my name is"
device="cpu"
# Set this to true to trigger hugging face login if needed
gated_model = False
```

```python
# If you need a specific head, uncomment this and specify the head
# %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git@head
# Otherwise, for running this on the latest release
%pip install transformer_lens
```

```python
if gated_model:
    %pip install huggingface_hub
    from huggingface_hub import login
    login()
```

```python
import einops
from torch.testing import assert_close
import torch
import matplotlib.pyplot as plt
from transformer_lens import HookedTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
```

## TransformerLens model

```python
tl_model = HookedTransformer.from_pretrained_no_processing(
    model_id,
    device=device,
)
```

```python
tl_model.generate(
    text,
    verbose=False,
    max_new_tokens=50,
)
```

## Huggingface Model

```python
tokenizer = AutoTokenizer.from_pretrained(model_id)
hf_model = AutoModelForCausalLM.from_pretrained(model_id)
```

```python
inputs = tokenizer(text, return_tensors="pt")
outputs = hf_model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Compare Model Weights

```python
torch.all(
    einops.rearrange(tl_model.blocks[0].attn.W_Q, "n m h -> (n h) m") ==
    hf_model.model.layers[0].self_attn.q_proj.weight
)
```

```python
tl_model.blocks[0].attn.W_K.shape, hf_model.model.layers[0].self_attn.k_proj.weight.shape
```

```python
torch.all(
    einops.reduce(
        tl_model.blocks[0].attn.W_K, "(n repeat) m h -> (n h) m",
        'max',
        n=tl_model.cfg.n_key_value_heads,
        repeat=4) ==
    hf_model.model.layers[0].self_attn.k_proj.weight
)
```

```python
torch.all(
    einops.reduce(
        tl_model.blocks[0].attn.W_V, "(n repeat) m h -> (n h) m",
        'max',
        n=tl_model.cfg.n_key_value_heads,
        repeat=4) ==
    hf_model.model.layers[0].self_attn.v_proj.weight
)
```

```python
torch.all(
    einops.rearrange(tl_model.blocks[0].attn.W_O, "n h m -> m (n h)") ==
    hf_model.model.layers[0].self_attn.o_proj.weight
)
```

```python
tl_model.blocks[0].attn.b_Q
```

```python
torch.all(hf_model.model.layers[0].block_sparse_moe.gate.weight.T == tl_model.blocks[0].mlp.W_gate)
```

```python
hf_model.model.layers[0].block_sparse_moe.gate.weight.dtype, tl_model.blocks[0].mlp.W_gate.dtype
```

## Compare Layer Outputs

```python
test_tensor = torch.randn((1, 1, 4096,))
```

```python
hf_model.model.layers[0](test_tensor)
```

```python
tl_model.blocks[0](test_tensor)
```

```python
hf_model.model.layers[0](test_tensor)[0] == tl_model.blocks[0](test_tensor)
```

```python
hf_model.model.layers[0](test_tensor)[0][0, 0, -2].item(), tl_model.blocks[0](test_tensor)[0, 0, -2].item()
```

```python
torch.sum(hf_model.model.layers[0](test_tensor)[0] == tl_model.blocks[0](test_tensor))
```

```python
differences = hf_model.model.layers[0](test_tensor)[0] - tl_model.blocks[0](test_tensor)

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between Layer Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

## Compare MLP Outputs

```python
torch.all(
    tl_model.blocks[0].mlp.experts[0].W_in ==
    hf_model.model.layers[0].block_sparse_moe.experts[0].w3.weight.T
)
```

```python
test_tensor = torch.randn((1, 1, 4096,))
```

```python
torch.all(
    hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] ==
    tl_model.blocks[0].mlp(test_tensor)
)
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0]
```

```python
tl_model.blocks[0].mlp(test_tensor)
```

```python
tl_model.blocks[0].mlp(test_tensor).shape
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] == tl_model.blocks[0].mlp(test_tensor)
```

```python
torch.sum(hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] == tl_model.blocks[0].mlp(test_tensor))
```

```python
differences = hf_model.model.layers[0].block_sparse_moe(test_tensor)[0] - tl_model.blocks[0].mlp(test_tensor)

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between MLP Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

```python
hf_model.model.layers[0].block_sparse_moe(test_tensor)[0][0, 0, 0].item()
```

```python
tl_model.blocks[0].mlp(test_tensor)[0, 0, 0].item()
```

## Compare Attention Outputs

```python
tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor)
```

```python
hf_model.model.layers[0].self_attn.forward(test_tensor)[0]
```

```python
(tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) ==
 hf_model.model.layers[0].self_attn.forward(test_tensor)[0])
```

```python
torch.sum(tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) ==
 hf_model.model.layers[0].self_attn.forward(test_tensor)[0])
```

```python
differences = tl_model.blocks[0].attn.forward(test_tensor, test_tensor, test_tensor) - hf_model.model.layers[0].self_attn.forward(test_tensor)[0]

# Flatten the differences to create a one-dimensional tensor
flattened_differences = differences.flatten().cpu().detach().numpy()

# Plot the histogram of the differences
plt.hist(flattened_differences, bins=50, alpha=0.75, color='blue')
plt.title('Differences Between Attention Outputs')
plt.xlabel('Difference')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

---

# hf-tl-logit-comparator.ipynb

# Logit Comparator for HuggingFace and TransformerLens Outputs
This notebook is a quick and dirty tool to compare the logit outputs of a HuggingFace model and a TransformerLens model via several different metrics. It is intended to help debug issues with the TransformerLens model, such as bugs in the model's implementation. If you identify any issues, please open an issue on the [GitHub repository](https://github.com/TransformerLensOrg/TransformerLens).

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformer_lens import HookedTransformer
import torch
import torch.nn.functional as F

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

torch.set_grad_enabled(False)
```

## Comparator Setup

```python
model_name = "EleutherAI/pythia-2.8b"  # You can change this to any model name
sentence = "The quick brown fox"
```

```python
from huggingface_hub import login
login(token="")
```

## Get Transformers Logits

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_model(model_name="gpt2"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return model, tokenizer

def get_logits(model, tokenizer, sentence, device):
    # Tokenize the input sentence
    inputs = tokenizer(sentence, return_tensors="pt")

    # Move inputs to the device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Generate the logits
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the logits for all tokens
    logits = outputs.logits

    return logits

model, tokenizer = load_model(model_name)
model = model.to(device)

hf_logits = get_logits(model, tokenizer, sentence, device)[:, -1, :]
```

## Get TransformerLens Logits

```python
model = HookedTransformer.from_pretrained_no_processing(model_name, device=device)
tokens = model.to_tokens(sentence, prepend_bos=False)
tl_logits = model(tokens)[:, -1, :]
```

## Compare Logit Distributions
Various metrics are used to compare the logit distributions of the two models. We don't yet have standard values for what constitutes a "good" logit comparison, so we are working on establishing benchmarks.

### Shape

```python
print(f"HF Logits Shape: {hf_logits.shape}")
print(f"TL Logits Shape: {tl_logits.shape}")
```

### Tensor Comparison

```python
are_close = torch.allclose(tl_logits, hf_logits, rtol=1e-5, atol=1e-3)
print(f"Are the logits close? {are_close}")
```

### Mean Squared Error

```python
# Compare the logits with MSE
mse = torch.nn.functional.mse_loss(hf_logits, tl_logits)
print(f"MSE: {mse}")
```

### Maximum Absolute Difference

```python
max_diff = torch.max(torch.abs(tl_logits - hf_logits))
print(f"Max Diff: {max_diff}")
```

### Cosine Similarity

```python
cosine_sim = F.cosine_similarity(tl_logits, hf_logits, dim=-1).mean()
print(f"Cosine Sim: {cosine_sim}")
```

### KL Divergence

```python
def kl_div(logits1: torch.Tensor, logits2: torch.Tensor) -> torch.Tensor:
    probs1 = F.softmax(logits1, dim=-1)
    probs2 = F.softmax(logits2, dim=-1)
    return F.kl_div(probs1.log(), probs2, reduction='batchmean')

kl_tl_hf = kl_div(tl_logits, hf_logits)
kl_hf_tl = kl_div(hf_logits, tl_logits)
print(f"KL(TL||HF): {kl_tl_hf}")
print(f"KL(HF||TL): {kl_hf_tl}")
```

```python

```

---

# stable_lm.ipynb


---

