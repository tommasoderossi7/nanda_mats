Directory Structure:

└── ./
    ├── docs
    │   ├── source
    │   │   ├── _static
    │   │   │   ├── css
    │   │   │   │   ├── custom.css
    │   │   │   │   ├── home.css
    │   │   │   │   ├── pygments.css
    │   │   │   │   └── status.css
    │   │   │   └── js
    │   │   │       ├── code.js
    │   │   │       └── custom.js
    │   │   ├── _templates
    │   │   │   ├── layout.html
    │   │   │   └── ndif_status.html
    │   │   ├── documentation
    │   │   │   ├── intervention.rst
    │   │   │   ├── modeling.rst
    │   │   │   ├── schema.rst
    │   │   │   ├── tracing.rst
    │   │   │   └── util.rst
    │   │   ├── notebooks
    │   │   │   └── tutorials
    │   │   │       └── tutorial_price_tagging_utils.py
    │   │   ├── pages
    │   │   │   └── home.html
    │   │   ├── about.rst
    │   │   ├── conf.py
    │   │   ├── documentation.rst
    │   │   ├── features.rst
    │   │   ├── index.rst
    │   │   ├── start.rst
    │   │   ├── status.rst
    │   │   └── tutorials.rst
    │   ├── sourcelatex
    │   │   ├── documentation
    │   │   │   ├── contexts.rst
    │   │   │   ├── intervention.rst
    │   │   │   ├── models.rst
    │   │   │   ├── module.rst
    │   │   │   ├── patching.rst
    │   │   │   ├── tracing.rst
    │   │   │   └── util.rst
    │   │   ├── conf.py
    │   │   └── index.rst
    │   └── contributing.md
    ├── src
    │   └── nnsight
    │       ├── intervention
    │       │   ├── backends
    │       │   │   ├── __init__.py
    │       │   │   ├── editing.py
    │       │   │   ├── noop.py
    │       │   │   └── remote.py
    │       │   ├── contexts
    │       │   │   ├── __init__.py
    │       │   │   ├── editing.py
    │       │   │   ├── globals.py
    │       │   │   ├── interleaving.py
    │       │   │   ├── invoker.py
    │       │   │   ├── local.py
    │       │   │   ├── session.py
    │       │   │   └── tracer.py
    │       │   ├── graph
    │       │   │   ├── __init__.py
    │       │   │   ├── graph.py
    │       │   │   ├── node.py
    │       │   │   └── proxy.py
    │       │   ├── protocols
    │       │   │   ├── __init__.py
    │       │   │   ├── entrypoint.py
    │       │   │   ├── grad.py
    │       │   │   ├── intervention.py
    │       │   │   ├── module.py
    │       │   │   ├── noop.py
    │       │   │   ├── parameter.py
    │       │   │   └── swap.py
    │       │   ├── __init__.py
    │       │   ├── base.py
    │       │   ├── envoy.py
    │       │   └── interleaver.py
    │       ├── modeling
    │       │   ├── mixins
    │       │   │   ├── __init__.py
    │       │   │   ├── loadable.py
    │       │   │   ├── meta.py
    │       │   │   └── remoteable.py
    │       │   ├── vllm
    │       │   │   ├── executors
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── GPUExecutor.py
    │       │   │   │   └── RayGPUExecutor.py
    │       │   │   ├── model_runners
    │       │   │   │   ├── __init__.py
    │       │   │   │   └── GPUModelRunner.py
    │       │   │   ├── workers
    │       │   │   │   ├── __init__.py
    │       │   │   │   └── GPUWorker.py
    │       │   │   ├── __init__.py
    │       │   │   ├── sampling.py
    │       │   │   └── vllm.py
    │       │   ├── __init__.py
    │       │   ├── diffusion.py
    │       │   └── language.py
    │       ├── schema
    │       │   ├── format
    │       │   │   ├── __init__.py
    │       │   │   ├── functions.py
    │       │   │   └── types.py
    │       │   ├── __init__.py
    │       │   ├── config.py
    │       │   ├── request.py
    │       │   ├── response.py
    │       │   └── result.py
    │       ├── tracing
    │       │   ├── backends
    │       │   │   ├── __init__.py
    │       │   │   └── base.py
    │       │   ├── contexts
    │       │   │   ├── __init__.py
    │       │   │   ├── base.py
    │       │   │   ├── conditional.py
    │       │   │   ├── globals.py
    │       │   │   ├── iterator.py
    │       │   │   └── tracer.py
    │       │   ├── graph
    │       │   │   ├── __init__.py
    │       │   │   ├── graph.py
    │       │   │   ├── node.py
    │       │   │   ├── proxy.py
    │       │   │   └── viz.py
    │       │   ├── hacks
    │       │   │   ├── __init__.py
    │       │   │   ├── comprehension.py
    │       │   │   ├── conditional.py
    │       │   │   ├── iterator.py
    │       │   │   └── util.py
    │       │   ├── protocols
    │       │   │   ├── __init__.py
    │       │   │   ├── base.py
    │       │   │   ├── lock.py
    │       │   │   ├── stop.py
    │       │   │   └── variable.py
    │       │   └── __init__.py
    │       ├── __init__.py
    │       ├── logger.py
    │       ├── test.py
    │       └── util.py
    ├── tests
    │   ├── __init__.py
    │   ├── test_diffusion.py
    │   ├── test_lm.py
    │   ├── test_tiny.py
    │   └── test_vllm.py
    ├── CHANGELOG.md
    ├── CODE_OF_CONDUCT.md
    ├── conftest.py
    └── README.md



---
File: /docs/source/_static/css/custom.css
---

/* Website light and dark theme colors */

html[data-theme="dark"] {
    --pst-color-secondary: #ce5d97;
    --pst-color-primary: #4385be;
    --pst-color-primary-highlight: #ce5d97;
    --pst-color-background: #100F0F;
    --pst-color-on-background: #1C1B1A;
    --pst-color-surface: #282726;
    --pst-color-on-surface: #CECDC3;
    --pst-color-text-base: #CECDC3;
    --pst-color-text-muted: #CECDC3;
    --pst-color-heading-color: #CECDC3;
    --pst-color-border: #1C1B1A;
    --pst-color-panel-background: #1C1B1A;
    --output-bg-color: #343331;
}

html[data-theme="light"] {
    --output-bg-color: #d8ebfb;
    --pst-color-primary: #4385be;
    --pst-color-secondary: #ce5d97;
    --pst-color-primary-highlight: #ce5d97;
}


/* Notebook h1 section heading styles */
.nb-heading {
    margin-top: 2.5rem;
}

/* Remove the shadow on the pydata navbar */
.bd-header {
    box-shadow: none !important;
}

/* Hide the theme version note at the bottom of the page */
.theme-version {
    visibility: hidden;
}

/* code cell output */
.output_area {
    background-color: var(--output-bg-color);
    margin-left: 10px !important;
}

/* Coloring, mostly for buttons */
.primary {
    background-color : var(--pst-color-primary) !important;
    border-color : var(--pst-color-primary) !important;
    color: var(--pst-color-background) !important;
}

.secondary {
    background-color : transparent !important;
    border-color : #878580 !important;
    color: #878580 !important;
}

.secondary:hover {
    background-color : transparent !important;
    border-color : var(--pst-color-on-surface) !important;
    color: var(--pst-color-on-surface) !important;
}


---
File: /docs/source/_static/css/home.css
---

.hero {
    height: 100vh !important;
}

.hero-image {
    width: 100% !important;
}

.fixed-background {
    background-attachment: fixed;
    background-position: center;
    background-repeat: no-repeat;
    background-size: cover;
    left: 0 !important;
    top: 0 !important;
    position: fixed !important;
    height: 100vh;
    width: 100%;
    z-index: -1;
}

.surface {
    background-color: var(--pst-color-background);
}

.transparent {
    background: transparent !important;
}


---
File: /docs/source/_static/css/pygments.css
---

pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */



---
File: /docs/source/_static/css/status.css
---

.custom-accordion-header {
  background-color: #007bff;
  /* Bootstrap's primary color for demonstration */
  color: white;
  /* Text color */
}

.custom-accordion-body {
  background-color: #f8f9fa;
  /* A light gray background */
  color: #212529;
  /* Bootstrap default text color */
}


---
File: /docs/source/_static/js/code.js
---

var simple = `from nnsight import NNsight, LanguageModel

net = torch.nn.Sequential(OrderedDict([
  ('layer1', torch.nn.Linear(input_size, hidden_dims)),
  ('layer2', torch.nn.Linear(hidden_dims, output_size)),
]))

model = NNsight(net)

...

model = LanguageModel('openai-community/gpt2')`;

var trace = `with model.trace('Who invented neural networks?'):

  hidden_state_output = model.layer1.output.save()
  hidden_state_input = model.layer2.input.save()

  output = model.output.save()
  
print(hidden_state_output)
print(hidden_state_input)
print(output)`;

var multi = `with model.trace() as tracer:
  
  with tracer.invoke('The Eiffel Tower is in the city of'):

    model.transformer.h[-1].mlp.output[0][:] = 0

    intervention = model.lm_head.output.argmax(dim=-1).save()

  with tracer.invoke('The Eiffel Tower is in the city of'):

    original = model.lm_head.output.argmax(dim=-1).save()

print(output)`;

document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('code.language-python.simple').forEach(el => {
        el.textContent = simple;
    });
    document.querySelectorAll('code.language-python.trace').forEach(el => {
        el.textContent = trace;
    });
    document.querySelectorAll('code.language-python.multi').forEach(el => {
        el.textContent = multi;
    });
    document.querySelectorAll('div.output_area.stderr').forEach(el => {
        el.style.visibility = "hidden";
        el.style.display = "none";
    });
    document.querySelectorAll('div.output_area.stderr').forEach(el => {
      el.style.visibility = "hidden";
      el.style.display = "none";
    });
    document.querySelectorAll('div.output_area').forEach(el => {
      el.style.marginBottom = "1em";
    });
    document.querySelectorAll('div.input_area').forEach(el => {
      el.style.border = "0px";
      el.style.marginTop = "0.5em";
      el.style.marginBottom = "0.5em";
    });
    document.querySelectorAll('summary').forEach(el => {
      el.style.marginTop = "1em";
      el.style.marginBottom = "1em";
      document.querySelectorAll('p').forEach(p => {
          const span = document.createElement('span');

          // Copy the innerHTML from the <p> to the <span>
          span.innerHTML = p.innerHTML;

          // Optional: Copy styles from the <p> to the <span>
          span.style.cssText = p.style.cssText;

          // Replace the <p> with the <span> in the DOM
          p.parentNode.replaceChild(span, p);
        });
  });
});




---
File: /docs/source/_static/js/custom.js
---

require.config({
  paths: {
    'three': 'https://cdnjs.cloudflare.com/ajax/libs/three.js/r134/three.min.js?MEH=BLAH',
    'VANTA': 'https://cdn.jsdelivr.net/npm/vanta@0.5.24/dist/vanta.dots.min.js?MEH=BLAH'
  },
  shim: {
    'VANTA': {
      deps: ['three'] 
    }
  }
});

var vantaEffect;

function initVanta() {
  require(['three'], function (THREE) {
    window.THREE = THREE; // Make THREE globally accessible

    require(['VANTA'], function () {
      if (vantaEffect) {
        vantaEffect.destroy();
      }
      
      const darkMode = document.documentElement.dataset.theme === 'dark';
      const color = darkMode ? 0xCECDC3 : 0x000000; // Example dark mode color
      const color2 = darkMode ? 0xCECDC3 : 0x000000; // Adjust color2 for dark mode as needed
      const backgroundColor = darkMode ? 0x100F0F : 0xFFFFFF; // Adjust background color for dark mode
      const size = darkMode ? 0.50 : 1.00; // Adjust size for dark mode as needed

      vantaEffect = VANTA.DOTS({
        el: ".fixed-background",
        mouseControls: false,
        touchControls: false,
        gyroControls: false,
        minHeight: 200.00,
        minWidth: 200.00,
        scale: 1.00,
        scaleMobile: 1.00,
        color: color,
        color2: color2,
        backgroundColor: backgroundColor,
        size: size,
        spacing: 10.00,
        showLines: false
      });
    });
  });
}

// Initial call
initVanta();

// Setup a mutation observer to detect theme changes
var observer = new MutationObserver(function(mutations) {
  mutations.forEach(function(mutation) {
    if (mutation.attributeName === "data-theme") {
      initVanta(); // Reinitialize Vanta with the new theme settings
    }
  });
});

// Start observing the document element for attribute changes
observer.observe(document.documentElement, {attributes: true, attributeFilter: ['data-theme']});


document.addEventListener("DOMContentLoaded", function() {
  fetch("https://ndif.dev/ping")
    .then(response => response.json())
    .then(data => {
      const statusElement = document.querySelector(".ndif");
      const status = (data === "pong") ? "Available" : "Unavailable";
      if (statusElement) {
        statusElement.setAttribute('data-bs-original-title', `Status: ${status}`);
      }
    })
    .catch(error => console.error('Error fetching NDIF status:', error));

    const data = document.querySelector("p.copyright");
    data.innerHTML = "(c) 2024 <a href='https://ndif.us/'>NDIF</a>"

});



---
File: /docs/source/_templates/layout.html
---

{% extends "!layout.html" %}

{% block extrahead %}
{{ super() }}
<link href="{{ pathto('_static/css/custom.css', 1) }}?v={{ version_identifier }}" rel="stylesheet" type="text/css" />
<link href="{{ pathto('_static/css/home.css', 1) }}?v={{ version_identifier }}" rel="stylesheet" type="text/css" />
{% endblock %}



---
File: /docs/source/_templates/ndif_status.html
---

<script>
    fetch("{{ndif_url}}")
        .then((response) => {
            if (response.status == 200) {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#66800b', 'important');
                    });
                });
            }
            else {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#af3029', 'important');
                    });
                });
                var statusIcon = document.querySelector('.ndif .fa-circle-check');
                if (statusIcon) {
                    // not here
                    statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark'); 
                }
            }
        })
        .catch((response) => {
            Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                    spanElement.style.setProperty('color', '#af3029', 'important');
                });
            });
            var statusIcon = document.querySelector('.ndif .fa-circle-check');
            if (statusIcon) {
                statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark');
            }
        })
</script>


---
File: /docs/source/documentation/intervention.rst
---

nnsight.intervention
--------------------

.. automodule:: nnsight.intervention
   :members:

.. automodule:: nnsight.intervention.protocols
   :members:

.. automodule:: nnsight.intervention.protocols.entrypoint
   :members:

.. automodule:: nnsight.intervention.protocols.grad
   :members:

.. automodule:: nnsight.intervention.protocols.intervention
   :members:

.. automodule:: nnsight.intervention.protocols.module
   :members:

.. automodule:: nnsight.intervention.protocols.swap
   :members:

.. automodule:: nnsight.intervention.contexts
   :members:

.. automodule:: nnsight.intervention.contexts.editing
   :members:

.. automodule:: nnsight.intervention.contexts.globals
   :members:

.. automodule:: nnsight.intervention.contexts.interleaving
   :members:

.. automodule:: nnsight.intervention.contexts.invoker
   :members:

.. automodule:: nnsight.intervention.contexts.local
   :members:

.. automodule:: nnsight.intervention.contexts.session
   :members:

.. automodule:: nnsight.intervention.contexts.tracer
   :members:

.. automodule:: nnsight.intervention.backends
   :members:

.. automodule:: nnsight.intervention.backends.editing
   :members:

.. automodule:: nnsight.intervention.backends.remote
   :members:

.. automodule:: nnsight.intervention.graph
   :members:

.. automodule:: nnsight.intervention.graph.graph
   :members:

.. automodule:: nnsight.intervention.graph.node
   :members:

.. automodule:: nnsight.intervention.graph.proxy
   :members:

.. automodule:: nnsight.intervention.base
   :members:

.. automodule:: nnsight.intervention.envoy
   :members:

.. automodule:: nnsight.intervention.interleaver
   :members:


---
File: /docs/source/documentation/modeling.rst
---

nnsight.modeling
----------------


.. automodule:: nnsight.modeling
   :members:

.. automodule:: nnsight.modeling.language
   :members:

.. automodule:: nnsight.modeling.diffusion
   :members:

.. automodule:: nnsight.modeling.vllm.vllm
   :members:

.. automodule:: nnsight.modeling.vllm.sampling
   :members:




---
File: /docs/source/documentation/schema.rst
---

nnsight.schema
---------------

.. automodule:: nnsight.schema
   :members:

.. automodule:: nnsight.schema.Config
   :members:

.. automodule:: nnsight.schema.Request
   :members:

.. automodule:: nnsight.schema.Response
   :members:

.. automodule:: nnsight.schema.format.functions
   :members:

.. automodule:: nnsight.schema.format.types
   :members:





---
File: /docs/source/documentation/tracing.rst
---

nnsight.tracing
---------------

.. automodule:: nnsight.tracing
   :members:

.. automodule:: nnsight.tracing.graph.graph
   :members:

.. automodule:: nnsight.tracing.graph.node
   :members:

.. automodule:: nnsight.tracing.graph.proxy
   :members:

.. automodule:: nnsight.tracing.protocols.base
   :members:

.. automodule:: nnsight.tracing.protocols.lock
   :members:

.. automodule:: nnsight.tracing.protocols.stop
   :members:

.. automodule:: nnsight.tracing.protocols.variable
   :members:

.. automodule:: nnsight.tracing.contexts.base
   :members:

.. automodule:: nnsight.tracing.contexts.conditional
   :members:

.. automodule:: nnsight.tracing.contexts.globals
   :members:

.. automodule:: nnsight.tracing.contexts.iterator
   :members:

.. automodule:: nnsight.tracing.contexts.tracer
   :members:


---
File: /docs/source/documentation/util.rst
---

nnsight.util
------------


.. automodule:: nnsight.util
   :members:


---
File: /docs/source/notebooks/tutorials/tutorial_price_tagging_utils.py
---

import itertools
import matplotlib.pyplot as plt
import numpy as np
from functools import partial
from typing import Dict, Optional, Sequence
from torch.nn import functional as F
import re
import os, random, argparse, sys, pickle, time, datasets, json
import copy, torch
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange
import numpy as np
import pandas as pd

os.environ["TOKENIZERS_PARALLELISM"] = "false"
from datasets import Dataset
from torch.utils.data import DataLoader
from dataclasses import dataclass, field
from collections import Counter
import networkx as nx
import ipywidgets as widgets
from ipywidgets import interact
from matplotlib.patches import Rectangle

IGNORE_INDEX = -100

"""
This is for tutorial

If the cost is between X and Y.ipynb

These dataset creation functions are copied from
https://github.com/frankaging/pyvene/blob/cf93a1a6491dba65e1422fe20428f5972d17137e/counterfactual_datasets/price_tagging_game.py
"""

alpaca_prompt_template = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
%s

### Input:
%s

### Response:
"""


def pricing_tag_game_config_sampler(amount, lower_bound, bound_width):
    if bound_width == None:
        bound_width_sample = round(random.uniform(2.50, 7.50), 2)
    else:
        bound_width_sample = bound_width
    if lower_bound == None:
        lower_bound_sample = round(random.uniform(0.05, 9.95 - bound_width_sample), 2)
        # left a little room to cover corner cases.
    else:
        lower_bound_sample = lower_bound
    upper_bound_sample = bound_width_sample + lower_bound_sample
    if amount == None:
        amount_sample = round(random.uniform(0.01, 9.99), 2)
    else:
        amount_sample = amount

    return lower_bound_sample, upper_bound_sample, amount_sample


def pricing_tag_game_example_sampler(
    tokenizer,
    amount,
    lower_bound,
    bound_width,
):
    (
        lower_bound_sample,
        upper_bound_sample,
        amount_sample,
    ) = pricing_tag_game_config_sampler(amount, lower_bound, bound_width)
    lower_bound_str = "%.2f" % lower_bound_sample
    upper_bound_str = "%.2f" % upper_bound_sample
    if amount_sample >= float(lower_bound_str) and amount_sample <= float(
        upper_bound_str
    ):
        label = tokenizer.convert_tokens_to_ids("Yes")
    else:
        label = tokenizer.convert_tokens_to_ids("No")

    amount_str = "%.2f dollars" % amount_sample
    instruction = f"Please say yes only if it costs between {lower_bound_str} and {upper_bound_str} dollars, otherwise no."
    alpaca_prompt = alpaca_prompt_template % (instruction, amount_str)
    input_ids = tokenizer(alpaca_prompt, return_tensors="pt").input_ids[0]
    output_ids = (torch.ones(input_ids.shape[0]) * -100).long().tolist()
    output_ids[-1] = label
    input_ids = input_ids.tolist()
    assert len(input_ids) == 82

    return input_ids, output_ids


def pricing_tag_game_example_sampler_with_info(
    tokenizer,
    amount,
    lower_bound,
    bound_width,
):
    (
        lower_bound_sample,
        upper_bound_sample,
        amount_sample,
    ) = pricing_tag_game_config_sampler(amount, lower_bound, bound_width)
    lower_bound_str = "%.2f" % lower_bound_sample
    upper_bound_str = "%.2f" % upper_bound_sample
    if amount_sample >= float(lower_bound_str) and amount_sample <= float(
        upper_bound_str
    ):
        label = tokenizer.convert_tokens_to_ids("Yes")
    else:
        label = tokenizer.convert_tokens_to_ids("No")

    amount_str = "%.2f dollars" % amount_sample
    instruction = f"Please say yes only if it costs between {lower_bound_str} and {upper_bound_str} dollars, otherwise no."
    alpaca_prompt = alpaca_prompt_template % (instruction, amount_str)
    input_ids = tokenizer(alpaca_prompt, return_tensors="pt").input_ids[0]
    output_ids = (torch.ones(input_ids.shape[0]) * -100).long().tolist()
    output_ids[-1] = label
    input_ids = input_ids.tolist()
    assert len(input_ids) == 82

    return (
        input_ids,
        output_ids,
        (lower_bound_sample, upper_bound_sample, amount_sample),
    )


def factual_sampler(
    tokenizer,
    max_n_training_examples,
    game="pricing_tag",
    amount=None,
    lower_bound=None,
    bound_width=None,
):
    all_input_ids = []
    all_output_ids = []  # this one does not have input ids, etc..
    for _ in range(max_n_training_examples):
        if "pricing_tag" in game:
            input_ids, output_ids = pricing_tag_game_example_sampler(
                tokenizer, amount, lower_bound, bound_width
            )
        elif game == "continent_retrieval":
            pass
        all_input_ids += [input_ids]
        all_output_ids += [output_ids]

    return all_input_ids, all_output_ids


def sample_with_region(region, lower_bound_sample, upper_bound_sample):
    if region == 1:
        amount_sample = round(random.uniform(0.01, lower_bound_sample - 0.01), 2)
    elif region == 2:
        amount_sample = round(random.uniform(lower_bound_sample, upper_bound_sample), 2)
    elif region == 3:
        amount_sample = round(random.uniform(upper_bound_sample + 0.01, 9.99), 2)
    return amount_sample


def lower_bound_alignment_example_sampler(
    tokenizer, amount=None, lower_bound=None, bound_width=None
):
    (
        base_lower_bound_sample,
        base_upper_bound_sample,
        _,
    ) = pricing_tag_game_config_sampler(amount, lower_bound, bound_width)
    (
        source_lower_bound_sample,
        source_upper_bound_sample,
        _,
    ) = pricing_tag_game_config_sampler(amount, lower_bound, bound_width)

    ctf_label_str = random.choice(["Yes", "No"])
    if ctf_label_str == "Yes":
        ctf_label = tokenizer.convert_tokens_to_ids("Yes")
        base_source_regions = [
            [1, 2],
            [1, 3],
            [2, 2],
            [2, 3],
        ]
    elif ctf_label_str == "No":
        ctf_label = tokenizer.convert_tokens_to_ids("No")
        base_source_regions = [[1, 1], [2, 1], [3, 1], [3, 2], [3, 3]]
    base_source_region = random.choice(base_source_regions)
    base_region = base_source_region[0]
    source_region = base_source_region[1]

    base_amount_sample = sample_with_region(
        base_region, base_lower_bound_sample, base_upper_bound_sample
    )
    source_amount_sample = sample_with_region(
        source_region, source_lower_bound_sample, source_upper_bound_sample
    )

    return (
        base_lower_bound_sample,
        base_upper_bound_sample,
        source_lower_bound_sample,
        source_upper_bound_sample,
        base_amount_sample,
        source_amount_sample,
        ctf_label,
        ctf_label_str,
    )


def bound_alignment_sampler(
    tokenizer,
    max_n_training_examples,
    bound_functors,
    amount=None,
    lower_bound=None,
    bound_width=None,
):
    all_base_input_ids = []
    all_source_input_ids = []
    all_ctf_output_ids = []  # this one does not have input ids, etc..
    all_intervention_ids = []

    for _ in range(max_n_training_examples):
        bound_functor = random.choice(bound_functors)
        (
            base_lower_bound_sample,
            base_upper_bound_sample,
            source_lower_bound_sample,
            source_upper_bound_sample,
            base_amount_sample,
            source_amount_sample,
            ctf_label,
            ctf_label_str,
        ) = bound_functor(
            tokenizer,
            amount,
            lower_bound,
            bound_width,
        )

        base_amount_str = "%.2f dollars" % base_amount_sample
        source_amount_str = "%.2f dollars" % source_amount_sample
        base_lower_bound_str = "%.2f" % base_lower_bound_sample
        base_upper_bound_str = "%.2f" % base_upper_bound_sample
        source_lower_bound_str = "%.2f" % source_lower_bound_sample
        source_upper_bound_str = "%.2f" % source_upper_bound_sample

        # print(f"base: [{base_lower_bound_str}, {base_upper_bound_str}], {base_amount_str}")
        # print(f"source: [{source_lower_bound_str}, {source_upper_bound_str}], {source_amount_str}")
        # print(f"ctf label: {ctf_label_str}")

        base_instruction = f"Please say yes only if it costs between {base_lower_bound_str} and {base_upper_bound_str} dollars, otherwise no."
        source_instruction = f"Please say yes only if it costs between {source_lower_bound_str} and {source_upper_bound_str} dollars, otherwise no."

        base_alpaca_prompt = alpaca_prompt_template % (
            base_instruction,
            base_amount_str,
        )
        source_alpaca_prompt = alpaca_prompt_template % (
            source_instruction,
            source_amount_str,
        )

        base_input_ids = tokenizer(base_alpaca_prompt, return_tensors="pt").input_ids[0]
        source_input_ids = tokenizer(
            source_alpaca_prompt, return_tensors="pt"
        ).input_ids[0]
        base_input_ids = base_input_ids.tolist()
        source_input_ids = source_input_ids.tolist()
        ctf_output_ids = (torch.ones(len(base_input_ids)) * -100).long().tolist()
        ctf_output_ids[-1] = ctf_label
        intervention_id = 0 if bound_functor == bound_functors[0] else 1

        all_base_input_ids += [base_input_ids]
        all_source_input_ids += [source_input_ids]

        all_ctf_output_ids += [ctf_output_ids]
        all_intervention_ids += [intervention_id]

        assert len(base_input_ids) == 82
        assert len(source_input_ids) == 82

    return (
        all_base_input_ids,
        all_source_input_ids,
        all_ctf_output_ids,
        all_intervention_ids,
    )


def midpoint_alignment_sampler(
    tokenizer,
    max_n_training_examples,
    amount=None,
    lower_bound=None,
    bound_width=None,
):

    all_base_input_ids = []
    all_source_input_ids = []
    all_ctf_output_ids = [] # this one does not have input ids, etc..
    all_intervention_ids = []
    
    for _ in range(max_n_training_examples):
        
        base_lower_bound_sample, base_upper_bound_sample, base_amount_sample = \
            pricing_tag_game_config_sampler(
                amount,
                lower_bound,
                bound_width
            )
        source_lower_bound_sample, source_upper_bound_sample, source_amount_sample = \
            pricing_tag_game_config_sampler(
                amount,
                lower_bound,
                bound_width
            )
        ctf_label = None
        ctf_label_str = None
        source_mid_point = (source_lower_bound_sample+source_upper_bound_sample)/2.0
        base_half = 0.5*abs(base_upper_bound_sample-base_lower_bound_sample)
        ctf_mid_diff = abs(base_amount_sample-source_mid_point)
        if ctf_mid_diff <= base_half:
            ctf_label = tokenizer.convert_tokens_to_ids("Yes")
            ctf_label_str = "Yes"
        else:
            ctf_label = tokenizer.convert_tokens_to_ids("No")
            ctf_label_str = "No"
            
        base_amount_str = "%.2f dollars" % base_amount_sample
        source_amount_str = "%.2f dollars" % source_amount_sample
        base_lower_bound_str = "%.2f" % base_lower_bound_sample
        base_upper_bound_str = "%.2f" % base_upper_bound_sample
        source_lower_bound_str = "%.2f" % source_lower_bound_sample
        source_upper_bound_str = "%.2f" % source_upper_bound_sample
        
        # print(f"base: [{base_lower_bound_str}, {base_upper_bound_str}], {base_amount_str}")
        # print(f"source: [{source_lower_bound_str}, {source_upper_bound_str}], {source_amount_str}")
        # print(f"ctf label: {ctf_label_str}")
        
        base_instruction = f"Please say yes only if it costs between {base_lower_bound_str} and {base_upper_bound_str} dollars, otherwise no."
        source_instruction = f"Please say yes only if it costs between {source_lower_bound_str} and {source_upper_bound_str} dollars, otherwise no."
        
        base_alpaca_prompt = alpaca_prompt_template % (base_instruction, base_amount_str)
        source_alpaca_prompt = alpaca_prompt_template % (source_instruction, source_amount_str)
        
        base_input_ids = tokenizer(base_alpaca_prompt, return_tensors="pt").input_ids[0]
        source_input_ids = tokenizer(source_alpaca_prompt, return_tensors="pt").input_ids[0]
        base_input_ids = base_input_ids.tolist()
        source_input_ids = source_input_ids.tolist()
        ctf_output_ids = (torch.ones(len(base_input_ids))*-100).long().tolist()
        ctf_output_ids[-1] = ctf_label
        
        all_base_input_ids += [base_input_ids]
        all_source_input_ids += [source_input_ids]
        all_ctf_output_ids += [ctf_output_ids]
        all_intervention_ids += [0]
        assert len(base_input_ids) == 82
        assert len(source_input_ids) == 82
        
    return all_base_input_ids, all_source_input_ids, all_ctf_output_ids, all_intervention_ids


def bracket_alignment_sampler(
    tokenizer,
    max_n_training_examples,
    amount=None,
    lower_bound=None,
    bound_width=None,
):

    all_base_input_ids = []
    all_source_input_ids = []
    all_ctf_output_ids = [] # this one does not have input ids, etc..
    all_intervention_ids = []
    
    for _ in range(max_n_training_examples):
        
        base_lower_bound_sample, base_upper_bound_sample, base_amount_sample = \
            pricing_tag_game_config_sampler(
                amount,
                lower_bound,
                bound_width
            )
        source_lower_bound_sample, source_upper_bound_sample, source_amount_sample = \
            pricing_tag_game_config_sampler(
                amount,
                lower_bound,
                bound_width
            )
        ctf_label = None
        ctf_label_str = None
        if base_amount_sample <= source_upper_bound_sample and base_amount_sample >= source_lower_bound_sample:
            ctf_label = tokenizer.convert_tokens_to_ids("Yes")
            ctf_label_str = "Yes"
        else:
            ctf_label = tokenizer.convert_tokens_to_ids("No")
            ctf_label_str = "No"
            
        base_amount_str = "%.2f dollars" % base_amount_sample
        source_amount_str = "%.2f dollars" % source_amount_sample
        base_lower_bound_str = "%.2f" % base_lower_bound_sample
        base_upper_bound_str = "%.2f" % base_upper_bound_sample
        source_lower_bound_str = "%.2f" % source_lower_bound_sample
        source_upper_bound_str = "%.2f" % source_upper_bound_sample
        
        # print(f"base: [{base_lower_bound_str}, {base_upper_bound_str}], {base_amount_str}")
        # print(f"source: [{source_lower_bound_str}, {source_upper_bound_str}], {source_amount_str}")
        # print(f"ctf label: {ctf_label_str}")
        
        base_instruction = f"Please say yes only if it costs between {base_lower_bound_str} and {base_upper_bound_str} dollars, otherwise no."
        source_instruction = f"Please say yes only if it costs between {source_lower_bound_str} and {source_upper_bound_str} dollars, otherwise no."
        
        base_alpaca_prompt = alpaca_prompt_template % (base_instruction, base_amount_str)
        source_alpaca_prompt = alpaca_prompt_template % (source_instruction, source_amount_str)
        
        base_input_ids = tokenizer(base_alpaca_prompt, return_tensors="pt").input_ids[0]
        source_input_ids = tokenizer(source_alpaca_prompt, return_tensors="pt").input_ids[0]
        base_input_ids = base_input_ids.tolist()
        source_input_ids = source_input_ids.tolist()
        ctf_output_ids = (torch.ones(len(base_input_ids))*-100).long().tolist()
        ctf_output_ids[-1] = ctf_label
        
        all_base_input_ids += [base_input_ids]
        all_source_input_ids += [source_input_ids]
        all_ctf_output_ids += [ctf_output_ids]
        all_intervention_ids += [0]
        assert len(base_input_ids) == 82
        assert len(source_input_ids) == 82
        
    return all_base_input_ids, all_source_input_ids, all_ctf_output_ids, all_intervention_ids



---
File: /docs/source/pages/home.html
---

<!-- Styles which override PyData site-wide themeing for the front page. -->
<style>
    .bd-main .bd-content .bd-article-container {
        max-width: 100%;  /* default is 60em */
    }

    .bd-article-container {
        padding-left: 0px !important;
        padding-right: 0px !important;
        padding-bottom: 0px !important;
    }

    .bd-page-width {
        max-width: 100%;  /* default is 88rem */
    }

    .bd-header__inner {
        max-width: 88rem !important;
    }

    .bd-footer__inner {
        max-width: 88rem !important;
    }

    .bd-footer {
        background: var(--pst-color-background) !important;
    }
</style>

<div class="fixed-background"></div>

<section class="hero col-lg-6 col-8 container">
    <div class="row h-100 align-items-center">
        <div class="col-md-6">
            <h1 class="display-5 fw-bold lh-1 mb-3">Interpretable Neural Networks</h1>
            <p class="lead">NNsight (/ɛn.saɪt/) is a package for interpreting and manipulating the internals of models.</p>
            <div class="d-grid gap-2 d-md-flex position-relative" style="z-index: 2;">
                <a href="start" role="button" class="btn btn-primary primary btn-md px-4">Start</a>
                <a href="documentation" role="button" class="btn btn-outline-secondary secondary btn-md px-4">Docs</a>
                <a href="features" role="button" class="btn btn-outline-secondary btn-md secondary px-4">Features</a>
                <a href="about" role="button" class="btn btn-outline-secondary btn-md secondary px-4">About</a>
            </div>
        </div>
        <div class="col-md-6 d-none d-md-block text-center">
            <img src="_static/images/nnsight_logo.svg" class="img-fluid transparent hero-image" alt="NNsight Logo" loading="lazy">
        </div>
    </div>
</section>

<div class="surface pb-5">
<section class="col-lg-6 col-8 container pb-5">


        <div class="row g-5 align-items-center" style="height: 45vh">

            <div class="col-md-5">
                <div style="width: 100%; margin: auto;">
                    <h3 class="fw-bold lh-1 mb-3 mt-0">Wrap Any PyTorch Model</h3>
                    <p class="lead" id="myElement">The NNsight class object wraps a given PyTorch model, enabling tracing capabilities.</p>
                    <a href="start">Get Started →</a>
                </div>
            </div>

            <div class="col-md-7 d-none d-md-block">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nnsight</span> <span class="kn">import</span> <span class="n">NNsight</span><span class="p">,</span> <span class="n">LanguageModel</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;layer1&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;layer2&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)),</span>
<span class="p">]))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">NNsight</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<span class="c1"># or</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s1">&#39;openai-community/gpt2&#39;</span><span class="p">)</span>
</pre></div>
            </div>

        </div>
        <div class="row g-5 align-items-center" style="height: 35vh">

            <div class="col-md-5">
                <div style="width: 100%; margin: auto;">
                <h3 class="fw-bold lh-1 mb-3 mt-0">Access Hidden States</h3>
                <p class="lead" id="myElement">Easily expose Module inputs and outputs.</p>
                <a href="notebooks/tutorials/walkthrough">Walkthrough →</a>
            </div>
            </div>

            <div class="col-md-7 d-none d-md-block">

<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s1">&#39;Who invented neural networks?&#39;</span><span class="p">):</span>

    <span class="n">hidden_state_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="n">hidden_state_input</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">hidden_state_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hidden_state_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>

            </div>
        </div>
        <div class="row g-5 align-items-center" style="height: 45vh">
            <div class="col-md-5">
                <div style="width: 100%; margin:auto;">
                    <h3 class="fw-bold lh-1 mb-3 mt-0">Develop Complex Interventions</h3>
                    <p class="lead" id="myElement">Edit Module outputs and measure effects.</p>
                    <a href="tutorials">Tutorials →</a>
                </div>
            </div>
            <!-- CODE BLOCK NUMBER THREE -->
            <div class="col-md-7 d-none d-md-block">
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span><span class="p">):</span>

        <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span><span class="p">):</span>

        <span class="n">original</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
            </div>
            <!-- END OF CODE BLOCK NUMBER THREE -->
        </div>
</section>
</div>


<script>

    // Fix navbar to top of page
    var navbar = document.querySelector('.navbar');

    if (navbar) {
        navbar.style.position = 'fixed';
    } else {
        console.log(".navbar element does not exist.");
    }

    // Remove article padding so content background is flush with sides
    var article = document.querySelector('.bd-article');

    if (article) {
        article.style.paddingTop = '0';
        article.style.paddingLeft = '0';
    } else {
        console.log(".article element does not exist.");
    }

</script>


---
File: /docs/source/about.rst
---

.. raw:: html

    <style>
        .accordion-header {
            margin: 0 !important;
        }
    </style>

    <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('figure.align-default').forEach(el => {
                el.style.marginBottom = "0px";
            });
        });
    </script>

About NNsight
=============

An API for transparent science on black-box AI.
-----------------------------------------------

.. card:: How can you study the internals of a deep network that is too large for you to run?

    In this era of large-scale deep learning, the most interesting AI models are massive black boxes
    that are hard to run. Ordinary commercial inference service APIs let you interact with huge
    models, but they do not let you see model internals.

    The NNsight library is different: it gives you full access to all the neural network internals.
    When used together with a remote service like the `National Deep Inference Fabric <https://ndif.us/>`_ (NDIF),
    it lets you run experiments on huge open models easily, with full transparent access. 
    NNsight is also terrific for studying smaller local models.

.. figure:: _static/images/interleaved.png


.. card::
    
    An overview of the NNsight/NDIF pipeline. Researchers write simple Python code to run along with the neural network locally or remotely. Unlike commercial inference, the experiment code can read or write any of the internal states of the neural networks being studied.  This code creates a computation graph that can be sent to the remote service and interleaved with the execution of the neural network.

How do I use NNsight?
---------------------

NNsight is built on PyTorch.

Running inference on a huge remote model with NNsight is very similar to running a neural network locally on your own workstation.  In fact, with NNsight, the same code for running experiments locally on small models can also be used on large models just by changing a few arguments.

The difference between NNsight and normal inference is that when you use NNsight, you do not treat the model as an opaque black box.
Instead, you set up a Python ``with`` context that enables you to get direct access to model internals while the neural network runs.
Here is how it looks:

.. code-block:: python
    :linenos:

    from nnsight import LanguageModel
    model = LanguageModel('meta-llama/Meta-Llama-3.1-70B')
    with model.trace('The Eiffel Tower is in the city of ', remote=True):
        hidden_state = model.layers[10].input[0].save()  # save one hidden state
        model.layers[11].mlp.output = 0  # change one MLP module output
        output = model.output.save() # save model output
    print('The model predicts', output)
    print('The internal state was', hidden_state)

The library is easy to use. Any HuggingFace language model can be loaded into a ``LanguageModel`` object, as you can see on line 2.  
Notice we are loading a 70-billion parameter model, which is ordinarily pretty difficult to load on a regular workstation since it would take 140-280 gigabytes of GPU RAM just to store the parameters. 

The trick that lets us work with this huge model is on line 3.  We set the flag ``remote=True`` to indicate that we want to actually run the network on the remote service. 
By default the remote service will be NDIF.  If we want to just run a smaller model locally on our machine, we could leave it as ``remote=False``.

Then when we trace and invoke the model on line 3, we do not just call it as a function. Instead, we access it using a ``with`` context manager. 
This allows NNsight to open up black box neural network models, providing direct access to model internals.

You can see what simple direct access looks like on lines 4-6. 
On line 4, we grab a hidden state at layer 10, and on line 5, we change the output of an MLP module inside the transformer at layer 11.

When you run this ``with``-block code on lines 3 through 6 on your local workstation, it creates a computation graph storing all your requested calculations on the model.  
When the ``with`` block is completed, all the defined calculations are sent to the remote server and executed there. 
Then when model execution is completed, saved results can be accessed on your local workstation, as shown on line 7 and 8.

What happens behind the scenes?
-------------------------------
When using NNsight, it is helpful to understand that the operations are not executed immediately but instead adds to an intervention graph that is executed alongside the model's computation graph upon exit of the with block.

An example of one such intervention graph can be seen below:

.. figure:: _static/images/execution.png

.. card::
    
    An example of an intervention graph. Operations in research code create nodes in the graph which depend on module inputs and outputs as well as other nodes. Then, this intervention graph is interleaved with the normal computation graph of the chosen model, and requested inputs and outputs are injected into the intervention graph for execution. 

Basic access to model internals can give you a lot of insight about what is going on inside a large model as it runs.  For example, you can use the `logit lens <https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens>`_ to read internal hidden states as text.  
And use can use `causal tracing <https://rome.baulab.info/>`_ or `path patching <https://arxiv.org/abs/2304.05969>`_ or `other circuit discovery methods <https://arxiv.org/abs/2310.10348>`_ to locate the layers and components within the network that play a decisive role in making a decision.

And with NNsight, you can use these methods on large models like Llama-3.1-70b or Llama-3.1-405b.

The NNsight library also provides full access to gradients and optimization methods, out of order module applications, cross prompt interventions, and many more features.

Next Steps
----------

See the :doc:`start` and :doc:`features` pages for more information on NNsight's functionality.

Join our forum https://discuss.ndif.us/ for updates, feature requests, bug reports, and opportunities to help with our efforts. 
If you'd like to report an issue or give our project a star, check out our GitHub: https://github.com/ndif-team/nnsight. 
We also welcome you to join the `NDIF Discord <https://discord.gg/6uFJmCSwW7>`_ for real-time discussions and community support.



---
File: /docs/source/conf.py
---

import time

# Configuration file for the Sphinx documentation builder.

# Project Information
project = "nnsight"
copyright = "2024 NDIF"
author = "Jaden Fiotto-Kaufman"


# General Configuration
extensions = [
    "sphinx.ext.autodoc",  # Auto documentation from docstrings
    "sphinx.ext.napoleon",  # Support for NumPy and Google style docstrings
    "sphinx_copybutton",  # Copy button for code blocks
    "sphinx_design",  # Boostrap design components
    "nbsphinx",  # Jupyter notebook support
    "sphinx.ext.viewcode",  # Add source links to the generated HTML files
    "sphinx.ext.extlinks",  # Add external links
]

templates_path = ["_templates"]
fixed_sidebar = True

# HTML Output Options

# See https://sphinx-themes.org/ for more
html_theme = "pydata_sphinx_theme"
html_title = "nnsight"
html_logo = "_static/images/nnsight_logo.svg"
html_static_path = ["_static"]
html_show_sphinx = False

html_favicon = "_static/images/icon.ico"
html_show_sourcelink = False

html_context = {
   "default_mode": "dark",
   "ndif_url": "https://ndif.dev/ping",
   "version_identifier": str(int(time.time())),
}


html_theme_options = {
    "show_nav_level": 2,
    "navbar_end": ["ndif_status", "theme-switcher","navbar-icon-links"],
    "navbar_align": "left",
    "icon_links": [
        {
            "name": "Status: Unknown",
            "url": "/status",
            "icon": "fa-solid fa-circle-check",
            "attributes": {"class": "ndif"},
        },
        {
            "name": "GitHub",
            "url": "https://github.com/ndif-team/nnsight",
            "icon": "fa-brands fa-github",
        },
        {
            "name": "Forum",
            "url": "https://discuss.ndif.us/",
            "icon": "fa-brands fa-discourse",
        },
        {
            "name": "Discord",
            "url": "https://forms.gle/1Y6myaXYzSh3oHf56",
            "icon": "fa-brands fa-discord",
        },
    ],
    "show_prev_next": False,
    "pygment_dark_style": "monokai",
}

extlinks = {'ndif': ('https://%s.com/ndif-team/nnsight',
                      '%s')}

html_js_files = [
    'js/custom.js',
    'js/code.js'
]


---
File: /docs/source/documentation.rst
---

Documentation
=============

.. toctree::
   :titlesonly:

   documentation/modeling
   documentation/tracing
   documentation/intervention
   documentation/schema
   documentation/util


Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ for debugging any issues and discussing features.


---
File: /docs/source/features.rst
---

Features
=========

.. raw:: html

   <script>
   document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('h5.card-title').forEach(el => {
      el.style.margin = '0';
      });
   });
   </script>

   <style>
      .toctree-wrapper {
         display: none !important;
      }
      h5 {
         margin-top: 0 !important;
      }
   </style>

.. grid:: 1 1 2 2
   :gutter: 3

   .. grid-item-card:: 
      :link: notebooks/features/getting.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-wrench fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Getting</h5>
               <p class="card-text">Access values</p>
            </div>
         </div>

   .. grid-item-card::
      :link: notebooks/features/setting.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-pull-request fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Setting</h5>
               <p class="card-text">Intervene on values</p>
            </div>
         </div>

   .. grid-item-card::
      :link: notebooks/features/scan_validate.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-binoculars fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Scan and Validate</h5>
               <p class="card-text">Debug tensor shapes</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/features/operations.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-glasses fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Operations</h5>
               <p class="card-text">Edit values</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/modules.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-cubes fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Modules</h5>
               <p class="card-text">Apply modules</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/custom_functions.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-atom fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Custom Functions</h5>
               <p class="card-text">Add thing to the Intervention Graph</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/gradients.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-backward fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Gradients</h5>
               <p class="card-text">Intervene on gradients</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/early_stopping.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-circle-stop fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Early Stopping</h5>
               <p class="card-text">Save computation time</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/conditionals.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-branch fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Conditional Interventions</h5>
               <p class="card-text">Use If Needed</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/cross_prompt.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-shuffle fa-2x"></i>
            </div>
            <div>
               <h5 class="card-title">Cross Prompts</h5>
               <p class="card-text">Edit in one pass</p>
            </div>
         </div>
   
   .. grid-item-card:: 
      :link: notebooks/features/multiple_token.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-gears fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Generation</h5>
               <p class="card-text">Generate multiple tokens</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/model_editing.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-pen-to-square fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Model Editing</h5>
               <p class="card-text">Add persistent interventions</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/features/remote_execution.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-satellite-dish fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Remote Execution</h5>
               <p class="card-text">Use our servers</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/sessions.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-bars fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Sessions</h5>
               <p class="card-text">Do many traces in one request</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/streaming.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-paper-plane fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Streaming</h5>
               <p class="card-text">Send remote values to local</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/iterator.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-arrow-rotate-left fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Iterative Interventions</h5>
               <p class="card-text">Make loops</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/lora_training.ipynb
      :class-card: surface
      :class-body: surface

   
      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-tower-broadcast fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">LORA</h5>
               <p class="card-text">Train one</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/features/vllm_support.ipynb
      :class-card: surface
      :class-body: surface


      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-stopwatch fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">vLLM Support</h5>
               <p class="card-text">Fast inference</p>
            </div>
         </div>
   

Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ to ask questions about our features or suggest new ones.

.. toctree::
   :glob:
   :maxdepth: 1
   
   notebooks/features/*




---
File: /docs/source/index.rst
---

:html_theme.sidebar_secondary.remove:
:sd_hide_title:

nnsight
=======

.. toctree::
  :maxdepth: 1
  :hidden:

  start
  documentation
  features
  tutorials
  About <about>

.. raw:: html
   :file: pages/home.html


---
File: /docs/source/start.rst
---

Getting Started
===============

**NNsight** (/ɛn.saɪt/) is a package for the interpreting and manipulating the internals of deep learning models.

.. _installation:

Installation
------------

To get started with NNsight, install it with ``pip``. 

.. code-block:: console

   pip install nnsight

Please give the project a :ndif:`star on Github` to support the project. NNsight is open-source and you can read and clone the full source at https://github.com/ndif-team/nnsight.

Remote Model Access
-------------------

To remotely access LLMs through NDIF, you must sign up for an NDIF API key.

:bdg-link-primary:`NDIF API Key Registration <https://login.ndif.us/>`

NDIF hosts multiple LLMs, including various sizes of the Llama 3.1 models and DeepSeek-R1 models. 
All of our models are open for public use, but you need to apply for access to the Llama-3.1-405B models. 
You can view the full list of hosted models at https://nnsight.net/status/.

If you have a clear research need for Llama-3.1-405B and would like more details about applying for access, 
please refer to our  `405B pilot program application <https://ndif.us/405b.html>`_.

Access LLM Internals
--------------------

Now that you have your NDIF API key, you can start exploring LLM internals with NDIF and NNsight. 
We've put together a Colab notebook to help you get started.

:bdg-link-primary:`Open Colab <https://colab.research.google.com/github/ndif-team/ndif-website/blob/onboarding-fixes/public/notebooks/NDIFGetStarted.ipynb>`

This notebook will walk you through the following steps:

#. Installing NNsight
#. Setting up your NDIF API key
#. Loading a LLM in NNsight
#. Accessing and altering LLM internals remotely


Next Steps
-----------

.. grid:: 2 2 2 2 
   :gutter: 2

   .. grid-item-card:: Walkthrough
      :link: notebooks/tutorials/walkthrough.ipynb

      Walk through the basic functionality of the package.

   .. grid-item-card:: Remote Access
      :link: notebooks/features/remote_execution.ipynb

      Configure API access for remote model execution.

   .. grid-item-card:: Features
      :link: features
      :link-type: doc

      Check out the basic features provided by :bdg-primary:`nnsight`.

   .. grid-item-card:: Tutorials
      :link: tutorials
      :link-type: doc

      See :bdg-primary:`nnsight` implementations of common interpretability techniques.

   .. grid-item-card:: Forum
      :link: https://discuss.ndif.us/

      Discuss :bdg-primary:`nnsight`, NDIF, and more!






---
File: /docs/source/status.rst
---

:html_theme.sidebar_secondary.remove:
:sd_hide_title:

.. raw:: html

    <style>
        .accordion-header {
            margin: 0 !important;
        }

        /* Custom accordion styles */
        .custom-accordion-header {
            background-color: var(--pst-color-surface);
            /* Default state background color */
            color: var(--pst-color-text-base);
            /* Text color */
            border-bottom: var(--pst-color-border);
            /* Border color */
        }

        .custom-accordion-header {
            background-color: var(--pst-color-surface);
            /* Default state background color */
            color: var(--pst-color-text-base);
            /* Text color */
            border-bottom: var(--pst-color-border);
            /* Border color */
        }

        .accordion {
            --bs-accordion-btn-icon: none;
            --bs-accordion-btn-active-icon: none;
        }

        .custom-accordion-header.collapsed {
            background-color: var(--pst-color-on-background);
            /* Collapsed state background color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .custom-accordion-header:not(.collapsed) {
            background-color: var(--pst-color-surface);
            /* Active/Expanded state background color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .custom-accordion-body {
            background-color: var(--pst-color-on-background);
            /* Body background color */
            border-color: var(--pst-color-border);
            /* Border color */
            color: var(--pst-color-text-base);
            /* Text color */
        }

        .sd-card {
            border-radius: 0 !important;
        }

        #loader {
            width: 120px;
            height: 120px;
            display: inline-block;
            position: relative;
        }
        #loader::after,
        #loader::before {
            content: '';  
            box-sizing: border-box;
            width:120px;
            height: 120px;
            border-radius: 50%;
            background: #FFF;
            position: absolute;
            left: 0;
            top: 0;
            animation: animloader 2s linear infinite;
        }
        #loader::after {
            animation-delay: 1s;
        }
        
        @keyframes animloader {
            0% {
                transform: scale(0);
                opacity: 1;
            }
            100% {
                transform: scale(1);
                opacity: 0;
            }
        }

    </style>


    <script>

        let ndif_url = "https://ndif.dev"
        let error_color = "#7e0000"
        let success_color = "#66800b"
        let warning_color = "#7d7106"

        function autoFormatJsonString(jsonString) {
            // Parse the JSON string into an object
            let jsonObject = JSON.parse(jsonString);

            // Convert the object back into a string with indentation
            let prettyPrintedJson = JSON.stringify(jsonObject, null, 2);

            // Replace keys in the JSON string with styled spans
            prettyPrintedJson = prettyPrintedJson.replace(/"([^"]+)":/g, '<span style="background-color: lightgrey;">"$1":</span>');

            // Set the formatted JSON string as the innerHTML of the element
            document.getElementById('jsonContainer').innerHTML = `<pre>${prettyPrintedJson}</pre>`;
        };

        function update(message, color) {
            document.querySelectorAll('div.sd-card-body.status-container').forEach(el => {
                el.style.backgroundColor = color;
                el.querySelectorAll('p.sd-card-text').forEach(el => {
                    el.textContent = message;
                });
            });
        }

        function loading(flag) {
            document.getElementById("loader").style.display = flag ? "block" : "none";
        }

        document.addEventListener('DOMContentLoaded', function() {

            loading(true);

            update("Fetching NDIF status...", warning_color);

            fetch(ndif_url + "/ping")

                .then((response) => {
                    if (response.status == 200) {

                        update("NDIF is up. Fetching model status...", warning_color);

                        console.log('Ping success');
                        // Nested fetch to ndif.dev/stats
                        fetch(ndif_url + "/stats")
                            .then((statsResponse) => {

                                loading(false);

                                if (statsResponse.status == 200) {
                                    statsResponse.json().then((parsed) => {
                                        // Initialize an empty string to accumulate information
                                        let infoString = '';

                                        let index = 0;

                                        let modelSummary = {};

                                        if (parsed.length === 0) {

                                            update("NDIF is up but there are no models deployed. Seems unintentional.", error_color);

                                            return
                                        }


                                        update("NDIF is operational.", success_color);

                                        Object.values(parsed).forEach((value) => {
                                            // Create a unique key for each model-config combination
                                            let modelConfigKey = `${value.repo_id}`;

                                            // Check if this model-config combination already exists in the summary
                                            if (modelSummary[modelConfigKey]) {
                                                // Increment the count if it does
                                                modelSummary[modelConfigKey].number_of_copies += 1;
                                            } else {
                                                // Otherwise, add a new entry
                                                modelSummary[modelConfigKey] = {
                                                    number_of_copies: 1,
                                                    config_string: value.config_json_string
                                                };
                                            }
                                        });

                                        // Now modelSummary contains the consolidated information
                                        console.log(modelSummary);

                                        // Iterate through the JSON dictionary and append information
                                        // Iterate through the modelSummary dictionary and append information
                                        Object.keys(modelSummary).forEach((key) => {
                                            var headingId = 'heading' + (index + 1);
                                            var collapseId = 'collapse' + (index + 1);

                                            const summaryItem = modelSummary[key];
                                            const configJsonString = summaryItem.config_string;

                                            let jsonObject = JSON.parse(configJsonString);

                                            // Convert the object back into a string with indentation
                                            let prettyPrintedJson = JSON.stringify(jsonObject, null, 4);

                                            prettyPrintedJson = prettyPrintedJson.replace(/"([^"]+)":/g, '"<b>$1</b>":');
                                            let huggingFaceLink = `<a href="http://huggingface.co/${key}" target="_blank">HuggingFace Model Repository ↗</a>`;

                                            infoString += `<div class="accordion-item">
                                                    <h2 class="accordion-header" id="${headingId}">
                                                        <button class="accordion-button custom-accordion-header collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#${collapseId}" aria-expanded="false" aria-controls="${collapseId}">
                                                            (${summaryItem.number_of_copies}x) ${key}
                                                        </button>
                                                    </h2>
                                                    <div id="${collapseId}" class="accordion-collapse collapse" aria-labelledby="${headingId}" data-bs-parent="#accordionExample">
                                                        <div class="accordion-body custom-accordion-body">${huggingFaceLink}<pre>${prettyPrintedJson}</pre></div>
                                                    </div>
                                                </div>`;


                                            index++;
                                        });

                                        var elm = document.getElementById("accordionHook");

                                        elm.innerHTML = infoString;


                                        console.log('Stats success');
                                    }).catch((jsonError) => {
                                        console.log('JSON parsing error:', jsonError);
                                    });
                                } else {
                                    update("Unable to get NDIF status.", error_color);

                                }
                            })
                            .catch((statsError) => {
                                update("Unable to get NDIF status.", error_color);
                                loading(false);

                                console.log('Stats error', statsError);
                            });
                    } else {
                        update("NDIF is unavailable", error_color);
                        loading(false);
                        console.log('Ping error');
                    }
                })
                .catch((pingError) => {
                    update("NDIF is unavailable", error_color);
                    loading(false);
                    console.error('Ping fetch failed:', pingError);
                });

        }, false);
    </script>


Status
======

.. card::
    :class-body: status-container
    :shadow: none

    Getting Status

.. card::
    :shadow: none
    
    The library can be used to run local models without requiring a key. However, running experiments on remote models requires a free server API key. To obtain a key, register for an `NDIF account <https://login.ndif.us>`_ which allows you to manage and generate keys.
    For information on API key configuration and remote system limits, please refer to our `Remote Execution Tutorial <https://nnsight.net/notebooks/features/remote_execution/>`_.

    We currently have engineers on call Monday to Friday from 9 AM to 5 PM ET to assist with any connectivity issues for our remote models. Please reach out to us on `Discord <https://discord.com/invite/6uFJmCSwW7>`_ or at mailto:info@ndif.us.

.. raw:: html

    <div style="
        width:100%;
        display: flex;
        justify-content: center;
        ">
        <div id="loader"></div>
    </div>
    


    <div class="accordion accordion-flush" id="accordionHook">
    </div>


---
File: /docs/source/tutorials.rst
---

.. role:: raw-html(raw)
   :format: html

.. raw:: html

   <script>
   document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('h5.card-title').forEach(el => {
      el.style.margin = '0';
      });
   });
   </script>

   <style>
      .toctree-wrapper {
         display: none !important;
      }
      h5 {
         margin-top: 0 !important;
      }
   </style>


Tutorials
=========

.. grid:: 1 1 2 2
   :class-container: tutorial-card-section
   :gutter: 3

   .. grid-item-card:: 
      :link: notebooks/tutorials/walkthrough.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-person-walking fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Walkthrough</h5>
               <p class="card-text">Learn the basics</p>
            </div>
         </div>


   .. grid-item-card:: 
      :link: notebooks/tutorials/start_remote_access.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-satellite-dish fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Access LLMs</h5>
               <p class="card-text">Use our hosted models</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/activation_patching.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-code-pull-request fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Activation Patching</h5>
               <p class="card-text">Causal intervention</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/attribution_patching.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-diagram-project fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Attribution Patching</h5>
               <p class="card-text">Approximate patching</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/boundless_DAS.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-magnifying-glass fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Boundless DAS</h5>
               <p class="card-text">Identifying causal mechanisms</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/dict_learning.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-book-open fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Dictionary Learning</h5>
               <p class="card-text">Sparse autoencoders</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/diffusion_lens.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-camera-retro fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Diffusion Lens</h5>
               <p class="card-text">Explore diffusion model text embedding</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/function_vectors.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-dharmachakra fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Function Vectors</h5>
               <p class="card-text">Steer model behavior</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/logit_lens.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-arrow-down-a-z fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">Logit Lens</h5>
               <p class="card-text">Decode activations</p>
            </div>
         </div>

   .. grid-item-card:: 
      :link: notebooks/tutorials/LoRA_tutorial.ipynb
      :class-card: surface
      :class-body: surface

      .. raw:: html

         <div class="d-flex align-items-center">
            <div class="d-flex justify-content-center" style="min-width: 50px; margin-right: 15px; height: 100%;">
               <i class="fa-solid fa-sliders fa-2x"></i>
            </div> 
            <div>
               <h5 class="card-title">LoRA</h5>
               <p class="card-text">Fine tuning for sentiment analysis</p>
            </div>
         </div>      

Report Issues
-------------

NNsight and NDIF are open-source and you can report issues, read, and clone the full source at https://github.com/ndif-team/nnsight. 
Also check out https://discuss.ndif.us/ to ask questions about our tutorials, share your projects in NNsight, or request new tutorials.

.. toctree::
   :glob:
   :maxdepth: 1

   notebooks/tutorials/*





---
File: /docs/sourcelatex/documentation/contexts.rst
---

nnsight.contexts
-----------------


.. automodule:: nnsight.contexts
   :members:

.. automodule:: nnsight.contexts.Runner
   :members:

.. automodule:: nnsight.contexts.Tracer
   :members:

.. automodule:: nnsight.contexts.Invoker
   :members:




---
File: /docs/sourcelatex/documentation/intervention.rst
---

nnsight.intervention
--------------------

.. automodule:: nnsight.intervention
   :members:


---
File: /docs/sourcelatex/documentation/models.rst
---

nnsight.models
--------------


.. automodule:: nnsight.models
   :members:


.. automodule:: nnsight.models.NNsightModel
   :members:


.. automodule:: nnsight.models.LanguageModel
   :members:


.. automodule:: nnsight.models.DiffuserModel
   :members:


---
File: /docs/sourcelatex/documentation/module.rst
---

nnsight.module
--------------

.. automodule:: nnsight.Module
   :members:


---
File: /docs/sourcelatex/documentation/patching.rst
---

nnsight.patching
----------------


.. automodule:: nnsight.patching
   :members:


---
File: /docs/sourcelatex/documentation/tracing.rst
---

nnsight.tracing
---------------

.. automodule:: nnsight.tracing
   :members:

.. automodule:: nnsight.tracing.Graph
   :members:

.. automodule:: nnsight.tracing.Node
   :members:

.. automodule:: nnsight.tracing.Proxy
   :members:



---
File: /docs/sourcelatex/documentation/util.rst
---

nnsight.util
------------


.. automodule:: nnsight.util
   :members:


---
File: /docs/sourcelatex/conf.py
---

# Configuration file for the Sphinx documentation builder.

# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'nnsight'
copyright = '2023, NDIF'
author = 'Jaden Fiotto-Kaufman'
release = '0.1'


fixed_sidebar=True

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',
    'sphinx_copybutton',
    'sphinx_design',
    'nbsphinx',
]

templates_path = ['_templates']
exclude_patterns = []



# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = "pydata_sphinx_theme"


html_static_path = ['_static']

html_theme_options = {
  "logo": {"text":"nnsight"},
  "show_nav_level": 2,
  "navbar_end": ["navbar-icon-links"],
  "search_bar_text": "Your text here...",
}


html_context = {
   "default_mode": "light"
}




---
File: /docs/sourcelatex/index.rst
---

Documentation
=============

.. toctree::
   :titlesonly:

   documentation/models
   documentation/module
   documentation/contexts
   documentation/util
   documentation/intervention
   documentation/tracing
   documentation/patching



---
File: /docs/contributing.md
---

# Installation

Run `pip install -r requirements.txt` while in the `nnsight/docs` directory.

Optionally, run `pip install -e nnsight` from the root directory to use the working dir

If you haven't already, you should also install pandoc. With brew run: `brew install pandoc`

# Adding Tutorials

Tutorials are written as Jupyter Notebooks in `.ipynb` format, and automatically converted to html by the `nbsphinx` extension.

The only requirement of `nbsphinx` is that you add a header to the notebook to act as the rendered title on Sphinx docs. To do this, create a markdown cell at the top of your notebook with a header `#`.

Then, just add your notebook to the `nnsight/docs/source/notebooks/tutorials` directory.

If you're adding a new notebook, navigate to `nnsight/docs/source/tutorials.rst`. At the bottom of the page, add the path to your notebook under the `toctree`.

```
.. toctree::
   :maxdepth: 1

   notebooks/tutorials/walkthrough.ipynb
   ...
   <YOUR NOTEBOOK PATH>

```

# Compiling Sphinx to HTML

Run `make dirhtml` from the `nnsight/docs` directory. The build is located in `nnsight/docs/build/dirhtml`. You can run `python3 -m http.server <PORT>` from that directory, and see the site at `http://localhost:<PORT>`.



---
File: /src/nnsight/intervention/backends/__init__.py
---

from .editing import EditingBackend
from .remote import RemoteBackend
from .noop import NoopBackend


---
File: /src/nnsight/intervention/backends/editing.py
---

from typing import TYPE_CHECKING
from ...tracing.backends import Backend

from ...tracing.graph import Graph
if TYPE_CHECKING:
    from .. import NNsight

class EditingBackend(Backend):
    """Backend to set the default graph to the current InterventionGraph. Assumes the final Node is an InterleavingTracer.
    """
    
    def __init__(self, model: "NNsight") -> None:
        
        self.model = model
    
    def __call__(self, graph: Graph) -> None:
                        
        self.model._default_graph = graph.nodes[-1].args[0]


---
File: /src/nnsight/intervention/backends/noop.py
---

from ...tracing.graph.graph import Graph
from ...tracing.backends import Backend


class NoopBackend(Backend):

    def __call__(self, graph: Graph) -> None:
        graph.nodes.clear()
        graph.stack.clear()


---
File: /src/nnsight/intervention/backends/remote.py
---

from __future__ import annotations

import io
import sys
import time
from datetime import datetime
from typing import Any, Dict, Optional, Tuple

import msgspec
import requests
import socketio
import torch
from tqdm.auto import tqdm

from ... import __IPYTHON__, __version__, CONFIG, remote_logger
from ...schema.request import RequestModel, StreamValueModel
from ...schema.response import ResponseModel
from ...schema.result import RESULT, ResultModel
from ...tracing.backends import Backend
from ...tracing.backends.base import frame_injection
from ...tracing.graph import Graph
from ...util import NNsightError
from ..contexts.local import LocalContext, RemoteContext


class RemoteBackend(Backend):
    """Backend to execute a context object via a remote service.

    Context object must inherit from RemoteMixin and implement its methods.

    Attributes:

        url (str): Remote host url. Defaults to that set in CONFIG.API.HOST.
    """

    def __init__(
        self,
        model_key: str,
        host: str = None,
        blocking: bool = True,
        job_id: str = None,
        ssl: bool = None,
        api_key: str = "",
        callback: str = "",
    ) -> None:

        self.model_key = model_key

        self.job_id = job_id or CONFIG.API.JOB_ID
        self.ssl = CONFIG.API.SSL if ssl is None else ssl
        self.zlib = CONFIG.API.ZLIB
        self.format = CONFIG.API.FORMAT
        self.api_key = api_key or CONFIG.API.APIKEY
        self.blocking = blocking
        self.callback = callback

        self.host = host or CONFIG.API.HOST
        self.address = f"http{'s' if self.ssl else ''}://{self.host}"
        self.ws_address = f"ws{'s' if CONFIG.API.SSL else ''}://{self.host}"

    def request(self, graph: Graph) -> Tuple[bytes, Dict[str, str]]:

        data = RequestModel.serialize(graph, self.format, self.zlib)

        headers = {
            "model_key": self.model_key,
            "format": self.format,
            "zlib": str(self.zlib),
            "ndif-api-key": self.api_key,
            "sent-timestamp": str(time.time()),
            "nnsight-version": __version__,
            "callback": self.callback,
        }

        return data, headers

    def __call__(self, graph: Graph):

        if self.blocking:

            # Do blocking request.
            result = self.blocking_request(graph)

        else:

            # Otherwise we are getting the status / result of the existing job.
            result = self.non_blocking_request(graph)

        if result is not None:
            ResultModel.inject(graph, result)
            
            if CONFIG.APP.FRAME_INJECTION:
                        
                frame_injection()

    def handle_response(
        self, response: ResponseModel, graph: Optional[Graph] = None
    ) -> Optional[RESULT]:
        """Handles incoming response data.

        Logs the response object.
        If the job is completed, retrieve and stream the result from the remote endpoint.
        Use torch.load to decode and load the `ResultModel` into memory.
        Use the backend object's .handle_result method to handle the decoded result.

        Args:
            response (Any): Json data to concert to `ResponseModel`

        Raises:
            Exception: If the job's status is `ResponseModel.JobStatus.ERROR`

        Returns:
            ResponseModel: ResponseModel.
        """
        
        if response.status == ResponseModel.JobStatus.ERROR:
            raise SystemExit(f"{response.description}\nRemote exception.")

        # Log response for user
        response.log(remote_logger)

        # If job is completed:
        if response.status == ResponseModel.JobStatus.COMPLETED:

            # If the response has no result data, it was too big and we need to stream it from the server.
            if response.data is None:

                result = self.get_result(response.id)
            else:

                result = response.data

            return result

        # If were receiving a streamed value:
        elif response.status == ResponseModel.JobStatus.STREAM:

            # Second item is index of LocalContext node.
            # First item is the streamed value from the remote service.

            index, dependencies = response.data

            ResultModel.inject(graph, dependencies)

            node = graph.nodes[index]

            node.execute()

        elif response.status == ResponseModel.JobStatus.NNSIGHT_ERROR:
            if graph.debug:
                error_node = graph.nodes[response.data["node_id"]]
                try:
                    raise NNsightError(
                        response.data["err_message"],
                        error_node.index,
                        response.data["traceback"],
                    )
                except NNsightError as nns_err:
                    if (
                        __IPYTHON__
                    ):  # in IPython the traceback content is rendered by the Error itself
                        # add the error node traceback to the the error's traceback
                        nns_err.traceback_content += "\nDuring handling of the above exception, another exception occurred:\n\n"
                        nns_err.traceback_content += error_node.meta_data["traceback"]
                    else:  # else we print the traceback manually
                        print(f"\n{response.data['traceback']}")
                        print(
                            "During handling of the above exception, another exception occurred:\n"
                        )
                        print(f"{error_node.meta_data['traceback']}")

                    sys.tracebacklimit = 0
                    raise nns_err from None
                finally:
                    if __IPYTHON__:
                        sys.tracebacklimit = None
            else:
                print(f"\n{response.data['traceback']}")
                raise SystemExit("Remote exception.")

    def submit_request(
        self, data: bytes, headers: Dict[str, Any]
    ) -> Optional[ResponseModel]:
        """Sends request to the remote endpoint and handles the response object.

        Raises:
            Exception: If there was a status code other than 200 for the response.

        Returns:
            (ResponseModel): Response.
        """

        from ...schema.response import ResponseModel

        headers["Content-Type"] = "application/octet-stream"

        response = requests.post(
            f"{self.address}/request",
            data=data,
            headers=headers,
        )

        if response.status_code == 200:

            response = ResponseModel(**response.json())

            self.handle_response(response)

            return response

        else:
            try:
                msg = response.json()["detail"]
            except:
                msg = response.reason
            raise ConnectionError(msg)

    def get_response(self) -> Optional[RESULT]:
        """Retrieves and handles the response object from the remote endpoint.

        Raises:
            Exception: If there was a status code other than 200 for the response.

        Returns:
            (ResponseModel): Response.
        """

        from ...schema.response import ResponseModel

        response = requests.get(
            f"{self.address}/response/{self.job_id}",
            headers={"ndif-api-key": self.api_key},
        )

        if response.status_code == 200:

            response = ResponseModel(**response.json())

            return self.handle_response(response)

        else:

            raise Exception(response.reason)

    def get_result(self, id: str) -> RESULT:

        result_bytes = io.BytesIO()
        result_bytes.seek(0)

        # Get result from result url using job id.
        with requests.get(
            url=f"{self.address}/result/{id}",
            stream=True,
        ) as stream:
            # Total size of incoming data.
            total_size = float(stream.headers["Content-length"])

            with tqdm(
                total=total_size,
                unit="B",
                unit_scale=True,
                desc="Downloading result",
            ) as progress_bar:
                # chunk_size=None so server determines chunk size.
                for data in stream.iter_content(chunk_size=None):
                    progress_bar.update(len(data))
                    result_bytes.write(data)

        # Move cursor to beginning of bytes.
        result_bytes.seek(0)

        # Decode bytes with pickle and then into pydantic object.
        result = torch.load(result_bytes, map_location="cpu", weights_only=False)

        result = ResultModel(**result).result

        # Close bytes
        result_bytes.close()

        return result

    def blocking_request(self, graph: Graph) -> Optional[RESULT]:
        """Send intervention request to the remote service while waiting for updates via websocket.

        Args:
            request (RequestModel):Request.
        """

        # We need to do some processing / optimizations on both the graph were sending remotely
        # and our local intervention graph. In order handle the more complex Protocols for streaming.

        # Create a socketio connection to the server.
        with socketio.SimpleClient(reconnection_attempts=10) as sio:
            # Connect
            sio.connect(
                self.ws_address,
                socketio_path="/ws/socket.io",
                transports=["websocket"],
                wait_timeout=10,
            )

            remote_graph = preprocess(graph)

            data, headers = self.request(remote_graph)

            headers["session_id"] = sio.sid

            # Submit request via
            response = self.submit_request(data, headers)

            LocalContext.set(
                lambda *args: self.stream_send(*args, job_id=response.id, sio=sio)
            )

            try:
                # Loop until
                while True:

                    # Get pickled bytes value from the websocket.
                    response = sio.receive()[1]
                    # Convert to pydantic object.
                    response = ResponseModel.unpickle(response)
                    # Handle the response.
                    result = self.handle_response(response, graph=graph)
                    # Break when completed.
                    if result is not None:
                        return result

            except Exception as e:

                raise e

            finally:
                LocalContext.set(None)

    def stream_send(
        self, values: Dict[int, Any], job_id: str, sio: socketio.SimpleClient
    ):
        """Upload some value to the remote service for some job id.

        Args:
            value (Any): Value to upload
            job_id (str): Job id.
            sio (socketio.SimpleClient): Connected websocket client.
        """

        sio.emit(
            "stream_upload",
            data=(StreamValueModel.serialize(values, self.format, self.zlib), job_id),
        )

    def non_blocking_request(self, graph: Graph):
        """Send intervention request to the remote service if request provided. Otherwise get job status.

        Sets CONFIG.API.JOB_ID on initial request as to later get the status of said job.

        When job is completed, clear CONFIG.API.JOB_ID to request a new job.

        Args:
            request (RequestModel): Request if submitting a new request. Defaults to None
        """

        if self.job_id is None:

            data, headers = self.request(graph)
            # Submit request via
            response = self.submit_request(data, headers)

            self.job_id = response.id
            
            CONFIG.API.JOB_ID = response.id

            CONFIG.save()

        else:

            try:

                result = self.get_response()

                if result is not None:

                    CONFIG.API.JOB_ID = None

                    CONFIG.save()

                    return result

            except Exception as e:

                CONFIG.API.JOB_ID = None

                CONFIG.save()

                raise e


def preprocess(graph: Graph):

    new_graph = graph.copy()

    for node in new_graph.nodes:

        if node.target is LocalContext:

            graph.nodes[node.index].kwargs["uploads"] = RemoteContext.from_local(node)

    return new_graph



---
File: /src/nnsight/intervention/contexts/__init__.py
---

from .invoker import Invoker
from .local import LocalContext, RemoteContext
from .tracer import InterventionTracer
from .session import Session
from .interleaving import InterleavingTracer
from .editing import EditingTracer
from .globals import *


---
File: /src/nnsight/intervention/contexts/editing.py
---

from typing import TYPE_CHECKING
from ..backends import EditingBackend
from . import InterleavingTracer
if TYPE_CHECKING:
    from .. import NNsight

class EditingTracer(InterleavingTracer):
    """The `EditingTracer` exists because we want to return the edited model from __enter__ not the Tracer itself
    While were here we might as well force the backend to be `EditingBackend`

    """

    def __init__(self, model:"NNsight", *args, inplace: bool = False, **kwargs) -> None:
        
        
        # If its not inplace we create a shallow copy of the model
        # With the same references to the underlying model.
        if not inplace:

            model = model._shallow_copy()
            
        super().__init__(model, *args, backend=EditingBackend(model), **kwargs)

    def __enter__(self):

        super().__enter__()

        return self._model



---
File: /src/nnsight/intervention/contexts/globals.py
---

"""
Global patching allows us to add un-traceable operations to nnsight by replacing them with ones that use the GLOBAL_TRACING_CONTEXT to add the operation to the current graph.
"""

from __future__ import annotations

from inspect import getmembers, isclass

import torch
from torch.utils import data

from ...tracing.contexts.globals import (
    GlobalTracingContext,
    global_patch,
    global_patch_method,
)
from ...tracing.graph.proxy import proxy_patch
from . import InterventionTracer

# Torch classes
global_patch(torch.nn.Parameter)
global_patch(torch.nn.Linear)

global_patch(data.DataLoader)
# Tensor creation operations
global_patch(torch.arange)
global_patch(torch.empty)
global_patch(torch.eye)
global_patch(torch.full)
global_patch(torch.linspace)
global_patch(torch.logspace)
global_patch(torch.ones)
global_patch(torch.rand)
global_patch(torch.randint)
global_patch(torch.randn)
global_patch(torch.randperm)
global_patch(torch.zeros)
global_patch(torch.cat)

# Module methods

global_patch_method(torch.nn.Module, torch.nn.Module.zero_grad)

# All Optimizers
for key, value in getmembers(torch.optim, isclass):

    if issubclass(value, torch.optim.Optimizer):

        global_patch(value)

import math
from inspect import getmembers, isbuiltin, isfunction

import einops

# Einops
for key, value in getmembers(einops.einops, isfunction):
    setattr(einops.einops, key, proxy_patch(value))
# math
for key, value in getmembers(math, isbuiltin):
    setattr(math, key, proxy_patch(value))


# Give it InterventionTracer methods
class GlobalInterventionTracingContext(GlobalTracingContext, InterventionTracer):
    GLOBAL_TRACING_CONTEXT: GlobalInterventionTracingContext


GlobalTracingContext.GLOBAL_TRACING_CONTEXT = GlobalInterventionTracingContext()



---
File: /src/nnsight/intervention/contexts/interleaving.py
---

import weakref
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    List,
    Optional,
    Tuple,
)


from ...tracing.backends import Backend
from ...tracing.graph import GraphType
from ..graph import (
    InterventionGraph,
    InterventionNode,
    InterventionNodeType,
    ValidatingInterventionNode,
)
from ..interleaver import Interleaver

from . import Invoker
from . import InterventionTracer
if TYPE_CHECKING:
    from .. import NNsight


class InterleavingTracer(InterventionTracer):
    """This is the Tracer type that actually interleaves an `InterventionGraph` with a PyTorch model upon execute.

    Attributes:
        _model (NNsight): NNsight model.
        invoker (Invoker): Current open invoker so we can prevent opening two at the same time.
        args (Tuple[...]): Positional arguments. First is which method to interleave with and subsequent args are invoker inputs.
        kwargs (Dict[str,Any]): Keyword arguments passed to the method to interleave. These are "global" keyword arguments for our chosen methof
            while kwargs for a given invoker are used for preprocessing the invoker input.
    """

    def __init__(
        self,
        model: "NNsight",
        method: Optional[str] = None,
        backend: Optional[Backend] = None,
        parent: Optional[GraphType] = None,
        validate: bool = False,
        debug: Optional[bool] = None,
        **kwargs,
    ) -> None:

        super().__init__(
            graph_class=InterventionGraph,
            model=model,
            node_class=ValidatingInterventionNode if validate else InterventionNode,
            proxy_class=model.proxy_class,
            backend=backend,
            parent=parent,
            graph=model._default_graph,
            debug=debug,
        )

        self._model = model

        # Tell all Envoy's about the current Tracer so they can use it to add InterventionProtocol Nodes.
        self._model._envoy._set_tracer(weakref.proxy(self))

        self.invoker: Optional[Invoker] = None

        self.args = [method]
        self.kwargs = kwargs
        

    def invoke(self, *inputs: Any, **kwargs) -> Invoker:
        """Create an Invoker context for a given input.

        Raises:
            Exception: If an Invoker context is already open

        Returns:
            Invoker: Invoker.
        """

        if self.invoker is not None:

            raise Exception("Can't create an invoker context with one already open!")

        return Invoker(self, *inputs, **kwargs)

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
    
        if self.invoker is not None:

            self.invoker.__exit__(None, None, None)

        self._model._envoy._reset()

        super().__exit__(exc_type, exc_val, exc_tb)

    @classmethod
    def _batch(
        cls, model: "NNsight", invoker_inputs: Tuple[Tuple[Tuple[Any], Dict[str, Any]]]
    ) -> Tuple[Tuple[Tuple[Any], Dict[str, Any]], List[Tuple[int, int]]]:
        """Batches together each set of inputs from each Invoker by iteratively calling the models ._prepare_input and ._batch methods.

        Args:
            model (NNsight): Model which defines its own logic for preparing and batching input
            invoker_inputs (Tuple[Tuple[Tuple[Any], Dict[str, Any]]]): Tuple of invoker inputs.

        Returns:
            Tuple[Tuple[Tuple[Any], Dict[str, Any]], List[Tuple[int, int]]]: One single batched input.
            List[Tuple[int, int]]: Batch groups
        """

        batch_groups = []
        batch_start = 0
        batched_input = None

        for args, kwargs in invoker_inputs:

            (args, kwargs), batch_size = model._prepare_input(*args, **kwargs)

            batch_groups.append((batch_start, batch_size))

            batched_input = model._batch(batched_input, *args, **kwargs)

            batch_start += batch_size

        if batched_input is None:

            return (tuple(), dict()), ((0, -1),)

        return batched_input, batch_groups

    @property
    def _invoker_group(self):

        return len(self.args) - 2
    
    @classmethod
    def execute(cls, node: InterventionNodeType):

        graph, method, *invoker_inputs = node.args
        
        
        
        graph: InterventionGraph
        model = graph.model

        # There may be Nodes in the inputs. Convert them to their value
        invoker_inputs, kwargs = node.prepare_inputs((invoker_inputs, node.kwargs))

        # Batch each invoker input into one input
        (invoker_args, invoker_kwargs), batch_groups = cls._batch(model, invoker_inputs)

        # Compile Intervention Graph
        graph.compile()

        graph.reset()
        
        interleaver = Interleaver(graph, batch_groups=batch_groups)
        
        graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)
        
        graph.cleanup()



---
File: /src/nnsight/intervention/contexts/invoker.py
---

from __future__ import annotations

import copy
from contextlib import AbstractContextManager
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple

import torch
from torch._subclasses.fake_tensor import FakeCopyMode, FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv

from ... import util
from ...tracing.contexts.globals import GlobalTracingContext
from ..graph import InterventionNode, InterventionProxy, InterventionProxyType

if TYPE_CHECKING:

    from . import InterleavingTracer


class Invoker(AbstractContextManager):
    """An Invoker is meant to work in tandem with a :class:`nnsight.intervention.contexts.InterleavingTracer` to enter input and manage intervention tracing.

    Attributes:
        tracer (nnsight.contexts.Tracer.Tracer): Tracer object to enter input and manage context.
        inputs (tuple[Any]): Initially entered inputs, then post-processed inputs from model's ._prepare_inputs(...) method.
        scan (bool): If to execute the model using `FakeTensor` in order to update the potential sizes/dtypes of all modules' Envoys' inputs/outputs as well as validate things work correctly.
            Scanning is not free computation wise so you may want to turn this to false when running in a loop.
            When making interventions, you made get shape errors if scan is false as it validates operations based on shapes so
            for looped calls where shapes are consistent, you may want to have scan=True for the first loop. Defaults to False.
        kwargs (Dict[str,Any]): Keyword arguments passed to the model's _prepare_inputs method.
        scanning (bool): If currently scanning.
    """

    def __init__(
        self,
        tracer: "InterleavingTracer",
        *args,
        scan: bool = False,
        **kwargs,
    ) -> None:

        self.tracer = tracer
        self.inputs = (args, kwargs)

        self.scan = scan

        self.scanning = False

        self.tracer.invoker = self

        self.batch_size: Optional[int] = None

    def __enter__(self) -> Invoker:
        """Enters a new invocation context with a given input.

        Calls the model's _prepare_inputs method using the input and other arguments.
        If scan is True, uses the model's ._execute method to update and validate module Envoy's inputs/outputs using a fake mode.
        Gets a batched version of the post processed input using the model's ._batched_inputs method to update the Tracer's
            current batch_size and batched_input.

        Returns:
            Invoker: Invoker.
        """

        has_proxies_in_inputs = False
        
        def check_for_proxies(proxy: InterventionProxyType):

            nonlocal has_proxies_in_inputs

            has_proxies_in_inputs = True

            return proxy

        # We need to check if there were any Proxies in the actual Invoker input. This might be True in a Session where values from one trace are used as an input to another.
        util.apply(self.inputs, check_for_proxies, InterventionProxy)

        # We dont want to create new proxies during scanning/prepare_inputs so we exit the global tracing context.
        with GlobalTracingContext.exit_global_tracing_context():

            # If we dont have proxies we can immediately prepare the input so the user can see it and the batch_size.
            if not has_proxies_in_inputs:

                self.inputs, self.batch_size = self.tracer._model._prepare_input(
                    *self.inputs[0], **self.inputs[1]
                )

            if self.scan:

                input = self.inputs

                if has_proxies_in_inputs:

                    input = util.apply(input, lambda x: x.fake_value, InterventionNode)

                    input, _ = self.tracer._model._prepare_input(*input[0], **input[1])

                # Clear all fake inputs and outputs because were going to re-populate them.
                self.tracer._model._envoy._clear()

                self.scanning = True

                with util.Patcher() as patcher:

                    # Some logic (like gpt-j rotary embeddings) gets "poisoned" by FakeTensors.
                    # This does not happen when `torch._jit_internal.is_scripting() returns True.`
                    patcher.add(
                        util.Patch(torch._jit_internal, lambda: True, "is_scripting")
                    )

                    with FakeTensorMode(
                        allow_non_fake_inputs=True,
                        shape_env=ShapeEnv(assume_static_by_default=True),
                    ) as fake_mode:
                        with FakeCopyMode(fake_mode):
                            fn = (
                                self.tracer._model._execute
                                if self.tracer.args[0] is None
                                else getattr(self.tracer._model, self.tracer.args[0])
                            )
                            fn(
                                *copy.deepcopy(input[0]),
                                **copy.deepcopy(input[1]),
                                **copy.deepcopy(self.tracer.kwargs),
                            )

                self.scanning = False

            else:
                self.tracer._model._envoy._reset()

            self.tracer.args.append(self.inputs)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:

        self.tracer.invoker = None

        if isinstance(exc_val, BaseException):
            raise exc_val



---
File: /src/nnsight/intervention/contexts/local.py
---

from typing import Callable, List, Optional

from nnsight.tracing.graph.node import Node

from ...tracing.contexts import Tracer
from ...tracing.graph import GraphType, NodeType
from ..protocols import EntryPoint, NoopProtocol


class LocalContext(Tracer):

    send: Optional[Callable] = None

    @classmethod
    def set(cls, fn: Callable):

        cls.send = fn

    @classmethod
    def execute(cls, node: NodeType):

        super().execute(node)

        uploads = node.kwargs.get("uploads", [])

        if uploads:

            values = {index: node.graph.nodes[index].value for index in uploads}

            cls.send(values)

            for index in uploads:

                node = node.graph.nodes[index]

                node.remaining_listeners -= 1

                if node.redundant:
                    node.destroy()


class RemoteContext(Tracer):

    send: Optional[Callable] = None
    receive: Optional[Callable] = None

    @classmethod
    def set(cls, send: Callable, receive: Callable):

        cls.send = send
        cls.receive = receive

    @classmethod
    def from_local(cls, local_node: NodeType):

        local_node.target = RemoteContext

        graph: GraphType = local_node.args[0]

        start = graph[0].index
        end = graph[-1].index

        uploads = []

        # TODO check for swap and error

        for node in graph.nodes[start : end + 1]:

            for dependency in node.dependencies:

                if (
                    isinstance(dependency.target, type)
                    and issubclass(dependency.target, EntryPoint)
                ) or dependency.index < start:

                    local_node.args.append(dependency)

            if isinstance(node.target, type) and issubclass(node.target, EntryPoint):
                continue

            node.args.clear()
            node.kwargs.clear()

            node.target = NoopProtocol

            for listener in node.listeners:

                if listener.index > end:

                    uploads.append(node.index)

        if len(uploads) > 0:
            local_node.kwargs["upload"] = True

        return uploads

    @classmethod
    def execute(cls, node: NodeType):

        graph, *dependencies = node.args

        dependencies = {
            dependency.index: dependency.value for dependency in dependencies
        }

        cls.send((node.index, dependencies))

        super().execute(node)

        if node.kwargs.get("upload", False):

            values = cls.receive()

            for index, value in values.items():
                graph.nodes[index]._value = value



---
File: /src/nnsight/intervention/contexts/session.py
---

from typing import TYPE_CHECKING, Optional

from typing_extensions import Self

from ..graph import (InterventionNode, InterventionProxy,
                     ValidatingInterventionNode)
from . import InterventionTracer

if TYPE_CHECKING:
    from .. import NNsight


class Session(InterventionTracer[InterventionNode, InterventionProxy]):
    """A Session simply allows grouping multiple Tracers in one computation graph.
    """

    def __init__(self, model: "NNsight", validate: bool = False, debug:Optional[bool] = None, **kwargs) -> None:

        super().__init__(
            node_class=ValidatingInterventionNode if validate else InterventionNode,
            proxy_class=model.proxy_class,
            debug=debug,
            **kwargs,
        )

        self.model = model
        
    def __enter__(self) -> Self:
        
        self.model._session = self
        
        return super().__enter__()


    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        self.model._session = None
        return super().__exit__(exc_type, exc_val, exc_tb)



---
File: /src/nnsight/intervention/contexts/tracer.py
---

import inspect
from functools import wraps
from typing import Any, Callable, Dict, Optional, TypeVar, Union

from ...tracing.contexts import Tracer
from ..graph import (InterventionNodeType, InterventionProxy,
                     InterventionProxyType)
from . import LocalContext
from ... import CONFIG

class InterventionTracer(Tracer[InterventionNodeType, InterventionProxyType]):
    """Extension of base Tracer to add additional intervention functionality and type hinting for intervention proxies.
    """

    R = TypeVar("R")

    def __init__(self, *args, **kwargs) -> None:
        if kwargs['debug'] == None:
            kwargs['debug'] = CONFIG.APP.DEBUG

        super().__init__(*args, **kwargs)

    def apply(
        self, target: Callable[..., R], *args, **kwargs
    ) -> Union[InterventionProxy, R]:
        return super().apply(target, *args, **kwargs)

    def local(self, fn: Optional[Callable] = None) -> Union[LocalContext, Callable]:

        if fn is None:

            return LocalContext(parent=self.graph)

        elif inspect.isroutine(fn):

            @wraps(fn)
            def inner(*args, **kwargs):

                with LocalContext(parent=self.graph) as context:

                    return context.apply(fn, *args, **kwargs)

        else:

            # TODO: error
            pass

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "purple", "shape": "polygon", "sides": 6}
        default_style["arg_kname"][1] = "method"

        return default_style



---
File: /src/nnsight/intervention/graph/__init__.py
---

from .proxy import InterventionProxy, InterventionProxyType
from .node import InterventionNode, ValidatingInterventionNode, InterventionNodeType
from .graph import InterventionGraph



---
File: /src/nnsight/intervention/graph/graph.py
---

import copy
import sys
from collections import defaultdict
from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, Union

from typing_extensions import Self

from ...tracing.contexts import Context
from ...tracing.graph import SubGraph
from ...util import NNsightError
from ..protocols import ApplyModuleProtocol, GradProtocol, InterventionProtocol
from . import InterventionNode, InterventionNodeType, InterventionProxyType

if TYPE_CHECKING:
    from .. import NNsight
    from ...tracing.graph.graph import GraphType, NodeType


class InterventionGraph(SubGraph[InterventionNode, InterventionProxyType]):
    """The `InterventionGraph` is the special `SubGraph` type that handles the complex intervention operations a user wants to make during interleaving.
    We need to `.compile()` it before execution to determine how to execute interventions appropriately.

    Attributes:
        model (NNsight): NNsight model.
        interventions
        grad_subgraph
        compiled
        call_counter
        deferred
    """

    def __init__(
        self,
        *args,
        model: Optional["NNsight"] = None,
        **kwargs,
    ) -> None:

        super().__init__(*args, **kwargs)

        self.model = model

        self.interventions: Dict[str, List[int]] = defaultdict(list)
        self.grad_subgraph: Set[int] = set()

        self.compiled = False
        self.call_counter: Dict[int, int] = defaultdict(int)
        self.deferred: Dict[int, List[int]] = defaultdict(list)

    def __getstate__(self) -> Dict:

        return {
            "subset": self.subset,
            "nodes": self.nodes,
            "interventions": self.interventions,
            "compiled": self.compiled,
            "call_counter": self.call_counter,
            "deferred": self.deferred,
            "grad_subgraph": self.grad_subgraph,
            "defer_stack": self.defer_stack,
        }

    def __setstate__(self, state: Dict) -> None:

        self.__dict__.update(state)

    def reset(self) -> None:
        self.call_counter = defaultdict(int)
        return super().reset()

    def set(self, model: "NNsight"):

        self.model = model

    def context_dependency(
        self,
        context_node: InterventionNode,
        intervention_subgraphs: List[SubGraph],
    ) -> None:

        context_graph: SubGraph = context_node.args[0]

        start = context_graph.subset[0]
        end = context_graph.subset[-1]

        for intervention_subgraph in intervention_subgraphs:

            # continue if the subgraph does not overlap with the context's graph
            if intervention_subgraph.subset[-1] < start or end < intervention_subgraph.subset[0]:
                continue

            for intervention_index in intervention_subgraph.subset:

                # if there's an overlapping node, make the context depend on the intervention node in the subgraph
                if start <= intervention_index and intervention_index <= end:

                    # the first node in the subgraph is an InterventionProtocol node
                    intervention_node = intervention_subgraph[0]

                    context_node._dependencies.add(intervention_node.index)
                    intervention_node._listeners.add(context_node.index)
                    # TODO: maybe we don't need this
                    intervention_subgraph.subset.append(context_node.index)

                    break

    def compile(self) -> Optional[Dict[str, List[InterventionNode]]]:

        if self.compiled:
            return self.interventions

        if len(self) == 0:
            self.compiled = True
            return
        
        intervention_subgraphs: List[SubGraph] = []

        start = self[0].index
        # is the first node corresponding to an executable graph?
        # occurs when a Conditional or Iterator context is explicitly entered by a user
        if isinstance(self[0].target, type) and issubclass(
            self[0].target, Context
        ):
            graph = self[0].args[0]

            # handle emtpy if statments or for loops
            if len(graph) > 0:
                start = graph[0].index

        end = self[-1].index + 1

        context_start: int = None
        defer_start: int = None
        context_node: InterventionNode = None

        # looping over all the nodes created within this graph's context
        for index in range(start, end):

            node: InterventionNodeType = self.nodes[index]

            # is this node part of an inner context's subgraph?
            if context_node is None and node.graph is not self:

                context_node = self.nodes[node.graph[-1].index + 1]

                context_start = self.subset.index(context_node.index)

                defer_start = node.index

                self.context_dependency(context_node, intervention_subgraphs)

            if node.target is InterventionProtocol:
                
                # build intervention subgraph
                subgraph = SubGraph(self, subset=sorted(list(node.subgraph())))

                module_path, *_ = node.args

                self.interventions[module_path].append(node.index)

                intervention_subgraphs.append(subgraph)

                # if the InterventionProtocol is defined within a sub-context
                if context_node is not None:
  
                    # make the current context node dependent on this intervention node
                    context_node._dependencies.add(node.index)
                    node._listeners.add(context_node.index)
                    # TODO: maybe we don't need this
                    self.subset.append(node.index)

                    graph: SubGraph = node.graph

                    graph.subset.remove(node.index)

                    node.kwargs["start"] = context_start
                    node.kwargs["defer_start"] = defer_start

                    node.graph = self

                else:

                    node.kwargs["start"] = self.subset.index(subgraph.subset[0])
                    node.kwargs["defer_start"] = node.kwargs["start"]

            elif node.target is GradProtocol:

                subgraph = SubGraph(self, subset=sorted(list(node.subgraph())))

                intervention_subgraphs.append(subgraph)

                self.grad_subgraph.update(subgraph.subset[1:])

                if context_node is not None:

                    context_node._dependencies.add(node.index)
                    node._listeners.add(context_node.index)
                    subgraph.subset.append(context_node.index)

                    graph: SubGraph = node.graph

                    graph.subset.remove(node.index)

                    node.kwargs["start"] = context_start

                    node.graph = self

                else:

                    node.kwargs["start"] = self.subset.index(subgraph.subset[1])

            elif node.target is ApplyModuleProtocol:

                node.graph = self

            elif context_node is not None and context_node is node:
                context_node = None

        self.compiled = True

    def execute(
        self,
        start: int = 0,
        grad: bool = False,
        defer: bool = False,
        defer_start: int = 0,
    ) -> None:

        err: Tuple[int, NNsightError] = None

        if defer_start in self.deferred:

            for index in self.deferred[defer_start]:

                self.nodes[index].reset()

            del self.deferred[defer_start]

        if defer:

            self.defer_stack.append(defer_start)

        for node in self[start:]:

            if node.executed:
                continue
            elif (
                node.index != self[start].index and node.target is InterventionProtocol
            ):
                break
            elif node.fulfilled:
                try:
                    node.execute()
                    if defer and node.target is not InterventionProtocol:
                        self.deferred[defer_start].append(node.index)
                except NNsightError as e:
                    err = (node.index, e)
                    break
            elif not grad and node.index in self.grad_subgraph:
                continue
            else:
                break

        if defer:
            self.defer_stack.pop()

        if err is not None:
            defer_stack = self.defer_stack
            self.defer_stack = []
            self.clean(err[0])
            self.defer_stack = defer_stack
            raise err[1]

    def count(self, index: int, iteration: Union[int, List[int], slice]) -> bool:
        """Increments the count of times a given Intervention Node has tried to be executed and returns if the Node is ready and if it needs to be deferred.

        Args:
            index (int): Index of intervention node to return count for.
            iteration (Union[int, List[int], slice]): What iteration(s) this Node should be executed for.

        Returns:
            bool: If this Node should be executed on this iteration.
            bool: If this Node and recursive listeners should have updating their remaining listeners (and therefore their destruction) deferred.
        """

        ready = False
        defer = False

        count = self.call_counter[index]

        if isinstance(iteration, int):
            ready = count == iteration
        elif isinstance(iteration, list):
            iteration.sort()

            ready = count in iteration
            defer = count != iteration[-1]

        elif isinstance(iteration, slice):

            start = iteration.start or 0
            stop = iteration.stop

            ready = count >= start and (stop is None or count < stop)

            defer = stop is None or count < stop - 1

        # if defer:
        #     self.deferred.add(index)
        # else:
        #     self.deferred.discard(index)

        self.call_counter[index] += 1

        return ready, defer

    def clean(self, start: Optional[int] = None):

        if start is None:
            start = self[0].index

        end = self[-1].index + 1

        # Loop over ALL nodes within the span of this graph.
        for index in range(start, end):

            node = self.nodes[index]

            if node.executed:
                break

            node.update_dependencies()

    def cleanup(self) -> None:
        """Because some modules may be executed more than once, and to accommodate memory management just like a loop,
        intervention graph sections defer updating the remaining listeners of Nodes if this is not the last time this section will be executed.
        If we never knew it was the last time, there may still be deferred sections after execution.
        These will be leftover in graph.deferred, and therefore we need to update their dependencies.
        """

        # For every intervention graph section (indicated by where it started)
        for start in self.deferred:

            # Loop through all nodes that got their dependencies deferred.
            for index in range(start, self.deferred[start][-1] + 1):

                node = self.nodes[index]

                # Update each of its dependencies
                for dependency in node.dependencies:
                    # Only if it was before start
                    # (not within this section, but before)
                    if dependency.index < start:
                        dependency.remaining_listeners -= 1

                        if dependency.redundant:
                            dependency.destroy()

    def copy(
        self,
        new_graph: Self = None,
        parent: Optional["GraphType"] = None,
        memo: Optional[Dict[int, "NodeType"]] = None,
    ) -> Self:

        if memo is None:
            memo = {}

        new_graph = super().copy(new_graph, parent=parent, memo=memo)

        new_graph.compiled = self.compiled

        for key, value in self.call_counter.items():
            new_graph.call_counter[memo[key]] = value

        if new_graph.compiled:

            for module_path, list_of_nodes in self.interventions.items():

                new_graph.interventions[module_path] = [
                    new_graph.nodes[memo[index]].index for index in list_of_nodes
                ]

            for key, values in self.deferred.items():

                new_graph.deferred[memo[key]] = [memo[index] for index in values]

            new_graph.grad_subgraph = [memo[index] for index in self.grad_subgraph]

        return new_graph

    # @classmethod
    # def shift(cls, mgraph: MultiGraph) -> MultiGraph:

    #     InterventionProtocol.compile(mgraph)

    #     intervention_subgraphs = InterventionProtocol.get_interventions(mgraph).values()

    #     graph_id_to_invoker_groups = defaultdict(set)
    #     graph_id_to_intervention_node = defaultdict(list)

    #     for subgraph in intervention_subgraphs:
    #         for (start, end) in subgraph:

    #             node = mgraph[start]

    #             invoker_group = node.args[1]

    #             offset = 0

    #             for graph in mgraph.id_to_graphs.values():
    #                 offset  += len(graph)
    #                 if start < offset:
    #                     graph_id_to_invoker_groups[graph.id].add(invoker_group)
    #                     graph_id_to_intervention_node[graph.id].append(node)
    #                     break

    #     global_offset = 0

    #     for graph_id, invoker_groups in graph_id_to_invoker_groups.items():

    #         min_group = min(invoker_groups)
    #         max_group = max(invoker_groups)

    #         offset = global_offset - min_group

    #         for node in graph_id_to_intervention_node[graph_id]:

    #             node.args[1] += offset

    #         global_offset += max_group + 1

    #     return mgraph



---
File: /src/nnsight/intervention/graph/node.py
---

from __future__ import annotations

import inspect
from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar, Union

import torch
from torch._subclasses.fake_tensor import FakeCopyMode, FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv

from ... import util
from ...tracing.contexts import GlobalTracingContext
from ...tracing.graph import Node, Proxy
from ...tracing.protocols import Protocol
from ..protocols import EntryPoint

if TYPE_CHECKING:
    from . import InterventionGraph


class InterventionNode(Node):
    """This is the intervention extension of the base Node type.

    It has a fake_value to see information about this Node's future value before execution.
    It adds additional functionality to Node.prepare_inputs to handle Tensors.
    """

    def __init__(
        self, *args, fake_value: Optional[Any] = inspect._empty, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)

        self.fake_value = fake_value

    @classmethod
    def prepare_inputs(
        cls,
        inputs: Any,
        device: Optional[torch.device] = None,
        fake: bool = False,
    ) -> Any:
        """Override prepare_inputs to make sure

        Args:
            inputs (Any): _description_
            device (Optional[torch.device], optional): _description_. Defaults to None.
            fake (bool, optional): _description_. Defaults to False.

        Returns:
            Any: _description_
        """

        inputs = util.apply(inputs, lambda x: x, inspect._empty)

        def inner(value: Union[InterventionNode, torch.Tensor]):

            nonlocal device

            if isinstance(value, Proxy):
                value = value.node

            if isinstance(value, InterventionNode):
                if fake:
                    value = value.fake_value
                else:
                    value = value.value

            if device is None and isinstance(value, torch.Tensor):
                device = value.device

            return value

        inputs = util.apply(
            inputs, inner, (InterventionNode, Proxy, torch.Tensor), inplace=not fake
        )

        if device is not None:

            def _to(value: torch.Tensor):
                return value.to(device)

            inputs = util.apply(inputs, _to, torch.Tensor, inplace=not fake)

        return inputs

    def update_dependencies(self):
        for dependency in self.dependencies:
            if len(self.graph.defer_stack) > 0 and (
                dependency.index < self.graph.defer_stack[-1]
                or (
                    EntryPoint.is_entrypoint(dependency.target)
                    and dependency.graph is not self.graph
                )
            ):
                continue

            dependency.remaining_listeners -= 1

            if dependency.redundant:
                dependency.destroy()


InterventionNodeType = TypeVar("InterventionNodeType", bound=InterventionNode)


class ValidatingInterventionNode(InterventionNode):
    """The ValidatingInterventionNode executes its target using the fake_values of all of its dependencies to calculate a new fake_value for this node.
    Does not do this if the Node is detached from any graph, already has a fake_value (specified by whoever created the Node) or is a Protocol.
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        if (
            self.attached
            and self.fake_value is inspect._empty
            and not Protocol.is_protocol(self.target)
        ):
            self.fake_value = validate(self.target, *self.args, **self.kwargs)


@staticmethod
def backwards_check(target: Callable, *args) -> bool:

    if target is Proxy.call:

        node: Node = args[0]

        if not isinstance(node, Node):
            return False

        if node.target is util.fetch_attr and node.args[1] == "backward":
            return True

    return False


@staticmethod
def validate(target: Callable, *args, **kwargs):

    # Enter FakeMode.
    with FakeTensorMode(
        allow_non_fake_inputs=True,
        shape_env=ShapeEnv(assume_static_by_default=True),
    ) as fake_mode:
        with FakeCopyMode(fake_mode):

            with GlobalTracingContext.exit_global_tracing_context():

                if backwards_check(target, *args):
                    return None

                args, kwargs = InterventionNode.prepare_inputs(
                    (args, kwargs), fake=True
                )

                return target(
                    *args,
                    **kwargs,
                )



---
File: /src/nnsight/intervention/graph/proxy.py
---

from __future__ import annotations

import inspect
from typing import TYPE_CHECKING, Any, Collection, TypeVar, Union

import torch
from typing_extensions import Self

from ... import util
from ...tracing.graph import Proxy
from .. import protocols

if TYPE_CHECKING:
    from . import InterventionNode


class InterventionProxy(Proxy):
        
    
    def __init__(self, node: "InterventionNode") -> None:
        super().__init__(node)

        self.__dict__["_grad"] = None

        self._grad: Self
        self.node: "InterventionNode"
    
    @property
    def grad(self) -> Self:
        """
        Calling denotes the user wishes to get the grad of proxy tensor and therefore we create a Proxy of that request.
        Only generates a proxy the first time it is references otherwise return the already set one.

        Returns:
            Proxy: Grad proxy.
        """

        self.__dict__["_grad"] = protocols.GradProtocol.add(self.node.graph, self.node, fake_value=self.node.fake_value)

        return self._grad

    @grad.setter
    def grad(self, value: Union[InterventionProxy, Any]) -> None:
        """
        Calling denotes the user wishes to set the grad of this proxy tensor and therefore we create a Proxy of that request via a SwapProtocol.

        Args:
            value (Union[InterventionProxy, Any]): Value to set output to.
        """
        protocols.SwapProtocol.add(self.node.graph, self._grad, value)
        
    def __setattr__(
        self, key: Union[InterventionProxy, Any], value: Union[Self, Any]
    ) -> None:

        # We catch setting .grad as that is a special Protocol vs. setting attributes generally.
        if key == "grad":
            return getattr(self.__class__, key).fset(self, value)

        return super().__setattr__(key, value)

    @property
    def shape(self) -> Collection[torch.Size]:
        """Property to retrieve the shape of the traced proxy value or real value.

        Returns:
            Union[torch.Size,Collection[torch.Size]]: Proxy value shape or collection of shapes.
        """

        if not self.node.attached:

            return util.apply(self.value, lambda x: x.shape, torch.Tensor)

        # If we haven't scanned in a proxy_value, just return a proxy to get the attribute.
        if self.node.fake_value is inspect._empty:

            return super().__getattr__("shape")

        return util.apply(self.node.fake_value, lambda x: x.shape, torch.Tensor)

    @property
    def device(self) -> Collection[torch.device]:
        """Property to retrieve the device of the traced proxy value or real value.

        Returns:
            Union[torch.Size,Collection[torch.device]]: Proxy value device or collection of devices.
        """

        if not self.node.attached:

            return util.apply(self.value, lambda x: x.device, torch.Tensor)

        # If we haven't scanned in a proxy_value, just return a proxy to get the attribute.
        if self.node.fake_value is inspect._empty:

            return super().__getattr__("device")

        return util.apply(self.node.fake_value, lambda x: x.device, torch.Tensor)

    @property
    def dtype(self) -> Collection[torch.device]:
        """Property to retrieve the dtype of the traced proxy value or real value.

        Returns:
            Union[torch.Size,Collection[torch.dtype]]: Proxy value dtype or collection of dtypes.
        """

        if not self.node.attached:

            return util.apply(self.value, lambda x: x.dtype, torch.Tensor)

        # If we haven't scanned in a proxy_value, just return a proxy to get the attribute.
        if self.node.fake_value is inspect._empty:

            return super().__getattr__("dtype")

        return util.apply(self.node.fake_value, lambda x: x.dtype, torch.Tensor)
    
    @classmethod
    def __torch_function__(cls, orig_method, types, args=None, kwargs=None) -> Self:
        if args is None:
            args = list()
        if kwargs is None:
            kwargs = dict()

        proxy: Proxy = None

        def get_proxy(arg):
            nonlocal proxy

            proxy = arg

        util.apply((args, kwargs), get_proxy, Proxy)
    
        return proxy.node.create(
            orig_method,
            *args,
            **kwargs,
        )
    
    
InterventionProxyType = TypeVar("InterventionProxyType", bound=InterventionProxy)


---
File: /src/nnsight/intervention/protocols/__init__.py
---

from .grad import GradProtocol
from .module import ApplyModuleProtocol
from .intervention import InterventionProtocol
from .swap import SwapProtocol
from .noop import NoopProtocol
from .parameter import ParameterProtocol
from .entrypoint import EntryPoint



---
File: /src/nnsight/intervention/protocols/entrypoint.py
---

from typing import Any
from ...tracing.protocols import Protocol

class EntryPoint(Protocol):
    """An EntryPoint Protocol should have its value set manually outside of normal graph execution.
    This makes these type of Nodes special and are handled differently in a variety of cases.
    Subclasses EntryPoint informs those cases to handle it differently.
    Examples are InterventionProtocol and GradProtocol.
    """
    
    @staticmethod
    def is_entrypoint(thing:Any):
        
        return isinstance(thing, type) and issubclass(thing, EntryPoint)
    
    @classmethod
    def add(cls,*args, **kwargs):
        return super().add(*args, redirect=False, **kwargs)



---
File: /src/nnsight/intervention/protocols/grad.py
---

from typing import TYPE_CHECKING, Any, Dict

import torch

from ...tracing.protocols import Protocol

if TYPE_CHECKING:
    from ..graph import InterventionNode, InterventionNodeType
    
class GradProtocol(Protocol):
    """Protocol which adds a backwards hook via .register_hook() to a Tensor. The hook injects the gradients into the node's value on hook execution.
    Nodes created via this protocol are relative to the next time .backward() was called during tracing allowing separate .grads to reference separate backwards passes:

    .. code-block:: python
        with model.trace(...):

            grad1 = model.module.output.grad.save()

            model.output.sum().backward(retain_graph=True)

            grad2 = model.module.output.grad.save()

            model.output.sum().backward()

    Uses an attachment to store number of times .backward() has been called during tracing so a given .grad hook is only value injected at the appropriate backwards pass.
    """
        
    @classmethod
    def execute(cls, node: "InterventionNode") -> None:

        args, kwargs = node.prepare_inputs((node.args, node.kwargs))

        # First arg is the Tensor to add hook to.
        tensor: torch.Tensor = args[0]

        # Hook to remove when hook is executed at the appropriate backward pass.
        hook = None

        def grad(value):
               
            # Set the value of the Node.
            node.set_value(value)

            node.graph.execute(start=node.kwargs['start'], grad=True)
            
            # There may be a swap Protocol executed during the resolution of this part of the graph.
            # If so get it and replace value with it.
            if 'swap' in node.kwargs:
                value:InterventionNodeType = node.kwargs.pop('swap')
                
            # Remove hook (if this is not done memory issues occur)
            hook.remove()

            return value
            
        # Register hook.
        hook = tensor.register_hook(grad)

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "green4", "shape": "box"}
    
        return default_style

        


---
File: /src/nnsight/intervention/protocols/intervention.py
---

from typing import TYPE_CHECKING, Any, Dict

import torch
from ... import util
from .entrypoint import EntryPoint

if TYPE_CHECKING:
    from ..graph import InterventionNodeType
    from ..interleaver import Interleaver

class InterventionProtocol(EntryPoint):

    @classmethod
    def intervene(
        cls,
        activations: Any,
        module_path: str,
        module: torch.nn.Module,
        key: str,
        interleaver: "Interleaver",
    ):
        """Entry to intervention graph. This should be hooked to all modules involved in the intervention graph.

        Forms the current module_path key in the form of <module path>.<output/input>
        Checks the graphs InterventionProtocol attachment attribute for this key.
        If exists, value is a list of (start:int, end:int) subgraphs to iterate through.
        Node args for intervention type nodes should be ``[module_path, (batch_start, batch_size), iteration]``.
        Checks and updates the counter (number of times this module has been called for this Node) for the given intervention node. If count is not ready yet compared to the iteration, continue.
        Using batch_size and batch_start, apply torch.narrow to tensors in activations to select
        only batch indexed tensors relevant to this intervention node. Sets the value of a node
        using the indexed values. Using torch.narrow returns a view of the tensors as opposed to a copy allowing
        subsequent downstream nodes to make edits to the values only in the relevant tensors, and have it update the original
        tensors. This both prevents interventions from effecting bathes outside their preview and allows edits
        to the output from downstream intervention nodes in the graph.

        Args:
            activations (Any): Either the inputs or outputs of a torch module.
            module_path (str): Module path of the current relevant module relative to the root model.
            key (str): Key denoting either "input" or "output" of module.
            intervention_handler (InterventionHandler): Handler object that stores the intervention graph and keeps track of module call count.

        Returns:
            Any: The activations, potentially modified by the intervention graph.
        """

        # Key to module activation intervention nodes has format: <module path>.<output/input>
        module_path = f"{module_path}.{key}"

        interventions = interleaver.graph.interventions

        if module_path in interventions:
            intervention_nodes = interventions[module_path]

            # Multiple intervention nodes can have same module_path if there are multiple invocations.
            # Is a set of node indexes making up the intervention subgraph
            for index in intervention_nodes:
                
                node = interleaver.graph.nodes[index]
                # Args for intervention nodes are (module_path, batch_group, iteration).
                _, batch_group, iteration = node.args

                # Updates the count of intervention node calls.
                # If count matches the Node's iteration, its ready to be executed.
                ready, defer = node.graph.count(node.index, iteration)
                
                # Dont execute if the node isnt ready (call count / iteration) or its not fulfilled (conditional)
                if not ready:
                    continue

                value = activations

                narrowed = False

                if len(interleaver.batch_groups) > 1:

                    batch_start, batch_size = interleaver.batch_groups[
                        batch_group
                    ]

                    def narrow(acts: torch.Tensor):

                        if acts.shape[0] == interleaver.batch_size:

                            nonlocal narrowed

                            narrowed = True

                            return acts.narrow(0, batch_start, batch_size)

                        return acts

                    value = util.apply(
                        activations,
                        narrow,
                        torch.Tensor,
                    )

                node.reset()

                # Value injection.
                node.set_value(value)
                
                node.executed = True
                # Execute starting from start
                node.graph.execute(start=node.kwargs['start'], defer=defer, defer_start=node.kwargs['defer_start'])

                # Check if through the previous value injection, there was a 'swap' intervention.
                # This would mean we want to replace activations for this batch with some other ones.
                if 'swap' in node.kwargs:
                    value:InterventionNodeType = node.kwargs.pop('swap')
                        
                    def _concat(values):
                        """
                        Concatenates or merges values from different batches.
                        
                        This function handles the 'swap' intervention by replacing a specific batch group's
                        activations with new values while preserving the structure of the original activations.
                        
                        Args:
                            values: A list containing [original_activations, replacement_activations]
                                   where replacement_activations will be inserted into original_activations
                                   at the position specified by batch_start and batch_size.
                        
                        Returns:
                            The merged activations with the batch group replaced by the new values.
                        """
                        data_type = type(values[0])

                        if data_type == torch.Tensor:
                            def _concat_tensor(values):
                                """
                                Concatenates tensors for the 'swap' intervention.
                                
                                This function handles tensor concatenation by:
                                1. Extracting the portion before the batch to be replaced (pre)
                                2. Extracting the portion after the batch to be replaced (post)
                                3. Concatenating [pre, replacement, post] if dimensions are compatible
                                
                                Args:
                                    values: A list containing [original_tensor, replacement_tensor]
                                           where replacement_tensor will replace the section of original_tensor
                                           specified by batch_start and batch_size
                                
                                Returns:
                                    torch.Tensor: The concatenated tensor with the batch section replaced,
                                                 or the original tensor if dimensions don't match
                                """
                                pre = values[0].narrow(0, 0, batch_start)
                                post = values[0].narrow(0, batch_start+batch_size, values[0].shape[0] - batch_start - batch_size) if interleaver.batch_size == values[0].shape[0] else values[0]

                                # Verify dimensions match before concatenating
                                if sum([pre.shape[0], values[1].shape[0], post.shape[0]]) == values[0].shape[0]:
                                    return torch.concatenate([pre, values[1], post])
                                else:
                                    return values[0]


                            if values[0].requires_grad:
                                if values[0].is_leaf:
                                    # For leaf tensors that require gradients, we need to use concatenation
                                    # to preserve the gradient flow
                                    return _concat_tensor(values)
                                else:
                                    # For non-leaf tensors, we can use in-place operations which are more efficient as long as 
                                    # the view is not the output of a function that returns multiple views
                                    if not torch.equal(values[0][batch_start:batch_start+batch_size], values[1]):
                                        try:
                                            values[0][batch_start:batch_start+batch_size] = values[1]
                                        except RuntimeError as e:
                                            if "This view is the output of a function that returns multiple views" in str(e):
                                                return _concat_tensor(values)
                                            else:
                                                raise e
                                        
                                    return values[0]
                            else:
                                values[0][batch_start:batch_start+batch_size] = values[1]
                                
                                return values[0]
                            
                        elif data_type == list:
                            # Recursively handle lists by concatenating each element
                            return [
                                _concat([value[value_idx] for value in values])
                                for value_idx in range(len(values[0]))
                            ]
                        elif data_type == tuple:
                            # Recursively handle tuples by concatenating each element
                            return tuple(
                                [
                                    _concat([value[value_idx] for value in values])
                                    for value_idx in range(len(values[0]))
                                ]
                            )
                        elif data_type == dict:
                            # Recursively handle dictionaries by concatenating each value
                            return {
                                key: _concat([value[key] for value in values])
                                for key in values[0].keys()
                            }
                        
                        # Default case: return the original value
                        return values[0]
                    
                    if narrowed:
                        # If the batch was narrowed, we need to merge the new values back into the original activations
                        activations = _concat([activations, value])
                    else:
                        # If the batch wasn't narrowed, we can directly replace the activations
                        activations = value

        return activations

    @classmethod
    def execute(cls, node: "InterventionNodeType"):
        # To prevent the node from looking like its executed when calling Graph.execute
        node.executed = False

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "green4", "shape": "box"}
        default_style["arg_kname"][0] = "module_path"
        default_style["arg_kname"][1] = "batch_group"
        default_style["arg_kname"][2] = "call_counter"

        return default_style



---
File: /src/nnsight/intervention/protocols/module.py
---

from typing import TYPE_CHECKING, Any, Dict

import torch
from typing_extensions import Self

from ... import util
from ...tracing.protocols import Protocol

if TYPE_CHECKING:
    from ..graph import InterventionGraph, InterventionNode

class ApplyModuleProtocol(Protocol):
    """Protocol that references some root model, and calls its .forward() method given some input.
    Using .forward() vs .__call__() means it wont trigger hooks.
    Uses an attachment to the Graph to store the model.
    """


    @classmethod
    def add(
        cls, graph: "InterventionGraph", module_path: str, *args, hook=False, **kwargs
    ) -> Self:
        """Creates and adds an ApplyModuleProtocol to the Graph.
        Assumes the attachment has already been added via ApplyModuleProtocol.set_module().

        Args:
            graph (Graph): Graph to add the Protocol to.
            module_path (str): Module path (model.module1.module2 etc), of module to apply from the root module.

        Returns:
            InterventionProxy: ApplyModule Proxy.
        """
        
        from ..graph.node import ValidatingInterventionNode, validate

        # If the Graph is validating, we need to compute the proxy_value for this node.
        if graph.node_class is ValidatingInterventionNode:

            # If the module has parameters, get its device to move input tensors to.
            module: torch.nn.Module = util.fetch_attr(
                graph.model._model, module_path
            )

            try:
                device = next(module.parameters()).device
            except:
                device = None

            # Enter FakeMode for proxy_value computing.
            kwargs['fake_value'] = validate(module.forward, *args, **kwargs)

        kwargs["hook"] = hook

        # Create and attach Node.
        return graph.create(
            cls,
            module_path,
            *args,
            **kwargs,
        )

    @classmethod
    def execute(cls, node: "InterventionNode") -> None:
        """Executes the ApplyModuleProtocol on Node.

        Args:
            node (Node): ApplyModule Node.
        """
        
        graph: InterventionGraph = node.graph
    
        module: torch.nn.Module = util.fetch_attr(
            graph.model._model, node.args[0]
        )

        try:
            device = next(module.parameters()).device
        except:
            device = None

        args, kwargs = node.prepare_inputs((node.args, node.kwargs), device=device)

        module_path, *args = args

        hook = kwargs.pop("hook")

        if hook:
            output = module(*args, **kwargs)
        else:
            output = module.forward(*args, **kwargs)

        node.set_value(output)

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "green4", "shape": "polygon", "sides": 6}

        return default_style



---
File: /src/nnsight/intervention/protocols/noop.py
---

from typing import TYPE_CHECKING, Any

from ...tracing.protocols import Protocol
if TYPE_CHECKING:
    from ..graph import  InterventionNode


class NoopProtocol(Protocol):
    

    @classmethod
    def execute(cls, node: "InterventionNode") -> None:
    
        node.set_value(None)


---
File: /src/nnsight/intervention/protocols/parameter.py
---

from typing import Dict, Any

import torch

from ... import util
from ...tracing.protocols import Protocol

class ParameterProtocol(Protocol):
    """ Protocol designed for safe access of model tensor parameter attributes (e.g. weights and biases).

    When a model attribute that's a tensor is accessed, the attribute value is clone before being returned.
    """

    @classmethod
    def execute(cls, node):

        module: torch.nn.Module = util.fetch_attr(
            node.graph.model._model, node.args[0]
        )

        attr = getattr(module, node.args[1]).clone()

        node.set_value(attr)

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "green4", "shape": "box"}

        default_style["arg_kname"][0] = "module_path"
        default_style["arg_kname"][1] = "key"

        return default_style


---
File: /src/nnsight/intervention/protocols/swap.py
---

from typing import TYPE_CHECKING, Any, Dict

from ...tracing.protocols import Protocol

if TYPE_CHECKING:
    from ..graph import InterventionNodeType


class SwapProtocol(Protocol):
    

    @classmethod
    def execute(cls, node: "InterventionNodeType") -> None:
        
        intervention_node, value = node.args
        intervention_node: "InterventionNodeType"
        
        value = node.prepare_inputs(value)
        
        node.set_value(None)
        
        intervention_node.kwargs['swap'] = value

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "green4", "shape": "ellipse"}

        return default_style



---
File: /src/nnsight/intervention/__init__.py
---


"""
The `intervention` module extends the `tracing` module to add PyTorch specific interventions to a given computation graph.
It defines its own: protocols, contexts, backends and graph primitives to achieve this.
"""
from .base import NNsight
from .envoy import Envoy


---
File: /src/nnsight/intervention/base.py
---

from __future__ import annotations

from typing import (TYPE_CHECKING, Any, Callable, Dict, Optional, Tuple, Type, Union)

import torch
from typing_extensions import Self

from .. import util
from ..tracing.backends import Backend
from .backends import NoopBackend
from .contexts import EditingTracer, InterleavingTracer, Session
from .envoy import Envoy
from .graph import (InterventionGraph, InterventionNode, InterventionProxy,
                    InterventionProxyType)
from .graph.proxy import Proxy
from .interleaver import Interleaver
from ..tracing.protocols import StopProtocol
from .. import CONFIG


class NNsight:
    """Main class to be implemented as a wrapper for PyTorch models wishing to gain this package's functionality.

    Class Attributes:

        proxy_class (Type[InterventionProxy]): InterventionProxy like type to use as a Proxy for this Model's inputs and outputs. Can have Model specific functionality added to a new sub-class.
        __methods__ (Dict[str,str]): Mapping of method name, which will open up a .trace context, and the actual method name to execute / interleave with.
            For example lets say I had a method on my underlying ._model called `.run` that I wanted to have the NNsight interleaving functionality applied to.
            I could define a method on my NNsight sub-class called `._run` which might look like:

             .. code-block:: python

                def _run(self, *inputs, **kwargs):

                    inputs, kwargs = some_preprocessing(inputs, kwargs)

                    return self._model.run(*args, **kwargs)

            I could then have my __methods__ attribute look like `__methods__ = {'run', '_run'}`
            This would allow me to do:

             .. code-block:: python

                with model.run(...):

                    output = model.output.save()



    Attributes:
        _model (torch.nn.Module): Underlying torch module.
        _envoy (Envoy): Envoy for underlying model.
        _session (Session): Session object if in a Session.
        _default_graph (Graph): Intervention graph to start from when calling NNsight.trace. This is set via the editing context NNsight.edit.
    """

    __methods__: Dict[str, str] = dict()

    proxy_class: Type[InterventionProxyType] = InterventionProxy

    def __init__(
        self,
        model: torch.nn.Module,
        rename: Optional[Dict[str,str]] = None
    ) -> None:

        self._model: torch.nn.Module = model
        self._envoy: Envoy[InterventionProxy, InterventionNode] = Envoy(self._model, rename=rename)

        self._session: Optional[Session] = None
        self._default_graph: Optional[InterventionGraph] = None

    #### Public API ##############

    def trace(
        self,
        *inputs: Any,
        trace: bool = True,
        scan: bool = False,
        method: Optional[str] = None,
        invoker_kwargs:Optional[Dict[str,Any]] = None,
        backend: Optional[Union[Backend, str]] = None,
        **kwargs: Dict[str, Any],
    ) -> Union[InterleavingTracer, Any]:
        """Entrypoint into the tracing and interleaving functionality nnsight provides.

        In short, allows access to the future inputs and outputs of modules in order to trace what operations you would like to perform on them.
        This can be as simple as accessing and saving activations for inspection, or as complicated as transforming the activations and gradients in a forward pass over multiple inputs.

        Args:
            inputs (tuple[Any]): When positional arguments are provided directly to .trace, we assume there is only one Invoker and therefore
                immediately create an enter an Invoker.
            trace (bool, optional): If to open a tracing context. Otherwise immediately run the model and return the raw output. Defaults to True.
            scan (bool): Exposed invoker kwarg to scan for the provided input. No effect if there is no input.
            method (Optional[str]): String name of method to interleave with. Defaults to None and therefore NNsight._execute
            invoker_kwargs (Dict[str, Any], optional): Keyword arguments to pass to Invoker initialization, and then downstream to the model's .prepare_inputs(...) method. Used when giving input directly to `.trace(...)`. Defaults to None.
            kwargs (Dict[str, Any]): Keyword arguments passed to Tracer initialization, and then downstream to the model's execution method.

        Raises:
            ValueError: If trace is False and no inputs were provided (nothing to run with)

        Returns:
            Union[Tracer, Any]: Either the Tracer used for tracing, or the raw output if trace is False.

        Examples:

            There are a few ways you can use ``.trace(...)`` depending in your use case.

            Lets use this extremely basic model for our examples:

            .. code-block:: python

                import torch
                from collections import OrderedDict

                input_size = 5
                hidden_dims = 10
                output_size = 2

                model = nn.Sequential(OrderedDict([
                    ('layer1', torch.nn.Linear(input_size, hidden_dims)),
                    ('sigma1', torch.nn.Sigmoid()),
                    ('layer2', torch.nn.Linear(hidden_dims, output_size)),
                    ('sigma2', torch.nn.Sigmoid()),
                ]))

                example_input = torch.rand((1, input_size))


            The first example has us running the model with a single example input, and saving the input and output of 'layer2' as well as the final output using the tracing context.

            .. code-block:: python

                from nnsight import NNsight

                with NNsight(model).trace(example_input) as model:

                    l2_input = model.layer2.input.save()
                    l2_output = model.layer2.output.save()

                    output = model.output.save()

                print(l2_input)
                print(l2_output)
                print(output)

            The second example allows us to divide up multiple inputs into one batch, and scope an inner invoker context to each one.
            We indicate this simply by not passing and positional inputs into `.trace(...)`. The Tracer object then expects you to enter each input via `Tracer.invoke(...)`

            .. code-block:: python

                example_input2 = torch.rand((1, input_size))

                with NNsight(model).trace() as model:

                    with model.invoke(example_input):

                        output1 = model.output.save()

                    with model.invoke(example_input2):

                        output2 = model.output.save()

                print(output1)
                print(output2)
        """

        # If were in a session, this trace is simple a child of the open trace.
        if self._session is not None:

            parent = self._session.graph

        else:
            parent = None

        # Create Tracer.
        tracer = InterleavingTracer(
            self,
            method=method,
            backend=backend,
            parent=parent,
            **kwargs,
        )

        # If user provided input directly to .trace(...).
        if len(inputs) > 0:
            
            if invoker_kwargs is None:
                invoker_kwargs = {}
            
            invoker_kwargs['scan'] = scan
            
            # Enter an invoker
            tracer.invoke(*inputs, **invoker_kwargs).__enter__()

            # If trace is False, we'll enter the Tracer context immediately and enter an Invoker context with the provided inputs as well.
            # We'll also save the output of the model and return its value directly.
            if not trace:

                with tracer:

                    output = self._envoy.output.save()

                if isinstance(output, Proxy):

                    output = output.value
                
                return output

        # If trace is False, you had to have provided an input.
        if not trace:

            raise ValueError("Can't execute on no inputs!")

        return tracer

    def scan(self, *inputs, **kwargs) -> InterleavingTracer:
        """Context just to populate fake tensor proxy values using scan and validate.
        Useful when looking for just the shapes of future tensors

        Examples:

            .. code-block:: python

                with model.scan(" "):

                    dim = model.module.output.shape[-1]

                print(dim)

        Returns:
            Tracer: Tracer context with Noop backend.
        """

        return self.trace(
            *inputs, **kwargs, scan=True, validate=True, backend=NoopBackend()
        )

    def edit(
        self,
        *inputs: Any,
        inplace: bool = False,
        **kwargs: Dict[str, Any],
    ) -> Union[InterleavingTracer, Any]:
        """Create a trace context with an edit backend and apply a list of edits.

        The edit backend sets a default graph on an NNsight model copy which is
        run on future trace calls.

        This operation is not inplace!

        Args:
            inplace (bool): If True, makes edits in-place.

        Returns:
            Union[Tracer, Any]: Either the Tracer used for tracing, or the raw output if trace is False.

        Example:
            .. code-block:: python
            from nnsight import LanguageModel

            gpt2 = LanguageModel("openai-community/gpt2)

            class ComplexModule(torch.nn.Module):
                def __init__(self):
                    super().__init__()
                    self.one = WrapperModule()

                def forward(self, x):
                    return self.one(x)

            l0 = gpt2.transformer.h[0]
            l0.attachment = ComplexModule()

            with gpt2.edit("test") as gpt2_edited:
                acts = l0.output[0]
                l0.output[0][:] = l0.attachment(acts, hook=True)

            with gpt2.trace(MSG_prompt):
                original = l0.output[0].clone().save()
                l0.output[0][:] *= 0.0
                original_output = gpt2.output.logits.save()

            with gpt2_edited.trace(MSG_prompt):
                one = l0.attachment.one.output.clone().save()
                l0.attachment.output *= 0.0
                edited_output = gpt2.output.logits.save()

            print(original_output)
            print(edited_output)
        """

        return EditingTracer(self, *inputs, inplace=inplace, **kwargs)

    def session(
        self,
        backend: Union[Backend, str] = None,
        **kwargs,
    ) -> Session:
        """Create a session context using a Session.

        Args:
            backend (Backend): Backend for this Session object.

        Returns:
            Session: Session.
        """
        if self._session is not None:

            raise ValueError("Can't create a Session with one already open!")

        return Session[InterventionNode, self.proxy_class](
            self, backend=backend, **kwargs
        )

    def interleave(
        self,
        interleaver: Interleaver,
        *args,
        fn: Optional[Union[Callable, str]] = None,
        **kwargs,
    ) -> Any:
        """This is the point in nnsight where we finally execute the model and interleave our custom logic.
        Simply resolves the function and executes it given some input within the Intreleaver context.
        This method is on here vs on the Interleaver because some models might want to define custom interleaving behavior. For example loading real model weights before execution.

        Args:
            interleaver (Interleaver): Interleaver.
            fn (Optional[Union[Callable, str]], optional): Function to interleave with. Defaults to None and therefore NNsight._execute.

        Returns:
            Any: _description_
        """

        if fn is None:
            fn = self._execute
        elif isinstance(fn, str):
            fn = getattr(self, fn)

        interleaver.graph.execute()

        try:
            with interleaver:
                return fn(*args, **kwargs)
        except StopProtocol.StopException as e:
            pass

    def to(self, *args, **kwargs) -> Self:
        """Override torch.nn.Module.to so this returns the NNSight model, not the underlying module when doing: model = model.to(...)

        Returns:
            Envoy: Envoy.
        """

        self._envoy.to(*args, **kwargs)

        return self

    @property
    def device(self) -> Optional[torch.device]:

        try:
            return next(self._model.parameters()).device
        except:
            return None

    def clear_edits(self) -> None:
        """Resets the default graph of this model."""
        self._default_graph = None
        
    def get(self, path:str) -> Union[Envoy, InterventionProxyType]:
        """Gets the Envoy/Proxy via its path.
        
        e.x:
            model = nnsight.LanguageModel("openai-community/gpt2")
            
            module = model.get('transformer.h.0.mlp')
            
            with model.trace("Hello"):
                value = model.get('transformer.h.0.mlp.output').save()

        Args:
            path (str): '.' separated path.

        Returns:
            Union[Envoy, InterventionProxyType]: Fetched Envoy/Proxy
        """
        return util.fetch_attr(self, path)
    
    #### Private API ##############

    def to_device(self, data: Any) -> Any:

        device = self.device

        if device is not None:

            data = util.apply(data, lambda x: x.to(device), torch.Tensor)

        return data

    def _shallow_copy(self) -> Self:
        """Creates a new instance copy of the same class with the all the attributes of the original instance.

        Returns:
            Self: NNsightModel
        """
        copy = self.__class__.__new__(self.__class__)
        for key, value in self.__dict__.items():
            copy.__dict__[key] = value

        return copy

    def __repr__(self) -> str:
        """Wrapper of ._model's representation as the NNsight model's representation.

        Returns:
            str: Representation.
        """
        return repr(self._envoy)

    def __setattr__(self, key: Any, value: Any) -> None:
        """Overload setattr to create and set an Envoy when trying to set a torch Module."""

        if key not in ("_model", "_model_key") and isinstance(value, torch.nn.Module):

            setattr(self._envoy, key, value)

        else:

            object.__setattr__(self, key, value)

    def __getattr__(self, key: Any):
        """Wrapper of ._envoy's attributes to access module's inputs and outputs.

        Returns:
            Any: Attribute.
        """

        if key in self.__methods__:
            return lambda *args, **kwargs: self.trace(
                *args, method=self.__methods__[key], **kwargs
            )

        return getattr(self._envoy, key)

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        return self._envoy(*args, **kwargs)

    ### NNsight VIRTUAL METHODS BELOW #####################################

    def _execute(self, *args, **kwargs) -> Any:

        args, kwargs = self.to_device((args, kwargs))

        return self._model(*args, **kwargs)

    def _prepare_input(
        self, *args, **kwargs
    ) -> Tuple[Tuple[Tuple[Any], Dict[str, Any]], int]:
        """Virtual method to prepare inputs before batching and execution and return batch size of prepared_inputs.

        Default implementation just returns inputs and length of first input.

        Args:
            inputs (tuple[Any]): Inputs to prepare for batching and execution.
            int: Batch size of prepared_inputs.

        Returns:
            Tuple[tuple[Any], int]: Prepared inputs, batch size of inputs.
        """
        return (args, kwargs), len(args[0])

    def _batch(
        self,
        batched_inputs: Optional[Tuple[Tuple[Any], Dict[str, Any]]],
        *args,
        **kwargs,
    ) -> Tuple[Tuple[Any], Dict[str, Any]]:
        """Virtual method to batch together results from _prepare_inputs.

        Default implementation returns list of all prepared_inputs.

        Args:
            batched_inputs (Any): Current state of batched_inputs. Initially None.
            prepared_inputs (tuple[Any]): Most recent result from _prepare_inputs.

        Returns:
            Any: Batched inputs.
        """

        if batched_inputs is None:
            return (args, kwargs)

        args = tuple(
            [
                torch.concatenate((batched_inputs[i], args[i]))
                for i in range(len(batched_inputs))
            ]
        )

        return args, kwargs


if TYPE_CHECKING:

    class NNsight(NNsight, Envoy[InterventionProxy, InterventionNode]):
        def __getattribute__(self, name: str) -> Union[Envoy[InterventionProxy]]:
            pass



---
File: /src/nnsight/intervention/envoy.py
---

from __future__ import annotations

import inspect
import weakref
import warnings
from contextlib import AbstractContextManager
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Generic,
    Iterator,
    List,
    Optional,
    Tuple,
    Union,
)

import torch
from typing_extensions import Self

from . import protocols
from .backends import EditingBackend
from .contexts import InterleavingTracer
from .graph import InterventionNodeType, InterventionProxyType


class Envoy(Generic[InterventionProxyType, InterventionNodeType]):
    """Envoy objects act as proxies for torch modules themselves within a model's module tree in order to add nnsight functionality.
    Proxies of the underlying module's output and input are accessed by `.output` and `.input` respectively.

    Attributes:
        path (str): String representing the attribute path of this Envoy's module relative the the root model. Separated by '.' e.x ('.transformer.h.0.mlp').
        output (nnsight.intervention.InterventionProxy): Proxy object representing the output of this Envoy's module. Reset on forward pass.
        inputs (nnsight.intervention.InterventionProxy): Proxy object representing the inputs of this Envoy's module. Proxy is in the form of (Tuple[Tuple[<Positional arg>], Dict[str, <Keyword arg>]])Reset on forward pass.
        input (nnsight.intervention.InterventionProxy): Alias for the first positional Proxy input i.e Envoy.inputs[0][0]
        iter (nnsight.envoy.EnvoyIterator): Iterator object allowing selection of specific .input and .output iterations of this Envoy.
        _module (torch.nn.Module): Underlying torch module.
        _children (List[Envoy]): Immediate Envoy children of this Envoy.
        _fake_outputs (List[torch.Tensor]): List of 'meta' tensors built from the outputs most recent _scan. Is list as there can be multiple shapes for a module called more than once.
        _fake_inputs (List[torch.Tensor]): List of 'meta' tensors built from the inputs most recent _scan. Is list as there can be multiple shapes for a module called more than once.
        _rename (Optional[Dict[str,str]]): Optional mapping of (old name -> new name).
            For example to rename all gpt 'attn' modules to 'attention' you would: rename={r"attn": "attention"}
            Not this does not actually change the underlying module names, just how you access its envoy. Renaming will replace Envoy.path but Envoy._path represents the pre-renamed true attribute path.
        _tracer (nnsight.context.Tracer.Tracer): Object which adds this Envoy's module's output and input proxies to an intervention graph. Must be set on Envoys objects manually by the Tracer.
    """

    def __init__(
        self,
        module: torch.nn.Module,
        module_path: str = "",
        alias_path: Optional[str] = None,
        rename: Optional[Dict[str, str]] = None,
    ):

        self.path = alias_path or module_path
        self._path = module_path

        self._module: torch.nn.Module = weakref.proxy(module)

        self._rename = rename

        self._iteration_stack = [0]

        self._fake_outputs: List[torch.Tensor] = []
        self._fake_inputs: List[torch.Tensor] = []

        self._output_stack: List[Optional[InterventionProxyType]] = [None]
        self._input_stack: List[Optional[InterventionProxyType]] = [None]

        self._tracer: InterleavingTracer = None

        self._children: List[Envoy] = []

        # Register hook on underlying module to update the _fake_outputs and _fake_inputs on forward pass.
        self._hook_handle = self._module.register_forward_hook(
            self._hook, with_kwargs=True
        )

        # Recurse into PyTorch module tree.
        for name, module in list(self._module.named_children()):

            setattr(self, name, module)

    # Public API ################

    def __call__(
        self, *args: List[Any], hook=False, **kwargs: Dict[str, Any]
    ) -> InterventionProxyType:
        """Creates a proxy to call the underlying module's forward method with some inputs.

        Returns:
            InterventionProxy: Module call proxy.
        """

        if not self._tracing() or self._scanning():
            return self._module(*args, **kwargs)

        if isinstance(self._tracer.backend, EditingBackend):
            hook = True

        return protocols.ApplyModuleProtocol.add(
            self._tracer.graph, self._path, *args, hook=hook, **kwargs
        )

    @property
    def output(self) -> InterventionProxyType:
        """
        Calling denotes the user wishes to get the output of the underlying module and therefore we create a Proxy of that request.
        Only generates a proxy the first time it is references otherwise return the already set one.

        Returns:
            InterventionProxy: Output proxy.
        """
        output = self._output_stack.pop()

        if output is None:

            if isinstance(self._module, torch.nn.ModuleList):
                output = self._tracer.apply(list)
                output.extend([envoy.output for envoy in self._children])
            else:

                iteration = self._iteration_stack[-1]

                if len(self._fake_outputs) == 0:
                    fake_output = inspect._empty
                elif iteration >= len(self._fake_outputs):
                    # TODO warning?
                    fake_output = self._fake_outputs[-1]
                else:
                    fake_output = self._fake_outputs[iteration]

                module_path = f"{self._path}.output"

                output = protocols.InterventionProtocol.add(
                    self._tracer.graph,
                    module_path,
                    self._tracer._invoker_group,
                    iteration,
                    fake_value=fake_output,
                )

        self._output_stack.append(output)

        return output

    @output.setter
    def output(self, value: Union[InterventionProxyType, Any]) -> None:
        """
        Calling denotes the user wishes to set the output of the underlying module and therefore we create a Proxy of that request.

        Args:
            value (Union[InterventionProxy, Any]): Value to set output to.
        """

        protocols.SwapProtocol.add(self.output.node.graph, self.output.node, value)

        self._output_stack[-1] = None

    @property
    def inputs(self) -> InterventionProxyType:
        """
        Calling denotes the user wishes to get the input of the underlying module and therefore we create a Proxy of that request.
        Only generates a proxy the first time it is references otherwise return the already set one.

        Returns:
            InterventionProxy: Input proxy.
        """

        input = self._input_stack.pop()

        if input is None:

            if isinstance(self._module, torch.nn.ModuleList):
                input = self._tracer.apply(list)
                input.extend([envoy.inputs for envoy in self._children])
            else:

                iteration = self._iteration_stack[-1]

                if len(self._fake_inputs) == 0:
                    fake_input = inspect._empty
                elif iteration >= len(self._fake_inputs):
                    # TODO warning?
                    fake_input = self._fake_inputs[-1]
                else:
                    fake_input = self._fake_inputs[iteration]

                module_path = f"{self._path}.input"

                input = protocols.InterventionProtocol.add(
                    self._tracer.graph,
                    module_path,
                    self._tracer._invoker_group,
                    iteration,
                    fake_value=fake_input,
                )

        self._input_stack.append(input)

        return input

    @inputs.setter
    def inputs(self, value: Union[InterventionProxyType, Any]) -> None:
        """
        Calling denotes the user wishes to set the input of the underlying module and therefore we create a Proxy of that request.

        Args:
            value (Union[InterventionProxy, Any]): Value to set input to.
        """

        protocols.SwapProtocol.add(self.inputs.node.graph, self.inputs.node, value)

        self._input_stack[-1] = None

    @property
    def input(self) -> InterventionProxyType:
        """Getting the first positional argument input of the model's module.

        Returns:
            InterventionProxy: Input proxy.
        """

        if isinstance(self._module, torch.nn.ModuleList):
            input = self._tracer.apply(list)
            input.extend([envoy.input for envoy in self._children])

            return input

        return self.inputs[0][0]

    @input.setter
    def input(self, value: Union[InterventionProxyType, Any]) -> None:
        """Setting the value of the input's first positional argument in the model's module.

        Args;
            value (Union[InterventionProxy, Any]): Value to set the input to.
        """

        self.inputs = ((value,) + self.inputs[0][1:],) + (self.inputs[1:])

    @property
    def iter(self) -> IterationEnvoy:

        return IterationEnvoy(self)

    @iter.setter
    def iter(self, iteration: Union[int, List[int], slice]) -> None:
        self._iteration_stack.append(iteration)

    def next(self, increment: int = 1) -> Envoy:
        """By default, this modules inputs and outputs only refer to the first time its called. Use `.next()`to select which iteration .input an .output refer to.

        Args:
            increment (int, optional): How many iterations to jump. Defaults to 1.

        Returns:
            Envoy: Self.
        """

        return self.iter[self._iteration_stack[-1] + increment].__enter__()

    def all(self, propagate: bool = True) -> Envoy:
        """By default, this modules inputs and outputs only refer to the first time its called. Use `.all()`to have .input and .output refer to all iterations.

        Returns:
            Envoy: Self.
        """

        return self.iter[:].__enter__()

    def to(self, *args, **kwargs) -> Envoy:
        """Override torch.nn.Module.to so this returns the Envoy, not the underlying module when doing: model = model.to(...)

        Returns:
            Envoy: Envoy.
        """

        self._module = self._module.to(*args, **kwargs)

        return self

    def modules(
        self,
        include_fn: Callable[[Envoy], bool] = None,
        names: bool = False,
        envoys: List = None,
    ) -> List[Envoy]:
        """Returns all Envoys in the Envoy tree.

        Args:
            include_fn (Callable, optional): Optional function to be ran against all Envoys to check if they should be included in the final collection of Envoys. Defaults to None.
            names (bool, optional): If to include the name/module_path of returned Envoys along with the Envoy itself. Defaults to False.

        Returns:
            List[Envoy]: Included Envoys
        """

        if envoys is None:
            envoys = list()

        included = True

        if include_fn is not None:
            included = include_fn(self)

        if included:
            if names:
                envoys.append((self.path, self))
            else:
                envoys.append(self)

        for sub_envoy in self._children:
            sub_envoy.modules(include_fn=include_fn, names=names, envoys=envoys)

        return envoys

    def named_modules(self, *args, **kwargs) -> List[Tuple[str, Envoy]]:
        """Returns all Envoys in the Envoy tree along with their name/module_path.

        Args:
            include_fn (Callable, optional): Optional function to be ran against all Envoys to check if they should be included in the final collection of Envoys. Defaults to None.

        Returns:
            List[Tuple[str, Envoy]]: Included Envoys and their names/module_paths.
        """

        return self.modules(*args, **kwargs, names=True)

    # Private API ###############################

    def _update(self, module: torch.nn.Module) -> None:
        """Updates the ._model attribute using a new model of the same architecture.
        Used when loading the real weights (dispatching) and need to replace the underlying modules.
        """

        self._hook_handle.remove()

        self._hook_handle = module.register_forward_hook(self._hook, with_kwargs=True)

        i = 0

        for i, child in enumerate(module.children()):

            self._children[i]._update(child)

        # Handle extra modules added after initialization: issues/376
        for name, child in list(self._module.named_children())[i + 1 :]:

            setattr(module, name, child)

        self._module = weakref.proxy(module)

    def _add_envoy(self, module: torch.nn.Module, name: str) -> None:
        """Adds a new Envoy for a given torch module under this Envoy.

        Args:
            module (torch.nn.Module): Module to create Envoy for.
            name (str): name of envoy/attribute.
        """

        alias_path = None

        module_path = f"{self._path}.{name}"

        if self._rename is not None and name in self._rename:

            name = self._rename[name]

            alias_path = f"{self.path}.{name}"

        envoy = Envoy(
            module, module_path=module_path, alias_path=alias_path, rename=self._rename
        )

        self._children.append(envoy)

        setattr(self._module, name, module)

        # If the module already has a sub-module named 'input' or 'output',
        # mount the proxy access to 'nns_input' or 'nns_output instead.
        if hasattr(Envoy, name):

            self._handle_overloaded_mount(envoy, name)

        else:

            super().__setattr__(name, envoy)

    def _handle_overloaded_mount(self, envoy: Envoy, mount_point: str) -> None:
        """If a given module already has an attribute of the same name as something nnsight wants to add, we need to rename it.

        Directly edits the underlying class to accomplish this.

        Args:
            envoy (Envoy): Envoy to handle.
            mount_point (str): Overloaded attribute name.
        """

        warnings.warn(
            f"Module of type `{type(self._module)}` has pre-defined a `{mount_point}` attribute. nnsight access for `{mount_point}` will be mounted at `.nns_{mount_point}` instead of `.{mount_point}` for this module only."
        )

        # If we already shifted a mount point dont create another new class.
        if "Preserved" in self.__class__.__name__:

            new_cls = self.__class__

        else:

            new_cls = type(
                f"{Envoy.__name__}.Preserved",
                (Envoy,),
                {},
            )

        # Get the normal proxy mount point
        mount = getattr(new_cls, mount_point)

        # Move it to nns_<mount point>
        setattr(new_cls, f"nns_{mount_point}", mount)
        # Set the sub-module/envoy to the normal mount point on the CLASS itself not the instance.
        setattr(new_cls, mount_point, envoy)

        # Update the class on the instance
        self.__class__ = new_cls

    def _set_tracer(self, tracer: InterleavingTracer, propagate=True):
        """Set tracer object on Envoy.

        Args:
            tracer (Tracer): Tracer to set.
            propagate (bool, optional): If to propagate to all sub-modules. Defaults to True.
        """

        self._tracer = tracer

        if propagate:
            for envoy in self._children:
                envoy._set_tracer(tracer, propagate=True)

    def _tracing(self) -> bool:
        """Whether or not tracing.

        Returns:
            bool: Is tracing.
        """

        try:

            return self._tracer.graph.alive

        except:

            return False

    def _scanning(self) -> bool:
        """Whether or not in scanning mode. Checks the current Tracer's Invoker.

        Returns:
            bool: Is scanning.
        """

        try:

            return self._tracer.invoker.scanning

        except:

            return False

    def _set_iteration(
        self, iteration: Optional[int] = None, propagate: bool = True
    ) -> None:

        if iteration is not None:
            self._iteration_stack.append(iteration)
            self._output_stack.append(None)
            self._input_stack.append(None)
        else:
            self._iteration_stack.pop()
            self._output_stack.pop()
            self._input_stack.pop()

        if propagate:
            for envoy in self._children:
                envoy._set_iteration(iteration, propagate=True)

    def _reset_proxies(self, propagate: bool = True) -> None:
        """Sets proxies to None.

        Args:
            propagate (bool, optional): If to propagate to all sub-modules. Defaults to True.
        """

        self._output_stack = [None]
        self._input_stack = [None]

        if propagate:
            for envoy in self._children:
                envoy._reset_proxies(propagate=True)

    def _reset(self, propagate: bool = True) -> None:
        """Sets _call_iter to zero. Calls ._reset_proxies as well.

        Args:
            propagate (bool, optional): If to propagate to all sub-modules. Defaults to True.
        """

        self._reset_proxies(propagate=False)

        self._iteration_stack = [0]

        if propagate:
            for envoy in self._children:
                envoy._reset(propagate=True)

    def _clear(self, propagate: bool = True) -> None:
        """Clears _fake_outputs and _fake_inputs. Calls ._reset as well.

        Args:
            propagate (bool, optional): If to propagate to all sub-modules. Defaults to True.
        """

        self._reset(propagate=False)

        self._fake_outputs = []
        self._fake_inputs = []

        if propagate:
            for envoy in self._children:
                envoy._clear(propagate=True)

    def _hook(
        self,
        module: torch.nn.Module,
        input: Any,
        input_kwargs: Dict,
        output: Any,
    ):

        if self._scanning():

            input = (input, input_kwargs)

            self._fake_outputs.append(output)
            self._fake_inputs.append(input)

    def _repr_module_list(self):

        list_of_reprs = [repr(item) for item in self._children]
        if len(list_of_reprs) == 0:
            return self._module._get_name() + "()"

        start_end_indices = [[0, 0]]
        repeated_blocks = [list_of_reprs[0]]
        for i, r in enumerate(list_of_reprs[1:], 1):
            if r == repeated_blocks[-1]:
                start_end_indices[-1][1] += 1
                continue

            start_end_indices.append([i, i])
            repeated_blocks.append(r)

        lines = []
        main_str = self._module._get_name() + "("
        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):
            local_repr = f"({start_id}): {b}"  # default repr

            if start_id != end_id:
                n = end_id - start_id + 1
                local_repr = f"({start_id}-{end_id}): {n} x {b}"

            local_repr = torch.nn.modules.module._addindent(local_repr, 2)
            lines.append(local_repr)

        main_str += "\n  " + "\n  ".join(lines) + "\n"
        main_str += ")"
        return main_str

    def __repr__(self) -> str:
        """Wrapper method for underlying module's string representation.

        Returns:
            str: String.
        """

        if isinstance(self._module, torch.nn.ModuleList):

            return self._repr_module_list()

        extra_lines = []
        extra_repr = self._module.extra_repr()
        # empty string will be split into list ['']
        if extra_repr:
            extra_lines = extra_repr.split("\n")
        child_lines = []
        for attribute_name, attribute in self.__dict__.items():

            if attribute_name == "_tracer":
                continue

            if isinstance(attribute, Envoy):

                mod_str = repr(attribute)
                mod_str = torch.nn.modules.module._addindent(mod_str, 2)
                child_lines.append("(" + attribute_name + "): " + mod_str)

        lines = extra_lines + child_lines

        main_str = self._module._get_name() + "("
        if lines:
            # simple one-liner info, which most builtin Modules will use
            if len(extra_lines) == 1 and not child_lines:
                main_str += extra_lines[0]
            else:
                main_str += "\n  " + "\n  ".join(lines) + "\n"

        main_str += ")"

        return main_str

    def __iter__(self) -> Iterator[Envoy[InterventionProxyType, InterventionNodeType]]:
        """Wrapper method for underlying ModuleList iterator.

        Returns:
            Iterator[Envoy]: Iterator.
        """

        return iter(self._children)

    def __getitem__(
        self, key: int
    ) -> Envoy[InterventionProxyType, InterventionNodeType]:
        """Wrapper method for underlying ModuleList getitem.

        Args:
            key (int): Key.

        Returns:
            Envoy: Envoy.
        """

        return self._children[key]

    def __len__(self) -> int:
        """Wrapper method for underlying ModuleList len.

        Returns:
            int: Length.
        """

        return len(self._module)

    def __getattr__(
        self, key: str
    ) -> Union[
        Envoy[InterventionProxyType, InterventionNodeType], InterventionProxyType, Any
    ]:
        """Wrapper method for underlying module's attributes.
        If the attribute is a tensor (e.g. weights or bias) and accessed during tracing, then an InterventionProxy is created.

        Args:
            key (str): Key.

        Returns:
            Union[InterventionProxyType, Any]: Attribute.
        """

        attr = getattr(self._module, key)

        if self._tracing() and isinstance(attr, torch.Tensor):
            attr_proxy = protocols.ParameterProtocol.add(
                self._tracer.graph, self._path, key
            )

            return attr_proxy

        return attr

    def __setattr__(self, key: Any, value: Any) -> None:
        """Overload setattr to create and set an Envoy when trying to set a torch Module."""

        if key != "_module" and isinstance(value, torch.nn.Module):

            self._add_envoy(value, key)

        else:

            super().__setattr__(key, value)


class IterationEnvoy(Envoy, AbstractContextManager):

    def __init__(self, envoy: Envoy) -> None:

        self.__dict__.update(envoy.__dict__)

        self._iteration = self._iteration_stack[-1]

        self._open_context = False

    @property
    def output(self) -> InterventionProxyType:

        self._output_stack.append(None)
        self._iteration_stack.append(self._iteration)

        output = super().output

        self._output_stack.pop()
        self._iteration_stack.pop()

        return output

    @property
    def input(self) -> InterventionProxyType:

        self._input_stack.append(None)
        self._iteration_stack.append(self._iteration)

        input = super().input

        self._input_stack.pop()
        self._iteration_stack.pop()

        return input

    def __getitem__(self, key: Union[int, List[int], slice]) -> Self:

        # TODO: Error if not valid key type

        if isinstance(key, tuple):

            key = list(key)

        self._iteration = key

        return self

    def __enter__(self) -> IterationEnvoy:

        if not self._open_context:

            self._set_iteration(self._iteration)

        self._open_context = True

        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:

        self._set_iteration()

        self._open_context = False

        if isinstance(exc_val, BaseException):
            raise exc_val



---
File: /src/nnsight/intervention/interleaver.py
---

from __future__ import annotations

from collections import defaultdict
from contextlib import AbstractContextManager
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union

import torch
from torch.utils.hooks import RemovableHandle

from .. import util
from .graph import InterventionGraph
from .protocols import InterventionProtocol

class Interleaver(AbstractContextManager):
    
    """The Interleaver is responsible for executing a function involving a PyTorch model alongside a user's custom functionality
    (represented by an `InterventionGraph`). This is called interleaving.
    
    The `InterventionGraph` has information about which components (modules) of the model the user's custom logic will interact with.
    As the `Interleaver` is a context, entering it adds the appropriate hooks to these components which act as a bridge between the model's
    original computation graph and the `InterventionGraph`. Exiting the `Interleaver` removes these hooks.
    
    Attributes:
        graph (InterventionGraph): The computation graph representing the user's custom intervention logic.
        batch_groups (Optional[List[Tuple[int, int]]]): A batch group is a section of tensor values related to a given intervention. 
            They are a tuple of (batch_start, batch_length). So if batch group 0 was (0, 4) it means it starts at index 0 and goes until index 3.
            The batch index is assumed to be the first dimension of all Tensors.
            InterventionProtocol Nodes know which batch group they are a part of in their arguments. That value is the index into the batch_groups.
        input_hook (Optional[Callable]). Function to hook onto the inputs of modules for interleaving. Defaults to None and therefore `InterventionProtocol.intervene`.
        output_hook (Optional[Callable]). Function to hook onto the outputs of modules for interleaving. Defaults to None and therefore `InterventionProtocol.intervene`.
        batch_size (Optional[int]). Total batch size. Used to determine which Tensors need to be narrowed to their batch group.
            i.e If a Tensor's first dimension isn't batch_size, we dont need to narrow it to convert it for its batch_group.
            Defaults to None and therefore the sum of the last batch_group.
    """

    def __init__(
        self,
        graph: InterventionGraph,
        batch_groups: Optional[List[Tuple[int, int]]] = None,
        input_hook: Optional[Callable] = None,
        output_hook: Optional[Callable] = None,
        batch_size: Optional[int] = None,
    ) -> None:

        self.graph = graph

        self.batch_groups = [] if batch_groups is None else batch_groups

        if input_hook is None:
            input_hook = (
                lambda activations, module_path, module: InterventionProtocol.intervene(
                    activations, module_path, module, "input", self
                )
            )

        if output_hook is None:
            output_hook = (
                lambda activations, module_path, module: InterventionProtocol.intervene(
                    activations, module_path, module, "output", self
                )
            )

        self.input_hook = input_hook
        self.output_hook = output_hook

        self.handles: List[RemovableHandle] = []
        
        if batch_size is None and len(self.batch_groups) != 0:
            
            self.batch_size = (
                sum(self.batch_groups[-1]) if batch_size is None else batch_size
            )
        else:
            self.batch_size = batch_size

    def __enter__(self) -> Interleaver:
        """Registers input and output hooks to modules involved in the `InterventionGraph`.

        Returns:
            Interleaver: Interleaver
        """

        # Keys of `InterventionGraph.interventions` are the module paths + if they are for input or output.
        # e.x 'transformer.h.0.mlp.output'
        for module_key in self.graph.interventions.keys():

            module_atoms = module_key.split(".")

            # Get just the hook type i.e input/output
            *module_atoms, hook_type = module_atoms

            # Get just the module path 
            module_path = ".".join(module_atoms)

            # Get the torch module using the module_path
            module: torch.nn.Module = util.fetch_attr(self.graph.model, module_path)

            if hook_type == "input":

                # Input hook activations are a tuple of (positional args, key-word arguments)
                # Include the module_path not the module
                def input_hook(module, input, kwargs, module_path=module_path):
                    return self.input_hook((input, kwargs), module_path, module)

                self.handles.append(
                    module.register_forward_pre_hook(
                        input_hook, with_kwargs=True, prepend=True
                    )
                )

            elif hook_type == "output":

                def output_hook(module, input, output, module_path=module_path):
                    return self.output_hook(output, module_path, module)

                self.handles.append(
                    module.register_forward_hook(output_hook, prepend=True)
                )

        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:

        # Remove all hooks
        for handle in self.handles:
            handle.remove()

        if isinstance(exc_val, Exception):
            raise exc_val






---
File: /src/nnsight/modeling/mixins/__init__.py
---

from .remoteable import RemoteableMixin
from .loadable import LoadableMixin
from .meta import MetaMixin


---
File: /src/nnsight/modeling/mixins/loadable.py
---

from typing import Dict, Optional

import torch

from ...intervention import NNsight


class LoadableMixin(NNsight):

    def __init__(self, *args, rename: Optional[Dict[str,str]] = None, **kwargs) -> None:

        if not isinstance(args[0], torch.nn.Module):

            model = self._load(*args, **kwargs)

        else:

            model = args[0]

        super().__init__(model, rename=rename)

    def _load(self, *args, **kwargs) -> torch.nn.Module:

        raise NotImplementedError()



---
File: /src/nnsight/modeling/mixins/meta.py
---

from typing import Dict, Optional

import torch
from accelerate import init_empty_weights

from ...intervention import NNsight
from .loadable import LoadableMixin


class MetaMixin(LoadableMixin):

    def __init__(
        self, *args, dispatch: bool = False, meta_buffers: bool = True, rename: Optional[Dict[str,str]] = None, **kwargs
    ) -> None:

        self.dispatched = False
        
        if isinstance(args[0], torch.nn.Module) or dispatch:
            
            self.dispatched = True

            super().__init__(*args, rename=rename, **kwargs)

        else:

            with init_empty_weights(include_buffers=meta_buffers):

                model = self._load_meta(*args, **kwargs)

            NNsight.__init__(self, model, rename=rename)

        self.args = args
        self.kwargs = kwargs

    def _load_meta(self, *args, **kwargs) -> torch.nn.Module:

        raise NotImplementedError()

    def dispatch(self) -> None:

        model = self._load(*self.args, **self.kwargs)
        self._envoy._update(model)
        self._model = model

        self.dispatched = True

    def interleave(self, *args, **kwargs):

        if not self.dispatched:
            self.dispatch()

        return super().interleave(*args, **kwargs)



---
File: /src/nnsight/modeling/mixins/remoteable.py
---

from typing import Any, Dict, Union

from typing_extensions import Self

from nnsight.intervention.contexts import Session

from ...intervention.backends import RemoteBackend
from ...tracing.backends import Backend
from ...util import from_import_path, to_import_path
from .meta import MetaMixin


class RemoteableMixin(MetaMixin):

    def trace(
        self,
        *inputs: Any,
        method: Union[str, None] = None,
        backend: Union[Backend, str, None] = None,
        remote: bool = False,
        blocking: bool = True,
        trace: bool = True,
        scan: bool = False,
        **kwargs: Dict[str, Any],
    ):

        if backend is not None:
            pass
        elif self._session is not None:
            pass
        elif remote:
            backend = RemoteBackend(self.to_model_key(), blocking=blocking)
        # If backend is a string, assume RemoteBackend url.
        elif isinstance(backend, str):
            backend = RemoteBackend(
                self.to_model_key(), host=backend, blocking=blocking
            )
        return super().trace(
            *inputs,
            method=method,
            backend=backend,
            trace=trace,
            scan=scan,
            **kwargs,
        )

    def session(
        self,
        backend: Union[Backend, str] = None,
        remote: bool = False,
        blocking: bool = True,
        **kwargs,
    ) -> Session:

        if backend is not None:
            pass
        elif remote:
            backend = RemoteBackend(self.to_model_key(), blocking=blocking)
        # If backend is a string, assume RemoteBackend url.
        elif isinstance(backend, str):
            backend = RemoteBackend(
                self.to_model_key(), host=backend, blocking=blocking
            )

        return super().session(backend=backend, **kwargs)

    def _remoteable_model_key(self) -> str:

        raise NotImplementedError()

    @classmethod
    def _remoteable_from_model_key(cls, model_key: str) -> Self:
        raise NotImplementedError()

    def to_model_key(self) -> str:

        return f"{to_import_path(type(self))}:{self._remoteable_model_key()}"

    @classmethod
    def from_model_key(cls, model_key: str, **kwargs) -> Self:

        import_path, model_key = model_key.split(":", 1)

        type: RemoteableMixin = from_import_path(import_path)

        return type._remoteable_from_model_key(model_key, **kwargs)



---
File: /src/nnsight/modeling/vllm/executors/__init__.py
---




---
File: /src/nnsight/modeling/vllm/executors/GPUExecutor.py
---


from vllm.executor.gpu_executor import GPUExecutor


class NNsightGPUExecutor(GPUExecutor):


    def _get_worker_module_and_class(self):
        return ("nnsight.modeling.vllm.workers.GPUWorker", "NNsightGPUWorker", None)



---
File: /src/nnsight/modeling/vllm/executors/RayGPUExecutor.py
---

from vllm.executor.ray_gpu_executor import RayGPUExecutor

class NNsightRayGPUExecutor(RayGPUExecutor):

    def _get_worker_module_and_class(self):
        return ("nnsight.modeling.vllm.workers.GPUWorker", "NNsightGPUWorker", None)


---
File: /src/nnsight/modeling/vllm/model_runners/__init__.py
---




---
File: /src/nnsight/modeling/vllm/model_runners/GPUModelRunner.py
---

import dataclasses
from functools import wraps
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Type, Union, Callable

from nnsight import NNsight

import torch
import torch.distributed

from nnsight.intervention import NNsight
from vllm.distributed import (get_pp_group,
                              get_tensor_model_parallel_rank,
                              get_tensor_model_parallel_world_size,
                              split_tensor_along_last_dim,
                              tensor_model_parallel_all_gather)
from vllm.forward_context import set_forward_context
from vllm.model_executor.layers.sampler import SamplerOutput
from vllm.model_executor.layers.linear import (ColumnParallelLinear,
                                               RowParallelLinear)
from vllm.multimodal import MultiModalInputs
from vllm.sequence import IntermediateTensors, SequenceGroupMetadata
from vllm.worker.model_runner import (ModelInputForGPUWithSamplingMetadata,
                                      ModelRunner)
from vllm.worker.model_runner_base import (
    _add_attn_metadata_broadcastable_dict,
    _init_attn_metadata_from_tensor_dict, dump_input_when_exception)

from ....intervention.protocols import InterventionProtocol
from ....util import Patch, Patcher

from ....intervention.interleaver import Interleaver

from .. import VLLM
from ..sampling import NNsightSamplingMetadata

if TYPE_CHECKING:
    from vllm.attention.backends.abstract import AttentionBackend

    from ..sampling import NNsightSamplingMetadata


class NNsightModelInputForGPUWithSamplingMetadata(ModelInputForGPUWithSamplingMetadata):

    sampling_metadata: Optional["NNsightSamplingMetadata"] = None

    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
        tensor_dict = {
            "input_tokens": self.input_tokens,
            "input_positions": self.input_positions,
            "lora_requests": self.lora_requests,
            "lora_mapping": self.lora_mapping,
            "multi_modal_kwargs": self.multi_modal_kwargs,
            "prompt_adapter_mapping": self.prompt_adapter_mapping,
            "prompt_adapter_requests": self.prompt_adapter_requests,
            "virtual_engine": self.virtual_engine,
            "request_ids_to_seq_ids": self.request_ids_to_seq_ids,
            "finished_requests_ids": self.finished_requests_ids,
        }
        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
        if self.sampling_metadata is not None:
            tensor_dict["selected_token_indices"] = (
                self.sampling_metadata.selected_token_indices)
            tensor_dict["intervention_graph"] = self.sampling_metadata.intervention_graph.copy()
            tensor_dict["batch_group"] = self.sampling_metadata.batch_groups
        return tensor_dict
    
    @classmethod
    def from_broadcasted_tensor_dict(
        cls,
        tensor_dict: Dict[str, Any],
        attn_backend: Optional["AttentionBackend"] = None,
    ) -> "ModelInputForGPUWithSamplingMetadata":
        selected_token_indices = tensor_dict.pop("selected_token_indices", None)
        intervention_graph = tensor_dict.pop("intervention_graph", None)
        intervention_graph.attachments = dict()
        batch_groups = tensor_dict.pop("batch_group", None)
        if selected_token_indices is not None:
            tensor_dict["sampling_metadata"] = NNsightSamplingMetadata(
                seq_groups=None,
                selected_token_indices=selected_token_indices,
                categorized_sample_indices=None,
                num_prompts=0,
                intervention_graph=intervention_graph,
                batch_groups=batch_groups
            )
        if attn_backend is not None:
            tensor_dict = _init_attn_metadata_from_tensor_dict(
                attn_backend, tensor_dict)
        return cls(**tensor_dict)


class NNsightGPUModelRunner(ModelRunner):

    _model_input_cls: Type[NNsightModelInputForGPUWithSamplingMetadata] = (
        NNsightModelInputForGPUWithSamplingMetadata
    )

    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)

        self.model: VLLM

    def load_model(self) -> None:
        super().load_model()

        self.model = VLLM(self.model)

    def make_model_input_from_broadcasted_tensor_dict(
        self,
        tensor_dict: Dict[str, Any],
    ) -> NNsightModelInputForGPUWithSamplingMetadata:
        model_input = \
            NNsightModelInputForGPUWithSamplingMetadata.from_broadcasted_tensor_dict(
                tensor_dict,
                attn_backend=self.attn_backend,
            )
        return model_input

    def prepare_model_input(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
        virtual_engine: int = 0,
        finished_requests_ids: Optional[List[str]] = None,
    ) -> NNsightModelInputForGPUWithSamplingMetadata:
        """Prepare the model input based on a given sequence group, including
        metadata for the sampling step.

        The API assumes seq_group_metadata_list is sorted by prefill -> decode.

        The result tensors and data structure also batches input in prefill
        -> decode order. For example,

        - input_tokens[:num_prefill_tokens] contains prefill tokens.
        - input_tokens[num_prefill_tokens:] contains decode tokens.

        If cuda graph is required, this API automatically pads inputs.
        """
        model_input = self._prepare_model_input_tensors(
            seq_group_metadata_list, finished_requests_ids
        )
        if get_pp_group().is_last_rank:
            # Sampling metadata is only required for the final pp group
            generators = self.get_generators(finished_requests_ids)
            sampling_metadata = NNsightSamplingMetadata.prepare(
                seq_group_metadata_list,
                model_input.seq_lens,
                model_input.query_lens,
                self.device,
                self.pin_memory,
                generators,
                self.sampling_metadata_cache,
            )
        else:
            sampling_metadata = None
        is_prompt = (
            seq_group_metadata_list[0].is_prompt if seq_group_metadata_list else None
        )
        return dataclasses.replace(
            model_input,
            sampling_metadata=sampling_metadata,
            is_prompt=is_prompt,
            virtual_engine=virtual_engine,
        )

    @torch.inference_mode()
    @dump_input_when_exception(exclude_args=[0], exclude_kwargs=["self"])
    def execute_model(
        self,
        model_input: NNsightModelInputForGPUWithSamplingMetadata,
        kv_caches: List[torch.Tensor],
        intermediate_tensors: Optional[IntermediateTensors] = None,
        num_steps: int = 1,
    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:

        if model_input.sampling_metadata.intervention_graph is None:

            return super().execute_model(
                model_input,
                kv_caches,
                intermediate_tensors=intermediate_tensors,
                num_steps=num_steps,
            )

        if num_steps > 1:
            raise ValueError("num_steps > 1 is not supported in ModelRunner")

        if self.lora_config:
            assert model_input.lora_requests is not None
            assert model_input.lora_mapping is not None
            self.set_active_loras(model_input.lora_requests, model_input.lora_mapping)

        if self.prompt_adapter_config:
            assert model_input.prompt_adapter_requests is not None
            assert model_input.prompt_adapter_mapping is not None
            self.set_active_prompt_adapters(
                model_input.prompt_adapter_requests, model_input.prompt_adapter_mapping
            )

        self.attn_state.begin_forward(model_input)

        # Currently cuda graph is only supported by the decode phase.
        assert model_input.attn_metadata is not None
        prefill_meta = model_input.attn_metadata.prefill_metadata
        decode_meta = model_input.attn_metadata.decode_metadata
        # TODO(andoorve): We can remove this once all
        # virtual engines share the same kv cache.
        virtual_engine = model_input.virtual_engine
        if prefill_meta is None and decode_meta.use_cuda_graph:
            assert model_input.input_tokens is not None
            graph_batch_size = model_input.input_tokens.shape[0]
            model_executable = self.graph_runners[virtual_engine][graph_batch_size]
        else:
            model_executable = self.model

        multi_modal_kwargs = model_input.multi_modal_kwargs or {}
        seqlen_agnostic_kwargs = (
            {
                "finished_requests_ids": model_input.finished_requests_ids,
                "request_ids_to_seq_ids": model_input.request_ids_to_seq_ids,
            }
            if self.has_inner_state
            else {}
        )
        if (
            self.observability_config is not None
            and self.observability_config.collect_model_forward_time
        ):
            model_forward_start = torch.cuda.Event(enable_timing=True)
            model_forward_end = torch.cuda.Event(enable_timing=True)
            model_forward_start.record()

        ## NNSIGHT #########################################

        intervention_graph = model_input.sampling_metadata.intervention_graph
        
        intervention_graph.set(self.model)

        batch_groups = model_input.sampling_metadata.batch_groups

        interleaver = Interleaver(
            intervention_graph, batch_groups=batch_groups, batch_size=len(model_input.input_tokens)
        )

        def inner():

            with set_forward_context(model_input.attn_metadata):
                hidden_or_intermediate_states = self.model._model(
                    input_ids=model_input.input_tokens,
                    positions=model_input.input_positions,
                    kv_caches=kv_caches,
                    attn_metadata=model_input.attn_metadata,
                    intermediate_tensors=intermediate_tensors,
                    **MultiModalInputs.as_kwargs(
                        multi_modal_kwargs, device=self.device
                    ),
                    **seqlen_agnostic_kwargs,
                )

            if (
                self.observability_config is not None
                and self.observability_config.collect_model_forward_time
            ):
                model_forward_end.record()

            # Compute the logits in the last pipeline stage.
            if not get_pp_group().is_last_rank:
                if (
                    self.is_driver_worker
                    and hidden_or_intermediate_states is not None
                    and isinstance(hidden_or_intermediate_states, IntermediateTensors)
                    and self.observability_config is not None
                    and self.observability_config.collect_model_forward_time
                ):
                    model_forward_end.synchronize()
                    model_forward_time = model_forward_start.elapsed_time(
                        model_forward_end
                    )
                    orig_model_forward_time = 0.0
                    if intermediate_tensors is not None:
                        orig_model_forward_time = intermediate_tensors.tensors.get(
                            "model_forward_time", torch.tensor(0.0)
                        ).item()
                    hidden_or_intermediate_states.tensors["model_forward_time"] = (
                        torch.tensor(model_forward_time + orig_model_forward_time)
                    )
                return hidden_or_intermediate_states

            logits = self.model.compute_logits(
                hidden_or_intermediate_states, model_input.sampling_metadata
            )

            # patching the batch_size to be the number of logits,
            # since vLLM optimizes the inference by turning the size of the input to be of size power of 2.
            patches = [Patch(interleaver, logits.shape[0], "batch_size")]

            # `batch_groups` is adapted to the token positions of the flattened input during the first token generation iteration
            # since the logit and sample tensors have different number of tokens, 
            # we need to patch `batch_groups` to reflect the correct batches specified by the invoker contexts defined by the user.
            if model_input.sampling_metadata.seq_groups[0].is_prompt:
                patches.append(Patch(interleaver, model_input.sampling_metadata.nns_batch_groups, "batch_groups"))

            with Patcher(patches):
                logits = self.model.logits(logits)

            if not self.is_driver_worker:
                return []

            if model_input.async_callback is not None:
                model_input.async_callback()

            # Sample the next token.
            output: SamplerOutput = self.model.sample(
                logits=logits,
                sampling_metadata=model_input.sampling_metadata,
            )

            og_sample_tokens = torch.tensor([token.samples[0].output_token for token in output.outputs])

            with Patcher(patches):
                sample_tokens = self.model.samples(og_sample_tokens)

            # inject any changes to the sampled tokens
            for idx, seq_out in enumerate(output.outputs):
                sample = seq_out.samples[0]
                sample.output_token = sample_tokens[idx].item()
                logprob = sample.logprobs.pop(og_sample_tokens[idx].item())
                sample.logprobs[sample_tokens[idx].item()] = logprob
            
            if (
                self.observability_config is not None
                and self.observability_config.collect_model_forward_time
                and output is not None
            ):
                model_forward_end.synchronize()
                model_forward_time = model_forward_start.elapsed_time(model_forward_end)
                orig_model_forward_time = 0.0
                if intermediate_tensors is not None:
                    orig_model_forward_time = intermediate_tensors.tensors.get(
                        "model_forward_time", torch.tensor(0.0)
                    ).item()
                # If there are multiple workers, we are still tracking the latency
                # from the start time of the driver worker to the end time of the
                # driver worker. The model forward time will then end up covering
                # the communication time as well.
                output.model_forward_time = orig_model_forward_time + model_forward_time

            if self.return_hidden_states:
                # we only need to pass hidden states of most recent token
                assert model_input.sampling_metadata is not None
                indices = model_input.sampling_metadata.selected_token_indices
                if model_input.is_prompt:
                    hidden_states = hidden_or_intermediate_states.index_select(
                        0, indices
                    )
                    output.prefill_hidden_states = hidden_or_intermediate_states
                elif decode_meta.use_cuda_graph:
                    hidden_states = hidden_or_intermediate_states[: len(indices)]
                else:
                    hidden_states = hidden_or_intermediate_states

                output.hidden_states = hidden_states

            return output
        
        def parallel_intervene(intervene_func: Callable) -> Callable:
            """ Create an intervene wrapper that handles tensor parallelism execution of vLLM models.

            Args:
                intervene_func (Callable): intervention function.
            
            Returns 
            """

            @wraps(intervene_func)
            def parallel_intervene_wrapper(
                activations: Any, 
                module_path: str, 
                module: torch.nn.Module, 
                key: str, 
                interleaver: Interleaver
            ) -> Any:
                """ InterventionProtocol.intervene wrapper handling the parallelized modules of vLLM.
                If some activations were parallelized, then they need to be gathered as a full tensor to intervene on them,
                and then split again before returning them.

                Args:
                    activations (Any): Either the inputs or outputs of a torch module.
                    module_path (str): Module path of the current relevant module relative to the root model.
                    module (torch.nn.Module): Module to be intervened on.
                    key (str): Key denoting either "input" or "output" of module.
                    interleaver (Interleaver): Handler object that stores the intervention graph and keeps track of module call count.

                Returns:
                    Any: The activations, potentially modified by the intervention graph.
                """
                # If the activations are parallelized, they must be gathered before intervening on them
                if isinstance(module, ColumnParallelLinear) and key == "output" and not module.gather_output:
                    full_tensor = tensor_model_parallel_all_gather(activations[0])
                    activations = (full_tensor, ) + activations[1:]
                if isinstance(module, RowParallelLinear) and key == "input" and module.input_is_parallel:
                    full_tensor = tensor_model_parallel_all_gather(activations[0][0])
                    activations = ((full_tensor,) + activations[0][1:], ) + activations[1:]

                activations = intervene_func(activations, module_path, module, key, interleaver)

                # If the activations were parallelized originally, they must be split again before returning them
                if isinstance(module, ColumnParallelLinear) and key == "output" and not module.gather_output:
                    tp_rank = get_tensor_model_parallel_rank()
                    splitted_input = split_tensor_along_last_dim(activations[0], num_partitions=get_tensor_model_parallel_world_size())
                    activations = (splitted_input[tp_rank].contiguous(),) + activations[1:]
                if isinstance(module, RowParallelLinear) and key == "input" and module.input_is_parallel:
                    tp_rank = get_tensor_model_parallel_rank()
                    splitted_input = split_tensor_along_last_dim(activations[0][0], num_partitions=get_tensor_model_parallel_world_size())
                    activations = ((splitted_input[tp_rank].contiguous(),) + activations[0][1:],) + activations[1:]

                return activations
            
            return parallel_intervene_wrapper

        if get_tensor_model_parallel_world_size() > 1:
            intervene_patch = Patch(InterventionProtocol, parallel_intervene(InterventionProtocol.intervene), "intervene")
        else:
            intervene_patch = Patch(InterventionProtocol, InterventionProtocol.intervene, "intervene")

        with Patcher([intervene_patch]):
            output = NNsight.interleave(
                self.model,
                fn=inner,
                interleaver=interleaver,
            )
 
        return [output]



---
File: /src/nnsight/modeling/vllm/workers/__init__.py
---




---
File: /src/nnsight/modeling/vllm/workers/GPUWorker.py
---

from vllm.worker.worker import Worker

from ..model_runners.GPUModelRunner import NNsightGPUModelRunner


class NNsightGPUWorker(Worker):

    def __init__(self, *args, **kwargs):

        super().__init__(*args, model_runner_cls=NNsightGPUModelRunner, **kwargs)



---
File: /src/nnsight/modeling/vllm/__init__.py
---

from .vllm import VLLM



---
File: /src/nnsight/modeling/vllm/sampling.py
---

import copy
from typing import Dict, List, Optional, Tuple

from nnsight.intervention.graph import InterventionGraph
import torch

from vllm.model_executor.sampling_metadata import (
    SamplingMetadata,
    SamplingMetadataCache,
    _prepare_seq_groups,
)
from vllm.sampling_params import SamplingParams
from vllm.sequence import SequenceGroupMetadata
from vllm.utils import async_tensor_h2d


class NNsightSamplingParams(SamplingParams):

    intervention_graph: Optional[InterventionGraph] = None
    nns_batch_groups: Optional[List[Tuple[int, int]]] = None
    invoker_group: Optional[int] = None
    is_default_param: bool = True

    def clone(self) -> "SamplingParams":
        """Deep copy excluding LogitsProcessor objects.

        LogitsProcessor objects are excluded because they may contain an
        arbitrary, nontrivial amount of data.
        See https://github.com/vllm-project/vllm/issues/3087
        """

        memo = {}

        if self.logits_processors is not None:
            for lp in self.logits_processors:
                memo[id(lp)] = lp

        if self.intervention_graph is not None:
            memo[id(self.intervention_graph)] = self.intervention_graph

        return copy.deepcopy(self, memo=memo)


class NNsightSamplingMetadata(SamplingMetadata):

    intervention_graph: Optional[InterventionGraph] = None
    nns_batch_groups: Optional[List[Tuple[int, int]]] = None
    batch_groups: Optional[List[Tuple[int, int]]] = None

    def __init__(
        self,
        *args,
        intervention_graph: InterventionGraph = None,
        nns_batch_groups: List[Tuple[int, int]] = None,
        batch_groups: Dict[int, Tuple[int, int]] = None,
        **kwargs,
    ):

        super().__init__(*args, **kwargs)

        self.intervention_graph = intervention_graph
        self.nns_batch_groups = nns_batch_groups
        self.batch_groups = batch_groups

    @staticmethod
    def prepare(
        seq_group_metadata_list: List[SequenceGroupMetadata],
        seq_lens: List[int],
        query_lens: List[int],
        device: str,
        pin_memory: bool,
        generators: Optional[Dict[str, torch.Generator]] = None,
        cache: Optional[SamplingMetadataCache] = None,
    ) -> "SamplingMetadata":
        (
            seq_groups,
            selected_token_indices,
            categorized_sample_indices,
            num_prompts,
        ) = _prepare_seq_groups(
            seq_group_metadata_list, seq_lens, query_lens, device, generators, cache
        )
        selected_token_indices = async_tensor_h2d(
            selected_token_indices,
            dtype=torch.long,
            target_device=device,
            pin_memory=pin_memory,
        )
        categorized_sample_indices = {
            t: async_tensor_h2d(
                seq_ids,
                dtype=torch.int,
                target_device=device,
                pin_memory=pin_memory,
            )
            for t, seq_ids in categorized_sample_indices.items()
        }
        
        
        ### NNSIGHT ###########################################

        intervention_graphs = []
        nns_batch_groups = []
        batch_groups = []
        batch_groups_offset = 0

        for idx, seq_group in enumerate(seq_group_metadata_list):

            if isinstance(seq_group.sampling_params, NNsightSamplingParams):

                seq_group_intervention_graph = (
                    seq_group.sampling_params.intervention_graph
                )

                seq_group_nns_batch_groups = seq_group.sampling_params.nns_batch_groups
                
                if isinstance(seq_group_intervention_graph, InterventionGraph):
                    
                    if seq_group_intervention_graph not in intervention_graphs:
                    
                        intervention_graphs.append(seq_group_intervention_graph)

                        nns_batch_groups.append(seq_group_nns_batch_groups)

                        batch_groups_offset = len(batch_groups)

                    seq_group_batch_group = (
                        seq_group.sampling_params.invoker_group + batch_groups_offset
                    )

                    batch_size = query_lens[idx]
            
                    if seq_group_batch_group >= len(batch_groups):
                        batch_start = sum(batch_groups[-1]) if len(batch_groups) > 0 else 0
                        batch_groups.append((batch_start, batch_size))
                    else:
                        batch_start, seq_group_batch_size = batch_groups[
                            seq_group_batch_group
                        ]
                        batch_size += seq_group_batch_size

                        batch_groups[seq_group_batch_group] = (batch_start, batch_size)
                    
        n_graphs = len(intervention_graphs)
        
        if n_graphs== 0:
            intervention_graph = None
            nns_batch_groups = None
        elif n_graphs == 1:
            intervention_graph =intervention_graphs[0]
            nns_batch_groups = nns_batch_groups[0]

        """ else:
            intervention_graph = MultiGraph(intervention_graphs.values())
            
            InterventionProtocol.shift(intervention_graph) """

        ###########################################

        sampling_metadata = NNsightSamplingMetadata(
            seq_groups=seq_groups,
            selected_token_indices=selected_token_indices,
            categorized_sample_indices=categorized_sample_indices,
            num_prompts=num_prompts,
            intervention_graph=intervention_graph,
            nns_batch_groups = nns_batch_groups,
            batch_groups=batch_groups,
        )

        return sampling_metadata



---
File: /src/nnsight/modeling/vllm/vllm.py
---

from typing import TYPE_CHECKING, Any, Callable, Dict, List, Tuple, Union, Optional

from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs

from ...util import WrapperModule
from ..mixins import RemoteableMixin
from .executors.GPUExecutor import NNsightGPUExecutor
from .executors.RayGPUExecutor import NNsightRayGPUExecutor
from .sampling import NNsightSamplingParams
from dataclasses import fields
from ...intervention.interleaver import Interleaver
from ...intervention import Envoy

if TYPE_CHECKING:
    from ...intervention.graph import InterventionGraph
    from torch.nn import Module
    from vllm.transformers_utils.tokenizer import AnyTokenizer
    from vllm.config import ModelConfig, SchedulerConfig, ParallelConfig

try:
    from vllm.distributed import (destroy_distributed_environment,
                                  destroy_model_parallel,
                                  init_distributed_environment,
                                  initialize_model_parallel)
    from vllm.engine.arg_utils import EngineArgs
    from vllm.entrypoints.llm import LLM
    from vllm.model_executor.model_loader.loader import _initialize_model
except Exception as e:
    raise type(e)(
        "Install vllm in your environment to use it with NNsight. "
        + "https://docs.vllm.ai/en/latest/getting_started/installation.html"
    ) from e



class VLLM(RemoteableMixin):
    """NNsight wrapper to conduct interventions on a vLLM inference engine.\
    
    Attributes:
        - vllm_entrypoint (vllm.LLM): vLLM language model.
        - tokenizer (vllm.transformers_utils.tokenizer.AnyTokenizer): tokenizer.
        - logits (nnsight.WrapperModule): logits.
        - samples (nnsight.WrapperModule): sampled tokens.

    .. code-block:: python
        from nnsight.models.VLLM import VLLM
        from vllm import SamplingParams

        model = VLLM("gpt2")

        prompt = ["The Eiffel Tower is in the city of"]

        with model.trace(prompt, temperature=0.0, top_p=0.95, stop=['.']) as tracer:
            model.transformer.h[8].output[-1][:] = 0

            output = model.output.save()

        print(model.tokenizer.decode(output.value.argmax(dim=-1)[-1]))
    """

    __methods__ = {"generate": "_execute"}

    def __init__(self, *args, **kwargs) -> None:

        self.vllm_entrypoint: LLM = None
        self.tokenizer: "AnyTokenizer" = None

        super().__init__(*args, **kwargs)

        self.logits: Envoy = WrapperModule()
        self.samples: Envoy = WrapperModule()

    def _load_meta(self, repo_id: str, **kwargs) -> "Module":

        # no parallelism during initialization
        kwargs["tensor_parallel_size"] = 1
        kwargs["pipeline_parallel_size"] = 1

        # creating vLLM Engine args
        engine_args = EngineArgs(
            model=repo_id,
            **kwargs,
        )

        # creating the vllm engine configuration
        vllm_config = engine_args.create_engine_config()
        vllm_config_dict = {field.name: getattr(vllm_config, field.name) for field in fields(type(vllm_config))}

        # starting the distributed environment
        init_distributed_environment(
            1,
            0,
            "tcp://127.0.0.1:47303",
            0,
            backend="gloo",
        )

        # start tensor parallel group
        initialize_model_parallel(backend="gloo")

        # initialize the model

        model = _initialize_model(vllm_config)

        # load the tokenzier
        self.tokenizer = self._load_tokenizer(
            model_config=vllm_config_dict["model_config"],
            scheduler_config=vllm_config_dict["scheduler_config"],
            parallel_config=vllm_config_dict["parallel_config"],
            enable_lora=bool(vllm_config_dict["lora_config"]),
        )

        return model
    
    def _load_tokenizer(
        self, 
        model_config: "ModelConfig", 
        scheduler_config: "SchedulerConfig", 
        parallel_config: "ParallelConfig", 
        enable_lora: bool) -> "AnyTokenizer":
        
        return init_tokenizer_from_configs(
            model_config=model_config,
            scheduler_config=scheduler_config,
            parallel_config=parallel_config,
            enable_lora=enable_lora,
        ).tokenizer

    def _load(self, repo_id: str, **kwargs) -> "Module":

        destroy_model_parallel()
        destroy_distributed_environment()

        distributed_executor_backend = NNsightGPUExecutor
        if (
            "tensor_parallel_size" in kwargs.keys()
            and kwargs["tensor_parallel_size"] > 1
        ):
            distributed_executor_backend = NNsightRayGPUExecutor

        llm = LLM(
            repo_id,
            **kwargs,
            distributed_executor_backend=distributed_executor_backend,
        )

        self.vllm_entrypoint = llm

        # load the tokenizer
        self.tokenizer = self._load_tokenizer(
            model_config=llm.llm_engine.model_config,
            scheduler_config=llm.llm_engine.scheduler_config,
            parallel_config=llm.llm_engine.parallel_config,
            enable_lora=bool(llm.llm_engine.lora_config),
        )

        if kwargs.get("tensor_parallel_size", 1) > 1:
            return llm.llm_engine.model_executor.driver_worker.worker.model_runner.model
        else:
            return llm.llm_engine.model_executor.driver_worker.model_runner.model

    def _prepare_input(
        self, *args, **kwargs
    ) -> Tuple[Tuple[Tuple[Any], Dict[str, Any]], int]:

        if "processed" in kwargs:
            return (args, kwargs), len(args[0])

        prompts = []
        params = []

        for arg in args:

            if not type(arg) is list:
                arg = [arg]

            for prompt in arg:

                param = NNsightSamplingParams(
                    **kwargs,
                )

                if kwargs != {}:
                    param.is_default_param = False

                prompts.append(prompt)
                params.append(param)

        return ((prompts, params), {"processed": True}), len(prompts)

    def _batch(
        self,
        batched_inputs: Union[Tuple[Tuple[Any], Dict[str, Any]], None],
        prompts: List[str],
        params: List[NNsightSamplingParams],
        **kwargs,
    ) -> Tuple[Union[Tuple[Any], Dict[str, Any]]]:

        if batched_inputs is None:
            batched_inputs = ([], []), {"invoker_group": 0}

        (bprompts, bparams), kwargs = batched_inputs

        invoker_group = kwargs["invoker_group"]

        for prompt in prompts:
            bprompts.append(prompt)

        for param in params:

            param.invoker_group = invoker_group

            bparams.append(param)

        kwargs["invoker_group"] += 1

        return (bprompts, bparams), kwargs

    def interleave(
        self,
        interleaver: Interleaver,
        *args,
        fn: Optional[Union[Callable, str]] = None,
        **kwargs,
    ) -> Any:

        """ if not self.dispatched:
            self.dispatch()

        for param in params:

            param.intervention_graph = intervention_graph

        fn(prompts, params, **kwargs)

        intervention_graph.alive = False """
        
        
        if not self.dispatched:
            self.dispatch()

        for param in args[1]:

            param.intervention_graph = interleaver.graph
            param.nns_batch_groups = interleaver.batch_groups

        if fn is None:
            fn = self._execute
        elif isinstance(fn, str):
            fn = getattr(self, fn)            
    
        return fn(*args, **kwargs)

    def _execute(
        self,
        prompts: List[str],
        params: List[NNsightSamplingParams],
        **kwargs,
    ) -> Any:

        kwargs.pop('invoker_group')

        for param in params:
            if param.is_default_param:
                for attr, value in kwargs.items():
                    if hasattr(NNsightSamplingParams, attr):
                        setattr(param, attr, value)

        self.vllm_entrypoint.generate(prompts, sampling_params=params)

if TYPE_CHECKING:
    
    class VLLM(VLLM,LLM):
        pass



---
File: /src/nnsight/modeling/__init__.py
---




---
File: /src/nnsight/modeling/diffusion.py
---

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union

import torch
from diffusers import DiffusionPipeline
from transformers import BatchEncoding
from typing_extensions import Self
from transformers import PreTrainedTokenizerBase
from ..intervention.contexts import InterventionTracer

from .. import util
from .mixins import RemoteableMixin


class Diffuser(util.WrapperModule):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__()

        self.pipeline = DiffusionPipeline.from_pretrained(*args, **kwargs)
        
        for key, value in self.pipeline.__dict__.items():
            if isinstance(value, torch.nn.Module) or isinstance(value, PreTrainedTokenizerBase):
                setattr(self, key, value)


class DiffusionModel(RemoteableMixin):
    
    __methods__ = {"generate": "_generate"}

    def __init__(self, *args, **kwargs) -> None:

        self._model: Diffuser = None

        super().__init__(*args, **kwargs)
        
    def _load_meta(self, repo_id:str, **kwargs):
        
        
        model = Diffuser(
            repo_id,
            device_map=None,
            low_cpu_mem_usage=False,
            **kwargs,
        )

        return model
        

    def _load(self, repo_id: str, device_map=None, **kwargs) -> Diffuser:

        model = Diffuser(repo_id, device_map=device_map, **kwargs)

        return model

    def _prepare_input(
        self,
        inputs: Union[str, List[str]],
    ) -> Any:

        if isinstance(inputs, str):
            inputs = [inputs]

        return ((inputs,), {}), len(inputs)

    def _batch(
        self,
        batched_inputs: Optional[Dict[str, Any]],
        prepared_inputs: BatchEncoding,
    ) -> torch.Tensor:

        if batched_inputs is None:

            return ((prepared_inputs, ), {})

        return (batched_inputs + prepared_inputs, )

    def _execute(self, prepared_inputs: Any, *args, **kwargs):

        return self._model.unet(
            prepared_inputs,
            *args,
            **kwargs,
        )

    def _generate(
        self, prepared_inputs: Any, *args, seed: int = None, **kwargs
    ):

        if self._scanning():

            kwargs["num_inference_steps"] = 1

        generator = torch.Generator(self.device)

        if seed is not None:

            if isinstance(prepared_inputs, list) and len(prepared_inputs) > 1:
                generator = [torch.Generator(self.device).manual_seed(seed + offset) for offset in range(len(prepared_inputs) * kwargs.get('num_images_per_prompt', 1))]
            else:
                generator = generator.manual_seed(seed)
            
        output = self._model.pipeline(
            prepared_inputs, *args, generator=generator, **kwargs
        )

        output = self._model(output)

        return output


if TYPE_CHECKING:

    class DiffusionModel(DiffusionModel, DiffusionPipeline):

        def generate(self, *args, **kwargs) -> InterventionTracer:
            return self._model.pipeline(*args, **kwargs)




---
File: /src/nnsight/modeling/language.py
---

from __future__ import annotations

import json
import warnings
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generic,
    List,
    Optional,
    Protocol,
    Tuple,
    Type,
    Union,
)

import torch
from torch.nn.modules import Module
from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForCausalLM,
    AutoTokenizer,
    BatchEncoding,
    PretrainedConfig,
    PreTrainedModel,
    PreTrainedTokenizer,
)
from transformers.models.auto import modeling_auto
from transformers.models.llama.configuration_llama import LlamaConfig
from typing_extensions import Self

from ..intervention import Envoy
from ..intervention.contexts import InterleavingTracer
from ..intervention.graph import InterventionNodeType, InterventionProxyType
from ..util import WrapperModule
from .mixins import RemoteableMixin


class LanguageModel(RemoteableMixin):
    """LanguageModels are NNsight wrappers around transformers language models.

    Inputs can be in the form of:
        Prompt: (str)
        Prompts: (List[str])
        Batched prompts: (List[List[str]])
        Tokenized prompt: (Union[List[int], torch.Tensor])
        Tokenized prompts: (Union[List[List[int]], torch.Tensor])
        Direct input: (Dict[str,Any])

    If using a custom model, you also need to provide the tokenizer like ``LanguageModel(custom_model, tokenizer=tokenizer)``

    Calls to generate pass arguments downstream to :func:`GenerationMixin.generate`

    Attributes:
        config (PretrainedConfig): Huggingface config file loaded from repository or checkpoint.
        tokenizer (PreTrainedTokenizer): Tokenizer for LMs.
        automodel (Type): AutoModel type from transformer auto models.
        model (PreTrainedModel): Meta version of underlying auto model.

    """

    __methods__ = {"generate": "_generate"}

    tokenizer: PreTrainedTokenizer

    class Generator(WrapperModule):

        class Streamer(WrapperModule):

            def put(self, *args):
                return self(*args)

            def end(self):
                pass

        def __init__(self) -> None:

            super().__init__()

            self.streamer = LanguageModel.Generator.Streamer()

    def __init__(
        self,
        *args,
        config: Optional[PretrainedConfig] = None,
        tokenizer: Optional[PreTrainedTokenizer] = None,
        automodel: Type[AutoModel] = AutoModelForCausalLM,
        **kwargs,
    ) -> None:

        self.automodel = (
            automodel
            if not isinstance(automodel, str)
            else getattr(modeling_auto, automodel)
        )

        self.config = config
        self.tokenizer = tokenizer
        self.repo_id: str = None

        super().__init__(*args, **kwargs)

        self.generator: Envoy[InterventionProxyType, InterventionNodeType] = (
            LanguageModel.Generator()
        )

    def _load_config(self, repo_id: str, **kwargs):

        if self.config is None:

            self.config = AutoConfig.from_pretrained(repo_id, **kwargs)

    def _load_tokenizer(self, repo_id: str, **kwargs):

        if self.tokenizer is None:

            if "padding_side" not in kwargs:
                kwargs["padding_side"] = "left"

            self.tokenizer = AutoTokenizer.from_pretrained(
                repo_id, config=self.config, **kwargs
            )

            if getattr(self.tokenizer, "pad_token", None) is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

    def _load_meta(
        self,
        repo_id: str,
        tokenizer_kwargs: Optional[Dict[str, Any]] = {},
        patch_llama_scan: bool = True,
        **kwargs,
    ) -> Module:

        self.repo_id = repo_id

        self._load_config(repo_id, **kwargs)

        self._load_tokenizer(repo_id, **tokenizer_kwargs)

        if (
            patch_llama_scan
            and isinstance(self.config, LlamaConfig)
            and isinstance(self.config.rope_scaling, dict)
            and "rope_type" in self.config.rope_scaling
        ):
            self.config.rope_scaling["rope_type"] = "default"

        model = self.automodel.from_config(self.config, trust_remote_code=True)

        return model

    def _load(
        self,
        repo_id: str,
        tokenizer_kwargs: Optional[Dict[str, Any]] = {},
        patch_llama_scan: bool = True,
        **kwargs,
    ) -> PreTrainedModel:

        self._load_config(repo_id, **kwargs)

        self._load_tokenizer(repo_id, **tokenizer_kwargs)

        if (
            patch_llama_scan
            and isinstance(self.config, LlamaConfig)
            and isinstance(self.config.rope_scaling, dict)
            and "rope_type" in self.config.rope_scaling
        ):
            self.config.rope_scaling["rope_type"] = "llama3"

        model = self.automodel.from_pretrained(repo_id, config=self.config, **kwargs)

        return model

    def _tokenize(
        self,
        inputs: Union[
            str,
            List[str],
            List[List[str]],
            List[int],
            List[List[int]],
            torch.Tensor,
            Dict[str, Any],
        ],
        **kwargs,
    ):

        if isinstance(inputs, str) or (
            isinstance(inputs, list) and isinstance(inputs[0], int)
        ):
            inputs = [inputs]

        if isinstance(inputs, torch.Tensor) and inputs.ndim == 1:
            inputs = inputs.unsqueeze(0)

        if not isinstance(inputs[0], str):
            inputs = [{"input_ids": ids} for ids in inputs]
            return self.tokenizer.pad(inputs, return_tensors="pt", **kwargs)

        return self.tokenizer(inputs, return_tensors="pt", padding=True, **kwargs)

    def _prepare_input(
        self,
        *inputs: Tuple[
            Union[
                str,
                List[str],
                List[List[str]],
                List[int],
                List[List[int]],
                torch.Tensor,
                List[torch.Tensor],
                Dict[str, Any],
                BatchEncoding,
            ]
        ],
        input_ids: Union[
            List[int], List[List[int]], torch.Tensor, List[torch.Tensor]
        ] = None,
        labels: Any = None,
        **kwargs,
    ) -> Tuple[BatchEncoding, int]:

        if input_ids is not None:

            assert len(inputs) == 0

            inputs = (input_ids,)

        assert len(inputs) == 1

        inputs = inputs[0]

        if isinstance(inputs, dict):
            inputs = BatchEncoding(inputs)
        elif isinstance(inputs, BatchEncoding):
            pass
        else:

            inputs = self._tokenize(inputs, **kwargs)

            if labels is not None:
                labels = self._tokenize(labels, **kwargs)["input_ids"]

        return ((inputs,), {"labels": labels}), len(inputs["input_ids"])

    def _batch(
        self,
        batched_inputs: Optional[Tuple[Tuple[BatchEncoding], Dict[str, Any]]],
        input: BatchEncoding,
        labels: Optional[torch.Tensor] = None,
    ) -> Tuple[Dict[str, Any]]:

        if batched_inputs is None:
            return ((input,), {"labels": labels})

        batched_labels = batched_inputs[1]["labels"]
        batched_inputs = batched_inputs[0][0]

        attention_mask = batched_inputs["attention_mask"]
        batched_inputs = [
            {"input_ids": ids}
            for ids in [
                *batched_inputs["input_ids"].tolist(),
                *input["input_ids"].tolist(),
            ]
        ]
        batched_inputs = self.tokenizer.pad(batched_inputs, return_tensors="pt")

        if labels is not None:

            batched_labels = torch.cat((batched_labels, labels))

        if self.tokenizer.padding_side == "left":

            batched_inputs["attention_mask"][
                : attention_mask.shape[0], -attention_mask.shape[1] :
            ] = attention_mask

        else:

            batched_inputs["attention_mask"][
                : attention_mask.shape[0], : attention_mask.shape[1]
            ] = attention_mask

        return ((batched_inputs,), {"labels": batched_labels})

    def _execute(self, inputs: BatchEncoding, **kwargs) -> Any:

        inputs = inputs.to(self.device)

        return self._model(
            **inputs,
            **kwargs,
        )

    def _generate(
        self,
        inputs: BatchEncoding,
        max_new_tokens=1,
        streamer: Any = None,
        **kwargs,
    ):

        if streamer is None:
            streamer = self.generator.streamer

        inputs = inputs.to(self.device)

        output = self._model.generate(
            **inputs,
            **kwargs,
            streamer=streamer,
            max_new_tokens=max_new_tokens,
        )

        self.generator(output)

        return output

    def _remoteable_model_key(self) -> str:
        return json.dumps(
            {"repo_id": self.repo_id}  # , "torch_dtype": str(self._model.dtype)}
        )

    @classmethod
    def _remoteable_from_model_key(cls, model_key: str, **kwargs) -> Self:

        kwargs = {**json.loads(model_key), **kwargs}

        repo_id = kwargs.pop("repo_id")

        return LanguageModel(repo_id, **kwargs)


if TYPE_CHECKING:

    class LanguageModel(LanguageModel, PreTrainedModel):

        def generate(self, *args, **kwargs) -> InterleavingTracer:
            pass



---
File: /src/nnsight/schema/format/__init__.py
---

from .functions import FUNCTIONS_WHITELIST, get_function_name


---
File: /src/nnsight/schema/format/functions.py
---

import operator
from inspect import (
    getmembers,
    isbuiltin,
    isclass,
    isfunction,
    ismethod,
    ismethoddescriptor,
)
from typing import Callable, Union

import einops
import torch
from torch.utils.data.dataloader import DataLoader

from ... import util
from ...intervention import protocols as intervention_protocols
from ...tracing import protocols
from ...tracing.graph import Proxy
from ...tracing import contexts
from ...intervention import contexts as intervention_contexts

def get_function_name(fn, module_name=None):
    if isinstance(fn, str):
        return fn

    if module_name is not None:
        return f"{module_name}.{fn.__name__}"

    module_name = getattr(fn, "__module__", None)

    if module_name is None:
        return fn.__qualname__

    return f"{module_name}.{fn.__qualname__}"


def update_function(function: Union[str,  Callable], new_function: Callable):

    if not isinstance(function, str):

        function = get_function_name(function)

    new_function.__name__ = function

    FUNCTIONS_WHITELIST[function] = new_function


FUNCTIONS_WHITELIST = {}

### Torch functions
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(torch._C._VariableFunctions, isbuiltin)
    }
)
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(torch.nn.functional, isfunction)
    }
)
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(torch._C._nn, isbuiltin)
    }
)
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value, module_name="Tensor"): value
        for key, value in getmembers(
            torch.Tensor, lambda x: ismethoddescriptor(x) or isfunction(x)
        )
    }
)
FUNCTIONS_WHITELIST.update({get_function_name(torch.nn.Parameter): torch.nn.Parameter})

FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(torch.optim, isclass)
        if issubclass(value, torch.optim.Optimizer)
    }
)
FUNCTIONS_WHITELIST.update({get_function_name(DataLoader): DataLoader})
### operator functions
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(operator, isbuiltin)
        if not key.startswith("_")
    }
)
### einops functions
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(value): value
        for key, value in getmembers(einops.einops, isfunction)
    }
)

### nnsight functions
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(bool): bool,
        get_function_name(bytes): bytes,
        get_function_name(int): int,
        get_function_name(float): float,
        get_function_name(str): str,
        get_function_name(complex): complex,
        get_function_name(bytearray): bytearray,
        get_function_name(tuple): tuple,
        get_function_name(list): list,
        get_function_name(set): set,
        get_function_name(dict): dict,
        get_function_name(print): print,
        get_function_name(setattr): setattr,
        get_function_name(util.fetch_attr): util.fetch_attr,
        get_function_name(Proxy.call): Proxy.call,
    }
)

### protocols
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(protocol): protocol
        for key, protocol in getmembers(protocols)
        if isinstance(protocol, type) and issubclass(protocol, protocols.Protocol)
    }
)

FUNCTIONS_WHITELIST.update(
    {
        get_function_name(protocol): protocol
        for key, protocol in getmembers(intervention_protocols)
        if isinstance(protocol, type) and issubclass(protocol, protocols.Protocol)
    }
)
FUNCTIONS_WHITELIST.update(
    {
        get_function_name(protocol): protocol
        for key, protocol in getmembers(contexts)
        if isinstance(protocol, type) and issubclass(protocol, protocols.Protocol)
    }
)

FUNCTIONS_WHITELIST.update(
    {
        get_function_name(protocol): protocol
        for key, protocol in getmembers(intervention_contexts)
        if isinstance(protocol, type) and issubclass(protocol, protocols.Protocol)
    }
)



---
File: /src/nnsight/schema/format/types.py
---

from __future__ import annotations

import threading
import weakref
from types import BuiltinFunctionType
from types import FunctionType as FuncType
from types import MethodDescriptorType
from typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional, Tuple, Union

import torch
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,
    Strict,
    ValidationError,
    field_validator,
    model_serializer,
)
from pydantic.functional_validators import AfterValidator, BeforeValidator
from typing_extensions import Annotated, Self

from ...intervention.graph import InterventionGraph, InterventionNode
from ...tracing.graph import Graph, Node, SubGraph
from . import FUNCTIONS_WHITELIST, get_function_name

if TYPE_CHECKING:
    from ... import NNsight

FUNCTION = Union[BuiltinFunctionType, FuncType, MethodDescriptorType, type]
PRIMITIVE = Union[int, float, str, bool, None]


class DeserializeHandler:

    def __init__(
        self,
        memo,
        model: "NNsight"
    ) -> None:

        self.memo = memo
        self.model = model
        self.graph = Graph(node_class=InterventionNode)



# Thread-local storage for MEMO
_local = threading.local()


def get_memo():
    """Get the thread-local MEMO dictionary."""
    if not hasattr(_local, 'MEMO'):
        _local.MEMO = {}
    return _local.MEMO


def clear_memo():
    """Clear the thread-local MEMO dictionary."""
    if hasattr(_local, 'MEMO'):
        _local.MEMO.clear()


class BaseNNsightModel(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    type_name: Literal["TYPE_NAME"]

    @classmethod
    def to_model(cls, value: Any) -> Self:
        raise NotImplementedError()

    def deserialize(self, handler: DeserializeHandler):
        raise NotImplementedError()


def try_deserialize(value: Union[BaseNNsightModel, Any], handler: DeserializeHandler):

    if isinstance(value, BaseNNsightModel):

        return value.deserialize(handler)

    return value


def memoized(fn):

    def inner(value):

        model = fn(value)

        _id = id(value)

        get_memo()[_id] = model

        return MemoReferenceModel(id=_id)

    return inner


### Custom Pydantic types for all supported base types
class NodeModel(BaseNNsightModel):

    type_name: Literal["NODE"] = "NODE"

    target: ValueTypes
    args: List[ValueTypes] = []
    kwargs: Dict[str, ValueTypes] = {}

    @staticmethod
    @memoized
    def to_model(value: Node) -> Self:

        return NodeModel(target=value.target, args=value.args, kwargs=value.kwargs)

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):

        dump = handler(self)

        if not self.kwargs:

            dump.pop("kwargs")

        if not self.args:

            dump.pop("args")

        return dump

    def deserialize(self, handler: DeserializeHandler) -> Node:

        return handler.graph.create(
            self.target.deserialize(handler),
            *[try_deserialize(value, handler) for value in self.args],
            **{
                key: try_deserialize(value, handler)
                for key, value in self.kwargs.items()
            }
        ).node

class TensorModel(BaseNNsightModel):

    type_name: Literal["TENSOR"] = "TENSOR"

    values: List
    dtype: str

    @staticmethod
    @memoized
    def to_model(value: torch.Tensor) -> Self:

        return TensorModel(values=value.tolist(), dtype=str(value.dtype).split(".")[-1])

    def deserialize(self, handler: DeserializeHandler) -> torch.Tensor:
        dtype = getattr(torch, self.dtype)
        return torch.tensor(self.values, dtype=dtype)


class SliceModel(BaseNNsightModel):

    type_name: Literal["SLICE"] = "SLICE"

    start: ValueTypes
    stop: ValueTypes
    step: ValueTypes

    @staticmethod
    @memoized
    def to_model(value: slice) -> Self:

        return SliceModel(start=value.start, stop=value.stop, step=value.step)

    def deserialize(self, handler: DeserializeHandler) -> slice:

        return slice(
            try_deserialize(self.start, handler),
            try_deserialize(self.stop, handler),
            try_deserialize(self.step, handler),
        )


class EllipsisModel(BaseNNsightModel):

    type_name: Literal["ELLIPSIS"] = "ELLIPSIS"

    def deserialize(
        self, handler: DeserializeHandler
    ) -> type(
        ...
    ):  # It will be better to use EllipsisType, but it requires python>=3.10
        return ...


class ListModel(BaseNNsightModel):

    type_name: Literal["LIST"] = "LIST"

    values: List[ValueTypes]

    @staticmethod
    def to_model(value: List) -> Self:

        return ListModel(values=value)

    def deserialize(self, handler: DeserializeHandler) -> list:
        return [try_deserialize(value, handler) for value in self.values]


class TupleModel(BaseNNsightModel):

    type_name: Literal["TUPLE"] = "TUPLE"

    values: List[ValueTypes]

    @staticmethod
    def to_model(value: Tuple) -> Self:

        return TupleModel(values=value)

    def deserialize(self, handler: DeserializeHandler) -> tuple:
        return tuple([try_deserialize(value, handler) for value in self.values])


class DictModel(BaseNNsightModel):

    type_name: Literal["DICT"] = "DICT"

    values: Dict[str, ValueTypes]

    @staticmethod
    def to_model(value: Dict) -> Self:

        return DictModel(values=value)

    def deserialize(self, handler: DeserializeHandler) -> dict:
        return {
            key: try_deserialize(value, handler) for key, value in self.values.items()
        }


class FunctionWhitelistError(Exception):
    pass


class FunctionModel(BaseNNsightModel):

    type_name: Literal["FUNCTION"] = "FUNCTION"

    function_name: str
    
    @staticmethod
    def to_model(value:FUNCTION):
        
        model = FunctionModel(function_name=get_function_name(value))
        
        FunctionModel.check_function_whitelist(model.function_name)
        
        return model

    @classmethod
    def check_function_whitelist(cls, qualname: str) -> str:
        if qualname not in FUNCTIONS_WHITELIST:
            raise FunctionWhitelistError(
                f"Function with name `{qualname}` not in function whitelist."
            )

        return qualname

    def deserialize(self, handler: DeserializeHandler) -> FUNCTION:

        FunctionModel.check_function_whitelist(self.function_name)

        return FUNCTIONS_WHITELIST[self.function_name]


class GraphModel(BaseNNsightModel):

    type_name: Literal["GRAPH"] = "GRAPH"

    # We have a reference to the real Graph in the pydantic to be used by optimization logic
    graph: Graph = Field(exclude=True, default=None, validate_default=False)

    nodes: List[Union[MemoReferenceModel, NodeType]]

    @staticmethod
    def to_model(value: Graph) -> Self:

        return GraphModel(graph=value, nodes=value.nodes)

    def deserialize(self, handler: DeserializeHandler) -> Graph:

        for node in self.nodes:

            node.deserialize(handler)

        return handler.graph


class SubGraphModel(BaseNNsightModel):

    type_name: Literal["SUBGRAPH"] = "SUBGRAPH"

    subset: List[int]

    @staticmethod
    def to_model(value: SubGraph) -> Self:

        return SubGraphModel(subset=value.subset)

    def deserialize(self, handler: DeserializeHandler) -> Graph:

        value = SubGraph(handler.graph, subset=self.subset)
        
        for node in value:
            node.graph = value
            
        return value


class InterventionGraphModel(SubGraphModel):

    type_name: Literal["INTERVENTIONGRAPH"] = "INTERVENTIONGRAPH"

    @staticmethod
    def to_model(value: InterventionGraph) -> Self:

        return InterventionGraphModel(subset=value.subset)

    def deserialize(self, handler: DeserializeHandler) -> Graph:
        value = InterventionGraph(handler.graph, model=handler.model, subset=self.subset)
    
        for node in value:
            node.graph = value
            
        return value


class MemoReferenceModel(BaseNNsightModel):

    type_name: Literal["REFERENCE"] = "REFERENCE"

    id: int

    def deserialize(self, handler: DeserializeHandler):
        
        value = try_deserialize(handler.memo[self.id], handler)

        handler.memo[self.id] = value

        return value


### Define Annotated types to convert objects to their custom Pydantic counterpart

GraphType = Annotated[
    Graph,
    AfterValidator(GraphModel.to_model),
]

SubGraphType = Annotated[
    SubGraph,
    AfterValidator(SubGraphModel.to_model),
]

InterventionGraphType = Annotated[
    InterventionGraph,
    AfterValidator(InterventionGraphModel.to_model),
]

TensorType = Annotated[torch.Tensor, AfterValidator(TensorModel.to_model)]

SliceType = Annotated[
    slice,
    AfterValidator(SliceModel.to_model),
]

EllipsisType = Annotated[
    type(...),  # It will be better to use EllipsisType, but it requires python>=3.10
    AfterValidator(lambda _: EllipsisModel()),
]


ListType = Annotated[list, AfterValidator(ListModel.to_model)]

TupleType = Annotated[
    tuple,
    Strict(),
    AfterValidator(TupleModel.to_model),
]

DictType = Annotated[dict, AfterValidator(DictModel.to_model)]

FunctionType = Annotated[
    FUNCTION,
    AfterValidator(FunctionModel.to_model),
]

NodeType = Annotated[
    Node,
    AfterValidator(NodeModel.to_model),
]


def check_memo(object: Any):

    _id = id(object)

    if _id in get_memo():

        return MemoReferenceModel(id=_id)

    raise ValueError()


MemoType = Annotated[object, BeforeValidator(check_memo)]

### Register all custom Pydantic objects to convert objects to
TOTYPES = Annotated[
    Union[
        MemoReferenceModel,
        NodeModel,
        SliceModel,
        TensorModel,
        TupleModel,
        ListModel,
        DictModel,
        FunctionModel,
        EllipsisModel,
        InterventionGraphModel,
        SubGraphModel,
        GraphModel,
    ],
    Field(discriminator="type_name"),
]
### Register all Annotated types objects to convert objects from
FROMTYPES = Annotated[
    Union[
        MemoType,
        NodeType,
        InterventionGraphType,
        SubGraphType,
        GraphType,
        FunctionType,
        SliceType,
        TensorType,
        TupleType,
        ListType,
        DictType,
        EllipsisType,
    ],
    Field(union_mode="left_to_right"),
]

### Final registration
ValueTypes = Union[
    PRIMITIVE,
    TOTYPES,
    FROMTYPES,
]



---
File: /src/nnsight/schema/__init__.py
---




---
File: /src/nnsight/schema/config.py
---

import os
from typing import Optional

import yaml
from pydantic import BaseModel

from ..logger import remote_logger


class ApiConfigModel(BaseModel):
    HOST: str = "ndif.dev"
    SSL: bool = True
    FORMAT: str = "json"
    ZLIB: bool = True
    APIKEY: Optional[str] = None
    JOB_ID: Optional[str] = None


class AppConfigModel(BaseModel):
    LOGGING: bool = False
    REMOTE_LOGGING: bool = True
    DEBUG: bool = True
    CONTROL_FLOW_HANDLING: bool = True
    FRAME_INJECTION: bool = True
    GLOBAL_TRACING: bool = True

    def __setattr__(self, name, value):
        if name == "REMOTE_LOGGING":
            self.on_remote_logging_change(value)
        super().__setattr__(name, value)

    def on_remote_logging_change(self, value: bool):
        if value != self.REMOTE_LOGGING:
            remote_logger.disabled = (not value)
        self.__dict__["REMOTE_LOGGING"] = value


class ConfigModel(BaseModel):
    API: ApiConfigModel = ApiConfigModel()
    APP: AppConfigModel = AppConfigModel()

    def set_default_api_key(self, apikey: str):

        self.API.APIKEY = apikey

        self.save()

    def set_default_app_debug(self, debug: bool):

        self.APP.DEBUG = debug

        self.save()

    def save(self):

        from .. import PATH

        with open(os.path.join(PATH, "config.yaml"), "w") as file:

            yaml.dump(self.model_dump(), file)



---
File: /src/nnsight/schema/request.py
---

from __future__ import annotations

import copy
import io
import json
import zlib
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, List, Union

import msgspec
import torch
from pydantic import BaseModel, ConfigDict

from .format.types import (clear_memo, get_memo, DeserializeHandler, Graph, GraphModel,
                           GraphType, ValueTypes, try_deserialize)

if TYPE_CHECKING:
    from .. import NNsight


class RequestModel(BaseModel):

    model_config = ConfigDict(arbitrary_types_allowed=True, protected_namespaces=())

    graph: Union[GraphModel, GraphType]
    memo: Dict[int, ValueTypes]

    def __init__(self, *args, memo: Dict = None, **kwargs):

        super().__init__(*args, memo=memo or dict(), **kwargs)

        if memo is None:

            self.memo = {**get_memo()}

            clear_memo()
    
    @staticmethod
    def serialize(graph: Graph, format:str, _zlib:bool) -> bytes:
        
        if format == "json":

            data = RequestModel(graph=graph)

            json = data.model_dump(mode="json")

            data = msgspec.json.encode(json)

        elif format == "pt":

            data = io.BytesIO()

            torch.save(graph, data)

            data.seek(0)

            data = data.read()
            
        if _zlib:

            data = zlib.compress(data)
            
        return data

    @staticmethod
    def deserialize(model: "NNsight", graph:bytes, format:str, _zlib:bool) -> Graph:
        
        if _zlib:

            graph = zlib.decompress(graph)

        if format == "json":

            nnsight_request = msgspec.json.decode(graph)

            request = RequestModel(**nnsight_request)
            
            handler = DeserializeHandler(request.memo, model)

            graph = request.graph.deserialize(handler)

        elif format == "pt":

            data = io.BytesIO(graph)

            data.seek(0)

            graph = torch.load(data, map_location="cpu", weights_only=False)

        return graph

class StreamValueModel(BaseModel):

    model_config = ConfigDict(arbitrary_types_allowed=True, protected_namespaces=())

    values: Dict[int, ValueTypes]
    memo: Dict[int, ValueTypes]
    
    def __init__(self, *args, memo: Dict = None, **kwargs):

        super().__init__(*args, memo=memo or dict(), **kwargs)

        if memo is None:

            self.memo = {**get_memo()}

            clear_memo()

    @staticmethod
    def serialize(values: Dict[int, Any], format:str, _zlib:bool) -> bytes:
        
        if format == "json":

            data = StreamValueModel(values=values)

            json = data.model_dump(mode="json")

            data = msgspec.json.encode(json)

        elif format == "pt":

            data = io.BytesIO()

            torch.save(values, data)

            data.seek(0)

            data = data.read()
            
        if _zlib:

            data = zlib.compress(data)
            
        return data

    @staticmethod
    def deserialize(values:bytes, format:str, _zlib:bool) -> Dict[int, Any]:
        
        if _zlib:

            values = zlib.decompress(values)

        if format == "json":

            nnsight_request = msgspec.json.decode(values)

            request = StreamValueModel(**nnsight_request)
            
            handler = DeserializeHandler(request.memo, None)

            values = {index: try_deserialize(value, handler) for index, value in request.values.items()}

        elif format == "pt":

            data = io.BytesIO(values)

            data.seek(0)

            values = torch.load(data, map_location="cpu", weights_only=False)

        return values



---
File: /src/nnsight/schema/response.py
---

from __future__ import annotations

import io
import logging
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Union

import torch
from pydantic import BaseModel, ConfigDict

from .result import RESULT


class ResponseModel(BaseModel):

    model_config = ConfigDict(arbitrary_types_allowed=True, protected_namespaces=())

    class JobStatus(Enum):
        RECEIVED = "RECEIVED"
        APPROVED = "APPROVED"
        RUNNING = "RUNNING"
        COMPLETED = "COMPLETED"
        LOG = "LOG"
        STREAM = "STREAM"
        ERROR = "ERROR"
        NNSIGHT_ERROR = "NNSIGHT_ERROR"

    id: str
    status: JobStatus

    description: Optional[str] = ""
    data: Optional[Union[RESULT, Any]] = None
    session_id: Optional[str] = None

    def __str__(self) -> str:
        return f"{self.id} - {self.status.name}: {self.description}"

    def log(self, logger: logging.Logger) -> ResponseModel:
       
        if self.status == ResponseModel.JobStatus.STREAM:
            pass
        else:
            logger.info(str(self))

        return self

    def pickle(self) -> bytes:
        """Pickles self and returns bytes.

        Returns:
            bytes: Pickled ResponseModel
        """

        with io.BytesIO() as file:

            torch.save(self.model_dump(exclude_unset=True), file)

            file.seek(0)

            return file.read()

    @classmethod
    def unpickle(cls, data: bytes) -> ResponseModel:
        """Loads a ResponseModel from pickled bytes.

        Args:
            data (bytes): Pickled ResponseModel.

        Returns:
            ResponseModel: Response.
        """

        with io.BytesIO(data) as file:
            return ResponseModel(
                **torch.load(file, map_location="cpu", weights_only=False)
            )



---
File: /src/nnsight/schema/result.py
---

from typing import Any, Dict
from pydantic import BaseModel, ConfigDict
from ..tracing.graph import Graph
RESULT = Dict[int, Any]

class ResultModel(BaseModel):
    
    
    model_config = ConfigDict(arbitrary_types_allowed=True, protected_namespaces=())
    
    id:str
    result: RESULT
    
    @classmethod
    def inject(cls, graph:Graph, result:RESULT):
    
                
        for index, value in result.items():
            
            graph.nodes[index]._value = value
            
    @classmethod
    def from_graph(cls, graph:Graph) -> RESULT:
        

        return {node.index: node.value for node in graph.nodes if node.done}


---
File: /src/nnsight/tracing/backends/__init__.py
---

from .base import Backend, ExecutionBackend


---
File: /src/nnsight/tracing/backends/base.py
---

import inspect
import sys

from ...util import NNsightError
from ..graph import Graph, Proxy
from ..protocols import StopProtocol
from ... import __IPYTHON__

class Backend:

    def __call__(self, graph: Graph) -> None:

        raise NotImplementedError()


class ExecutionBackend(Backend):

    def __init__(self, injection: bool = True) -> None:
        self.injection = injection

    def __call__(self, graph: Graph) -> None:

        try:

            graph.nodes[-1].execute()

            if self.injection:
                
                frame_injection()

        except StopProtocol.StopException:

            pass

        except NNsightError as e:
            if graph.debug:
                node_traceback = graph.nodes[e.node_id].meta_data['traceback']

                if __IPYTHON__: # in IPython the traceback content is rendered by the Error itself
                    # add the error node traceback to the the error's traceback
                    e.traceback_content += "\nDuring handling of the above exception, another exception occurred:\n\n"
                    e.traceback_content += node_traceback
                else: # else we print the traceback manually
                    print(f"\n{e.traceback_content}")
                    print(
                        "During handling of the above exception, another exception occurred:\n"
                    )
                    print(f"{node_traceback}")
                    
                sys.tracebacklimit = 0
                raise e from None
            else:
                raise e

        finally:
            if __IPYTHON__:
                sys.tracebacklimit = None
            graph.nodes.clear()
            graph.stack.clear()


def frame_injection():
    
    from ..contexts import Context
    import ctypes

    frame = inspect.currentframe().f_back
    while frame.f_back is not None and 'self' in frame.f_locals and isinstance(frame.f_locals['self'], (Context,Backend)):
        frame = frame.f_back
        
    for key, value in frame.f_locals.items():
                
        if isinstance(value, Proxy) and value.node.done:
            frame.f_locals[key] = value.value
            ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(frame), 0)


---
File: /src/nnsight/tracing/contexts/__init__.py
---

from .base import Context
from .iterator import Iterator
from .conditional import Condition
from .tracer import Tracer
from .globals import GlobalTracingContext


---
File: /src/nnsight/tracing/contexts/base.py
---

from __future__ import annotations

from contextlib import AbstractContextManager
from typing import Generic, Optional, Type

from typing_extensions import Self

from ... import CONFIG
from ...tracing.graph import Node, NodeType, Proxy, ProxyType
from ..backends import Backend, ExecutionBackend
from ..graph import Graph, GraphType, SubGraph, viz_graph
from ..protocols import Protocol

class Context(Protocol, AbstractContextManager, Generic[GraphType]):
    """A `Context` represents a scope (or slice) of a computation graph with specific logic for adding and executing nodes defined within it.
    It has a `SubGraph` which contains the nodes that make up the operations of the context.
    As an `AbstractContextManager`, entering adds its sub-graph to the stack, making new nodes created while within this context added to it's sub-graph.
        Exiting pops its sub-graph off the stack, allowing nodes to be added to its parent, and adds itself as a node to its parent `Context`/`SubGraph`. ( To say, "execute me")
        If the `Context` has a backend, it pops its parent off the stack and passes it to the `Backend` object to execute.
        (This only happens if the context is the root-most context, and its parent is therefore the root `Graph`)
    As a `Context` is itself a `Protocol`, it defines how to execute it's sub-graph in the `execute` method.
    

    Attributes:
    
        backend (Backend): Backend to execute the deferred root computation graph
    """

    def __init__(
        self,
        *args,
        backend: Optional[Backend] = None,
        parent: Optional[GraphType] = None,
        graph: Optional[GraphType] = None,
        graph_class: Type[SubGraph] = SubGraph,
        node_class: Type[NodeType] = Node,
        proxy_class: Type[ProxyType] = Proxy,
        debug: bool = False,
        **kwargs,
    ) -> None:

        # If this is the root graph, we want to execute it upon exit.
        # Otherwise its a child context/graph and all we want to
        if backend is None and parent is None:
            backend = ExecutionBackend(injection=CONFIG.APP.FRAME_INJECTION)

        self.backend = backend

        if parent is None:
            parent = Graph(node_class=node_class, proxy_class=proxy_class, debug=debug)
            parent.stack.append(parent)

        self.graph = graph_class(*args, parent, **kwargs)

        self.graph.stack.append(self.graph)

        if graph is not None:
            graph.copy(self.graph)

        self.args = []
        self.kwargs = {}

    def __enter__(self) -> Self:

        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:

        graph = self.graph.stack.pop()

        if isinstance(exc_val, BaseException):
            raise exc_val

        self.add(graph.stack[-1], graph, *self.args, **self.kwargs)

        if self.backend is not None:
            
            graph = graph.stack.pop()
            
            graph.alive = False

            self.backend(graph)

    def vis(self, *args, **kwargs):
        viz_graph(self.graph, *args, **kwargs)

    @classmethod
    def execute(cls, node: NodeType):

        graph: GraphType = node.args[0]

        graph.reset()
        graph.execute()

        node.set_value(None)
    


---
File: /src/nnsight/tracing/contexts/conditional.py
---

from __future__ import annotations

from typing import Any, Dict, Optional

from ...tracing.graph import NodeType, SubGraph
from ..contexts import Context


class Condition(Context[SubGraph]):

    def __init__(
        self, condition: Optional[NodeType], branch: Optional[NodeType] = None, *args, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)

        self.args = [condition, branch]
        self.index = None

    def else_(self, condition: Optional[Any] = None):
        
        return Condition(
            condition,
            branch=self.graph.nodes[self.index],
            parent=self.graph.stack[-1],
        )
        
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        super().__exit__(exc_type, exc_val, exc_tb)
        
        self.index = self.graph.nodes[-1].index

    @classmethod
    def execute(cls, node: NodeType):
        graph, condition, branch = node.args

        graph: SubGraph

        condition: Any
        condition, branch = node.prepare_inputs((condition, branch))

        # else case has a True condition
        if condition is None and not branch:
            condition = True

        if not branch and condition:
            
            graph.reset()
            graph.execute()

            node.set_value(True)
        else:
            graph.clean()
            node.set_value(branch)
            
    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """
    
        default_style = super().style()

        default_style["node"] = {"color": "#FF8C00", "shape": "polygon", "sides": 6}
        default_style["edge"][2] = {"style": "solid", "label": "branch", "color": "#FF8C00", "fontsize": 10}

        return default_style




---
File: /src/nnsight/tracing/contexts/globals.py
---

from __future__ import annotations

import inspect
from contextlib import AbstractContextManager
from functools import wraps
from types import FunctionType, MethodType
from typing import Any, Type, Union

from ... import util, CONFIG
from ..graph import Graph
from . import Tracer


def global_patch_class(cls: type) -> util.Patch:

    if cls.__new__ is object.__new__:

        def super_new(cls, *args, **kwargs):

            return object.__new__(cls)

        cls.__new__ = super_new

    fn = cls.__new__

    @wraps(fn)
    def inner(cls, *args, **kwargs):

        if not GlobalTracingContext.GLOBAL_TRACING_CONTEXT:
            return cls(*args, **kwargs)

        return GlobalTracingContext.GLOBAL_TRACING_CONTEXT.apply(cls, *args, **kwargs)

    return util.Patch(cls, inner, "__new__")


def global_patch_fn(fn: FunctionType) -> util.Patch:

    @wraps(fn)
    def inner(*args, **kwargs):

        if not GlobalTracingContext.GLOBAL_TRACING_CONTEXT:
            return fn(*args, **kwargs)

        return GlobalTracingContext.GLOBAL_TRACING_CONTEXT.apply(fn, *args, **kwargs)

    return util.Patch(inspect.getmodule(fn), inner, fn.__name__)


def global_patch_method(cls: type, fn: MethodType) -> None:

    @wraps(fn)
    def inner(*args, **kwargs):

        if not GlobalTracingContext.GLOBAL_TRACING_CONTEXT:
            return fn(*args, **kwargs)

        return GlobalTracingContext.GLOBAL_TRACING_CONTEXT.apply(fn, *args, **kwargs)

    patch = util.Patch(cls, inner, fn.__name__)

    GlobalTracingContext.PATCHER.add(patch)


def global_patch(obj: Union[FunctionType, Type]):

    if isinstance(obj, type):

        patch = global_patch_class(obj)

    else:

        patch = global_patch_fn(obj)

    GlobalTracingContext.PATCHER.add(patch)


class GlobalTracingContext(Tracer):
    """The Global Tracing Context handles adding tracing operations globally without reference to a given `GraphBasedContext`.
    There should only be one of these and that is `GlobalTracingContext.GLOBAL_TRACING_CONTEXT`.
    `GlobalTracingContext.TORCH_HANDLER` handles adding torch functions without reference to a given `GraphBasedContext`.

    """

    GLOBAL_TRACING_CONTEXT: GlobalTracingContext
    PATCHER: util.Patcher = util.Patcher()

    class GlobalTracingExit(AbstractContextManager):

        def __enter__(self) -> Any:

            if CONFIG.APP.GLOBAL_TRACING:

                GlobalTracingContext.PATCHER.__exit__(None, None, None)

            return self

        def __exit__(self, exc_type, exc_val, traceback):

            if CONFIG.APP.GLOBAL_TRACING:

                GlobalTracingContext.PATCHER.__enter__()

            if isinstance(exc_val, BaseException):

                raise exc_val

    def __init__(self) -> None:
        """We create an empty `GraphBasedContext` by default."""

        self.graph: Graph = None

    @staticmethod
    def exit_global_tracing_context():

        return GlobalTracingContext.GlobalTracingExit()

    @staticmethod
    def try_register(graph_based_context: Tracer) -> bool:
        """Attempts to register a `Graph` globally.]
        Will not if one is already registered.

        Args:
            graph_based_context (GraphBasedContext): `GraphBasedContext` to register.

        Returns:
            bool: True if registering ws successful, False otherwise.
        """

        if CONFIG.APP.GLOBAL_TRACING:

            if GlobalTracingContext.GLOBAL_TRACING_CONTEXT:

                return False

            GlobalTracingContext.register(graph_based_context)

        return True

    @staticmethod
    def try_deregister(graph_based_context: Tracer) -> bool:
        """Attempts to deregister a `Graph` globally.
        Will not if `graph_based_context` does not have the same `Graph` as the currently registered one.

        Args:
            graph_based_context (GraphBasedContext): `GraphBasedContext` to deregister.

        Returns:
            bool: True if deregistering ws successful, False otherwise.
        """

        if CONFIG.APP.GLOBAL_TRACING:

            if (
                not GlobalTracingContext.GLOBAL_TRACING_CONTEXT
                or graph_based_context.graph
                is not GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph
            ):

                return False

            GlobalTracingContext.deregister()

        return True

    @staticmethod
    def register(graph_based_context: Tracer) -> None:
        """Register `GraphBasedContext` globally.

        Args:
            graph_based_context (GraphBasedContext): GraphBasedContext to register.
        """

        assert GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph is None

        GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph = graph_based_context.graph

        GlobalTracingContext.PATCHER.__enter__()

    @staticmethod
    def deregister() -> None:
        """Deregister `GraphBasedContext` globally.

        Args:
            graph_based_context (GraphBasedContext): GraphBasedContext to deregister.
        """

        assert GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph is not None

        GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph = None

        GlobalTracingContext.PATCHER.__exit__(None, None, None)

    def __bool__(self) -> bool:
        """True if there is a `GraphBasedContext` registered globally. False otherwise."""

        return GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph is not None


GlobalTracingContext.GLOBAL_TRACING_CONTEXT = GlobalTracingContext()



---
File: /src/nnsight/tracing/contexts/iterator.py
---

import copy
from typing import Collection, Dict, Any

from ...tracing.graph import SubGraph
from ...tracing.graph import Node
from ...tracing.graph import Proxy
from . import Context
from ..protocols import VariableProtocol, StopProtocol

class Iterator(Context[SubGraph]):

    def __init__(self, collection: Collection, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.args = [collection]
        
    def __enter__(self) -> Proxy:
        
        super().__enter__()
        
        return VariableProtocol.add(self.graph)
        

    @classmethod
    def execute(cls, node: Node):

        graph, collection = node.args

        graph: SubGraph
        collection: Collection

        collection = node.prepare_inputs(collection)
        
        variable_node = next(iter(graph))
        
        graph.defer_stack.append(variable_node.index)

        for idx, value in enumerate(copy.copy(collection)):

            VariableProtocol.set(variable_node, value)

            if idx == len(collection) - 1:
                graph.defer_stack.pop()


            graph.reset()
            try:
                graph.execute()
            except Exception as e:
                
                if idx != len(collection) - 1:
                
                    graph.defer_stack.pop()
                    
                if not isinstance(e, StopProtocol.StopException):
                
                    raise e
                
                else:
                    break
            
        node.set_value(None)

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()
        
        default_style["node"] = {"color": "blue", "shape": "polygon", "sides": 6}

        return default_style



---
File: /src/nnsight/tracing/contexts/tracer.py
---

from typing import Callable, TypeVar, Union
from typing_extensions import Self

from ..graph import ProxyType, SubGraph, NodeType, Proxy
from ..protocols import StopProtocol
from . import Condition, Context, Iterator

class Tracer(Context[SubGraph[NodeType, ProxyType]]):
    
    
    def __enter__(self) -> Self:
        
        from .globals import GlobalTracingContext
        
        GlobalTracingContext.try_register(self)
        
        return super().__enter__()
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        
        from .globals import GlobalTracingContext
        
        GlobalTracingContext.try_deregister(self)
        
        return super().__exit__(exc_type, exc_val, exc_tb)

    def iter(self, collection):

        return Iterator(collection, parent=self.graph)

    def cond(self, condition):

        return Condition(condition, parent=self.graph)
    
    def stop(self):

        StopProtocol.add(self.graph)
        
    def log(self, *args):
        
        self.apply(print, *args)

    R = TypeVar('R')
    
    def apply(self, target: Callable[..., R], *args, **kwargs) -> Union[Proxy, R]:

        return self.graph.create(
            target,
            *args,
            **kwargs,
        )



---
File: /src/nnsight/tracing/graph/__init__.py
---

from .proxy import Proxy, ProxyType
from .node import Node, NodeType
from .graph import Graph, SubGraph, GraphType
from .viz import viz_graph



---
File: /src/nnsight/tracing/graph/graph.py
---

from __future__ import annotations
from typing import (Callable, Dict, Generic, Iterator, List, Optional,
                    Tuple, Type, TypeVar, Union, overload)

from typing_extensions import Self

from ... import util
from ...util import NNsightError
from .. import protocols
from . import Node, NodeType, Proxy, ProxyType


class Graph(Generic[NodeType, ProxyType]):
    """The `Graph` class represents a computation graph composed of individual `Node`s (operations).
    It contains logic to both trace/build the computation graph, as well as how to execute it.
    Sections of the graph can be divided into `SubGraphs`, but there will always be one root `Graph`.
    The final `Node` of the graph (graph[-1]) should be the root `Node` which when executed, downstream executes the entire `Graph`.

    Attributes:
        node_class (Type[NodeType]): Class used to create `Node`s. Can be changed to add additional functionality to `Node's. Defaults to `Node`.
        proxy_class (Type[ProxyType]): Class used to create `Proxy`s for 'Node's. Can be changed to add additional functionality to `Proxy's. Defaults to `Proxy`.
        nodes (List[Node]): Ordered list of all `Node`s. Used to access `Nodes` via their index.
        stack (List[Graph]): List of `Graph`s as a stack. Used to move `Node`s onto the most recent graph, as opposed to the `Graph` used to create the `Node`.
            Managed outside the `Graph` class by the `Context` objects.
        defer_stack (List[int]): List of `Node` indexes as a stack. Used to prevent destruction/memory cleanup of `Node`s whose index is less than the most recent index on the stack.
            This happens when you have `Node`s that will be executed more than once. In a loop for example, you only want to destroy a `Node`s dependencies on the final iteration.
            Also managed outside the `Graph` object.
        alive (bool): If the `Graph` is "alive". Alive meaning its still open for tracing (adding new `Node`s). Set to False before executing the `Graph`.
    """

    def __init__(
        self,
        node_class: Type[NodeType] = Node,
        proxy_class: Type[ProxyType] = Proxy,
        debug: bool = False,
    ) -> None:

        self.node_class = node_class
        self.proxy_class = proxy_class
        self.debug = debug

        self._alive = [True]

        self.nodes: List[Node] = []
        self.stack: List[Graph] = []
        self.defer_stack: List[int] = []

    @property
    def alive(self) -> bool:
        return self._alive[0]

    @alive.setter
    def alive(self, value: bool):

        self._alive[0] = value

    def reset(self) -> None:
        """Resets the `Graph` to prepare for execution.
        Simply resets all `Node`s in the `Graph`.
        """

        for node in self:
            node.reset()

    def execute(self) -> None:
        """Executes all `Node`s (operations) in this `Graph`.

        Raises:
            exception: If there is an exception during executing a `Node`. If so, we need to clean up the dependencies of `Node`s yet to be executed.
        """

        err: Tuple[int, NNsightError] = None

        for node in self:
            try:
                node.execute()
            except NNsightError as e:
                err = (node.index, e)
                break

        if err is not None:
            defer_stack = self.defer_stack.copy()
            self.defer_stack.clear()
            self.clean(err[0])
            self.defer_stack.extend(defer_stack)
            raise err[1]

    def clean(self, start: Optional[int] = None):
        """Cleans up dependencies of `Node`s so their values are appropriately memory managed.
        Cleans all `Node`s from start to end regardless if they are on this `Graph`.

        Args:
            start (Optional[int], optional): `Node` index to start cleaning up from. Defaults to None.
        """
        
        if len(self) == 0:
            return

        if start is None:
            start = self[0].index

        end = self[-1].index + 1

        # Loop over ALL nodes within the span of this graph.
        for index in range(start, end):
            
            node = self.nodes[index]

            node.update_dependencies()

    def create(
        self,
        target: Union[Callable, protocols.Protocol],
        *args,
        redirect: bool = True,
        **kwargs,
    ) -> ProxyType:
        """Creates a new `Node` using this `Graph`'s node_class and returns a `Proxy` for it with this `Graph`'s proxy_class.

        Args:
            target (Union[Callable, protocols.Protocol]): Target for the new `Node`.
            redirect (bool, optional): If to move the newly created `Node` to the most recent `Graph` on the Graph.stack. Defaults to True.

        Returns:
            ProxyType: `Proxy` for newly created `Node`.
        """

        # Redirection.
        graph = self.stack[-1] if redirect and self.stack else self

        return self.proxy_class(self.node_class(target, *args, graph=graph, **kwargs))

    def add(self, node: NodeType) -> None:
        """Adds a `Node` to this `Graph`.
        Sets the `Node`'s .index attribute so it knows its own index within the entire computation graph.

        Args:
            node (NodeType): `Node` to add.
        """

        # Tag the Node with its own index.
        node.index = len(self.nodes)

        # Add Node.
        self.nodes.append(node)

    def copy(self, new_graph: Optional[Graph[NodeType, ProxyType]] = None) -> Graph:
        """Creates a shallow copy of the root `Graph` object.

        Args:
            new_graph (Optional[Graph[NodeType, ProxyType]], optional): `Graph` to copy into. Defaults to None and creates a new `Graph`.

        Returns:
            Graph: New `Graph`.
        """

        if new_graph is None:
            new_graph = Graph(node_class=self.node_class, proxy_class=self.proxy_class, debug=self.debug)
        
        node = self[-1]

        def process(arg: Union[Node, SubGraph]):

            if isinstance(arg, SubGraph):
                return arg.copy(parent=new_graph)

            if arg.done:
                return arg.value

        new_graph.create(
            node.target,
            *util.apply(node.args, process, (Node, SubGraph)),
            **util.apply(node.kwargs, process, (Node, SubGraph)),
        )

        return new_graph

    ### Magic Methods ######################################

    def __str__(self) -> str:
        result = f"{self.__class__.__name__}:\n"

        for node in self:
            result += f"  {str(node)}\n"

        return result

    @overload
    def __getitem__(self, key: int) -> Node: ...

    @overload
    def __getitem__(self, key: Union[slice, List[int]]) -> List[Node]: ...

    def __getitem__(self, key: Union[int, Union[slice, List[int]]]) -> Union[Node, List[Node]]:
        return self.nodes[key]

    def __iter__(self) -> Iterator[Node]:
        return iter(self.nodes)

    def __len__(self) -> int:
        return len(self.nodes)


class SubGraph(Graph[NodeType, ProxyType]):
    """Represents a slice of the greater computation graph. It has a reference to the same underlying list of nodes and simply maintains a subset of node indexes.

    Attributes:
        subset (List[int]): Node indexes for `Node`s contained within this subgraph.
    """

    def __init__(
        self,
        parent: GraphType,
        subset: Optional[List[int]] = None,
    ):
        """Init

        Args:
            parent (GraphType): Graph to inherit attributes from.
            subset (Optional[List[int]], optional): Subset to start from when loading a pre-defined `SubGraph`
        """

        self.__dict__.update(parent.__dict__)

        self.subset: List[int] = [] if subset is None else subset

    def __getstate__(self):

        return {
            "nodes":self.nodes,
            "subset":self.subset,
            "defer_stack": self.defer_stack,
        }
    
    def __setstate__(self, state: Dict) -> None:

        self.__dict__.update(state)

    def add(self, node: NodeType) -> None:

        super().add(node)

        # Also add the index to this SubGraph's subset upon adding.
        self.subset.append(self.nodes[-1].index)

    @overload
    def __getitem__(self, key: int) -> Node: ...

    @overload
    def __getitem__(self, key: Union[slice, List[int]]) -> List[Node]: ...

    def __getitem__(self, key: Union[int, Union[slice, List[int]]]) -> Union[Node, List[Node]]:        

        index = self.subset[key]

        # We iterate over indexes and get their Nodes.
        node = (
            [self.nodes[idx] for idx in index]
            if isinstance(index, list)
            else self.nodes[index]
        )

        return node

    def __iter__(self) -> Iterator[Node]:
        return self.Iterator(self)

    def __len__(self) -> int:
        return len(self.subset)

    class Iterator(Iterator):

        def __init__(self, subgraph: SubGraph[GraphType]) -> None:

            self.subgraph = subgraph
            self.start = 0
            self.end = len(self.subgraph)

        def __next__(self) -> NodeType:

            if self.start < self.end:
                value = self.subgraph[self.start]
                self.start += 1
                return value

            raise StopIteration

    def copy(
        self,
        new_graph: Optional[SubGraph[NodeType, ProxyType]] = None,
        parent: Optional[Graph[NodeType, ProxyType]] = None,
        memo: Optional[Dict[int, NodeType]] = None,
    ) -> Self:
        """Creates a shallow copy of this SubGraph.

        Args:
            new_graph (Optional[SubGraph[NodeType, ProxyType]], optional): SubGraph to copy into. Defaults to None and creates a new SubGraph of the same type.
            parent (Optional[Graph[NodeType, ProxyType]], optional): Parent graph. Defaults to None and will create a root `Graph` as the parent.

        Returns:
            Self: New graph.
        """
        if parent is None:
            parent = Graph(node_class=self.node_class, proxy_class=self.proxy_class)

        if new_graph is None:
            new_graph = type(self)(parent)

        if memo is None:
            memo = {}

        def process(arg: Union[Node, SubGraph]):

            if isinstance(arg, SubGraph):
                return arg.copy(parent=new_graph, memo=memo)

            if arg.done:
                return arg.value

            return new_graph.nodes[memo[arg.index]]

        for node in self:

            new_node = new_graph.create(
                node.target,
                *util.apply(node.args, process, (Node, SubGraph)),
                **util.apply(node.kwargs, process, (Node, SubGraph)),
            ).node

            memo[node.index] = new_node.index

        return new_graph


# class MultiGraph(Graph):


#     def __init__(self, *args, **kwargs) -> None:
#         super().__init__(proxy_class, validate)


GraphType = TypeVar("GraphType", bound=SubGraph)



---
File: /src/nnsight/tracing/graph/node.py
---

from __future__ import annotations

import inspect
import re
import traceback
from typing import (TYPE_CHECKING, Any, Callable, Dict, List,
                    Optional, Set, TypeVar, Union)

from typing_extensions import Self

from ... import util
from ..protocols import Protocol
from .proxy import Proxy, ProxyType

from ...util import NNsightError

if TYPE_CHECKING:
    from .graph import Graph


class Node:
    """A computation `Graph` is made up of individual `Node`s which represent a single operation.
    It has a `target` which the operation this `Node` will execute.
    It has `args` and `kwargs` to execute its `target` with. These may contain other `Node`s and are therefore `dependencies` of this `Node`.
    Conversely this `Node` is a `listener` of its `dependencies`.
    
    During execution of the computation graph and therefore the `Node`s, each 
    
    Attributes:
        index (Optional[int]): Integer index of this `Node` within its greater computation graph.
        graph (Graph): 
        target (Union[Callable, Protocol]): Callable to execute as this `Node`'s operation. Might be a `Protocol` which is handled differently in node execution.
    """

    def __init__(
        self,
        target: Union[Callable, Protocol],
        *args,
        graph: "Graph" = None,
        **kwargs,
    ) -> None:

        self.index: Optional[int] = None

        # No tuples. Only lists.
        args = list(args)

        self.graph: "Graph" = graph

        self.target = target

        self.args = args
        self.kwargs = kwargs

        self._listeners: Set[int] = set()
        self._dependencies: Set[int] = set()

        self._value: Any = inspect._empty
        self.remaining_listeners = 0
        self.remaining_dependencies = 0
        self.executed = False

        self.meta_data = self._meta_data()

        # If theres an alive Graph, add it.
        if self.attached:

            self.graph.add(self)
            
            # Preprocess args.
            self.preprocess()

    def __getstate__(self):

        state = self.__dict__.copy()

        return state
    
    def __setstate__(self, state: Dict) -> None:

        self.__dict__.update(state)
            
    @property
    def listeners(self) -> List[Self]:
        """Iterator from index to `Node`.

        Returns:
            List[Self]: List of listener `Node`s.
        """
        
        return [self.graph.nodes[index] for index in self._listeners]
    
    @property
    def dependencies(self) -> List[Self]:
        """Iterator from index to `Node`.

        Returns:
            List[Self]: List of dependency `Node`s.
        """
        
        return [self.graph.nodes[index] for index in self._dependencies]

    def preprocess(self) -> None:
        """Preprocess Node.args and Node.kwargs.
        Converts Proxies to their Node.
        Converts Nodes that are done to their value.
        Adds Node arguments to self dependencies.
        Add self to Node argument listeners.
        """

        def preprocess_node(node: Union[NodeType, ProxyType]):

            if isinstance(node, Proxy):

                node = node.node

            if node.done:

                return node.value

            self._dependencies.add(node.index)
            node._listeners.add(self.index)

            return node

        self.args, self.kwargs = util.apply(
            (self.args, self.kwargs), preprocess_node, (Node, Proxy)
        )

    ### Properties ########################
    @property
    def value(self) -> Any:
        """Property to return the value of this node.

        Returns:
            Any: The stored value of the node, populated during execution.

        Raises:
            ValueError: If the underlying ._value is inspect._empty (therefore never set or was destroyed).
        """

        if not self.done:
            raise ValueError("Accessing value before it's been set.")

        return self._value

    @property
    def attached(self) -> bool:
        """Checks to see if the `Graph` this `Node` is a part of is alive..
        Alive meaning the Graph is still open to tracing new Nodes.

        Returns:
            bool: Is Node attached.
        """

        try:

            return self.graph.alive

        except:
            return False

    @property
    def done(self) -> bool:
        """Returns true if the value of this node has been set.

        Returns:
            bool: If done.
        """
        return self._value is not inspect._empty

    @property
    def fulfilled(self) -> bool:
        """Returns true if remaining_dependencies is 0.

        Returns:
            bool: If fulfilled.
        """
        return self.remaining_dependencies == 0

    @property
    def redundant(self) -> bool:
        """Returns true if remaining_listeners is 0.

        Returns:
            bool: If redundant.
        """
        return self.remaining_listeners == 0

    ### API #############################
    def reset(self) -> None:
        """Resets this Nodes remaining_listeners and remaining_dependencies."""

        self.executed = False
        self._value = inspect._empty

        self.remaining_listeners = len(self._listeners)
        self.remaining_dependencies = sum(
            [not node.executed for node in self.dependencies]
        )

    def create(
        self,
        *args,
        **kwargs,
    ) -> Union[NodeType, Any]:
        """We use Node.create vs Graph.create in case graph is dead.
        If the graph is dead, we first check the GlobalTracing Context to add
        assume this node is ready to execute and therefore we try and execute it and then return its value.

        Returns:
            Union[NodeType, Any]: Proxy or value
        """

        if not self.attached:

            from ..contexts.globals import GlobalTracingContext

            if GlobalTracingContext.GLOBAL_TRACING_CONTEXT:

                return GlobalTracingContext.GLOBAL_TRACING_CONTEXT.graph.create(
                    *args,
                    **kwargs,
                )

            # Create dangling Node.
            node = type(self)(
                *args,
                **kwargs,
            )

            # Reset it.
            node.reset()

            # So it doesn't get destroyed.
            node.remaining_listeners = 1

            # Execute Node
            node.execute()

            # Get value.
            value = node.value

            # Destroy.
            node.destroy()

            return value

        # Otherwise just create the Node on the Graph like normal.
        return self.graph.create(
            *args,
            **kwargs,
        )

    @classmethod
    def prepare_inputs(cls, inputs: Any) -> Any:
        """Prepare arguments for executing this node's target.
        Converts Nodes in args and kwargs to their value.

        Returns:
            Any: Prepared inputs.
        """

        inputs = util.apply(inputs, lambda x: x, inspect._empty)

        def _value(node: Union[ProxyType, NodeType]):

            if isinstance(node, Proxy):
                node = node.node

            return node.value

        inputs = util.apply(inputs, _value, (Node, Proxy))

        return inputs

    def execute(self) -> None:
        """Actually executes this node.
        Lets protocol execute if target is Protocol.
        Else prepares args and kwargs and passes them to target. Gets output of target and sets the Node's value to it.
        """

        self.executed = True

        try:

            if isinstance(self.target, type) and issubclass(self.target, Protocol):

                self.target.execute(self)

            else:

                # Prepare arguments.
                args, kwargs = self.prepare_inputs((self.args, self.kwargs))

                # Call the target to get value.
                output = self.target(*args, **kwargs)
    

                # Set value.
                self.set_value(output)
        except NNsightError as e:
            raise e
        except Exception as e:
            traceback_content = traceback.format_exc()
            raise NNsightError(str(e), self.index, traceback_content)

    def set_value(self, value: Any) -> None:
        """Sets the value of this Node and logs the event.
        Updates remaining_dependencies of listeners. If they are now fulfilled, execute them.
        Updates remaining_listeners of dependencies. If they are now redundant, destroy them.

        Args:
            value (Any): Value.
        """
        self._value = value
        
        if self.graph is not None:

            self.update_listeners()

            self.update_dependencies()

            if self.done and self.redundant:
                self.destroy()

    def update_listeners(self):
        """Updates remaining_dependencies of listeners."""

        for listener in self.listeners:
            listener.remaining_dependencies -= 1

    def update_dependencies(self):
        """Updates remaining_listeners of dependencies. If they are now redundant, destroy them."""
        
        for dependency in self.dependencies:
            if len(self.graph.defer_stack) > 0 and dependency.index  < self.graph.defer_stack[-1]:
                continue
            
            dependency.remaining_listeners -= 1

            if dependency.redundant:
                dependency.destroy()

    def destroy(self) -> None:
        """Removes the reference to the node's value and logs it's destruction."""

        self._value = inspect._empty

    def subgraph(self, subgraph: Optional[Set[int]] = None) -> Set[int]:
        """Returns a Set of indexes starting from this node, and recursively iterating over all the Node's listeners.

        Args:
            subgraph (Optional[Set[int]], optional): Current subgraph. Defaults to None.

        Returns:
            Set[int]: Set of Node indexes.
        """

        if subgraph is None:
            subgraph = set()

        if self.index in subgraph:
            return subgraph
    
        subgraph.add(self.index)

        for listener in self.listeners:
            listener.subgraph(subgraph)

        return subgraph
    
    def _meta_data(self) -> Dict[str, Any]:
        """ Creates a dictionary of meta-data for this node.
        Contains the following key-value pairs:
            - traceback: Optional[str]: If the Graph is in debug mode, 
                a traceback string is compiled to be used if the execution of this Node raises an error.
        
        Returns:
            Dict[str, Any]: Meta-Data dictionary.
        """

        meta_data = dict()

        def traceback_str() -> str:
            """ Compiles a string of all the lines in the Traceback up until nnsight code is called.
            Returns:
                Str: Call Stack
            """
            traceback_str = ""
            stack = traceback.extract_stack()
            for frame in stack:
                # exclude frames created by nnsight or from the python environment
                if not bool(re.search((r'/lib/python3\.\d+/'), frame.filename)) and not ('/nnsight/src/nnsight/' in frame.filename):
                    traceback_str += f"  File \"{frame.filename}\", line {frame.lineno}, in {frame.name}\n"
                    traceback_str += f"    {frame.line}\n"
                else:
                    if traceback_str == "":
                        continue
                    else:
                        break

            traceback_str = "Traceback (most recent call last):\n" + traceback_str

            return traceback_str

        if self.attached and self.graph.debug:
            meta_data["traceback"] = traceback_str()

        return meta_data

    ### Magic Methods #####################################
    def __str__(self) -> str:
        return f"{self.target.__name__} {self.index}"

    def __repr__(self) -> str:
        return f"&lt;{self.__class__.__name__} at {hex(id(self))}&gt;"

    def __hash__(self) -> int:
        return id(self)


NodeType = TypeVar("NodeType", bound=Node)



---
File: /src/nnsight/tracing/graph/proxy.py
---

from __future__ import annotations

import inspect
import operator
from functools import wraps
from typing import TYPE_CHECKING, Any, Callable, Iterator, TypeVar, Union

from typing_extensions import Self

from ... import CONFIG, util
from .. import protocols

if TYPE_CHECKING:
    from .node import Node


class Proxy:
    """Proxy objects are the actual objects that interact with operations in order to update the graph to create new Nodes.

    The operations that are traceable on base Proxy objects are many python built-in and magic methods.

    Attributes:
        node (NodeType): This proxy's Node.
    """

    def __init__(self, node: "Node") -> None:

        self.__dict__["node"] = node

        self.node: "Node"

    ### API ##############################

    def save(self) -> Self:
        """Adds a lock Node to prevent its value from being cleared where normally it would be cleared when its no longer needed to save memory.
        Used to access values outside of the tracing context, after execution.

        Returns:
            InterventionProxy: Proxy.
        """

        # Add a 'lock' node with the save proxy as an argument to ensure the values are never deleted.
        # This is because 'lock' nodes never actually get set and therefore there will always be a
        # dependency for the save proxy.

        protocols.LockProtocol.add(self.node)

        return self

    def stop(self) -> None:
        protocols.StopProtocol.add(
            self.node.graph,
            self.node,
        )

    @property
    def value(self) -> Any:
        """Property to return the value of this proxy's node.

        Returns:
            Any: The stored value of the proxy, populated during execution of the model.
        """

        return self.node.value

    def __str__(self) -> str:

        if not self.node.attached:

            return str(self.value)

        return f"{type(self).__name__} ({self.node.target.__name__})"

    def __repr__(self) -> str:

        if not self.node.attached:

            return repr(self.value)

        return str(self)

    ### Special ################

    @staticmethod
    def call(callable: Callable, *args, **kwargs) -> Self:
        return callable(*args, **kwargs)

    def __call__(self, *args, **kwargs) -> Self:
        """
        Calling a Proxy object just creates a Proxy.proxy_call operation.

        Returns:
            Proxy: New call proxy.
        """

        return self.node.create(
            Proxy.call,
            *([self.node] + list(args)),
            **kwargs,
        )

    def __getattr__(self, key: Union[Self, Any]) -> Self:
        return self.node.create(util.fetch_attr, self.node, key)

    def __setattr__(self, key: Union[Proxy, Any], value: Union[Self, Any]) -> None:

        if key == "__dict__":

            super().__setattr__(key, value)

            return

        return self.node.create(
            setattr,
            self.node,
            key,
            value,
        )

    ### Regular Operators #########################

    def __getitem__(self, key: Union[Self, Any]) -> Self:
        return self.node.create(operator.getitem, self.node, key)

    def __setitem__(self, key: Union[Self, Any], value: Union[Self, Any]) -> None:
        self.node.create(
            operator.setitem,
            self.node,
            key,
            value,
        )

    def __abs__(self) -> Self:
        return self.node.create(
            operator.abs,
            self.node,
        )

    def __invert__(self) -> Self:
        return self.node.create(
            operator.invert,
            self.node,
        )

    def __neg__(self) -> Self:
        return self.node.create(
            operator.neg,
            self.node,
        )

    def __add__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.add, self.node, other)

    def __radd__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.add,
            other,
            self.node,
        )

    def __sub__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.sub,
            self.node,
            other,
        )

    def __rsub__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.sub, other, self.node)

    def __pow__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.pow,
            self.node,
            other,
        )

    def __rpow__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.pow,
            other,
            self.node,
        )

    def __mul__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.mul,
            self.node,
            other,
        )

    def __rmul__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.mul,
            other,
            self.node,
        )

    def __mod__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.mod,
            self.node,
            other,
        )

    def __rmod__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.mod,
            other,
            self.node,
        )

    def __matmul__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.matmul,
            self.node,
            other,
        )

    def __rmatmul__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.matmul,
            other,
            self.node,
        )

    def __truediv__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.truediv,
            self.node,
            other,
        )

    def __rtruediv__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.truediv,
            other,
            self.node,
        )

    def __floordiv__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.floordiv,
            self.node,
            other,
        )

    def __rfloordiv__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(
            operator.floordiv,
            other,
            self.node,
        )

    def __eq__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.eq, self.node, other)

    def __ne__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.ne, self.node, other)

    def __lt__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.lt, self.node, other)

    def __gt__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.gt, self.node, other)

    def __le__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.le, self.node, other)

    def __ge__(self, other: Union[Self, Any]) -> Self:
        return self.node.create(operator.ge, self.node, other)

    def __index__(self) -> Self:
        return self.node.create(operator.index, self.node)

    def __len__(self) -> Self:
        return self.node.create(
            len,
            self.node,
        )

    ### Hacks ##############################

    def __iter__(self) -> Iterator[Self]:

        if not CONFIG.APP.CONTROL_FLOW_HANDLING:
            raise Exception(
                'Iteration control flow encountered but "CONFIG.APP.CONTROL_FLOW_HACKS" is set to False'
            )

        from ..hacks import iterator

        return iterator.handle_proxy(inspect.currentframe().f_back, self)

    def __bool__(self) -> Self:

        if not CONFIG.APP.CONTROL_FLOW_HANDLING:
            raise Exception(
                'Conditional control flow encountered but "CONFIG.APP.CONTROL_FLOW_HACKS" is set to False'
            )

        from ..hacks import conditional

        return conditional.handle_proxy(inspect.currentframe().f_back, self)

    def __instancecheck__(self, __instance: Any) -> bool:
        return self.node.fake_value.__instancecheck__(__instance)


ProxyType = TypeVar("ProxyType", bound=Proxy)


def proxy_patch(fn: Callable):

    @wraps(fn)
    def inner(*args, **kwargs):

        found: Proxy = None

        def find(proxy: Proxy):

            nonlocal found

            found = proxy

        util.apply((args, kwargs), find, Proxy)

        if found is not None:

            return found.node.graph.create(
                fn,
                *args,
                **kwargs,
            )

        return fn(*args, **kwargs)

    return inner



---
File: /src/nnsight/tracing/graph/viz.py
---

import tempfile
from collections import defaultdict
from collections.abc import Iterable
from typing import TYPE_CHECKING, Dict


import torch
from PIL import Image as PILImage

from ..protocols import Protocol
from . import Node, SubGraph

if TYPE_CHECKING:
    from . import Graph


def viz_graph(
    graph: "Graph",
    title: str = "graph",
    display: bool = True,
    save: bool = False,
    path: str = ".",
    recursive: bool = False,
    group: bool = False,
    ) -> None:
    """
    Utility funciton to visualize the NNsight Graph structures built during tracing.

    Args:
        - graph (Graph): NNsight Graph to be visualized.
        - title (str): Title given to the visualization. Default: "graph".
        - display (bool): Displays the rendered graph visualization. Default: True.
        - save (bool): Saves the rendered graph to a file with the title as the name. Default: False.
        - path (str): Path to store the saved visualization. Default: ".".
        - recursive (bool): Recursively visualize all the inner Subgraphs of a given Graph. Default: False.
        - group (bool): Visually group all the nodes belonging to the same Subgraph together. Default: False.
    """

    from ..contexts.globals import GlobalTracingContext

    with GlobalTracingContext.exit_global_tracing_context():
        
        try:

            import pygraphviz as pgv

        except Exception as e:

            raise type(e)(
                "Visualization of the Graph requires `pygraphviz` which requires `graphviz` to be installed on your machine."
            ) from e
        
        if group and not recursive:
            print("Warning: set `recursive=True` to visualize all subgraphs and make use of the 'group' functionality.")
            group = False
        
        from IPython.display import Image
        from IPython.display import display as IDisplay

        graph_viz: pgv.AGraph = pgv.AGraph(strict=True, directed=True)

        graph_viz.graph_attr.update(
            label=title, fontsize="20", labelloc="t", labeljust="c"
        )

        def style_node(node: Node) -> Dict:
            """Gets the style of the node based on it's target.
            If the target is a Protocol, then it gets the style directly from the protocol class.

            Args:
                - node (Node): node.

            Returns:
                - Dict: dictionary style.
            """

            if isinstance(node.target, type) and issubclass(node.target, Protocol):
                return node.target.style()
            else:
                return {
                    "node": {"color": "black", "shape": "ellipse"},
                    "label": (node.target if isinstance(node.target, str) else node.target.__name__),
                    "arg": defaultdict(lambda: {"color": "gray", "shape": "box"}),
                    "arg_kname": defaultdict(lambda: None),
                    "edge": defaultdict(lambda: {"style": "solid"}),
                }
        
        subgraphs: Dict[int, pgv.AGraph] = {}
        subgraph_names_count: Dict[str, int] = defaultdict(lambda: 0)
        def get_subgraph(node: Node) -> pgv.AGraph:
            """ Returns the Graph Visualization Object where this node should be rendered.

            Args:
                - node (Node: )
            

            Returns:
                - pgv.AGraph: Graph Visualization Object.
            """

            if group:
                if id(node.graph) != id(graph):
                    if not id(node.graph) in subgraphs.keys():
                        subgraph = graph_viz.subgraph(name=f"cluster_{id(node.graph)}")
                        subgraph.graph_attr['penwidth'] = 0.25
                        subgraphs[id(node.graph)] = subgraph

                    return subgraphs[id(node.graph)]
                else:
                    return graph_viz
            else:
                return graph_viz
        
        if recursive:
            nodes = [node for node in graph.nodes if id(node.graph) >= id(graph)]
        else:
            nodes = graph

        visualized_nodes = set()
        for node in nodes:

            styles: Dict = style_node(node)

            subgraph: pgv.AGraph = get_subgraph(node)

            subgraph.add_node(node.index, label=styles["label"], **styles["node"])
            visualized_nodes.add(node.index)

            for idx, arg in enumerate(node.args):
                if isinstance(arg, SubGraph):
                    name: str = f"{node.index}_{arg}_{idx}"
                    label: str = f"Subgraph"
                    
                    subgraph.add_node(name, label=label, **{"color": "purple", "shape": "box"})
                    
                    if recursive:
                        for sub_node in arg:
                            root_node: bool = True
                            for dep_idx in sub_node._dependencies:
                                root_node = root_node and (dep_idx not in arg.subset)
                                
                            if root_node:
                                graph_viz.add_edge(node.index, sub_node.index, **{"style": "dashed", "color": styles["node"]["color"]})

                    if group:
                        subgraph_label: str = styles['label']
                        subgraphs[id(arg)].graph_attr['label'] = f"{subgraph_label}_{subgraph_names_count[subgraph_label]}"
                        subgraph_names_count[subgraph_label] += 1
                    
                elif isinstance(arg, Node):
                    name = arg.index
                    label = node.index

                    if arg.index not in visualized_nodes:
                        arg_label = (arg.target if isinstance(arg.target, str) else arg.target.__name__)

                        subgraph.add_node(arg.index, label=arg_label, **{"color": "brown", "shape": "box"})

                        visualized_nodes.add(arg.index)
                else:
                    name = str(node.index)
                    if isinstance(arg, torch.Tensor):
                        name += f"_Tensor_{idx}"
                        label = "Tensor"
                    elif isinstance(arg, str):
                        name += f"_{arg}_{idx}"
                        label = f'"{arg}"'
                    else:
                        name += f"_{arg}_{idx}"
                        label = str(arg)

                    if not styles["arg_kname"][idx] is None:
                            label = f"{styles['arg_kname'][idx]}={label}"
                    else:
                        label = label

                    subgraph.add_node(name, label=label, **{"color": "gray", "shape": "box"})

                    if isinstance(arg, Iterable):
                        for idx, element in enumerate(arg):
                            if isinstance(element, Node):
                                if element.index not in visualized_nodes:

                                    element_label = (element.target if isinstance(element.target, str) else element.target.__name__)
                                    subgraph.add_node(element.index, label=element_label, color="brown", shape="box")
                                    visualized_nodes.add(element.index)
                                    
                                graph_viz.add_edge(element.index, name, style="dashed", color="gray", label=f"{idx}", fontsize=10)
                    
                subgraph.add_edge(name, node.index, **styles["edge"][idx])

        def display_graph(file_name):
            in_notebook = True

            # Credit: Till Hoffmann - https://stackoverflow.com/a/22424821
            try:
                from IPython import get_ipython

                if "IPKernelApp" not in get_ipython().config:
                    in_notebook = False
            except ImportError:
                in_notebook = False
            except AttributeError:
                in_notebook = False

            if in_notebook:
                IDisplay(Image(filename=file_name))
            else:
                img = PILImage.open(file_name)
                img.show()
                img.close()

        if not save:
            with tempfile.NamedTemporaryFile(suffix=".png") as temp_file:
                graph_viz.draw(temp_file.name, prog="dot")
                if display:
                    display_graph(temp_file.name)
        else:
            graph_viz.draw(f"{path}/{title}.png", prog="dot")
            if display:
                display_graph(f"{path}/{title}.png")



---
File: /src/nnsight/tracing/hacks/__init__.py
---

import ast
from types import FrameType

from ..graph import Graph
from .conditional import handle as handle_conditional
from .iterator import handle as handle_iterator


def handle_inner(node:ast.stmt, frame: FrameType, graph: Graph):
    
    if isinstance(node, ast.If):
        
        handle_conditional(node, frame, graph)
        
        return True
    
    elif isinstance(node, ast.For):
        
        handle_iterator(node, frame, graph)
        
        return True
    
    return False


---
File: /src/nnsight/tracing/hacks/comprehension.py
---

import ast
import ctypes
import inspect
import sys
from types import FrameType
from typing import TYPE_CHECKING

from ..contexts import Iterator
from ..graph import Graph
from .util import execute, execute_body, execute_until, visit

if TYPE_CHECKING:
    from ..graph import Proxy

COMPS = [ast.SetComp, ast.DictComp, ast.ListComp, ast.GeneratorExp]

def handle(node: ast.For, frame: FrameType, graph: Graph):

    iter_expr = ast.Expression(
        body=node.iter, lineno=node.lineno, col_offset=node.col_offset
    )

    iter = execute(iter_expr, frame)

    context = Iterator(iter, parent=graph)

    target = node.target

    with context as item:
        if isinstance(target, ast.Name):
            frame.f_locals[target.id] = item
        elif isinstance(target, ast.Tuple):
            for t, v in zip(target.elts, item):
                if isinstance(t, ast.Name):
                    frame.f_locals[t.id] = v
                    
        ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(frame), 0)
        
        execute_body(node.body, frame, context.graph)


def handle_proxy(node:ast.stmt, frame: FrameType, collection:"Proxy"):
    
    graph = collection.node.graph
        
    
    iterator = Iterator(collection, parent=graph)
        
    item = iterator.__enter__()
    
    def callback(new_frame:FrameType, list_proxy, iterator:Iterator):
        
        
        key, result = next(iter(new_frame.f_locals.items()))
        print(node, node.elt.elt.ctx.__dict__)
        
        # list_proxy.append(result[0]) 
            
        # new_frame.f_locals[key] = list_proxy
        # ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(new_frame), 0)
        
        iterator.__exit__(None, None, None)
        
    
    execute_until(frame.f_lineno -1, frame.f_lineno - 1, frame, callback= lambda new_frame: callback(new_frame, [], iterator))

    return iter([item])



---
File: /src/nnsight/tracing/hacks/conditional.py
---

import ast
import inspect
import sys
from types import FrameType
from typing import TYPE_CHECKING

from ..contexts import Condition
from .util import execute, execute_body, execute_until, visit
from ..graph import Graph
if TYPE_CHECKING:
    from ..graph import Proxy

def get_else(node: ast.If):

    return (
        node.orelse[0]
        if isinstance(node.orelse[0], ast.If)
        else ast.If(
            test=ast.Constant(value=None),
            body=node.orelse,
            orelse=[],
            lineno=node.lineno,
            col_offset=node.col_offset,
        )
    )
    
def handle(node: ast.If, frame:FrameType, graph:Graph, branch:Condition = None):

    condition_expr = ast.Expression(
        body=node.test, lineno=node.lineno, col_offset=node.col_offset
    )

    condition = execute(condition_expr, frame)
        
    context = Condition(condition, parent = graph) if branch is None else branch.else_(condition)

    with context as branch:
        execute_body(node.body, frame, branch.graph)

    if node.orelse:
        return handle(get_else(node), frame, graph, branch)
    
def handle_proxy(frame: FrameType, condition: "Proxy"):

    class Visitor(ast.NodeVisitor):
        def __init__(self, line_no):
            self.target = None
            self.line_no = line_no

        def visit_If(self, node):
            if node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)

    visitor = visit(frame, Visitor)

    if_node:ast.If = visitor.target
    
    graph = condition.node.graph

    branch = Condition(condition, parent=graph)

    def callback(node: ast.If, frame: FrameType, graph:Graph, branch:Condition):
        
        branch.__exit__(None, None, None)

        if node.orelse:
            handle(get_else(if_node), frame, graph, branch)

    branch.__enter__()
    end = frame.f_lineno + (if_node.end_lineno - if_node.lineno)
    execute_until(frame.f_lineno, end, frame, callback=lambda _: callback(if_node, frame, graph, branch))

    return True


---
File: /src/nnsight/tracing/hacks/iterator.py
---

import ast
import ctypes
import inspect
import sys
from types import FrameType
from typing import TYPE_CHECKING

from ..contexts import Iterator
from ..graph import Graph
from .util import execute, execute_body, execute_until, visit
from .comprehension import handle_proxy as handle_comprehension
if TYPE_CHECKING:
    from ..graph import Proxy

COMPS = [ast.SetComp, ast.DictComp, ast.ListComp, ast.GeneratorExp]

def handle(node: ast.For, frame: FrameType, graph: Graph):

    iter_expr = ast.Expression(
        body=node.iter, lineno=node.lineno, col_offset=node.col_offset
    )

    iter = execute(iter_expr, frame)

    context = Iterator(iter, parent=graph)

    target = node.target

    with context as item:
        if isinstance(target, ast.Name):
            frame.f_locals[target.id] = item
        elif isinstance(target, ast.Tuple):
            for t, v in zip(target.elts, item):
                if isinstance(t, ast.Name):
                    frame.f_locals[t.id] = v
                    
        ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(frame), 0)
        
        execute_body(node.body, frame, context.graph)


def handle_proxy(frame: FrameType, collection: "Proxy"):
    
    class Visitor(ast.NodeVisitor):
        def __init__(self, line_no):
            self.target = None
            self.line_no = line_no
            self.assign = None
            self.nodes_on_line = []
            
            
        def generic_visit(self, node):
            if hasattr(node, 'lineno') and node.lineno == self.line_no:
                self.nodes_on_line.append(node)
            super().generic_visit(node)
        def visit_Assign(self, node):
            if node.lineno == self.line_no:
                self.assign = node
            self.generic_visit(node)

        def visit_For(self, node):
            if node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)
            
        def visit_ListComp(self, node):
            if self.target is None and node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)

        def visit_DictComp(self, node):
            if self.target is None and node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)

        def visit_SetComp(self, node):
            if self.target is None and node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)

        def visit_GeneratorExp(self, node):
            if self.target is None and node.lineno == self.line_no:
                self.target = node
            self.generic_visit(node)

    visitor = visit(frame, Visitor)

    for_node:ast.If = visitor.target 

 
    if type(for_node) in COMPS:
        return handle_comprehension(for_node, frame, collection)
    
    graph = collection.node.graph
    
    iterator = Iterator(collection, parent=graph)

    item = iterator.__enter__()
    
    def callback(iterator: Iterator):
                
        iterator.__exit__(None, None, None)
    end = frame.f_lineno + (for_node.end_lineno - for_node.lineno)
    execute_until(frame.f_lineno, end, frame, callback=lambda _: callback(iterator))

    return iter([item])



---
File: /src/nnsight/tracing/hacks/util.py
---

import ast
import ctypes
import inspect
import sys
from types import FrameType
from typing import Any, Callable, List, Optional, Type

from ..contexts import Context
from ..graph import Graph


def execute(expr: ast.expr, frame: FrameType) -> Any:
    ast.fix_missing_locations(expr)
    return eval(
        compile(expr, "<string>", "eval"),
        frame.f_globals,
        frame.f_locals,
    )


def execute_body(body: List[ast.stmt], frame: FrameType, graph: Graph) -> None:

    from . import handle_inner

    for stmt in body:

        if not handle_inner(stmt, frame, graph):
            module = ast.Module(body=[stmt], type_ignores=[])
            ast.fix_missing_locations(module)
            exec(
                compile(module, "<string>", "exec"),
                frame.f_globals,
                frame.f_locals,
            )


def execute_until(
    first_line: int,
    last_line: int,
    frame: FrameType,
    callback: Optional[Callable] = None,
):

    prev_trace = frame.f_trace

    def trace(new_frame: FrameType, *args):

        if new_frame.f_code.co_filename == frame.f_code.co_filename and (
            new_frame.f_lineno > last_line or new_frame.f_lineno < first_line
        ):

            frame.f_trace = prev_trace
            sys.settrace(prev_trace)

            if prev_trace is not None:

                prev_trace(new_frame, *args)

            if callback is not None:

                callback(new_frame)

    frame.f_trace = trace
    sys.settrace(trace)


def is_ipython():
    return "_ih" in locals()


def visit(frame: FrameType, visitor_cls: Type[ast.NodeVisitor]) -> ast.stmt:

    line_no = frame.f_lineno

    if "_ih" in frame.f_locals:
        import IPython

        ipython = IPython.get_ipython()
        source_lines = ipython.user_global_ns["_ih"][-1]
        inner_line_no = 0

    else:
        source_lines, inner_line_no = inspect.getsourcelines(frame)

    if inner_line_no > 0:
        line_no = line_no - inner_line_no + 1

        shift = len(source_lines[0]) - len(source_lines[0].lstrip())

        if shift > 0:

            source_lines = [source_line[shift:] for source_line in source_lines]

    source = "".join(source_lines)

    tree = ast.parse(source)

    visitor = visitor_cls(line_no)
    visitor.visit(tree)

    return visitor



---
File: /src/nnsight/tracing/protocols/__init__.py
---

from .base import Protocol
from .variable import VariableProtocol
from .stop import StopProtocol
from .lock import LockProtocol


---
File: /src/nnsight/tracing/protocols/base.py
---

from collections import defaultdict
from typing import TYPE_CHECKING, Any, Dict

if TYPE_CHECKING:
    from ..graph import GraphType, Node, Proxy
    
class Protocol:
    
    @staticmethod
    def is_protocol(thing:Any):
        
        return isinstance(thing, type) and issubclass(thing, Protocol)

    @classmethod
    def add(cls, graph:"GraphType",*args, **kwargs) -> "Proxy":
        
        return graph.create(
            cls,
            *args,
            **kwargs
            
        )

    @classmethod
    def execute(cls, node: "Node"):
      
        pass

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        return {
            "node": {"color": "black", "shape": "ellipse"},  # Node display
            "label": cls.__name__,
            "arg": defaultdict(
                lambda: {"color": "gray", "shape": "box"}
            ),  # Non-node argument display
            "arg_kname": defaultdict(lambda: None),  # Argument label key word
            "edge": defaultdict(lambda: {"style": "solid"}),
        }  # Argument edge display



---
File: /src/nnsight/tracing/protocols/lock.py
---

from typing import TYPE_CHECKING, Any, Dict

from . import Protocol

if TYPE_CHECKING:
    from ..graph import ProxyType, NodeType


class LockProtocol(Protocol):

    @classmethod
    def add(cls, node: "NodeType") -> "ProxyType":
        return node.create(
            cls,
            node,
            fake_value=None,
        )
    
    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "brown", "shape": "ellipse"}

        return default_style



---
File: /src/nnsight/tracing/protocols/stop.py
---

from typing import TYPE_CHECKING, Any, Dict

from . import Protocol
from ...util import NNsightError

if TYPE_CHECKING:
    from ..graph import Node


class StopProtocol(Protocol):

    class StopException(NNsightError):
        pass

    @classmethod
    def execute(cls, node: "Node") -> None:

        raise cls.StopException("Early Stop Exception!", node.index)
    
    @classmethod
    def style(cls) -> Dict[str, Any]:
        """Visualization style for this protocol node.

        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "red", "shape": "polygon", "sides": 6}

        return default_style




---
File: /src/nnsight/tracing/protocols/variable.py
---

from typing import TYPE_CHECKING, Any, Dict

from . import Protocol

if TYPE_CHECKING:
    from ..graph import Node

class VariableProtocol(Protocol):

    @classmethod
    def set(cls, node: "Node", value: Any):

        node.args = [value]

    @classmethod
    def execute(cls, node: "Node"):

        value = node.prepare_inputs(node.args[0])

        node.set_value(value)

    @classmethod
    def style(cls) -> Dict[str, Any]:
        """ Visualization style for this protocol node.
        
        Returns:
            - Dict: dictionary style.
        """

        default_style = super().style()

        default_style["node"] = {"color": "blue", "shape": "box"}
        
        return default_style



---
File: /src/nnsight/tracing/__init__.py
---

"""The `tracing` module acts as a standalone library to trace and execute Python based deferred computation graphs.

The `graph` sub-module defines the computation graph primitives.
The `protocol` sub-module contains logic for adding custom operations to the computation graph.
The `contexts` sub-module contains logic for defining scoped sub-graphs that handle execution of their piece of the computation graph.
The `backends` sub-module contains logic for executing the traced computation graph.
"""


---
File: /src/nnsight/__init__.py
---

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#   _____  ___   _____  ___    ________  __     _______    __    __  ___________        ______     ___  ___     # 
#  (\"   \|"  \ (\"   \|"  \  /"       )|" \   /" _   "|  /" |  | "\("     _   ")      /    " \   (: "||_  |    #
#  |.\\   \    ||.\\   \    |(:   \___/ ||  | (: ( \___) (:  (__)  :))__/  \\__/      // ____  \  |  (__) :|    # 
#  |: \.   \\  ||: \.   \\  | \___  \   |:  |  \/ \       \/      \/    \\_ /        /  /    ) :)  \____  ||    # 
#  |.  \    \. ||.  \    \. |  __/  \\  |.  |  //  \ ___  //  __  \\    |.  |       (: (____/ //_____  _\ '|    # 
#  |    \    \ ||    \    \ | /" \   :) /\  |\(:   _(  _|(:  (  )  :)   \:  |        \        /))_  ")/" \_|\   # 
#   \___|\____\) \___|\____\)(_______/ (__\_|_)\_______)  \__|  |__/     \__|         \"_____/(_____((_______)  # 
#                                                                                                               #
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
import os
from functools import wraps

from importlib.metadata import PackageNotFoundError, version
from typing import Any, Callable, Dict, Union

try:
    __version__ = version("nnsight")
except PackageNotFoundError:
    __version__ = "unknown version"

from IPython import get_ipython

try:
    __IPYTHON__ = get_ipython() is not None
except NameError:
    __IPYTHON__ = False

import torch
import yaml

from .schema.config import ConfigModel
from .util import Patch, Patcher

PATH = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(PATH, "config.yaml"), "r") as file:
    CONFIG = ConfigModel(**yaml.safe_load(file))


from .logger import logger, remote_logger
from .intervention import Envoy, NNsight
from .modeling.language import LanguageModel

logger.disabled = not CONFIG.APP.LOGGING
remote_logger.disabled = not CONFIG.APP.REMOTE_LOGGING

# Below do default patching:
DEFAULT_PATCHER = Patcher()


# Tensor creation operations
from torch._subclasses.fake_tensor import FakeTensor


def fake_bool(self):
    return True


DEFAULT_PATCHER.add(Patch(FakeTensor, fake_bool, "__bool__"))


from torch.amp.autocast_mode import autocast

def wrap_autocast(func):
    
    @wraps(func)
    def inner(self, device_type:str, *args, **kwargs):
        
        if device_type == "meta":
            device_type = "cpu"
            
        return func(self, device_type, *args, **kwargs)
        
    return inner


DEFAULT_PATCHER.add(Patch(autocast, wrap_autocast(autocast.__init__), "__init__"))

DEFAULT_PATCHER.__enter__()

from .intervention.contexts import GlobalInterventionTracingContext

apply = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.apply
log = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.log
local = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.local
cond = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.cond
iter = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.iter
stop = GlobalInterventionTracingContext.GLOBAL_TRACING_CONTEXT.stop

def trace(fn):
    """Helper decorator to add a function to the intervention graph via `.apply(...)`.
    This is opposed to entering the function during tracing and tracing all inner operations.

    Args:
        fn (Callable): Function to apply.

    Returns:
        Callable: Traceable function.
    """

    @wraps(fn)
    def inner(*args, **kwargs):
        
        return apply(fn, *args, **kwargs)

    return inner


bool = trace(bool)
bytes = trace(bytes)
int = trace(int)
float = trace(float)
str = trace(str)
complex = trace(complex)
bytearray = trace(bytearray)
tuple = trace(tuple)
list = trace(list)
set = trace(set)
dict = trace(dict)


---
File: /src/nnsight/logger.py
---

import logging
import os
from logging.handlers import RotatingFileHandler

# Work out the log directory:
# – Look for an environment variable called NNSIGHT_LOG_PATH (stripped of whitespace);
# – if it’s unset or empty, fall back to the default.
# Then make sure that directory actually exists, creating it if necessary.
_DEFAULT_PATH = os.path.dirname(os.path.abspath(__file__))
_env_path = os.getenv("NNSIGHT_LOG_PATH", "").strip()
PATH = _env_path or _DEFAULT_PATH
os.makedirs(PATH, exist_ok=True)

logging_handler = RotatingFileHandler(
    os.path.join(PATH, f"nnsight.log"),
    mode="a",
    maxBytes=5 * 1024 * 1024,
    backupCount=2,
    encoding=None,
    delay=0,
)
logging_handler.setFormatter(
    logging.Formatter(
        "%(asctime)s %(processName)-10s %(name)s %(levelname)-8s %(message)s"
    )
)
logging_handler.setLevel(logging.INFO)

logger = logging.getLogger("nnsight")
logger.addHandler(logging_handler)
logger.setLevel(logging.INFO)

# set up a std out logger
remote_logger = logging.getLogger("nnsight_remote")
remote_handler = logging.StreamHandler()
remote_handler.setFormatter(
    logging.Formatter("%(asctime)s %(message)s")
)
remote_handler.setLevel(logging.INFO)
remote_logger.addHandler(remote_handler)
remote_logger.setLevel(logging.INFO)


---
File: /src/nnsight/test.py
---



from .tracing.contexts import Tracer

with Tracer() as session:
    
    ls = session.graph.create(list)
    
    with session.trace()as tracer:
        
        ls.append(4)
        
        
        ls.append(1)
        
        
        
    for itt in ls:
        ls.append(itt)
        

    
    if ls[-1] == 4:
        ls.append(10001)

    elif ls[-1] == 1:
        ls.append(11111111)   
    else:
        ls.append(123)
    
    with session.trace() as tracer:
        
        ls.append(3)
        
    
    
    ls.append(5)
    session.graph.create(print, ls)
    
    print(session.graph)



---
File: /src/nnsight/util.py
---

"""Module for utility functions and classes used throughout the package."""

import importlib
from contextlib import AbstractContextManager
from typing import Any, Callable, Collection, List, Optional, Type, TypeVar

import torch
from typing_extensions import Self

# TODO Have an Exception you can raise to stop apply early

T = TypeVar("T")
C = TypeVar("C", bound=Collection[T])


def apply(
    data: C, fn: Callable[[T], Any], cls: Type[T], inplace: bool = False
) -> C:
    """Applies some function to all members of a collection of a give type (or types)

    Args:
        data (Any): Collection of data to apply function to.
        fn (Callable): Function to apply.
        cls (type): Type or Types to apply function to.
        inplace (bool): If to apply the fn inplace. (For lists and dicts)

    Returns:
        Any: Same kind of collection as data, after then fn has been applied to members of given type.
    """

    if isinstance(data, cls):
        return fn(data)

    data_type = type(data)

    if data_type == list:
        if inplace:
            for idx, _data in enumerate(data):
                data[idx] = apply(_data, fn, cls, inplace=inplace)
            return data
        return [apply(_data, fn, cls, inplace=inplace) for _data in data]

    elif data_type == tuple:
        return tuple([apply(_data, fn, cls, inplace=inplace) for _data in data])

    elif data_type == dict:
        if inplace:
            for key, value in data.items():
                data[key] = apply(value, fn, cls, inplace=inplace)
            return data
        return {
            key: apply(value, fn, cls, inplace=inplace)
            for key, value in data.items()
        }

    elif data_type == slice:
        return slice(
            apply(data.start, fn, cls, inplace=inplace),
            apply(data.stop, fn, cls, inplace=inplace),
            apply(data.step, fn, cls, inplace=inplace),
        )

    return data

def fetch_attr(object: object, target: str) -> Any:
    """Retrieves an attribute from an object hierarchy given an attribute path. Levels are separated by '.' e.x (transformer.h.1)

    Args:
        object (object): Root object to get attribute from.
        target (str): Attribute path as '.' separated string.

    Returns:
        Any: Fetched attribute.
    """
    if target == "":
        return object

    target_atoms = target.split(".")

    for atom in target_atoms:

        if not atom:
            continue

        object = getattr(object, atom)

    return object

def to_import_path(type: type) -> str:

    return f"{type.__module__}.{type.__name__}"


def from_import_path(import_path: str) -> type:

    *import_path, classname = import_path.split(".")
    import_path = ".".join(import_path)

    return getattr(importlib.import_module(import_path), classname)


class Patch:
    """Class representing a replacement of an attribute on a module.

    Attributes:
        obj (Any): Object to replace.
        replacement (Any): Object that replaces.
        parent (Any): Module or class to replace attribute.
    """

    def __init__(self, parent: Any, replacement: Any, key: str) -> None:
        self.parent = parent
        self.replacement = replacement
        self.key = key
        self.orig = getattr(self.parent, key)

    def patch(self) -> None:
        """Carries out the replacement of an object in a module/class."""
        setattr(self.parent, self.key, self.replacement)

    def restore(self) -> None:
        """Carries out the restoration of the original object on the objects module/class."""

        setattr(self.parent, self.key, self.orig)

class Patcher(AbstractContextManager):
    """Context manager that patches from a list of Patches on __enter__ and restores the patch on __exit__.

    Attributes:
        patches (List[Patch]):
    """

    def __init__(self, patches: Optional[List[Patch]] = None) -> None:
        self.patches = patches or []
        
        self.entered = False

    def add(self, patch: Patch) -> None:
        """Adds a Patch to the patches. Also calls `.patch()` on the Patch.

        Args:
            patch (Patch): Patch to add.
        """
        self.patches.append(patch)

        if self.entered:
            patch.patch()

    def __enter__(self) -> Self:
        """Enters the patching context. Calls `.patch()` on all patches.

        Returns:
            Patcher: Patcher
        """
        
        self.entered = True
        
        for patch in self.patches:
            patch.patch()

        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Calls `.restore()` on all patches."""
        self.entered = False
        for patch in self.patches:
            patch.restore()


class WrapperModule(torch.nn.Module):
    """Simple torch module which passes it's input through. Useful for hooking.
    If there is only one argument, returns the first element.
    """

    def forward(self, *args, **kwargs):
        if len(args) == 1:
            args = args[0]

        return args

class NNsightError(Exception):
    """NNsight Execption class for raising error during execution.
    
    Attributes:
        - message (str): error message.
        - node_id (int): node id.
        - traceback_content (Optional[str]): traceback of the original exception being raised.
    """

    def __init__(self, message: str, node_id: int, traceback_content: Optional[str] = None):
        self.message = message
        self.node_id = node_id
        self.traceback_content = traceback_content
        super().__init__(self.message)


    def _render_traceback_(self) -> List[str]:
        """
        This function allows custom rendering of traceback in IPython
        
        Returns:
            - List of string lines.
        """
        traceback_list = self.traceback_content.split("\n")
        traceback_list.append(f"{str(self.__class__.__name__)}: {self.message}")

        return traceback_list



---
File: /tests/__init__.py
---




---
File: /tests/test_diffusion.py
---

import pytest
import torch
import PIL

if not torch.cuda.is_available():
    pytest.skip("no GPU available.", allow_module_level=True)

from nnsight.modeling.diffusion import DiffusionModel

@pytest.fixture(scope="module")
def tiny_sd():
    return DiffusionModel("segmind/tiny-sd", torch_dtype=torch.float16, dispatch=True).to("cuda")

@pytest.fixture(scope="module")
def cat_prompt():
    return "A brown and white cat staring off with pretty green eyes"

def test_generation(tiny_sd, cat_prompt):
    num_images_per_prompt = 3

    images = tiny_sd.generate(cat_prompt, 
                              num_inference_steps=20, 
                              num_images_per_prompt=num_images_per_prompt,
                              seed=423, 
                              trace=False).images

    assert len(images) == num_images_per_prompt
    assert all([type(img) == PIL.Image.Image for img in images])
    assert all(images[i] != images[j] for i in range(num_images_per_prompt) for j in range(i + 1, num_images_per_prompt))    



---
File: /tests/test_lm.py
---

import pytest
import torch
import msgspec
import zlib
import nnsight
from nnsight.intervention.contexts import InterventionTracer, Session
from nnsight.tracing.backends import Backend
from nnsight.tracing.graph import Graph
from nnsight.tracing.protocols import StopProtocol
from nnsight.tracing.contexts import GlobalTracingContext
from nnsight.schema.request import RequestModel


class AssertSavedLenBackend(Backend):

    def __init__(self, len: int) -> None:
        self.len = len

    def __call__(self, graph: Graph) -> None:

        try:

            graph.nodes[-1].execute()

        except StopProtocol.StopException:

            pass

        finally:

            assert self.len == len([node for node in graph.nodes if node.done])

            graph.nodes.clear()
            graph.stack.clear()


@pytest.fixture(scope="module")
def gpt2(device: str):
    return nnsight.LanguageModel(
        "openai-community/gpt2", device_map=device, dispatch=True
    )


@pytest.fixture
def MSG_prompt():
    return "Madison Square Garden is located in the city of"


def _test_serialize(tracer: InterventionTracer):

    with GlobalTracingContext.exit_global_tracing_context():
        request = RequestModel.serialize(tracer.graph.stack[0], "json", True)

        model = tracer.model if isinstance(tracer, Session) else tracer._model

        graph = RequestModel.deserialize(model, request, "json", True)
    assert isinstance(graph, Graph)


@torch.no_grad()
def test_generation(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(
        max_new_tokens=3, validate=True, backend=AssertSavedLenBackend(1)
    ) as generator:
        with generator.invoke(MSG_prompt, scan=True) as invoker:
            output = gpt2.generator.output.save()

        _test_serialize(generator)

    output = gpt2.tokenizer.decode(output.value[0])

    assert output == "Madison Square Garden is located in the city of New York City"


@torch.no_grad()
def test_save(gpt2: nnsight.LanguageModel):
    with gpt2.generate(
        "Hello world", validate=True, scan=True, backend=AssertSavedLenBackend(2)
    ) as tracer:

        hs = gpt2.transformer.h[-1].output[0].save()
        hs_input = gpt2.transformer.h[-1].input.save()

        _test_serialize(tracer)

    assert hs.value is not None
    assert isinstance(hs.value, torch.Tensor)
    assert hs.value.ndim == 3

    assert hs_input.value is not None
    assert isinstance(hs_input.value, torch.Tensor)
    assert hs_input.value.ndim == 3


@torch.no_grad()
def test_set1(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(validate=True, backend=AssertSavedLenBackend(3)) as tracer:
        with tracer.invoke(MSG_prompt, scan=True) as invoker:
            pre = gpt2.transformer.h[-1].output[0].clone().save()

            gpt2.transformer.h[-1].output[0][:] = 0

            post = gpt2.transformer.h[-1].output[0].save()

            output = gpt2.generator.output.save()

        _test_serialize(tracer)

    output = gpt2.tokenizer.decode(output.value[0])

    assert not (pre.value == 0).all().item()
    assert (post.value == 0).all().item()
    assert output != "Madison Square Garden is located in the city of New"


@torch.no_grad()
def test_set2(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(validate=True, backend=AssertSavedLenBackend(3)) as generator:
        with generator.invoke(MSG_prompt, scan=True) as invoker:
            pre = gpt2.transformer.wte.output.clone().save()

            gpt2.transformer.wte.output = gpt2.transformer.wte.output * 0

            post = gpt2.transformer.wte.output.save()

            output = gpt2.generator.output.save()

        _test_serialize(generator)

    output = gpt2.tokenizer.decode(output.value[0])

    assert not (pre.value == 0).all().item()
    assert (post.value == 0).all().item()
    assert output != "Madison Square Garden is located in the city of New"

@torch.no_grad()
def test_set3(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(validate=True, backend=AssertSavedLenBackend(3)) as generator:
        with generator.invoke(MSG_prompt, scan=True) as invoker:
            pre = gpt2.transformer.h[-1].output.save()

            gpt2.transformer.h[-1].output = (torch.zeros_like(gpt2.transformer.h[-1].output[0]),) + gpt2.transformer.h[-1].output[1:]

            post = gpt2.transformer.h[-1].output.save()

            output = gpt2.generator.output.save()

        _test_serialize(generator)

    output = gpt2.tokenizer.decode(output.value[0])

    assert not (pre.value[0] == 0).all().item()
    assert (post.value[0] == 0).all().item()
    assert output != "Madison Square Garden is located in the city of New"

@torch.no_grad()
def test_set4(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(2)) as tracer:
        with tracer.invoke(MSG_prompt, scan=True):
            gpt2.transformer.h[-1].output = (torch.zeros_like(gpt2.transformer.h[-1].output[0]),) + gpt2.transformer.h[-1].output[1:]

        with tracer.invoke(MSG_prompt, scan=True):
            hs = gpt2.transformer.h[-1].output.save()
            out = gpt2.lm_head.output[0][-1].argmax(dim=-1).save()

    assert isinstance(hs.value, tuple)
    assert torch.all(hs.value[0] != 0)
    assert gpt2.tokenizer.decode(out.value) == " New"


def test_set5(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(2)) as tracer:
        gpt2.transformer.h[0].output[0].requires_grad_(True)
        with tracer.invoke(MSG_prompt, scan=True):
            gpt2.transformer.h[0].output = (torch.zeros_like(gpt2.transformer.h[0].output[0]),) + gpt2.transformer.h[0].output[1:]
        
        with tracer.invoke(MSG_prompt, scan=True):
            hs = gpt2.transformer.h[0].output[0].save()
            out = gpt2.lm_head.output[0][-1].argmax(dim=-1).save()
    
    assert torch.all(hs.value != 0)
    assert gpt2.tokenizer.decode(out.value) == " New"


@torch.no_grad()
def test_adhoc_module(gpt2: nnsight.LanguageModel):
    with gpt2.generate(validate=True, backend=AssertSavedLenBackend(1)) as generator:
        with generator.invoke(
            "The Eiffel Tower is in the city of", scan=True
        ) as invoker:
            hidden_states = gpt2.transformer.h[-1].output[0]
            hidden_states = gpt2.lm_head(gpt2.transformer.ln_f(hidden_states))
            tokens = torch.softmax(hidden_states, dim=2).argmax(dim=2).save()

        _test_serialize(generator)

    output = gpt2.tokenizer.decode(tokens.value[0])

    assert output == "\n-el Tower is a the middle centre Paris"


@torch.no_grad()
def test_embeddings_set1(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(
        max_new_tokens=3, validate=True, backend=AssertSavedLenBackend(2)
    ) as generator:
        with generator.invoke(MSG_prompt, scan=True) as invoker:
            embeddings = gpt2.transformer.wte.output

            output1 = gpt2.generator.output.save()

        with generator.invoke("_ _ _ _ _ _ _ _ _", scan=True) as invoker:
            gpt2.transformer.wte.output = embeddings

            output2 = gpt2.generator.output.save()

        _test_serialize(generator)

    output1 = gpt2.tokenizer.decode(output1.value[0])
    output2 = gpt2.tokenizer.decode(output2.value[0])

    assert output1 == "Madison Square Garden is located in the city of New York City"

    assert output2 == "_ _ _ _ _ _ _ _ _ New York City"


@torch.no_grad()
def test_embeddings_set2(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.generate(
        max_new_tokens=3, validate=True, backend=AssertSavedLenBackend(2)
    ) as generator:
        with generator.invoke(MSG_prompt, scan=True) as invoker:
            embeddings = gpt2.transformer.wte.output.save()

            output = gpt2.generator.output.save()

    output1 = gpt2.tokenizer.decode(output.value[0])

    with gpt2.generate(
        max_new_tokens=3, validate=True, backend=AssertSavedLenBackend(1)
    ) as generator:
        with generator.invoke("_ _ _ _ _ _ _ _ _", scan=True) as invoker:
            gpt2.transformer.wte.output = embeddings.value

            output = gpt2.generator.output.save()

        _test_serialize(generator)

    output2 = gpt2.tokenizer.decode(output.value[0])

    assert output1 == "Madison Square Garden is located in the city of New York City"
    assert output2 == "_ _ _ _ _ _ _ _ _ New York City"


def test_retain_grad(gpt2: nnsight.LanguageModel):
    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(1)) as tracer:
        with tracer.invoke("Hello World", scan=True) as invoker:
            hidden_states = gpt2.transformer.h[-1].output[0].save()
            hidden_states.retain_grad()

            logits = gpt2.lm_head.output

            logits.sum().backward()

        _test_serialize(tracer)

    assert hidden_states.value.grad is not None


def test_grad(gpt2: nnsight.LanguageModel):
    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(2)) as tracer:
        with tracer.invoke("Hello World", scan=True) as invoker:
            hidden_states = gpt2.transformer.h[-1].output[0].save()
            hidden_states_grad = hidden_states.grad.save()
            hidden_states_grad[:] = 0

            logits = gpt2.lm_head.output

            logits.sum().backward()

        _test_serialize(tracer)

    hidden_states.value

    assert (hidden_states_grad.value == 0).all().item()

    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(1)) as tracer:
        with tracer.invoke("Hello World", scan=True) as invoker:
            hidden_states = gpt2.transformer.h[-1].output[0].save()
            grad = hidden_states.grad.clone()
            grad[:] = 0
            hidden_states.grad = grad

            logits = gpt2.lm_head.output

            logits.sum().backward()

        _test_serialize(tracer)

    hidden_states.value
    assert (hidden_states_grad.value == 0).all().item()


def test_other_device_tensors(gpt2: nnsight.LanguageModel):

    device = next(gpt2.parameters())

    lin = torch.nn.Linear(768, 768).to(device)
    bias = torch.randn(768).to(device)

    def fun(x):
        return torch.nn.ReLU()(lin(x) - bias)

    with gpt2.trace(
        "fish", validate=True, scan=True, backend=AssertSavedLenBackend(1)
    ) as tracer:
        x = gpt2.transformer.h[0].mlp.output
        y = fun(x)
        z = y.save()

        # TODO
        # _test_serialize(tracer)

    z.value


def test_multi_grad(gpt2: nnsight.LanguageModel):
    with gpt2.trace(validate=True, backend=AssertSavedLenBackend(3)) as tracer:
        with tracer.invoke("Hello World", scan=True) as invoker:
            hidden_states = gpt2.transformer.h[-1].output[0].save()

            hidden_states_grad1 = hidden_states.grad.save()

            logits = gpt2.lm_head.output

            logits.sum().backward(retain_graph=True)

            hidden_states_grad2 = hidden_states.grad.save()

            logits = logits * 2

            logits.sum().backward()

        _test_serialize(tracer)

    assert not torch.all(hidden_states_grad1.eq(hidden_states_grad2))


def test_editing(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    from nnsight.util import WrapperModule

    class ComplexModule(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.one = WrapperModule()

        def forward(self, x):
            return self.one(x)

    l0 = gpt2.transformer.h[0]
    l0.attachment = ComplexModule()

    with gpt2.edit() as gpt2_edited:
        acts = l0.output[0]
        l0.output[0][:] = l0.attachment(acts, hook=True)

    # Get values pre editing
    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(2)):
        original = l0.output[0].clone().save()
        l0.output[0][:] *= 0.0
        original_output = gpt2.output.logits.save()

    with gpt2_edited.trace(MSG_prompt, backend=AssertSavedLenBackend(2)):
        one = l0.attachment.one.output.clone().save()
        l0.attachment.output *= 0.0
        edited_output = gpt2.output.logits.save()

    # Check that submodule in attached model
    # is equal to original output.
    assert torch.equal(original, one)
    # Check that edits propagate from attached module
    assert torch.equal(original_output, edited_output)


def test_non_inplace_editing(gpt2: nnsight.LanguageModel, MSG_prompt: str):

    with gpt2.edit(inplace=True):
        gpt2.transformer.h[1].output[0][:, 0] = 0

    with gpt2.edit() as gpt2_edited:
        gpt2.transformer.h[1].output[0][:, 1] = 0

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        l1_out = gpt2.transformer.h[1].output[0].save()

    with gpt2_edited.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        l1_out_edited = gpt2_edited.transformer.h[1].output[0].save()

    assert torch.all(l1_out[:, 0] == 0) and torch.all(l1_out[:, 1] != 0)
    assert torch.all(l1_out_edited[:, 0] == 0) and torch.all(l1_out_edited[:, 1] == 0)


def test_clear_edits(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.edit(inplace=True):
        gpt2.transformer.h[1].output[0][:] = 0

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        l1_out = gpt2.transformer.h[1].output[0].save()

    gpt2.clear_edits()

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        l1_out_unedited = gpt2.transformer.h[1].output[0].save()

    assert torch.all(l1_out == 0)
    assert torch.all(l1_out_unedited != 0)


def test_batched_editing(gpt2: nnsight.LanguageModel):
    from nnsight.util import WrapperModule

    class ComplexModule(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.one = WrapperModule()

        def forward(self, x):
            return self.one(x)

    l0 = gpt2.transformer.h[0]
    l0.attachment = ComplexModule()

    batch = ["a", "b"]
    single = "a"

    with gpt2.edit(single) as gpt2_edited:
        acts = l0.output[0]
        l0.output[0][:] = l0.attachment(acts, hook=True)

    with gpt2_edited.trace(batch, backend=AssertSavedLenBackend(1)):
        edited = l0.attachment.output.save()

    # Check that the batch size does not narrow
    assert edited.shape[0] == 2


def test_conditional_interventions(gpt2: nnsight.LanguageModel):
    with gpt2.session(backend=AssertSavedLenBackend(1)) as session:
        with gpt2.trace("Hello World", validate=True, scan=True) as tracer:
            with tracer.cond(torch.all(gpt2.transformer.h[5].output[0] < 100000)):
                gpt2.transformer.h[-1].output[0][:] = 0

            output = gpt2.transformer.h[-1].output[0].save()

        _test_serialize(session)

    assert torch.all(output.value == 0)


def test_input_setting(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.session(backend=AssertSavedLenBackend(2)):
        with gpt2.trace(MSG_prompt):
            hs = gpt2.transformer.h[6].inputs
            tokens_out_1 = gpt2.lm_head.output.argmax(dim=-1).save()

        with gpt2.trace(MSG_prompt):
            gpt2.transformer.h[6].input = hs[0][0]
            tokens_out_2 = gpt2.lm_head.output.argmax(dim=-1).save()

    prediction_1 = gpt2.tokenizer.decode(tokens_out_1[0][-1])
    prediction_2 = gpt2.tokenizer.decode(tokens_out_2[0][-1])

    assert prediction_1 == prediction_2


def test_parameter_protocol(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    og_weights = gpt2.transformer.h[0].mlp.c_proj.weight
    og_bias = gpt2.transformer.h[0].mlp.c_proj.bias

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(2)):
        cl_weights = gpt2.transformer.h[0].mlp.c_proj.weight.save()
        cl_bias = gpt2.transformer.h[0].mlp.c_proj.bias.save()

    assert not (cl_weights is og_weights)
    assert not (cl_bias is og_bias)

    assert torch.equal(og_weights, cl_weights)
    assert torch.equal(og_bias, cl_bias)


def test_parameter_protocol_meta(MSG_prompt: str):
    model = nnsight.LanguageModel("openai-community/gpt2")

    with model.trace(MSG_prompt, backend=AssertSavedLenBackend(2)):
        weights = model.transformer.h[0].mlp.c_proj.weight.save()
        bias = model.transformer.h[0].mlp.c_proj.bias.save()

    assert weights.device.type != "meta"
    assert bias.device.type != "bias"


def test_parameter_protocol_result(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(2)):
        weights = gpt2.transformer.h[0].mlp.c_proj.weight
        bias = gpt2.transformer.h[0].mlp.c_proj.bias

        c_proj_in = gpt2.transformer.h[0].mlp.c_proj.input

        c_proj_out_2 = torch.addmm(
            bias, c_proj_in.view(-1, c_proj_in.size(-1)), weights
        ).save()

        c_proj_out = gpt2.transformer.h[0].mlp.c_proj.output[0].save()

    assert torch.equal(c_proj_out_2, c_proj_out)


def test_parameter_protocol_safety(gpt2: nnsight.LanguageModel, MSG_prompt: str):
    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        hs_1 = gpt2.transformer.h[0].mlp.c_proj.output[0].save()

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(0)):
        gpt2.transformer.h[0].mlp.c_proj.weight[:] = 0

    with gpt2.trace(MSG_prompt, backend=AssertSavedLenBackend(1)):
        hs_2 = gpt2.transformer.h[0].mlp.c_proj.output[0].save()

    assert torch.equal(hs_1, hs_2)


def test_undispatched_extra_module(device: str, MSG_prompt: str):

    model = nnsight.LanguageModel(
        "openai-community/gpt2", device_map=device, dispatch=False
    )

    with model.generate(MSG_prompt, max_new_tokens=1, backend=AssertSavedLenBackend(1)):

        output = model.generator.output.save()

    output.value



---
File: /tests/test_tiny.py
---

from collections import OrderedDict
from enum import auto

import pytest
import torch

from nnsight.tracing.backends import Backend
from nnsight.tracing.graph import Graph
from nnsight.tracing.protocols import StopProtocol
from nnsight import NNsight
class AssertSavedLenBackend(Backend):
    
    def __init__(self, len:int) -> None:
        self.len = len
    
    def __call__(self, graph: Graph) -> None:
        
        try:
         
            graph.nodes[-1].execute()
            
        except StopProtocol.StopException:
            
            pass
        
        finally:
            
            assert self.len == len([node for node in graph.nodes if node.done])
                            
            graph.nodes.clear()
            graph.stack.clear()
            
input_size = 5
hidden_dims = 10
output_size = 2

@pytest.fixture(scope="module")
def tiny_model(device: str):

    net = torch.nn.Sequential(
        OrderedDict(
            [
                ("layer1", torch.nn.Linear(input_size, hidden_dims)),
                ("layer2", torch.nn.Linear(hidden_dims, output_size)),
            ]
        )
    )

    return NNsight(net).to(device)

@pytest.fixture(autouse=True)
def model_clear(tiny_model: NNsight):
    # clear the model before each test
    tiny_model._clear()
    return tiny_model

@pytest.fixture
def tiny_input():
    return torch.rand((1, input_size))


@torch.no_grad()
def test_tiny(tiny_model: NNsight, tiny_input: torch.Tensor):

    with tiny_model.trace(tiny_input, backend=AssertSavedLenBackend(1)):

        hs = tiny_model.layer2.output.save()

    assert isinstance(hs.value, torch.Tensor)


def test_grad_setting(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(2)):
        l1_grad = tiny_model.layer1.output.grad.clone().save()

        tiny_model.layer1.output.grad = (
            tiny_model.layer1.output.grad.clone() * 2
        )

        l1_grad_double = tiny_model.layer1.output.grad.save()

        loss = tiny_model.output.sum()
        loss.backward()

    assert torch.equal(l1_grad.value * 2, l1_grad_double.value)


def test_external_proxy_intervention_executed_locally(
    tiny_model: NNsight, tiny_input: torch.Tensor
):
    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(1)) as sesh:
        with tiny_model.trace(tiny_input, validate=True, scan=True) as tracer_1:
            l1_out = tiny_model.layer1.output.save()

        with tiny_model.trace(tiny_input, validate=True, scan=True) as tracer_2:
            l1_out[:, 2] = 5

    assert l1_out[:, 2] == 5


def test_early_stop_protocol(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(1)) as tracer:
        l1_out = tiny_model.layer1.output.save()
        tracer.stop()
        l2_out = tiny_model.layer2.output.save()
        

    assert isinstance(l1_out.value, torch.Tensor)

    with pytest.raises(ValueError):
        l2_out.value


def test_true_conditional_protocol(
    tiny_model: NNsight, tiny_input: torch.Tensor
):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(1)) as tracer:
        num = 5
        with tracer.cond(num > 0):
            tiny_model.layer1.output[:] = 1
            l1_out = tiny_model.layer1.output.save()

    assert isinstance(l1_out.value, torch.Tensor)
    assert torch.all(l1_out.value == 1).item()


def test_false_conditional_protocol(
    tiny_model: NNsight, tiny_input: torch.Tensor
):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(1)) as tracer:
        num = 5
        with tracer.cond(num < 0):
            tiny_model.layer1.output[:] = 1
            l1_out = tiny_model.layer1.output.save()

        # check that the condition does not persist outside the context
        l2_out = tiny_model.layer2.output.save()

    with pytest.raises(ValueError):
        l1_out.value
    assert isinstance(l2_out.value, torch.Tensor)


def test_node_as_condition(tiny_model: NNsight, tiny_input: torch.Tensor):
    """Test a Tensor a boolean value as a result of a boolean operation on an InterventionProxy"""

    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(0)) as tracer:
        out = tiny_model.layer1.output
        out[:, 0] = 1
        with tracer.cond(out[:, 0] != 1):
            tiny_model.layer1.output[:] = 1
            l1_out = tiny_model.layer1.output.save()

    with pytest.raises(ValueError):
        l1_out.value


def test_multiple_dependent_conditionals(
    tiny_model: NNsight, tiny_input: torch.Tensor
):
    """Test that interventions defined within different Intervention contexts can be referenced if their conditions evaluated to True."""

    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(1)) as tracer:
        num = 5
        l1_out = tiny_model.layer1.output
        l2_out = tiny_model.layer2.output.save()
        with tracer.cond(num > 0):
            l1_out[:] = 1

        with tracer.cond(l1_out[:, 0] != 1):
            tiny_model.layer2.output[:] = 2

        with tracer.cond(l1_out[:, 0] == 1):
            tiny_model.layer2.output[:] = 3

    assert torch.all(l2_out.value == 3).item()


def test_nested_conditionals(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(1)) as tracer:
        num = 5
        with tracer.cond(num > 0):  # True
            l1_out = tiny_model.layer1.output.save()

            with tracer.cond(num > 0):  # True
                tiny_model.layer1.output[:] = 1

            with tracer.cond(num < 0):  # False
                tiny_model.layer1.output[:] = 2

        with tracer.cond(num < 0):  # False
            tiny_model.layer2.output[:] = 0

            with tracer.cond(num > 0):  # True
                l2_out = tiny_model.layer2.output.save()

    assert isinstance(l1_out.value, torch.Tensor)
    assert torch.all(l1_out.value == 1).item()
    with pytest.raises(ValueError):
        l2_out.value


def test_conditional_trace(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(1)) as session:
        num = 5
        with session.cond(num > 0):
            with tiny_model.trace(tiny_input, validate=True, scan=True):
                output = tiny_model.output.save()

    assert isinstance(output.value, torch.Tensor)


def test_conditional_iteration(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.session(validate=False, backend=AssertSavedLenBackend(1)) as session:
        result = session.apply(list).save()
        with session.iter([0, 1, 2]) as item:
            with session.cond(item % 2 == 0):
                with tiny_model.trace(tiny_input, validate=True, scan=True):
                    result.append(item)

    assert result.value == [0, 2]


def test_bridge_protocol(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(1)) as session:
        val = session.apply(int, 0)
        with tiny_model.trace(tiny_input, validate=True, scan=True):
            tiny_model.layer1.output[:] = (
                val  # fetches the val proxy using the bridge protocol
            )
            l1_out = tiny_model.layer1.output.save()

    assert torch.all(l1_out.value == 0).item()


def test_sequential_graph_based_context_exit(tiny_model: NNsight):
    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(1)) as session:
        l = session.apply(list).save()
        l.append(0)

        with session.iter([1, 2, 3, 4]) as item:
            with session.cond(item == 3):
                session.stop()
            l.append(item)
        l.append(5)
        session.stop()
        l.append(6)

    assert l.value == [0, 1, 2, 5]


def test_tracer_stop(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.trace(tiny_input, validate=True, scan=True, backend=AssertSavedLenBackend(0)) as tracer:
        l1_out = tiny_model.layer1.output
        tracer.stop()
        l1_out_double = l1_out * 2

    with pytest.raises(ValueError):
        l1_out.value


def test_stop_forward_pass_in_loop(tiny_model: NNsight):

    inp_1 = torch.rand((1, input_size))
    inp_2 = torch.rand((1, input_size))
    inp_3 = torch.rand((1, input_size))
    
    with tiny_model.session(validate=True) as session:
        res_list = session.apply(list).save()
        dataloader = torch.utils.data.DataLoader([inp_1, inp_2, inp_3], batch_size=1)

        with session.iter(dataloader) as batch:
            with tiny_model.trace(batch) as tracer:
                hs = tiny_model.layer1.output
                tiny_model.layer1.output.stop()
            res_list.append(hs)

    assert len(res_list) == 3       


def test_bridged_node_cleanup(tiny_model: NNsight):
    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(0)) as session:
        l = session.apply(list)
        with session.iter([0, 1, 2]) as item:
            with session.cond(item == 2):
                session.stop()
            l.append(item)

    with pytest.raises(ValueError):
        l.value


def test_nested_iterator(tiny_model: NNsight):

    with tiny_model.session(validate=True, backend=AssertSavedLenBackend(1)) as session:
        l = session.apply(list)
        l.append([0])
        l.append([1])
        l.append([2])
        l2 = session.apply(list).save()
        with session.iter(l) as item:
            with session.iter(item) as item_2:
                l2.append(item_2)

    assert l2.value == [0, 1, 2]

def test_nnsight_builtins(tiny_model: NNsight):
    with tiny_model.session(backend=AssertSavedLenBackend(3)) as session:
        nn_list = session.apply(list).save()
        sesh_list = session.apply(list).save()
        apply_list = session.apply(list).save()

        with session.iter([nn_list, sesh_list, apply_list]) as l:
            l.append(session.apply(int))
            l.append(session.apply(str, "Hello World"))
            l.append(session.apply(dict, {"a": "1"}))

    assert nn_list.value == sesh_list.value
    assert sesh_list.value == apply_list.value

def test_torch_creation_operations_patch(tiny_model: NNsight, tiny_input: torch.Tensor):
    with tiny_model.trace(tiny_input, scan=False, validate=False, backend=AssertSavedLenBackend(0)):
        l1_output = tiny_model.layer1.output
        torch.arange(l1_output.shape[0], l1_output.shape[1])
        torch.empty(l1_output.shape)
        torch.eye(l1_output.shape[0])
        torch.full(l1_output.shape, 5)
        torch.linspace(l1_output.shape[0], l1_output.shape[1], 5)
        torch.logspace(l1_output.shape[0], l1_output.shape[1], 5)
        torch.ones(l1_output.shape)
        torch.rand(l1_output.shape)
        torch.randint(5, l1_output.shape)
        torch.randn(l1_output.shape)
        torch.randperm(l1_output.shape[0])
        torch.zeros(l1_output.shape)



---
File: /tests/test_vllm.py
---

import pytest
import nnsight
import torch
from typing import TYPE_CHECKING

from nnsight.tracing.backends import Backend
from nnsight.tracing.protocols import StopProtocol

if TYPE_CHECKING:
    from nnsight.tracing.graph import Graph

try:
    from nnsight.modeling.vllm import VLLM
except:
    pytest.skip("Skipping VLLM tests", allow_module_level=True)


class AssertSavedLenBackend(Backend):
    
    def __init__(self, len:int) -> None:
        self.len = len
    
    def __call__(self, graph: "Graph") -> None:
        
        try:
         
            graph.nodes[-1].execute()
            
        except StopProtocol.StopException:
            
            pass
        
        finally:
            
            assert self.len == len([node for node in graph.nodes if node.done])
                            
            graph.nodes.clear()
            graph.stack.clear()


@pytest.fixture(scope="module")
def tp(request):
    tp = request.config.getoption("--tp")
    if tp > torch.cuda.device_count() or tp < 1:
        pytest.exit("--tp can't be higher than the number of availale GPUs.")
    return tp

@pytest.fixture(scope="module")
def vllm_gpt2(tp: int):
    return VLLM("gpt2", tensor_parallel_size=tp, dispatch=True)

@pytest.fixture
def ET_prompt():
    return "The Eiffel Tower is located in the city of"

@pytest.fixture
def MSG_prompt():
    return "Madison Square Garden is located in the city of"


def test_single_logit(vllm_gpt2, ET_prompt: str):
    with vllm_gpt2.trace(ET_prompt, temperature=0.0, top_p=1, backend=AssertSavedLenBackend(1)):
        logits = vllm_gpt2.logits.output.save()

    next_token = vllm_gpt2.tokenizer.decode(logits.argmax(dim=-1))
    assert next_token == " Paris"


def test_multi_token_generation(vllm_gpt2, MSG_prompt: str):
    with vllm_gpt2.trace(MSG_prompt, temperature=0.0, top_p=1.0, max_tokens=3):
        logits = nnsight.list().save()
        for ii in range(3):
            logits.append(vllm_gpt2.logits.output)
            vllm_gpt2.logits.next()

    assert vllm_gpt2.tokenizer.batch_decode([logit.argmax(dim=-1) for logit in logits]) == [" New", " York", " City"]


def test_sampling(vllm_gpt2, MSG_prompt: str):
    with vllm_gpt2.trace(max_tokens=3) as tracer:
        with tracer.invoke(MSG_prompt, temperature=0.0, top_p=1.0, max_tokens=3):
            samples_1 = nnsight.list().save()
            for ii in range(3):
                samples_1.append(vllm_gpt2.samples.output)
                vllm_gpt2.samples.next()
        with tracer.invoke(MSG_prompt, temperature=0.8, top_p=0.95):
            samples_2 = nnsight.list().save()
            for ii in range(3):
                samples_2.append(vllm_gpt2.samples.output)
                vllm_gpt2.samples.next()

    assert vllm_gpt2.tokenizer.batch_decode(samples_1) == [" New", " York", " City"]
    assert vllm_gpt2.tokenizer.batch_decode(samples_2) == [" Richmond", " on", " the"]


""" def test_max_token_generation(vllm_gpt2, ET_prompt: str):
    with vllm_gpt2.trace(ET_prompt, max_tokens=10):
        logits = nnsight.list().save()
        with vllm_gpt2.logits.all():
            logits.append(vllm_gpt2.logits.output)

    assert len(logits) == 10 """


""" def test_sampling(vllm_gpt2, ET_prompt:str):
    with vllm_gpt2.trace(ET_prompt, temperature=0.8, top_p=0.95, max_tokens=3):
        samples = nnsight.list().save()
        with vllm_gpt2.sample.all():
            li.append(vllm_gpt2.sample.output)

    samples = vllm_gpt2.batch_decode([sample.argmax(dim=-1) for sample in samples])
    assert samples == [' Canary', ' Wh', 'arf'] """


def test_intervention(vllm_gpt2, ET_prompt: str):
    with vllm_gpt2.trace(ET_prompt, temperature=0.0, top_p=1, backend=AssertSavedLenBackend(2)) as tracer:
        vllm_gpt2.transformer.h[-2].mlp.output[:] = 0
        hs = vllm_gpt2.transformer.h[-2].mlp.output.save()
        logits = vllm_gpt2.logits.output.save()

    next_token = vllm_gpt2.tokenizer.decode(logits.argmax(dim=-1))
    assert next_token == " London"
    assert torch.all(hs == 0)


def test_swap_intervention(vllm_gpt2, ET_prompt: str):
    with vllm_gpt2.trace(ET_prompt, temperature=0.0, top_p=1, backend=AssertSavedLenBackend(2)) as tracer:
        vllm_gpt2.transformer.h[-2].mlp.output = torch.zeros_like(vllm_gpt2.transformer.h[-2].mlp.output)
        hs = vllm_gpt2.transformer.h[-2].mlp.output.save()
        logits = vllm_gpt2.logits.output.save()

    next_token = vllm_gpt2.tokenizer.decode(logits.argmax(dim=-1))
    assert next_token == " London"
    assert torch.all(hs == 0)


def test_batched_intervention(vllm_gpt2, ET_prompt: str,):
    with vllm_gpt2.trace(temperature=0.0, top_p=1, backend=AssertSavedLenBackend(4)) as tracer:

        with tracer.invoke(ET_prompt):
            clean_hs = vllm_gpt2.transformer.h[-2].mlp.output.save()
            clean_logits = vllm_gpt2.logits.output.save()
        with tracer.invoke(ET_prompt):
            vllm_gpt2.transformer.h[-2].mlp.output[:] = 0
            corrupted_hs = vllm_gpt2.transformer.h[-2].mlp.output.save()
            corrupted_logits = vllm_gpt2.logits.output.save()

    clean_token = vllm_gpt2.tokenizer.decode(clean_logits.argmax(dim=-1))
    corrupted_token = vllm_gpt2.tokenizer.decode(corrupted_logits.argmax(dim=-1))

    assert clean_token == " Paris"
    assert corrupted_token == " London"
    assert not torch.all(clean_hs == 0)
    assert torch.all(corrupted_hs == 0)


def test_batched_multi_token_generation(vllm_gpt2, ET_prompt: str, MSG_prompt: str):
    max_token_1: int = 3
    max_token_2: int = 5

    num_prompts_1: int = 2
    num_prompts_2: int = 1

    with vllm_gpt2.trace() as tracer:
        with tracer.invoke([MSG_prompt, ET_prompt], max_tokens=max_token_1):
            MSG_ET_hs = nnsight.list().save()
            MSG_ET_logits = nnsight.list().save()
            MSG_ET_samples = nnsight.list().save()
            for ii in range(max_token_1):
                MSG_ET_hs.append(vllm_gpt2.transformer.h[5].output)
                vllm_gpt2.transformer.h[5].next()
                MSG_ET_logits.append(vllm_gpt2.logits.output)
                vllm_gpt2.logits.next()
                MSG_ET_samples.append(vllm_gpt2.samples.output)
                vllm_gpt2.samples.next()
        with tracer.invoke(MSG_prompt, max_tokens=max_token_2):
            MSG_hs = nnsight.list().save()
            MSG_logits = nnsight.list().save()
            MSG_samples = nnsight.list().save()
            for ii in range(max_token_2):
                MSG_hs.append(vllm_gpt2.transformer.h[5].output)
                vllm_gpt2.transformer.h[5].next()
                MSG_logits.append(vllm_gpt2.logits.output)
                vllm_gpt2.logits.next()
                MSG_samples.append(vllm_gpt2.samples.output)
                vllm_gpt2.samples.next()

    assert len(MSG_ET_hs) == max_token_1
    assert all(hs.shape[0] == num_prompts_1 for hs in MSG_ET_hs[1:])

    assert len(MSG_ET_logits) == max_token_1
    assert all(logit.shape[0] == num_prompts_1 for logit in MSG_ET_logits)

    assert len(MSG_ET_samples) == max_token_1
    assert all(sample.shape[0] == num_prompts_1 for sample in MSG_ET_samples)


    assert len(MSG_hs) == max_token_2
    assert all(hs.shape[0] == num_prompts_2 for hs in MSG_hs[1:])

    assert len(MSG_logits) == max_token_2
    assert all(logit.shape[0] == num_prompts_2 for logit in MSG_logits)

    assert len(MSG_samples) == max_token_2
    assert all(sample.shape[0] == num_prompts_2 for sample in MSG_samples)


""" def test_batched_multi_token_generation_with_iter(vllm_gpt2, ET_prompt: str, MSG_prompt: str):
    with vllm_gpt2.trace(max_tokens=10) as tracer:
        with tracer.invoke(ET_prompt):
            ET_logits = nnsight.list().save()
            with vllm_gpt2.logits.iter[:3]:
                ET_logits.append(vllm_gpt2.logits.output)
            #vllm_gpt2.output.save()
        with tracer.invoke(MSG_prompt, max_tokens=5):
            MSG_logits = nnsight.list().save()
            with vllm_gpt2.logits.iter[:5]:
                MSG_logits.append(vllm_gpt2.logits.output)

    assert len(ET_logits.value) == 3
    assert len(MSG_logits.value) == 5 """


def test_mutli_token_generation_with_intervention(tp, vllm_gpt2, MSG_prompt: str):
    with vllm_gpt2.trace(MSG_prompt, temperature=0.0, top_p=1.0, max_tokens=5) as tracer:
        logits = nnsight.list().save()
        hs_list = nnsight.list().save()
        for ii in range(5):
            if ii == 2:
                vllm_gpt2.transformer.h[-2].output[0][:] = 0
            hs_list.append(vllm_gpt2.transformer.h[-2].output[0])
            vllm_gpt2.transformer.h[-2].next()
            logits.append(vllm_gpt2.logits.output)
            vllm_gpt2.logits.next()

    assert [torch.all(hs == 0) for hs in hs_list] == [False, False, True, False, False]

    if tp == 1:
        assert vllm_gpt2.tokenizer.batch_decode([logit.argmax(dim=-1) for logit in logits]) == [' New', ' York', '\n', '\n', 'The']


""" def test_multi_referenced_module(vllm_gpt2, ET_prompt: str):
    with vllm_gpt2.trace(ET_prompt):
        act_in = vllm_gpt2.transformer.h[0].mlp.act.input.save()
        vllm_gpt2.transformer.h[0].mlp.act.next()
        act_in_other = vllm_gpt2.transformer.h[1].mlp.act.input.save()

    assert not torch.equal(act_in, act_in_other) """


def test_tensor_parallelism(tp, vllm_gpt2, ET_prompt: str):
    if tp < 2:
        pytest.skip("Skipping test for tp>1!")

    with vllm_gpt2.trace(ET_prompt, temperature=0.0, top_p=1.0):
        vllm_gpt2.transformer.h[5].mlp.c_fc.output[0][:, 2000:] = 0
        hs = vllm_gpt2.transformer.h[5].mlp.c_fc.output[0].save()
        logit = vllm_gpt2.logits.output.save()

    next_token = vllm_gpt2.tokenizer.decode(logit.argmax(dim=-1))

    #assert next_token != " Paris"
    assert hs.shape == torch.Size([11, 3072])
    assert torch.all(hs[:, 2000:] == 0)



---
File: /CHANGELOG.md
---

# Changelog

## `0.3.0`

_released: 2024-08-29_

We are excited to announce the release of `nnsight0.3`.

This version significantly enhances the library's remote execution capabilities. It improves the integration experience with the [NDIF](https://ndif.us) backend and allows users to define and execute optimized training loop workflows directly on the remote server, including LoRA and other PEFT methods.

### Breaking Changes

-  Module `input` access has a syntactic change:
    - Old: `nnsight.Envoy.input`
    - New: `nnsight.Envoy.inputs`
    - Note: `nnsight.Envoy.input` now provides access to the first positional argument of the module's input.

- `scan` & `validate` are set to `False` by default in the `Tracer` context.

### New Features

- [<ins>Session context</ins>](https://nnsight.net/notebooks/features/sessions/): efficiently package multi-tracing experiments into a single request, enabling faster, more scalable remote experimentation.

- [<ins>Iterator context</ins>](https://nnsight.net/notebooks/features/iterator/): define an intervention loop for iterative execution.

- [<ins>Model editing</ins>](nnsight.net/notebooks/features/model_editing/): alter a model by setting default edits and interventions in an editing context, applied before each forward pass.

- [<ins>Early stopping</ins>](https://nnsight.net/notebooks/features/early_stopping/): interrup a model's forward pass at a chosen module before execution completes. 

- [<ins>Conditional context</ins>](https://nnsight.net/notebooks/features/conditionals/): define interventions within a Conditional context, executed only when the specified condition evaluates to be True.

- [<ins>Scanning context</ins>](https://nnsight.net/notebooks/features/scan_validate/): perform exclusive model scanning to gather important insights.

- <ins>`nnsight` builtins</ins>: define traceable `Python` builtins as part of the intervention graph.

- <ins>Proxy update</ins>: assign new values to existing proxies. 
     
- <ins>In-Trace logging</ins>: add log statements to be called during the intervention graph execution.

- [<ins>Traceable function calls</ins>](https://nnsight.net/notebooks/features/custom_functions/): make unsupported functions traceable by the intervention graph. Note that [<ins>all pytorch functions are now traceable</ins>](https://nnsight.net/notebooks/features/operations/) by `nnsight` by default.



---
File: /CODE_OF_CONDUCT.md
---


# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official email address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at j.bell@northeastern.edu .
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations



---
File: /conftest.py
---

import pytest
import toml

def pytest_addoption(parser):
    parser.addoption("--device", action="store", default="cuda:0")
    parser.addoption(
        "--tp", 
        action="store", 
        type=int, 
        default="1", 
        help="An argument for specifying the number of gpus to be used by VLLM"
    )

def pytest_generate_tests(metafunc):
    # This is called for every test. Only get/set command line arguments
    # if the argument is specified in the list of test "fixturenames".
    option_value = metafunc.config.option.device
    if "device" in metafunc.fixturenames and option_value is not None:
        metafunc.parametrize("device", [option_value], scope="module")

@pytest.fixture(scope="session")
def load_pyproject_toml():
    """Fixture to load and parse the pyproject.toml file."""
    try:
        with open("pyproject.toml", "r") as f:
            data = toml.load(f)
        return data
    except toml.TomlDecodeError as e:
        pytest.fail(f"Failed to load pyproject.toml: {e}")

collect_ignore = ["examples/test_server.py", "examples/test_server_llama.py"]



---
File: /README.md
---

<img src="./docs/source/_static/images/nnsight_logo.svg" alt="drawing" style="width:200px;float:left"/>

# nnsight 

<a href="https://arxiv.org/abs/2407.14561"><img src="https://img.shields.io/badge/READ%20THE%20PAPER%20HERE!-orange" style="transform: scale(3);"></a>

<a href="https://www.nnsight.net"><img src="https://img.shields.io/badge/-Read%20the%20Docs%20Here-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white"></img></a> <a href="https://discord.gg/6uFJmCSwW7"><img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white"></a>

The `nnsight`  package enables interpreting and manipulating the internals of deep learned models. Read our [paper!](https://arxiv.org/abs/2407.14561)

#### Installation

Install this package through pip by running:

`pip install nnsight`

#### Examples

Here is a simple example where we run the nnsight API locally on gpt2 and save the hidden states of the last layer:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace('The Eiffel Tower is in the city of') as tracer:

      hidden_states = model.transformer.h[-1].output[0].save()

      output = model.output.save()
```

Lets go over this piece by piece.

We import the `LanguageModel` object from the `nnsight` module and create a gpt2 model using the huggingface repo ID for gpt2, `'openai-community/gpt2'`. This accepts arguments to create the model including `device_map` to specify which device to run on.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2',device_map='auto')
```

Then, we create a tracing context block by calling `.trace(...)` on the model object. This denotes we want to run the model with our prompt.


```python
with model.trace('The Eiffel Tower is in the city of') as tracer:
```

Now calling `.trace(...)` does not actually initialize or run the model. Only after the tracing` block is exited, is the actual model loaded and ran. All operations in the block are "proxies" which essentially creates a graph of operations we wish to carry out later.

Within this context, all operations/interventions will be applied to the processing of the given prompt.

```python
hidden_states = model.transformer.h[-1].output[0].save()
```

On this line were saying, access the last layer of the transformer `model.transformer.h[-1]`, access its output `.output`, index it at 0 `.output[0]`, and save it `.save()`

A few things, we can see the module tree of the model by printing the model. This allows us to know what attributes to access to get to the module we need.
Running `print(model)` results in:

```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

`.output` returns a proxy for the output of this module. This essentially means were saying, when we get to the output of this module during inference, grab it and perform any operations we define on it (which also become proxies). There are two operational proxies here, one for getting the 0th index of the output, and one for saving the output. We take the 0th index because the output of gpt2 transformer layers are a tuple where the first index are the actual hidden states (last two indicies are from attention). We can call `.shape` on any proxies to get what shape the value will eventually be. 
Running `print(model.transformer.h[-1].output.shape)` returns `(torch.Size([1, 10, 768]), (torch.Size([1, 12, 10, 64]), torch.Size([1, 12, 10, 64])))`

During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed. However calling `.save()` on the proxy informs the computation graph to save the value of this proxy and never destroy it, allowing us to access to value after generation.

After exiting the generator context, the model is ran with the specified arguments and intervention graph. `output` is populated with the actual output and `hidden_states` will contain the hidden value.

```python
print(output)
print(hidden_states)
```

returns:

```
tensor([[ 464,  412,  733,  417, 8765,  318,  287,  262, 1748,  286, 6342]],
       device='cuda:0')
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
```



---

###### Operations

Most basic operations and torch operations work on proxies and are added to the computation graph. 

```python
from nnsight import LanguageModel
import torch 

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.trace('The Eiffel Tower is in the city of'):

  hidden_states_pre = model.transformer.h[-1].output[0].save()

  hs_sum = torch.sum(hidden_states_pre).save()

  hs_edited = hidden_states_pre + hs_sum

  hs_edited = hs_edited.save()

print(hidden_states_pre)
print(hs_sum)
print(hs_edited)
```

In this example we get the sum of the hidden states and add them to the hidden_states themselves (for whatever reason). By saving the various steps, we can see how the values change.

```
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
tensor(501.2957, device='cuda:0')
tensor([[[501.3461, 501.1229, 501.1267,  ..., 500.2860, 501.4237, 500.2270],
         [510.0451, 504.2014, 506.5981,  ..., 493.2538, 502.5920, 498.4279],
         [501.5916, 505.9643, 497.6315,  ..., 501.5348, 498.6892, 504.5219],
         ...,
         [503.4493, 508.1874, 505.1607,  ..., 501.3545, 499.3091, 507.2145],
         [500.8496, 508.7242, 491.9892,  ..., 503.3485, 498.5010, 501.8512],
         [507.9242, 503.0215, 506.0926,  ..., 508.9671, 504.3639, 503.3438]]],
       device='cuda:0')
       
```

---
###### Setting

We often not only want to see whats happening during computation, but intervene and edit the flow of information. 

```python
from nnsight import LanguageModel
import torch 

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.trace('The Eiffel Tower is in the city of') as tracer:

  hidden_states_pre = model.transformer.h[-1].mlp.output.clone().save()

  noise = (0.001**0.5)*torch.randn(hidden_states_pre.shape)

  model.transformer.h[-1].mlp.output = hidden_states_pre + noise

  hidden_states_post = model.transformer.h[-1].mlp.output.save()

print(hidden_states_pre)
print(hidden_states_post)
```
In this example, we create a tensor of noise to add to the hidden states. We then add it, use the assigment `=` operator to update the value of `.output` with these new noised activations. 

We can see the change in the results:

```
tensor([[[ 0.0505, -0.1728, -0.1690,  ..., -1.0096,  0.1280, -1.0687],
         [ 8.7494,  2.9057,  5.3024,  ..., -8.0418,  1.2964, -2.8677],
         [ 0.2960,  4.6686, -3.6642,  ...,  0.2391, -2.6064,  3.2263],
         ...,
         [ 2.1537,  6.8917,  3.8651,  ...,  0.0588, -1.9866,  5.9188],
         [-0.4460,  7.4285, -9.3065,  ...,  2.0528, -2.7946,  0.5556],
         [ 6.6286,  1.7258,  4.7969,  ...,  7.6714,  3.0682,  2.0481]]],
       device='cuda:0')
tensor([[[ 0.0674, -0.1741, -0.1771,  ..., -0.9811,  0.1972, -1.0645],
         [ 8.7080,  2.9067,  5.2924,  ..., -8.0253,  1.2729, -2.8419],
         [ 0.2611,  4.6911, -3.6434,  ...,  0.2295, -2.6007,  3.2635],
         ...,
         [ 2.1859,  6.9242,  3.8666,  ...,  0.0556, -2.0282,  5.8863],
         [-0.4568,  7.4101, -9.3698,  ...,  2.0630, -2.7971,  0.5522],
         [ 6.6764,  1.7416,  4.8027,  ...,  7.6507,  3.0754,  2.0218]]],
       device='cuda:0')
```

---
###### Multiple Token Generation

When generating more than one token, use `.generate(...) ` and `.next()`  on the module you want to get the next value of to denote following interventions should be applied to the subsequent generations.

Here we again generate using gpt2, but generate three tokens and save the hidden states of the last layer for each one:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate('The Eiffel Tower is in the city of', max_new_tokens=3) as tracer:
 
  hidden_states1 = model.transformer.h[-1].output[0].save()

  invoker.next()

  hidden_states2 = model.transformer.h[-1].next().output[0].save()

  invoker.next()

  hidden_states3 = model.transformer.h[-1].next().output[0].save()

```
---

###### Cross Prompt Intervention


Intervention operations work cross prompt! Use two invocations within the same generation block and operations can work between them.

You can do this by not passing a prompt into `.trace`/`.generate`, but by calling `.invoke(...)` on the created tracer object.

In this case, we grab the token embeddings coming from the first prompt, `"Madison square garden is located in the city of New"` and replace the embeddings of the second prompt with them.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate(max_new_tokens=3) as tracer:
    
    with tracer.invoke("Madison square garden is located in the city of New"):

        embeddings = model.transformer.wte.output

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):

        model.transformer.wte.output = embeddings

        output = model.generator.output.save()

print(model.tokenizer.decode(output[0]))
print(model.tokenizer.decode(output[1]))
```

This results in:

```
Madison square garden is located in the city of New York City.
_ _ _ _ _ _ _ _ _ _ York City.
```

We also could have entered a pre-saved embedding tensor as shown here:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='cuda')

with model.generate(max_new_tokens=3) as tracer:
    
    with tracer.invoke("Madison square garden is located in the city of New") as invoker:

        embeddings = model.transformer.wte.output.save()

with model.generate(max_new_tokens=3) as tracer:

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _") as invoker:

        model.transformer.wte.output = embeddings.value

```
---

###### Ad-hoc Module

Another thing we can do is apply modules in the model's module tree at any point during computation, even if it's out of order.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel("openai-community/gpt2", device_map='cuda')

with model.generate('The Eiffel Tower is in the city of') as generator:

  hidden_states = model.transformer.h[-1].output[0]
  hidden_states = model.lm_head(model.transformer.ln_f(hidden_states)).save()
  tokens = torch.softmax(hidden_states, dim=2).argmax(dim=2).save()
        
print(hidden_states)
print(tokens)
print(model.tokenizer.decode(tokens[0]))

```

Here we get the hidden states of the last layer like usual. We also chain apply `model.transformer.ln_f` and `model.lm_head` in order to "decode" the hidden states into vocabularly space.
Applying softmax and then argmax allows us to then transform the vocabulary space hidden states into actually tokens which we can then use the tokenizer to decode.

The output looks like:

```
tensor([[[ -36.2874,  -35.0114,  -38.0793,  ...,  -40.5163,  -41.3759,
           -34.9193],
         [ -68.8886,  -70.1562,  -71.8408,  ...,  -80.4195,  -78.2552,
           -71.1206],
         [ -82.2950,  -81.6519,  -83.9941,  ...,  -94.4878,  -94.5194,
           -85.6998],
         ...,
         [-113.8675, -111.8628, -113.6634,  ..., -116.7652, -114.8267,
          -112.3621],
         [ -81.8531,  -83.3006,  -91.8192,  ...,  -92.9943,  -89.8382,
           -85.6898],
         [-103.9307, -102.5054, -105.1563,  ..., -109.3099, -110.4195,
          -103.1395]]], device='cuda:0')
tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]],
       device='cuda:0')

-el Tower is a the middle centre Paris
```

---

More examples can be found at [nnsight.net](https://www.nnsight.net)

### Citation

If you use `nnsight` in your research, please cite using the following

```bibtex
@article{fiottokaufman2024nnsightndifdemocratizingaccess,
      title={NNsight and NDIF: Democratizing Access to Foundation Model Internals}, 
      author={Jaden Fiotto-Kaufman and Alexander R Loftus and Eric Todd and Jannik Brinkmann and Caden Juang and Koyena Pal and Can Rager and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Michael Ripa and Adam Belfki and Nikhil Prakash and Sumeet Multani and Carla Brodley and Arjun Guha and Jonathan Bell and Byron Wallace and David Bau},
      year={2024},
      eprint={2407.14561},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.14561}, 
}
``````

# LoRA_tutorial.ipynb

# LoRA for Sentiment Analysis
📗 You can find an interactive Colab version of this tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/LoRA_tutorial.ipynb).

[Low Rank Adaptation (LoRA)](https://github.com/microsoft/LoRA) is a technique used to modify and fine tune large language models in a more efficient way. Rather than modifying all of the model weights, LoRAs find two low dimensional matrices that have the lowest rank. It then multiplies the two matrices to find the fine tuned weight matrix. This fine tuned weight matrix will be the same size as the original pre trained weight matrix. Once the fine tuned matrix has been found it can then be applied to the model's layers.

![TRAIN FIGURE](https://github.com/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/images/LoRA_tutorial_figure_1.png?raw=1)

<br>
<br>

Fine tuning with a LoRA is a part of the [Parameter Efficient Fine Tuning (PEFT)](https://github.com/huggingface/) family because it keeps the original model unchanged and introduces a small number of layers or parameters instead. Once the fine tuned matrix has been calculated, it is applied to the last Multilayer Perceptron (MLP) layer of the model. Once the LoRA has been applied, the model is fine tuned based on a knowledge base or domain specific dataset.

![TEST FIGURE](https://github.com/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/images/LoRA_tutorial_figure_2.png?raw=1)

# Setup

Make sure you have obtained your [NDIF API key](https://login.ndif.us/) and configured your workspace for [remote execution](https://nnsight.net/notebooks/features/remote_execution/).

The following packages need to be installed for this tutorial:
```
!pip install nnsight
!pip install pyarrow==15.0.2
!pip install datasets
!pip install datasets torch
```

```python
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
    !pip install pyarrow==15.0.2
    !pip install datasets
    !pip install datasets torch
```

```python
from nnsight import CONFIG

if is_colab:
    # include your HuggingFace Token and NNsight API key on Colab secrets
    from google.colab import userdata
    NDIF_API = userdata.get('NDIF_API')
    HF_TOKEN = userdata.get('HF_TOKEN')

    CONFIG.set_default_api_key(NDIF_API)
```

Here are the imports needed for this tutorial.

```python
import torch
import torch.nn as nn
import pandas as pd
from nnsight import LanguageModel
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM
from transformers import TrainingArguments, Trainer
from torch.utils.data import DataLoader, Subset
from datasets import load_dataset
```

# Prepare Data

For this tutorial we will be using the The Stanford Sentiment Treebank (SST2). It consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence as being either positive or negative. In the dataset, the positive/negative labels of each phrase are represented by a 0 for each negative statement and a 1 for each positive statement.

```python
# GLUE is a standard Natural Language Processing (NLP) benchmark which is commonly used for sentiment analysis tasks.
# It is responisble for assessing the effectiveness of language models across various NLP tasks.
# It serves as a standard for evaluating a model's ability to understand and process language.
dataset = load_dataset("glue", "sst2")

# 0 = neg, 1 = pos
def label_to_str(example):
    example['label'] = 'positive' if example['label'] == 1 else 'negative'
    return example

train_data = [(dataset['sentence'], 'positive' if dataset['label'] == 1 else 'negative') for dataset in dataset['train']]
validation_data = [(dataset['sentence'], 'positive' if dataset['label'] == 1 else 'negative') for dataset in dataset['validation']]
```

Next, we need to tokenize our data. Tokenizing involves converting text into a numerical representation. It is a popular technique in NLP because it helps the models better understand the text and output a more accurate result.

```python
tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2', add_prefix_space=True)
tokenizer.pad_token = tokenizer.eos_token

# Uses the tokenizer from the model to tokenize a given sentence with padding and truncation
def tokenize_function(text):
  return tokenizer(text['sentence'], padding='max_length', truncation=True, max_length=10, return_tensors='pt')

# We use .map() in order to apply the tokenization function to all the training data.
tokenized_train_dataset = dataset['train'].map(tokenize_function, batched=True, batch_size=10)
tokenized_train_dataset = tokenized_train_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask'], 'labels': x['label']})
```

# Prepare our Model

For this tutorial we will be using the [Llama-70B](https://huggingface.co/meta-llama/Llama-2-70b) language model.

```python
# Use the LanguageModel wrapper class to load in the Llama model
model_name = "meta-llama/Meta-Llama-3.1-70B"
model = LanguageModel(model_name, device_map='auto')
```

This is the model architechure before the LoRA has been applied. After the model has been fine tuned with the LoRA, the last MLP layer of the model will be replaced with the LoRA.

<br>
<br>

We’re going to train a very simple LORA that, when applied, will make our model determine whether a sentence is displaying a positive sentiment or a negative sentiment.

```python
from nnsight import Envoy

# We will define a LORA class.
# The LORA class call method operations are simply traced like you would normally do in a .trace.
class LORA(nn.Module):
    def __init__(self, module: Envoy, dim: int, r: int) -> None:
        """Init.

        Args:
            module (Envoy): Which model Module we are adding the LORA to.
            dim (int): Dimension of the layer we are adding to (This could potentially be auto populated if the user scanned first so we know the shape)
            r (int): Inner dimension of the LORA
        """
        super(LORA, self).__init__()
        self.r = r
        self.module = module
        self.WA = torch.nn.Parameter(torch.randn(dim, self.r), requires_grad=True).save()
        self.WB = torch.nn.Parameter(torch.zeros(self.r, dim), requires_grad=True).save()

    # The Call method defines how to actually apply the LORA.
    # happens after the forward pass
    def __call__(self, alpha: float = 1.0):
        """Call.

        Args:
            alpha (float, optional): How much to apply the LORA. Can be altered after training for inference. Defaults to 1.0.
        """

        # We apply WA to the first positional arg (the hidden states)
        A_x = torch.matmul(self.module.input, self.WA)
        BA_x = torch.matmul(A_x, self.WB)

        # LORA is additive
        h = BA_x + self.module.output

        # Replace the output with our new one * alpha
        # Could also have been self.module.output[:] = h * alpha, for in-place
        self.module.output = h * alpha

    def parameters(self):
        # Some way to get all the parameters.
        return [self.WA, self.WB]
```

# LLM Fine Tuning

```python
# Inner LORA dimension
lora_dim = 4

# Module to train LORA on
# Accesses the last mlp layer of the model
module = model.model.layers[-1].mlp
```

We can use the `.scan()` method to get the shape of the module without having to fully run the model.

```python
with model.scan(" "):
    dim = module.output.shape[-1]

print(dim)
```

```python
import nnsight
# The LORA object itself isn't transmitted to the server. Only the forward / call method.
# The parameters are created remotely and never sent only retrieved
with model.session(remote=True) as session:

    dataset = tokenized_train_dataset

    # Smaller chunks to run faster, feel free to increase
    indices = list(range(0, 5000))
    subset = Subset(dataset, indices)

    # Create a dataloader from it.
    dataloader = DataLoader(subset, batch_size=10)

    # Create our LORA on the last mlp and apply it to the model
    lora = LORA(module, dim, lora_dim)

    # Create an optimizer. Use the parameters from LORA
    optimizer = torch.optim.AdamW(lora.parameters(), lr=3)

    # Iterate over dataloader using .iter.
    with session.iter(dataloader) as batch:

        # Accesses the phrase that contains either a positive/negative sentiment
        prompt = batch['sentence']

        # Determines whether the phrase is positive/negative
        correct_token = batch['label']

        # Run .trace with prompt
        with model.trace(prompt) as tracer:

            # Apply LORA to intervention graph just by calling it with .trace
            # This is invoke the __call__() method of the LORA class defined above
            lora()

            # Get logits
            # Logits are the output of the neural network before the
            # activation function has been applied.
            logits = model.lm_head.output

            # Do cross entropy on last predicted token and correct_token
            loss = torch.nn.functional.cross_entropy(logits[:, -1], batch['label'])

            # Call backward
            loss.backward()

        # Call methods on optimizer. Graphs that arent from .trace (so in this case session and iterator both have their own graph) are executed sequentially.
        # The Graph of Iterator here will be:
        # 1.) Index batch at 0 for prompt
        # 2.) Index batch at 1 for correct_token
        # 3.) Execute the .trace using the prompt
        # 4.) Call .step() on optimizer
        optimizer.step()
        # 5.) Call .zero_grad() in optimizer
        optimizer.zero_grad()
        # 6.) Print out the lora WA weights to show they are indeed changing
        nnsight.log(lora.WA)

```

```python
print(model)
```

In addition to the weights changing, we know the LoRA has been applied because there is a difference in the model's architecture. The 11th block of the model no longer has the standard MLP layer and instead contains the LoRA.

Now it is time to test out whether our fine tuned model is able to predict the sentiment of a given sentence.

```python
# With lora. Will output "negative".
with model.generate("I'm upset", remote=True) as generator:
  lora()
  out = model.lm_head.output.save()

# The model outputs the sentiment as tokens first.
token_ids = out.argmax(dim=-1)

# Convert the tokens to either positive or negative
count_positive = (token_ids == 1).sum().item()
count_negative = (token_ids == 0).sum().item()

# Determine the overall sentiment of the entire sentence
if count_positive > count_negative:
  print("\nPrediction with LoRA: Positive\n")
else:
  print("\nPrediction with LoRA: Negative\n")

# Then without. It will try to complete the sentence rather than output the
# sentiment analysis.

with model.generate("I'm upset", remote=True) as generator:
    out = model.lm_head.output.save()

print("\nPrediction without LoRA:", model.tokenizer.decode(out.argmax(dim=-1)[0]))
```

---

# NNsight_Walkthrough.ipynb

<a href="https://colab.research.google.com/github/ndif-team/nnsight/blob/main/NNsight_Walkthrough.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<img src="https://nnsight.net/_static/images/nnsight_logo.svg" alt="drawing" width="200"/>

# **NNsight**

## The API for a transparent science on black-box AI

In this era of large-scale deep learning, the most interesting AI models are
massive black boxes that are hard to run. Ordinary commercial inference service
APIs let us interact with huge models, but they do not let us access model
internals.

The `nnsight` library is different: it provides full access to all the neural
network internals. When used together with a remote service like the
[National Deep Inference Fabric](https://thevisible.net/docs/NDIF-proposal.pdf)
(NDIF), it makes possible to run complex experiments on huge open  models easily,
with fully transparent access.

Our team wants to enable entire labs and independent researchers alike, as we
believe a large, passionate, and collaborative community will produce the next
big insights on a profoundly important field.

# 1 First, let's start small

## Setup

```python
# Install nnsight
!pip install nnsight
!pip install --upgrade transformers torch

from IPython.display import clear_output

clear_output()
```

## Tracing Context

To demonstrate the core functionality and syntax of nnsight, we'll define and
use a tiny two layer neural network.

Our little model here is composed of two submodules – linear layers 'layer1' and 'layer2'. We specify the sizes of each of these modules and create
some complementary example input.

```python
from collections import OrderedDict
import torch

input_size = 5
hidden_dims = 10
output_size = 2

net = torch.nn.Sequential(
    OrderedDict(
        [
            ("layer1", torch.nn.Linear(input_size, hidden_dims)),
            ("layer2", torch.nn.Linear(hidden_dims, output_size)),
        ]
    )
).requires_grad_(False)
```

The core object of the nnsight package is `NNsight`. This wraps around a given
PyTorch model to enable investigation of its internal parameters.

```python
import nnsight
from nnsight import NNsight

tiny_model = NNsight(net)
```

Printing a PyTorch model shows a named hierarchy of modules which is very useful
when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed.

```python
print(tiny_model)
```

Before we actually get to using the model we just created, let's talk about
Python contexts.

Python contexts define a scope using the `with` statement and are often used to
create some object, or initiate some logic, that you later want to destroy or
conclude.

The most common application is opening files as in the following example:

```python
with open('myfile.txt', 'r') as file:
  text = file.read()
```

Python uses the `with` keyword to enter a context-like object. This object
defines logic to be run at the start of the `with` block, as well as logic to be
run when exiting. When using `with` for a file, entering the context opens the
file and exiting the context closes it. Being within the context means we can
read from the file.

Simple enough! Now we can discuss how `nnsight` uses
contexts to enable intuitive access into the internals of a neural network.

The main tool with `nnsight` is a context for tracing.

We enter the tracing context by calling `model.trace(<input>)` on an `NNsight`
model, which defines how we want to run the model. Inside the context, we will
be able to customize how the neural network runs. The model is actually run upon
exiting the tracing context.

```python
# random input
input = torch.rand((1, input_size))

with tiny_model.trace(input) as tracer:
    pass
```

But where's the output? To get that, we'll have to learn how to request it from
within the tracing context.

## Getting

Earlier, when we wrapped our little neural net with the `NNsight` class. This
added a couple properties to each module in the model (including the root model
itself). The two most important ones are `.input` and `.output`.

```python
model.input
model.output
```

The names are self explanatory. They correspond to the inputs and outputs of
their respective modules during a forward pass of the model. We can use these
attributes inside the `with` block.

However, it is important to understand that the model is not executed until the
end of the tracing context. How can we access inputs and outputs before the
model is run? The trick is deferred execution.

`.input` and `.output` are Proxies for the eventual inputs and outputs of a
module. In other words, when we access `model.output` what we are
communicating to `nnsight` is, "When you compute the output of `model`, please
grab it for me and put the value into its corresponding Proxy object. Let's try it:

```python
with tiny_model.trace(input) as tracer:

    output = tiny_model.output

print(output)
```

Oh no an error! "Accessing value before it's been set."

Why doesn't our `output` have a `value`?

Proxy objects will only have their value at the end of a context if we call
`.save()` on them. This helps to reduce memory costs. Adding `.save()` fixes the
error:

```python
with tiny_model.trace(input) as tracer:

    output = tiny_model.output.save()

print(output)
```

Success! We now have the model output. We just completed out first
intervention using `nnsight`.

Each time we access a module's input or output, we create an _intervention_ in
the neural network's forward pass. Collectively these requests form the
_intervention graph_. We call the process of executing it alongside the model's
normal computation graph, _interleaving_.

<details>
<summary>On Model output</summary>

---

If we don't need to access anything other than the final model output, we can
call the tracing context with `trace=False` and not use it as a context. This could be especially useful for easy remote inference.

```python
  output = model.trace(<inputs>, trace=False)
```

---

</details>

Just like we saved the output of the model as a whole, we can save the output of
any of its submodules. We use normal Python attribute syntax. We can discover
how to access them by name by printing out the model:

```python
print(tiny_model)
```

Let's access the output of the first layer (which we've named 'layer1'):

```python
with tiny_model.trace(input) as tracer:

    l1_output = tiny_model.layer1.output.save()

print(l1_output)
```

Let's do the same for the input of layer2. While we're at it, let's also drop
the `as tracer`, as we won't be needing the tracer object itself for a few
sections:

```python
with tiny_model.trace(input):

    l2_input = tiny_model.layer2.input.save()

print(l2_input)
```

<details>
  <summary>On module inputs</summary>

---

Notice how the value for `l2_input`, is just a single tensor. By default, the `.input` attribute of a module will return the **first** tensor input to the module.

We can also access the full input to a module by using the `.inputs` attribute which will return the values in the form of:

      tuple(tuple(args), dictionary(kwargs))

Where the first index of the tuple is itself a tuple of all positional
arguments, and the second index is a dictionary of the keyword arguments.

---

</details>

Until now we were saving the output of the model and its submodules within the `Trace` context to then print it after exiting the context. We will continuing doing this in the rest of the tutorial since it's a good practice to save the computation results for later analysis.

However, we can also log the outputs of the model and its submodules within the `Trace` context. This is useful for debugging and understanding the model's behavior while saving memory. Let's see how to do this:

```python
with tiny_model.trace(input) as tracer:
  tracer.log("Layer 1 - out: ", tiny_model.layer1.output)
```

## Functions, Methods, and Operations

Now that we can access activations, we also want to do some post-processing on
it. Let's find out which dimension of layer1's output has the highest value.

We could do this by calling `torch.argmax(...)` after the tracing context or we
can just leverage the fact that `nnsight` handles Pytorch functions and methods within
the tracing context, by creating a Proxy request for it:

```python
with tiny_model.trace(input):

    # Note we don't need to call .save() on the output,
    # as we're only using its value within the tracing context.
    l1_output = tiny_model.layer1.output

    # We do need to save the argmax tensor however,
    # as we're using it outside the tracing context.
    l1_amax = torch.argmax(l1_output, dim=1).save()

print(l1_amax[0])
```

Nice! That worked seamlessly, but hold on, how come we didn't need to call
`.value[0]` on the result? In previous sections, we were just being explicit to
get an understanding of Proxies and their value. In practice, however, `nnsight`
knows that when outside of the tracing context we only care about the actual
value, and so printing, indexing, and applying functions all immediately return
and reflect the data in `.value`. So for the rest of the tutorial we won't use
it.

The same principles work for Pytorch methods and all operators as well:

```python
with tiny_model.trace(input):

    value = (tiny_model.layer1.output.sum() + tiny_model.layer2.output.sum()).save()

print(value)
```

The code block above is saying to `nnsight`, "Run the model with
the given `input`. When the output of `tiny_model.layer1` is computed, take its sum. Then do
the same for `tiny_model.layer2`. Now that both of those are computed, add them and make sure
not to delete this value as I wish to use it outside of the tracing context."

## Custom Functions

Everything within the tracing context operates on the intervention graph. Therefore, for `nnsight` to trace a  function it must also be a part of the intervention graph.

Out-of-the-box `nnsight` supports PyTorch functions and methods, all operators, as well the `einops` library. We don't need to do anything special to use them. But what do we do if we want to use custom functions? How do we add them to the intervention graph?

Enter `nnsight.apply()`. It allows us to add new functions to the intervention graph. Let's see how it works:

```python
# Take a tensor and return the sum of its elements
def tensor_sum(tensor):
    flat = tensor.flatten()
    total = 0
    for element in flat:
        total += element.item()

    return torch.tensor(total)

with tiny_model.trace(input) as tracer:

    # Specify the function name and its arguments (in a comma-separated form) to add to the intervention graph
    custom_sum = nnsight.apply(tensor_sum, tiny_model.layer1.output).save()
    sum = tiny_model.layer1.output.sum()
    sum.save()

print(custom_sum, sum)
```

`nnsight.apply()` executes the function it wraps and returns its output as a Proxy object. We can then use this Proxy object as we would any other.

The applications of `nnsight.apply` are wide: it can be used to wrap any custom function or functions from libraries that `nnsight` does not support out-of-the-box.

## Setting

Getting and analyzing the activations from various points in a model can be
really insightful, and a number of ML techniques do exactly that. However, often we not only want to view the computation of a model, but also to influence it.

To demonstrate the effect of editing the flow of information through the model,
let's set the first dimension of the first layer's output to 0. `NNsight` makes
this really easy using the '=' operator:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    # Access the 0th index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, 0] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

Seems our change was reflected. Now let's do the same for the last dimension:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    # Access the last index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, hidden_dims] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

Oh no, we are getting an error! Ah of course, we needed to index at `hidden_dims - 1` not `hidden_dims`.

If you've been using `nnsight`, you are probably familiar with error messages that can be quite difficult to troubleshoot. In `nnsight 0.4` we've now improved error messaging to be descriptive and line-specific, as you should see in the above example!

<details>

<summary>
Old NNsight error messaging
</summary>

If you've been using NNsight prior to the NNsight 0.4 release, you will be familiar with the following non-descriptive error messaging. If you choose to turn off NNsight 0.4's new error messaging feature, this is how errors within the tracing context will appear.

```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in execute(self)
    379                 # Call the target to get value.
--> 380                 output = self.target(*args, **kwargs)
    381

IndexError: index 10 is out of bounds for dimension 1 with size 10

The above exception was the direct cause of the following exception:

IndexError                                Traceback (most recent call last)
20 frames
<ipython-input-16-5c81de91fb1f> in <cell line: 0>()
----> 1 with tiny_model.trace(input):
      2
      3     # Save the output before the edit to compare.
      4     # Notice we apply .clone() before saving as the setting operation is in-place.
      5     l1_output_before = tiny_model.layer1.output.clone().save()

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/Tracer.py in __exit__(self, exc_type, exc_val, exc_tb)
    100
    101
--> 102         super().__exit__(exc_type, exc_val, exc_tb)
    103
    104     def invoke(self, *inputs: Any, **kwargs) -> Invoker:

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/GraphBasedContext.py in __exit__(self, exc_type, exc_val, exc_tb)
    215             raise exc_val
    216
--> 217         self.backend(self)
    218
    219     ### BACKENDS ########

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/backends/LocalBackend.py in __call__(self, obj)
     25     def __call__(self, obj: LocalMixin):
     26
---> 27         obj.local_backend_execute()

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/Tracer.py in local_backend_execute(self)
    144         self.graph.execute()
    145
--> 146         self.model.interleave(
    147             self.model._execute,
    148             self.graph,

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in interleave(self, fn, intervention_graph, *inputs, **kwargs)
    467         module_paths = InterventionProtocol.get_interventions(intervention_graph).keys()
    468
--> 469         with HookHandler(
    470             self._model,
    471             list(module_paths),

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in __exit__(self, exc_type, exc_val, exc_tb)
    579
    580         if isinstance(exc_val, Exception):
--> 581             raise exc_val
    582
    583

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in interleave(self, fn, intervention_graph, *inputs, **kwargs)
    478         ):
    479             try:
--> 480                 fn(*inputs, **kwargs)
    481             except protocols.EarlyStopProtocol.EarlyStopException:
    482                 # TODO: Log.

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in _execute(self, *prepared_inputs, **kwargs)
    585             pass
    586
--> 587         return self._model(
    588             *prepared_inputs,
    589             **kwargs,

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735         else:
-> 1736             return self._call_impl(*args, **kwargs)
   1737
   1738     # torchrec tests the code consistency with the following code

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1842
   1843         try:
-> 1844             return inner()
   1845         except Exception:
   1846             # run always called hooks if they have not already been run

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in inner()
   1788                 args = bw_hook.setup_input_hook(args)
   1789
-> 1790             result = forward_call(*args, **kwargs)
   1791             if _global_forward_hooks or self._forward_hooks:
   1792                 for hook_id, hook in (

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py in forward(self, input)
    248     def forward(self, input):
    249         for module in self:
--> 250             input = module(input)
    251         return input
    252

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735         else:
-> 1736             return self._call_impl(*args, **kwargs)
   1737
   1738     # torchrec tests the code consistency with the following code

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1842
   1843         try:
-> 1844             return inner()
   1845         except Exception:
   1846             # run always called hooks if they have not already been run

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in inner()
   1801                         hook_result = hook(self, args, kwargs, result)
   1802                     else:
-> 1803                         hook_result = hook(self, args, result)
   1804
   1805                     if hook_result is not None:

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in output_hook(module, input, output, module_path)
    564
    565                 def output_hook(module, input, output, module_path=module_path):
--> 566                     return self.output_hook(output, module_path)
    567
    568                 self.handles.append(

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in <lambda>(activations, module_path)
    473                 activations, module_path, "input", intervention_handler
    474             ),
--> 475             output_hook=lambda activations, module_path: InterventionProtocol.intervene(
    476                 activations, module_path, "output", intervention_handler
    477             ),

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in intervene(cls, activations, module_path, key, intervention_handler)
    454
    455                 # Value injection.
--> 456                 node.set_value(value)
    457
    458                 # Check if through the previous value injection, there was a 'swap' intervention.

/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in set_value(self, value)
    408
    409             if listener.fulfilled() and not self.graph.sequential:
--> 410                 listener.execute()
    411
    412         for dependency in self.arg_dependencies:

/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in execute(self)
    385         except Exception as e:
    386
--> 387             raise type(e)(
    388                 f"Above exception when execution Node: '{self.name}' in Graph: '{self.graph.id}'"
    389             ) from e

IndexError: Above exception when execution Node: 'setitem_0' in Graph: '132147685816016'

```

</details>

The error messaging feature can be toggled using `nnsight.CONFIG.APP.DEBUG` which defaults to true.

<details>

<summary>
Toggle Error Messaging
</summary>

Turn off debugging:
```
import nnsight

nnsight.CONFIG.APP.DEBUG = False
nnsight.CONFIG.save()
```

Turn on debugging:
```
import nnsight

nnsight.CONFIG.APP.DEBUG = True
nnsight.CONFIG.save()
```
</details>

Now that we know more about NNsight's error messaging, let's try our setting operation again with the correct indexing and view the shape of the output
before leaving the tracing context:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    print(f"Layer 1 output shape: {tiny_model.layer1.output.shape}")

    # Access the last index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, hidden_dims - 1] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

## Scan and Validate
Error codes are helpful, but sometimes you may want to quickly troubleshoot your code without actually running it.

Enter "Scanning" and "Validating"! We can enable this features by setting the `scan=True` and `validate=True` flag in the `trace` method.

"Scanning" runs "fake" inputs throught the model to collect information like shapes and types (i.e., scanning will populate all called `.inputs` and `.outputs`).

"Validating" attempts to execute the intervention proxies with "fake" inputs to check if they work (i.e., executes all interventions in your code with fake tensors).

"Validating" is dependent on "Scanning" to work correctly, so we need to run the scan of the model at least once to debug with validate. Let's try it out on our example above.

```python
# turn on scan and validate
with tiny_model.trace(input, scan=True, validate=True):

    l1_output_before = tiny_model.layer1.output.clone().save()

    # the error is happening here
    tiny_model.layer1.output[:, hidden_dims] = 0

    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

The operations are never executed using tensors with real values so it doesn't incur any memory costs. Then, when creating proxy requests like the setting one above, `nnsight` also attempts to execute the request on the "fake" values we recorded. Hence, it lets us know if our request is feasible before even running the model. [Here](https://nnsight.net/notebooks/features/scan_validate/) is a more detailed example of scan and validate in action!

<details>
<summary>A word of caution</summary>

---

Some pytorch operations and related libraries don't work well with fake tensors

If you are doing anything in a loop where efficiency is important, you should keep scanning and validating off. It's best to use them only when debugging or when you are unsure if your intervention will work.

---

</details>

We can also use the `.scan()` method to get the shape of a module without having to fully run the model. If scan  is enabled, our input is run though the model under its own "fake" context. This means the input makes its way through all of the model operations, allowing `nnsight` to record the shapes and data types of module inputs and outputs!

```python
with tiny_model.scan(input):

    dim = tiny_model.layer1.output.shape[-1]

print(dim)
```

We can also just replace proxy inputs and outputs with tensors of the same shape
and type. Let's use the shape information we have at our disposal to add noise
to the output, and replace it with this new noised tensor:

## Gradients

`NNsight` also lets us apply backpropagation and access gradients with respect to a
loss. Like `.input` and `.output` on modules, `nnsight` exposes `.grad` on
Proxies themselves (assuming they are proxies of tensors):

```python
with tiny_model.trace(input):

    # We need to explicitly have the tensor require grad
    # as the model we defined earlier turned off requiring grad.
    tiny_model.layer1.output.requires_grad = True

    # We call .grad on a tensor Proxy to communicate we want to store its gradient.
    # We need to call .save() since .grad is its own Proxy.
    layer1_output_grad = tiny_model.layer1.output.grad.save()
    layer2_output_grad = tiny_model.layer2.output.grad.save()

    # Need a loss to propagate through the later modules in order to have a grad.
    loss = tiny_model.output.sum()
    loss.backward()

print("Layer 1 output gradient:", layer1_output_grad)
print("Layer 2 output gradient:", layer2_output_grad)
```

All of the features we learned previously, also apply to `.grad`. In other
words, we can apply operations to and edit the gradients. Let's zero the grad of
`layer1` and double the grad of `layer2`.

```python
with tiny_model.trace(input):

    # We need to explicitly have the tensor require grad
    # as the model we defined earlier turned off requiring grad.
    tiny_model.layer1.output.requires_grad = True

    tiny_model.layer1.output.grad[:] = 0
    tiny_model.layer2.output.grad = tiny_model.layer2.output.grad * 2

    layer1_output_grad = tiny_model.layer1.output.grad.save()
    layer2_output_grad = tiny_model.layer2.output.grad.save()

    # Need a loss to propagate through the later modules in order to have a grad.
    loss = tiny_model.output.sum()
    loss.backward()

print("Layer 1 output gradient:", layer1_output_grad)
print("Layer 2 output gradient:", layer2_output_grad)
```

## Early Stopping

If we are only interested in a model's intermediate computations, we can halt a forward pass run at any module level, reducing runtime and conserving compute resources. One examples where this could be particularly useful would if we are working with SAEs - we can train an SAE on one layer and then stop the execution.

```python
with tiny_model.trace(input):
   l1_out = tiny_model.layer1.output.save()
   tiny_model.layer1.output.stop()

# get the output of the first layer and stop tracing
print("L1 - Output: ", l1_out)
```

Interventions within the tracing context do not necessarily execute in the order they are defined. Instead, their execution is tied to the module they are associated with.

As a result, if the forward pass is terminated early any interventions linked to modules beyond that point will be skipped, even if they were defined earlier in the context.

In the example below, the output of layer 2 _**cannot**_ be accessed since the model's execution was stopped at layer 1.

```python
with tiny_model.trace(input):
   l2_out = tiny_model.layer2.output.save()
   tiny_model.layer1.output.stop()

print("L2 - Output: ", l2_out)
```

## Conditional Interventions

Interventions can also be made conditional.

Inside the tracing context we can specify a new - conditional - context. This context will only execute the interventions within it if the condition is met.

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  with tracer.cond(rand_int % 2 == 0):
    tracer.log("Random Integer ", rand_int, " is Even")

  with tracer.cond(rand_int % 2 == 1):
    tracer.log("Random Integer ", rand_int, " is Odd")
```

Conditional contexts can also be nested, if we want our interventions to depend on more than one condition at a time.

```python
with tiny_model.trace(input) as tracer:

  non_rand_int = 8

  with tracer.cond(non_rand_int > 0):
    with tracer.cond(non_rand_int % 2 == 0):
      tracer.log("Rand Int ", non_rand_int, " is Positive and Even")
```

With `nnsight 0.4` we can now also use Python `if` statements within the tracing context to create a conditional context!

*Note: Colab behaves a little strangely with this feature the first time you run it - expect some lagging and warnings*

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")

  if rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

`elif` statements should also work as `if` statements within the tracing context:

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")
  elif rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

## Iterative Interventions

With the iterator context, you can now run an intervention loop at scale. It iteratively executes and updates a single intervention graph. Use a `.session()` to define the Iterator context and pass in a sequence of items that you want to loop over at each iteration

```python
with tiny_model.session() as session:

  li = nnsight.list() # an NNsight built-in list object
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

With `nnsight 0.4` we can now also use Python `for` loops within a tracer context at scale.

*NOTE: inline for loops (i.e., `[x for x in <Proxy object>`]) are not currently supported.*

```python
# New: Using Python for loops for iterative interventions
with tiny_model.session() as session:

    li = nnsight.list()
    [li.append([num]) for num in range(0, 3)]
    li2 = nnsight.list().save()

    # Using regular for loops
    for item in li:
        for item_2 in item: # for loops can be nested!
            li2.append(item_2)

print("\nList: ", li2)
```

# 2️ Bigger

Now that we have the basics of `nnsight` under our belt, we can scale our model
up and combine the techniques we've learned into more interesting experiments.

The `NNsight` class is very bare bones. It wraps a pre-defined model and does no
pre-processing on the inputs we enter. It's designed to be extended with more
complex and powerful types of models, and we're excited to see what can be done
to leverage its features!

However, if you'd like to load a Language Model from HuggingFace with its tokenizer, the`LanguageModel` subclass greatly simplifies this process.

## LanguageModel

`LanguageModel` is a subclass of `NNsight`. While we could define and create a
model to pass in directly, `LanguageModel` includes special support for
Huggingface language models, including automatically loading models from a
Huggingface ID, and loading the model together with the appropriate tokenizer.

Here is how we can use `LanguageModel` to load `GPT-2`:

```python
from nnsight import LanguageModel

llm = LanguageModel("openai-community/gpt2", device_map="auto")

print(llm)
```

<details>
<summary>On Model Initialization</summary>

---

A few important things to note:

Keyword arguments passed to the initialization of `LanguageModel` is forwarded
to HuggingFace specific loading logic. In this case, `device_map` specifies
which devices to use and its value `auto` indicates to evenly distribute it to
all available GPUs (and CPU if no GPUs available). Other arguments can be found
here:
https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM

When we initialize `LanguageModel`, we aren't yet loading the parameters of the
model into memory. We are actually loading a 'meta' version of the model which
doesn't take up any memory, but still allows us to view and trace actions on it.
After exiting the first tracing context, the model is then fully loaded into
memory. To load into memory on initialization, you can pass `dispatch=True` into
`LanguageModel` like
`LanguageModel('openai-community/gpt2', device_map="auto", dispatch=True)`.

---

</details>

Let's now apply some of the features that we used on the small model to `GPT-2`. Unlike `NNsight`, `LanguageModel` does define logic to pre-process
inputs upon entering the tracing context. This makes interacting with the model
simpler (i.e., you can send prompts to the model without having to directly access the tokenizer).

In the following example, we ablate the value coming from the last layer's MLP
module and decode the logits to see what token the model predicts without
influence from that particular module:

```python
with llm.trace("The Eiffel Tower is in the city of"):

    # Access the last layer using h[-1] as it's a ModuleList
    # Access the first index of .output as that's where the hidden states are.
    llm.transformer.h[-1].mlp.output[0][:] = 0

    # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.
    token_ids = llm.lm_head.output.argmax(dim=-1).save()

print("\nToken IDs:", token_ids)

# Apply the tokenizer to decode the ids into words after the tracing context.
print("Prediction:", llm.tokenizer.decode(token_ids[0][-1]))
```

We just ran a little intervention on a much more complex model with many more
parameters! However, we're missing an important piece of information: what the
prediction would have looked like without our ablation.

We could just run two tracing contexts and compare the outputs. However, this would require two forward passes through the model. `NNsight` can do
better than that with batching.

<a name="batching-id"></a>

## Batching

Batching is a way to process multiple inputs in one forward pass. To better understand how batching works, we're going to bring back the `Tracer` object that we dropped before.

When we call `.trace(...)`, it's actually creating two different contexts behind the scenes. The first one is the tracing context that we've discussed previously, and the second one is the invoker context. The invoker context defines the values of the `.input` and `.output` Proxies.

If we call `.trace(...)` with some input, the input is passed on to the invoker. As there is only one input, only one invoker context is created.

If we call `.trace()` without an input, then we can call `tracer.invoke(input1)` to manually create the invoker context with an input, `input1`. We can also repeatedly call `tracer.invoke(...)` to create the invoker context for additional inputs. Every subsequent time we call
`.invoke(...)`, interventions within its context will only refer to the input in that particular invoke statement.

When exiting the tracing context, the inputs from all of the invokers will be batched together, and they will be executed in one forward pass! To test this out, let's do the same ablation experiment, but also add a 'control' output for comparison:

<details>
<summary>More on the invoker context</summary>

---

Note that when injecting data to only the relevant invoker interventions, `nnsight` tries, but can't guarantee, to narrow the data into the right
batch indices. Thus, there are cases
where all invokes will get all of the data. Specifically, if the input or output data is stored
as an object that is not an arbitrary collection of tensors, it will be broadcasted to all invokes.

Just like `.trace(...)` created a `Tracer` object, `.invoke(...)` creates an `Invoker` object. For `LanguageModel` models, the `Invoker` prepares the input by running a tokenizer on it.
`Invoker` stores pre-processed inputs at `invoker.inputs`, which can be accessed to see information about our inputs.
In a case where we pass a single input to `.trace(...)` directly, we can still access the invoker
object at `tracer.invoker` without having to call `tracer.invoke(...)`.

Keyword arguments given to `.invoke(..)` make their way to the input pre-processing.
`LanguageModel` has keyword arguments `max_length` and `truncation` used for tokenization which can be
passed to the invoker. If we want to pass keyword arguments to the invoker for a single-input `.trace(...)`, we can pass `invoker_args` as a dictionary of invoker keyword arguments.

Here is an example to demonstrate everything we've described:

**This snippet**

```
with llm.trace("hello", invoker_args={"max_length":10}) as tracer:
  invoker = tracer.invoker

```
  **does the same as**

```
with llm.trace() as tracer:
  with tracer.invoke("hello", max_length=10) as invoker:
    invoker = invoker
```

---

</details>

```python
with llm.trace() as tracer:

    with tracer.invoke("The Eiffel Tower is in the city of"):

        # Ablate the last MLP for only this batch.
        llm.transformer.h[-1].mlp.output[0][:] = 0

        # Get the output for only the intervened on batch.
        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()

    with tracer.invoke("The Eiffel Tower is in the city of"):

        # Get the output for only the original batch.
        token_ids_original = llm.lm_head.output.argmax(dim=-1).save()

print("Original token IDs:", token_ids_original)
print("Modified token IDs:", token_ids_intervention)

print("Original prediction:", llm.tokenizer.decode(token_ids_original[0][-1]))
print("Modified prediction:", llm.tokenizer.decode(token_ids_intervention[0][-1]))
```

Based on our control results, our ablation did end up affecting what the model predicted. That's pretty neat!

Another cool thing with multiple invokes is that Proxies can interact between them.

Here, we transfer the token embeddings from a real prompt into another placeholder prompt. Therefore the latter prompt produces the output of the former prompt:

```python
with llm.trace() as tracer:

    with tracer.invoke("The Eiffel Tower is in the city of"):
        embeddings = llm.transformer.wte.output

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):
        llm.transformer.wte.output = embeddings
        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):
      token_ids_original = llm.lm_head.output.argmax(dim=-1).save()

print("original prediction shape", token_ids_original[0][-1].shape)
print("Original prediction:", llm.tokenizer.decode(token_ids_original[0][-1]))

print("modified prediction shape", token_ids_intervention[0][-1].shape)
print("Modified prediction:", llm.tokenizer.decode(token_ids_intervention[0][-1]))
```

## Multiple Token Generation

### .next()

Some HuggingFace models define methods to generate multiple outputs at a time.
`LanguageModel` wraps that functionality to provide the same tracing features by
using `.generate(...)` instead of `.trace(...)`. This calls the underlying
model's `.generate` method. It passes the output through a `.generator`
module that we've added onto the model, allowing us to get the generate output
at `.generator.output`.

In a case like this, the underlying model is called more than once; the modules
of said model produce more than one output. Which iteration should a given
`module.output` refer to? That's where `Module.next()` comes in!

Each module has a call index associated with it and `.next()` simply increments
that attribute. At the time of execution, data is injected into the intervention
graph only at the iteration that matches the call index.

```python
with llm.generate('The Eiffel Tower is in the city of', max_new_tokens=3) as tracer:

    hidden_states1 = llm.transformer.h[-1].output[0].save()

    # use module.next() to access the next intervention
    hidden_states2 = llm.transformer.h[-1].next().output[0].save()

    # saving the output allows you to save the hidden state across the initial prompt
    out = llm.generator.output.save()

print(hidden_states1.shape)
print(hidden_states2.shape)
print(out)
```

### New! using .all()

With `nnsight 0.4` you can now use `.all()` to recursively apply interventions to a model. Calling `.all()` on a module within a model will recursively apply its `.input` and `.output` across all iterations. Previously, we'd need to loop across each new generated token, saving the intervention for every generated token and calling `.next()` to move forward.

```python
# Old approach:
prompt = 'The Eiffel Tower is in the city of'
layers = llm.transformer.h
n_new_tokens = 3
hidden_states = []
with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    for i in range(n_new_tokens):
        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output.save())

        # Move to next generated token
        layers[0].next()

print("Hidden state length: ",len(hidden_states))
```

We can use also `.all()` to streamline the multiple token generation process. We simply call `.all` on the module where we are applying the intervention (in this case GPT-2's layers), apply our intervention, and append our hidden states (stored in an `nnsight.list()` object).
<br> <br>

Let's test this out for the multiple token generation case:

```python
# using .all():
prompt = 'The Eiffel Tower is in the city of'
layers = llm.transformer.h
n_new_tokens = 3
with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() to apply intervention to each new token
    layers.all()

    # Apply intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(layers[-1].output) # no need to call .save
    # Don't need to loop or call .next()!

print("Hidden state length: ",len(hidden_states))
```

Easy! Note that because `.all()` is recursive, it will only work to append outputs called on children of the module that `.all()` was called on. See example below for more information. TL;DR: apply `.all()` on the highest-level accessed module if interventions and outputs have different hierarchies within model structure.

<details>
<summary>Recursive properties of .all()</summary>

`.all()` recursively acts on model components. In the below code example, only the first token generation is saved, because `.all()` applied to `layers`, while the saved variable `hidden_states` is produced from `model.lm_head`, which is not a child of `layers`.

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on layers
    layers.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save, it's already initialized

print("Hidden state length: ",len(hidden_states)) # length is 1, meaning it only saved the first token generation
```

If you want to apply an intervention during multiple token generation while saving the state of a model component that isn't a child of that module, you can instead apply `.all()` to the full model:

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on model
    model.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save

print("Hidden state length: ",len(hidden_states)) # length is 3, as expected!
```

</details>

## Model Editing

NNsight's model editing feature allows you to create persistently modified versions of a model with a use of `.edit()`. Unlike interventions in a tracing context, which are temporary, the **Editor** context enables you to make lasting changes to a model instance.

This feature is useful for:
* Creating modified model variants without altering the original
* Applying changes that persist across multiple forward passes
* Comparing interventions between original and edited models

Let's explore how to use the **Editor** context to make a simple persistent change to a model:

```python
# we take the hidden states with the expected output "Paris"
with llm.trace("The Eiffel Tower is located in the city of") as tracer:
    hs11 = llm.transformer.h[11].output[0][:, -1, :].save()

# the edited model will now always predict "Paris" as the next token
with llm.edit() as llm_edited:
    llm.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with llm.trace("Vatican is located in the city of") as tracer:
    original_tokens = llm.lm_head.output.argmax(dim=-1).save()

# ...with the output of the edited model
with llm_edited.trace("Vatican is located in the city of") as tracer:
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("\nOriginal Prediction: ", llm.tokenizer.decode(original_tokens[0][-1]))
print("Modified Prediction: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

Edits defined within an **Editor** context create a new, modified version of the model by default, preserving the original. This allows for safe experimentation with model changes. If you wish to modify the original model directly, you can set `inplace=True` when calling `.edit()`.

Use this option cautiously, as in-place edits alter the base model for all the consequent model calls.

```python
# we use the hidden state we saved above (hs11)
with llm.edit(inplace=True) as llm_edited:
    llm.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with llm.trace("Vatican is located in the city of") as tracer:
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("Modified In-place: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

If you've made in-place edits to your model and need to revert these changes, you can apply `.clear_edits()`. This method removes all edits applied to the model, effectively restoring it to its original state.

```python
llm.clear_edits()

with llm.trace("Vatican is located in the city of"):
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("Edits cleared: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

# 3 I thought you said huge models?

`NNsight` is only one part of our project to democratize access to AI internals. The other half is the National Deep Inference Fabric, or `NDIF`. `NDIF` hosts large models for shared access using `NNsight`, so you don't have to worry about any of the headaches of hosting large models yourself!

The interaction between `NDIF` and `NNsight` is fairly straightforward. The
**intervention graph** we create via the tracing context can be encoded into a
custom json format and sent via an http request to the `NDIF` servers. `NDIF`
then decodes the **intervention graph** and **interleaves** it alongside the
specified model.

To see which models are currently being hosted, check out the following status
page: https://nnsight.net/status/

## Remote execution

In its current state, `NDIF` requires you to receive an API key. Therefore, to
run the rest of this walkthrough, you need one of your own. To get one, simply
register at https://login.ndif.us.

With a valid API key, you then can configure `nnsight` as follows:

```python
from nnsight import CONFIG

CONFIG.set_default_api_key("YOUR_API_KEY")
```

If you're running in a local IDE, this only needs to be run once as it will save the API key as the default in a
.config file along with your `nnsight` installation. You can also add your API key to Google Colab secrets.

To amp things up a few levels, let's demonstrate using `nnsight`'s tracing
context with `Llama-3.1-8b`!

```python
import os

# Llama 3.1 8b is a gated model, so you need to apply for access on HuggingFace and include your token.
os.environ['HF_TOKEN'] = "YOUR_HUGGING_FACE_TOKEN"
```

```python
from nnsight import LanguageModel

# We'll never actually load the parameters locally, so no need to specify a device_map.

llama = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
# All we need to specify using NDIF vs executing locally is remote=True.
with llama.trace("The Eiffel Tower is in the city of", remote=True) as runner:

    hidden_states = llama.model.layers[-1].output.save()

    output = llama.output.save()

print(hidden_states)

print(output["logits"])
```

It really is as simple as `remote=True`. All of the techniques we went through
in earlier sections work just the same when running locally or remotely.

## Sessions

NDIF uses a queue to handle concurrent requests from multiple users. To optimize the execution of our experiments we can use the `session` context to efficiently package multiple interventions together as one single request to the server.

This offers the following benefits:
1.   All interventions within a session will be executed one after another without additional wait in the NDIF queue
2.   All intermediate outputs for each intervention are stored on the server and can be accessed by other interventions in the same session without moving the data back and forth between NDIF and the local machine

Let's take a look:

```python
with llama.session(remote=True) as session:

  with llama.trace("The Eiffel Tower is in the city of") as t1:
    # capture the hidden state from layer 32 at the last token
    hs_31 = llama.model.layers[31].output[0][:, -1, :] # no .save()
    t1_tokens_out = llama.lm_head.output.argmax(dim=-1).save()

  with llama.trace("Buckingham Palace is in the city of") as t2:
    llama.model.layers[1].output[0][:, -1, :] = hs_31[:]
    t2_tokens_out = llama.lm_head.output.argmax(dim=-1).save()

print("\nT1 - Original Prediction: ", llama.tokenizer.decode(t1_tokens_out[0][-1]))
print("T2 - Modified Prediction: ", llama.tokenizer.decode(t2_tokens_out[0][-1]))
```

In the example above, we are interested in replacing the hidden state of a later layer with an earlier one. Since we are using a `session`, we don't have to save the hidden state from Tracer 1 to reference it in Tracer 2.

It is important to note that all the traces defined within the `session` context are executed sequentially, strictly following the order of definition (i.e. `t2` being executed after `t1` and `t3` after `t2` etc.).

The `session` context object has its own methods to log values and be terminated early.

```python
with llama.session(remote=True) as session:
  session.log("-- Early Stop --")
  nnsight.stop
```

In addition to the benefits mentioned above, the `session` context also enables interesting experiments not possible with other `nnsight` tools — since every trace is run on its own model, it means that within one session we can run interventions between different models — for example, we could swap activations between base and instruct versions of the Llama model and compare their outputs. And `session` can also be used to run similar experiments entirely locally!

## Streaming

Streaming enables users apply functions and datasets locally during remote model execution. This allows users to stream results for immediate consumption (i.e., seeing tokens as they are generated) or applying non-whitelisted functions such as model tokenizers, large local datasets, and more!

*   `nnsight.local()` context sends values immediately to user's local machine from server
*   Intervention graph is executed locally on downstream nodes
*   Exiting local context uploads data back to server
*   `@nnsight.trace` function decorator enables custom functions to be added to intervention graph when using `nnsight.local()`

### `nnsight.local()`

You may sometimes want to locally access and manipulate values during remote execution. Using `.local()` on a proxy, you can send remote content to your local machine and apply local functions. The intervention graph is then executed locally on downstream nodes (until you send execution back to the remote server by exiting the `.local()` context).

There are a few use cases for streaming with `.local()`, including live chat generation and applying large datasets or non-whitelisted local functions to the intervention graph.

Now let's explore how streaming works. We'll start by grabbing some hidden states of the model and printing their value using `tracer.log()`. Without calling `nnsight.local()`, these operations will all occur remotely.

```python
# This will give you a remote LOG response because it's coming from the remote server
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

Now, let's try the same operation using the `nnsight.local()` context. This will send the operations to get and print the hidden states to your local machine, changing how the logging message is formatted (local formatting instead of remote).

```python
# This will print locally because it's already local
with llama.trace("hello", remote=True) as tracer:

    with nnsight.local():
        hs = llama.model.layers[-1].output[0]
        tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

### `@nnsight.trace` function decorator

We can also use function decorators to create custom functions to be used during `.local` calls. This is a handy way to enable live streaming of a chat or to train probing classifiers on model hidden states.

Let's try out `@nnsight.trace` and `nnsight.local()` to access a custom function during remote execution.

```python
# first, let's define our function
@nnsight.trace # decorator that enables this function to be added to the intervention graph
def my_local_fn(value):
    return value * 0

# We use a local function to ablate some hidden states
# This downloads the data for the .local context, and then uploads it back to set the value.
with llama.generate("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    with nnsight.local():

        hs = my_local_fn(hs)

    llama.model.layers[-1].output[0][:] = hs

    out =  llama.lm_head.output.save()
```

Note that without calling `.local`, the remote API does not know about `my_local_fn` and will throw a whitelist error. A whitelist error occurs because you are being allowed access to the function.

```python
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    hs = my_local_fn(hs) # no .local - will cause an error

    llama.model.layers[-1].output[0][:] = hs * 2

    out =  llama.lm_head.output.save()

print(out)
```

### Example: Live-streaming remote chat

Now that we can access data within the tracing context on our local computer, we can apply non-whitelisted functions, such as the model's tokenizer, within our tracing context.

Let's build a decoding function that will decode tokens into words and print the result.

```python
@nnsight.trace
def my_decoding_function(tokens, model, max_length=80, state=None):
    # Initialize state if not provided
    if state is None:
        state = {'current_line': '', 'current_line_length': 0}

    token = tokens[-1] # only use last token

    # Decode the token
    decoded_token = llama.tokenizer.decode(token).encode("unicode_escape").decode()

    if decoded_token == '\\n':  # Handle explicit newline tokens
        # Print the current line and reset state
        print('',flush=True)
        state['current_line'] = ''
        state['current_line_length'] = 0
    else:
        # Check if adding the token would exceed the max length
        if state['current_line_length'] + len(decoded_token) > max_length:
            print('',flush=True)
            state['current_line'] = decoded_token  # Start a new line with the current token
            state['current_line_length'] = len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line
        else:
            # Add a space if the line isn't empty and append the token
            if state['current_line']:
                state['current_line'] += decoded_token
            else:
                state['current_line'] = decoded_token
            state['current_line_length'] += len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line

    return state
```

Now we can decode and print our model outputs throughout token generation by accessing our decoding function through `nnsight.local()`.

```python
import torch

nnsight.CONFIG.APP.REMOTE_LOGGING = False

prompt = "A press release is an official statement delivered to members of the news media for the purpose of"
# prompt = "Your favorite board game is"

print("Prompt: ",prompt,'\n', end ="")

# Initialize the state for decoding
state = {'current_line': '', 'current_line_length': 0}

with llama.generate(prompt, remote=True, max_new_tokens = 50) as generator:
    # Call .all() to apply to each new token
    llama.all()

    all_tokens = nnsight.list().save()

    # Access model output
    out = llama.lm_head.output.save()

    # Apply softmax to obtain probabilities and save the result
    probs = torch.nn.functional.softmax(out, dim=-1)
    max_probs = torch.max(probs, dim=-1)
    tokens = max_probs.indices.cpu().tolist()
    all_tokens.append(tokens[0]).save()

    with nnsight.local():
        state = my_decoding_function(tokens[0], llama, max_length=20, state=state)
```

## Looping across sessions

We mention earlier that the `session` context enables multi-tracing execution. But how can we optimize a process that would require running an intervention graph in a loop? If we create a simple `for` loop with a **Tracer context** inside, this will result in creating a new intervention graph at each iteration, which is not scalable.

We solve this problem the `nnsight` way via the **Iterator context**: an intervention loop that iteratively executes and updates a single intervention graph.

Use a `session` to define the **Iterator context** and pass in a sequence of items that you want to loop over at each iteration:

```python
with llama.session(remote=True) as session:

  with session.iter([0, 1, 2]) as item:
    # define intervention body here ...

    with llama.trace("_"):
      # define interventions here ...
      pass

    with llama.trace("_"):
      # define interventions here ...
      pass
```

The `Iterator` context extends all the `nnsight` graph-based functionalities, but also closely mimics the conventional `for` loop statement in Python, which allows it to support all kind of iterative operations with a use of `as item` syntax:

```python
with llama.session(remote=True) as session:

  li = nnsight.list()
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

Notice how we used the `nnsight.list()` method to create a list of lists to loop over. This type of method is what we call an **NNsight Built-in**. It is a special type of methods that serve as a wrapper around `nnsight.apply()` to provide a more user-friendly interface for adding common datatypes to the Intervention Graph.

<details>
<summary>A full list of NNsight Built-ins</summary>

`nnsight.bool()` creates a traceable Boolean

`nnsight.bytes()` creates a traceable Bytes

`nnsight.int()` creates a traceable Integer

`nnsight.float()` creates a traceable Float

`nnsight.str()` creates a traceable String

`nnsight.comples()` creates a traceable Complex number

`nnsight.bytearray()` creates a traceable Bytearray

`nnsight.tuple()` creates a traceable Tuple

`nnsight.list()` creates a traceable List

`nnsight.set()` creates a traceable Set

`nnsight.dict()` creates a traceable Dictionary

</details>

We can also expose the `iterator` context object via a `return_context` flag. You can then use it to `exit` out of the Iteration loop early and log the intermediate outputs within the loop:

```python
with llama.session(remote=True) as session:

  # with session.iter([0, 1, 2, 3], return_context=True) as (item, iterator):
  with session.iter([0, 1, 2, 3]) as item:

      nnsight.log(item)

      with nnsight.cond(item == 2):
        nnsight.stop()
```

The **Iterator** context is a niece piece of functionality that allows you to define a bunch of basic code operations that can now be "traceable" by `nnsight`.

But in what kind of experimental scenario would someone need iterators?

In the next section, we delve into a powerful use case of the `Iterator` context and see how it enables it!

## Training a LoRA

Here is an example of a task that uses everything we have covered in the last section - remote execution, **Session** context and iterative interventions. Using session and iterator contexts, we're going apply a very simple fine-tuning approach called low-rank adaptation (LoRA).

Let's try training a LoRA that, when applied, makes our model always predict "Paris" no matter what.

```python
import torch
import torch.nn as nn
import nnsight
# from nnsight.envoy import Envoy # this moved in 0.4
from nnsight import Envoy

# We will define a LORA class.
# The LORA class call method operations are simply traced like you would normally do in a .trace.
class LORA(nn.Module):
    def __init__(self, module: Envoy, dim: int, r: int) -> None:
        """Init.

        Args:
            module (Envoy): Which model Module we are adding the LORA to.
            dim (int): Dimension of the layer we are adding to (This could potentially be auto populated if the user scanned first so we know the shape)
            r (int): Inner dimension of the LORA
        """
        super(LORA, self).__init__()
        self.r = r
        self.module = module
        self.WA = torch.nn.Parameter(torch.randn(dim, self.r), requires_grad=True).save()
        self.WB = torch.nn.Parameter(torch.zeros(self.r, dim), requires_grad=True).save()

    # The Call method defines how to actually apply the LORA.
    def __call__(self, alpha: float = 1.0):
        """Call.

        Args:
            alpha (float, optional): How much to apply the LORA. Can be altered after training for inference. Defaults to 1.0.
        """

        # We apply WA to the first positional arg (the hidden states)
        A_x = torch.matmul(self.module.input[0][0], self.WA)
        BA_x = torch.matmul(A_x, self.WB)

        # LORA is additive
        h = BA_x + self.module.output

        # Replace the output with our new one * alpha
        # Could also have been self.module.output[:] = h * alpha, for in-place
        self.module.output = h * alpha

    def parameters(self):
        # Some way to get all the parameters.
        return [self.WA, self.WB]
```

Let's define all the variables to use in LoRA training.

```python
# We need the token id of the correct answer.
answer = " Paris"
answer_token = llama.tokenizer.encode(answer)[1]
# Inner LORA dimension
lora_dim = 4
# Module to train LORA on
module = llama.model.layers[-1].mlp
```

We can use the `.scan()` method to get the shape of the module without having to fully run the model.

```python
with llama.scan(" "):
    dim = module.output.shape[-1]

print(dim)
```

It's time to run the LORA training loop! We using the **Session** and the **Iterator** contexts to achieve this.

```python
from torch.utils.data import DataLoader

# The LORA object itself isn't transmitted to the server. Only the forward / call method.
# The parameters are created remotely and never sent only retrieved
with llama.session(remote=True) as session:

    # Create dataset of 100 pairs of a blank prompt and the " Paris " id
    dataset = [["_", answer_token]] * 100

    # Create a dataloader from it.
    dataloader = DataLoader(dataset, batch_size=10)

    # Create our LORA on the last mlp
    lora = LORA(module, dim, lora_dim)

    # Create an optimizer. Use the parameters from LORA
    optimizer = torch.optim.AdamW(lora.parameters(), lr=3)

    # Iterate over dataloader using .iter.
    with session.iter(dataloader) as batch:

        prompt = batch[0]
        correct_token = batch[1]

        # Run .trace with prompt
        with llama.trace(prompt) as tracer:

            # Apply LORA to intervention graph just by calling it with .trace
            lora()

            # Get logits
            logits = llama.lm_head.output

            # Do cross entropy on last predicted token and correct_token
            loss = torch.nn.functional.cross_entropy(logits[:, -1], batch[1])
            # Call backward
            loss.backward()

        # Call methods on optimizer. Graphs that arent from .trace (so in this case session and iterator both have their own graph) are executed sequentially.
        # The Graph of Iterator here will be:
        # 1.) Index batch at 0 for prompt
        # 2.) Index batch at 1 for correct_token
        # 3.) Execute the .trace using the prompt
        # 4.) Call .step() on optimizer
        optimizer.step()
        # 5.) Call .zero_grad() in optimizer
        optimizer.zero_grad()
        # 6.) Print out the lora WA weights to show they are indeed changing
        nnsight.log(lora.WA)

```

Now `WA` and `WB` are optimized! So we generate with the LoRA just by calling `lora()` in the `.generate` and save the output to then de-tokenize it.

```python
# With lora. Should produce "Hello Paris"
with llama.generate("Hello", remote=True) as generator:

    lora()

    out = llama.generator.output.save()

print(llama.tokenizer.batch_decode(out.value))

# Then without. Should produce "Hello,"
with llama.generate("Hello", remote=True) as generator:

    out = llama.generator.output.save()

print(llama.tokenizer.batch_decode(out.value))

```

# Next Steps
Check out [nnsight.net/tutorials](https://nnsight.net/tutorials) for more walkthroughs implementating classic interpretability techniques using `nnsight`.

# Getting Involved!

Note that both `nnsight` and `NDIF` are in active development, so changes may be made and errors may arise during use. If you’re interested in following updates to `nnsight`, contributing, giving feedback, or finding collaborators, please join the [NDIF discord](https://discord.gg/6uFJmCSwW7). We’d love to hear about your work using nnsight!

You can also follow us on [LinkedIn](https://www.linkedin.com/company/national-deep-inference-fabric/), Bluesky: [@ndif-team.bsky.social](https://bsky.app/profile/ndif-team.bsky.social), and X: [@ndif_team](https://x.com/ndif_team).

💟


---

# NNsight_v0_3_guide.ipynb

# NNsight 0.3 - User Guide

## Set up

```python
from IPython.display import clear_output

!pip install nnsight
!pip install --upgrade transformers torch

clear_output()
```

```python
from google.colab import userdata
from nnsight import CONFIG

from nnsight.logger import remote_logger
remote_logger.propagate = False

CONFIG.set_default_api_key('422220a9817141e49c5add1868af07a5')
```

```python
from collections import OrderedDict
from nnsight import NNsight
import torch

input_size = 5
hidden_dims = 10
output_size = 2

torch.manual_seed(423)

net = torch.nn.Sequential(
    OrderedDict(
        [
            ("layer1", torch.nn.Linear(input_size, hidden_dims)),
            ("layer2", torch.nn.Linear(hidden_dims, hidden_dims))
        ]
    )
).requires_grad_(False)

input = torch.rand((1, input_size))
input_2 = torch.rand((1, input_size))

tiny_model = NNsight(net)
```

```python
from nnsight import LanguageModel

lm = LanguageModel("openai-community/gpt2", dispatch=True)
llm = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
```

# Breaking Changes

## input/inputs

Module input access has a syntactic change:

- Old: `nnsight.Envoy.input`

- New: `nnsight.Envoy.inputs`

- Note: `nnsight.Envoy.input` now provides access to the first positional argument of the module's input.

```python
with lm.trace("Hello World"):
  l2_ins = lm.transformer.h[2].inputs.save()
  l2_in = lm.transformer.h[2].input.save()

print("Inputs: ", l2_ins)
print("First Positional Argument Input: ", l2_in)
```

## `scan` and `validate`

`scan` and `validate` are now set to `False` by default in the `Tracer` context.

# New Features

### Scanning

You can scan a model without executing it to gather important insights. This is useful for looking at internal modules' shapes for example. You can pass a dummy input to the model, and it will not be executed. This is also means that you don't have to call `save()` on any variable.

```python
with tiny_model.scan(torch.tensor([0, 0, 0, 0, 0])):
  dim = tiny_model.layer2.input.shape

print(dim)
```

## nnsight builtins

You can now define multiple `Python` builtins to be traceable by the Intervention graph.

Simply use the `nnsight` import to call constructors for these data structures.

```python
import nnsight

with tiny_model.trace(input):
  num = nnsight.int(5).save()
  l = nnsight.list().save()
  l.append(num)
  d = nnsight.dict({"five": num}).save()

print("Interger: ", num)
print("List: ", l)
print("Dictionary: ", d)
```

Here is the complete list of supported `Python` builtins:
- bool
- bytes
- int
- float
- str
- complex
- bytearray
- tuple
- list
- set
- dict

## Proxy Update

For literals created and traced by `nnsight`, there is no direct way of setting their values.

Use our `.update()` method on Intervention Proxies to assign it a new value.

```python
import nnsight

with tiny_model.trace(input):
  input_str = nnsight.str("I am a ").save()
  input_str.update(input_str + "Transformer")

print("Input: ", input_str)
```

This is also useful for calculating running sums and other statistics.

## Logging

We are probably all guilty, at least once, of trying to print an Intervention Proxy from within the tracing context to look at its value:

```python
with tiny_model.trace(input):
    print(tiny_model.layer1.output)
```

The reason this does not print any actual value is because the model is only executed upon exiting the `Tracer` context, and thus, the proxies' values have not been populated yet.

If you are still only interested in looking at some intermediate values without necessary saving them, you can call our logging feature which will be executed as an `nnsight` node during the model's execution and show you the actual values.

```python
import nnsight

with tiny_model.trace(input) as tracer:
  nnsight.log("Layer 1 - out: ", tiny_model.layer1.output)
```

## Tracing function calls

Everything within the tracing context operates on the intervention graph. Therefore for `nnsight` to trace a function it must also be a part of the intervention graph.

Out-of-the-box `nnsight` supports `Pytorch` functions and methods, all operators, as well the `einops` library. We don’t need to do anything special to use them.

For custom functions we can use `nnsight.apply()` to add them to the intervention graph.

```python
import nnsight
import torch

# We define a simple custom function that sums all the elements of a tensor
def tensor_sum(tensor):
    flat = tensor.flatten()
    total = 0
    for element in flat:
        total += element.item()

    return torch.tensor(total)

with lm.trace("The Eiffel Tower is in the city of") as tracer:

    # Specify the function name and its arguments (in a coma-separated form) to add to the intervention graph
    custom_sum = nnsight.apply(tensor_sum, lm.transformer.h[0].output[0]).save()
    sum = lm.transformer.h[0].output[0].sum().save()

print("PyTorch sum: ", sum)
print("Our sum: ", custom_sum)
```

## Early Stopping

If you are only interested in a model's intermediate computations, you can halt a forward pass run at any module level, reducing runtime and conserving computational resources. This is particularly useful if you are working with SAEs.

```python
with tiny_model.trace(input):
   l1_out = tiny_model.layer1.output.save()
   tiny_model.layer1.output.stop()

print("L1 - Output: ", l1_out)
```

Interventions within the `Tracer` context do not necessarily execute in the order they are defined. Instead, their execution is tied to the module they are associated with.

As a result, if the forward pass is terminated early any interventions linked to modules beyond that point will be skipped, even if they were defined earlier in the context.

In the example below, the output of layer 2 **CANNOT** be accessed since the model's execution was stopped at layer 1.

```python
with tiny_model.trace(input):
   l2_out = tiny_model.layer2.output.save()
   tiny_model.layer1.output.stop()

print("L2 - Output: ", l2_out)
```

## Conditional Interventions

You can make interventions conditional!

Create a Conditional context and pass it a value to be evaluated as a boolean. The context will wrap all the interventions that you wish to be dependent on the condition specified.

Let's take a look at how you can do that:

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,)).item()

  with tracer.cond(rand_int % 2 == 0):
    tracer.apply(print, "Random Integer ", rand_int, " is Even")

  with tracer.cond(rand_int % 2 == 1):
    tracer.apply(print, "Random Integer ", rand_int, " is Odd")
```

In the example above, we have two Conditional contexts with mutually exclusive conditions, mimicking a conventional `If`-`Else` statement.

The condition passed to the Conditional context is evaluated directly by calling `bool()` on the proxy value, so be mindful of how your Intervention Proxy condition evaluates to boolean.

```python
with tiny_model.trace(input) as tracer:
  l1_out = tiny_model.layer1.output
  with tracer.cond(l1_out != 1):
    tracer.apply(print, "Condition is True")
```

The code above throws the **ERROR**: `Boolean value of Tensor with more than one value is ambiguous`, because the condition specified in the Conditional context cannot be handled properly.

Instead, use something like this:

```python
with tiny_model.trace(input) as tracer:
  l1_out = tiny_model.layer1.output
  with tracer.cond(torch.all(l1_out != 1)):
    tracer.apply(print, "Condition is True")
```

Conditional contexts can also be nested, if you want your interventions to depend on more than one condition at a time.

```python
with tiny_model.trace(input) as tracer:
  rand_int = tracer.apply(int, 6)
  with tracer.cond(rand_int > 0):
    with tracer.cond(rand_int % 2 == 0):
      tracer.apply(print, "Rand Int ", rand_int, " is Positive and Even")
```

## Model Editing

You can alter a model by setting default edits and interventions in an editing context, applied before each forward pass. This can be used to attach additional modules like SAEs.

```python
with tiny_model.edit() as edited_model:
  tiny_model.layer1.output[0][:] = 0

with tiny_model.trace(input):
  l1_out = tiny_model.layer1.output.save()

with edited_model.trace(input):
  l1_out_edited = edited_model.layer1.output.save()

print("L1 - Out: ", l1_out)
print("L1 - Out [edited]: ", l1_out_edited)
```

Let's look at anotehr example

```python
from nnsight.util import WrapperModule

class ComplexModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.one = WrapperModule()

    def forward(self, x):
        return self.one(x)

l0 = lm.transformer.h[0]
l0.attachment = ComplexModule()

with lm.edit() as gpt2_edited:
    acts = l0.output[0]
    l0.output[0][:] = l0.attachment(acts, hook=True)

# Get values pre editing
with lm.trace("Madison Square Garden is located in the city of"):
    original = l0.output[0].clone().save()
    l0.output[0][:] *= 0.
    original_output = lm.output.logits.save()

with gpt2_edited.trace("Madison Square Garden is located in the city of"):
    one = l0.attachment.one.output.clone().save()
    l0.attachment.output *= 0.
    edited_output = lm.output.logits.save()

print("Original output: ", original_output)
print("Edited output: ", edited_output)
```

Your edit call can be customized by choosing to perform edits in-place on the model andgetting access to the editor context (`nnsight.context.Tracer`).

You can also choose to remove edits perfomerd on a model at a later stage.

```python
with tiny_model.edit(inplace=True, return_context=True) as editor:
  tiny_model.layer1.output[0][:] = 0

with tiny_model.trace(input):
  l1_out = tiny_model.layer1.output.save()

print("L1 - Out: ", l1_out)

tiny_model.clear_edits()

with tiny_model.trace(input):
  l1_out = tiny_model.layer1.output.save()

print("L1 - Out [unedited]: ", l1_out)
```

Note that setting new modules with remote execution is currently not supported!

## Session Context

`nnsight 0.3` focuses on enhancing the capabilities of our remote execution API, powered by the [NDIF](https://ndif.us) backend.

To achieve this, we introduce the **Session** context: an overarching structure for efficiently handling multi-tracing experiments. This means that, multiple `Tracer` contexts can be packaged together as part of one single request to the server.

The `Session` context can also be used entirely for local usage, as it enables useful functionalities and optimizes experiments.

```python
with llm.session(remote=True):
  with llm.trace("_") as t1:
    # define interventions here
    pass

  with llm.trace("_") as t2:
    # define interventions here
    pass

  with llm.trace("_") as t3:
    # define interventions here
    pass
```

All operations defined within a `Session` context are executed at the very end (upon exiting the overarching context) and it is conducted sequentially, strictly following the order of definition (`t2` being executed after `t1` and `t3` after `t2`).

In a `Session`, interventions defined at any early stage can be seamlessly referenced.

```python
with llm.session(remote=True) as session:
  with llm.trace("The Eiffel Tower is in the city of") as t1:
    hs_11 = llm.model.layers[-1].output[0][:, -1, :] # no .save()
    t1_tokens_out = llm.output.save()

  with llm.trace("Buckingham Palace is in the city of") as t2:
    llm.model.layers[-2].output[0][:, -1, :] = hs_11[:]
    t2_tokens_out = llm.output.save()

print("\nT1 - Prediction: ", llm.tokenizer.decode(t1_tokens_out["logits"].argmax(dim=-1)[0][-1]))
print("T2 - Prediction: ", llm.tokenizer.decode(t2_tokens_out["logits"].argmax(dim=-1)[0][-1]))
```

In the example above, we are interested in patching the hidden state of a later layer into an earlier one. This experiment can only be conducted with two `Tracer` contexts; since we are using a `Session`, it is not required to save the hidden state from Tracer 1 to reference it in Tracer 2.

The `Session` context can also be terminated early.

```python
import nnsight

with llm.session(remote=True) as session:
  l = nnsight.list().save()

  l.append(0)
  l.append(1)
  nnsight.log("-- Early Stop --")
  session.exit()
  l.append(2)

print("List: ", l)
```

## Iterator Context

We mention earlier that the `Session` context enables multi-tracing execution. But how can we optimize a process that would require running an intervention graph in a loop?

If you create a `for` loop with a `Tracer` context inside of it, this will result in creating a new intervention graph at each iteration, which is not scalable.

We solve this problem the `nnsight` way by introducing the **Iterator** context: an intervention loop that iteratively executes a single intervention graph with an updated parameter.

```python
import nnsight

with llm.session(remote=True) as session:

  prompts = nnsight.list(["This is nnsight 0.3",
                          "It works with NDIF",
                          "pip install it now!"])
  results = nnsight.list().save()
  with session.iter(prompts) as prompt:

    with llm.trace(prompt):
      results.append(llm.lm_head.output)
```

Use a `Session` to define the `Iterator` context and pass in a sequence of items that you want to loop over at each executed iteration.

The sequence must be iterable or be a Proxy with an iterable value.

The iterable's item can be referenced in the inner intervention body of the `Iterator`.

### loop

The `Iterator` context extends all the `nnsight` graph-based functionalities, but also closely mimics the conventional `for` loop statement in Python, which allows it to support all kind of iterative operations.

```python
import nnsight

with llm.session(remote=True) as session:
  l = nnsight.list()
  [l.append(num) for num in range(0, 3)] # adding 0, 1, 2 to l
  with session.iter(l) as item: # with session.iter([0, 1, 2]) also works!
    nnsight.log(item)
```

You can create nested `Iterator` contexts:

```python
import nnsight

with llm.session() as session:
  l = nnsight.list([[10]] * 5)

  l2 = nnsight.list().save()
  with session.iter(l) as item:
    with session.iter(item) as item_2:
      l2.append(item_2)

print("List: ", l2)
```

You can skip some iterations:

```python
import nnsight

with llm.session(remote=True) as session:

  with session.iter([0, 1, 2, 3], return_context=True) as (item, iterator):
    with iterator.cond(item % 2 == 0):
      nnsight.log(item)
```

Or, you can choose to `break` out of the Iteration loop early:

```python
import nnsight

with llm.session(remote=True) as session:

  with session.iter([0, 1, 2, 3], return_context=True) as (item, iterator):
      with iterator.cond(item == 2):
        iterator.exit()

      nnsight.log(item)
```

The `Iterator` context is a niece piece of functionality that allows you to define a bunch of basic code operations that can now be "traceable" by `nnsight`.

But in what kind of experimental scenario would someone even need to use it?

In the next section, we delve into a powerful use case of the `Iterator` context and see how it enables it!

---

# NNsight_v0_4_guide.ipynb

# `nnsight 0.4`: walkthrough
**We have many exciting new features in this update, including:**

*   Descriptive error messages
*   .all() for multiple token generation
*   vLLM Integration
*   Streaming remote execution to local machine
*   Support for traditional Python syntax for `if` statements and `for` loops on proxies within tracing contexts
*   Ability to rename model modules

...and more!

The following walkthrough guides you through how to access `nnsight 0.4` and use all of its individual features.

**Breaking Changes**
* The InterventionGraph now follows a <u>sequential execution order</u>. Module envoys are expected to be referenced following the model’s architecture hierarchy. This means that out-of-order in-place operations will not take effect.

* Saved node values are automatically injected into their proxy reference in the Python frames post graph execution. If you are calling `.value` in your code after tracing, this could lead to the wrong behavior.

# Access `nnsight` 0.4

```python
from IPython.display import clear_output
!pip install nnsight
clear_output()
```

Import `nnsight` and load the GPT-2 model.

```python
# import packages
import nnsight
from nnsight import LanguageModel
```

```python
model = LanguageModel('openai-community/gpt2', device_map='auto')
print(model)
```

# Improved Error Messaging

If you've been using `nnsight`, you are probably familiar with the following type of error message:

```
IndexError: Above exception when execution Node: 'setitem_0' in Graph: '6063279136'
```
It can be quite difficult to troubleshoot with these errors, so in `nnsight 0.4` we've now improved error messaging to be descriptive and line-specific! Let's check it out:

```python
prompt = 'The Eiffel Tower is in the city of'

with model.trace(prompt) as tracer:

    # try to access a layer of model that doesn't exist
    model.transformer.h[12].output[0][:] = 0
    output = model.lm_head.output.save()

print("lm_head output = ",output)
```

Great! Now we know that our list index was out of range within the tracing context, and if we expand to see the full message, we can tell that it's happening in line 7.

Let's try again, now using the correct index for the final layer:

```python
prompt = 'The Eiffel Tower is in the city of'

with model.trace(prompt) as tracer:

    # ablate last layer output
    model.transformer.h[11].output[0][:] = 0
    output = model.lm_head.output.save()

print("lm_head output = ",output)
```

The error messaging feature can be toggled using `nnsight.CONFIG.APP.DEBUG` which defaults to true.

```python
# Turn off debugging:
import nnsight

nnsight.CONFIG.APP.DEBUG = False
nnsight.CONFIG.save()
```

# .all()

Sometimes you may want to recursively apply interventions to a model (e.g., when generating many tokens or for models like RNNs, where modules are called multiple times).

*   Calling `.all()` on a model or its submodules will recursively apply its `.input` and `.output` across all iterations.
*   When generating multiple tokens with `.generate` (see: [Multiple Token Generation](https://nnsight.net/notebooks/features/multiple_token/)), using `.all()` before applying an intervention will ensure that the model undergoes the intervention for *all* new tokens generated, not just the first.

## About

## .all() now streamlines multiple token generation

With .all, applying interventions during multiple token generation becomes much easier. Let's test this out!

We can use `.all()` to streamline the multiple token generation process. We simply call `.all` on the module where we are applying the intervention (in this case GPT-2's layers), apply our intervention, and append our hidden states (stored in an `nnsight.list()` object).

```python
# New: using .all():
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() to apply intervention to each new token
    layers.all()

    # Apply intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(layers[-1].output) # no need to call .save
    # Don't need to loop or call .next()!

print("Hidden state length: ",len(hidden_states))
```

Easy! Note that because `.all()` is recursive, it will only work to append outputs called on children of the module that `.all()` was called on. See example below for more information. TL;DR: apply `.all()` on the highest-level accessed module if interventions and outputs have different hierarchies within model structure.

### Note: (Old method) Applying interventions during multiple token generation without .all()

Without `.all()`, we would need to loop across each new generated token, saving the intervention for every generated token and calling `.next()` to move forward.

```python
# Old approach:
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
hidden_states = []
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    for i in range(n_new_tokens):
        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output.save())

        # Move to next generated token
        layers[0].next()

print("Hidden state length: ",len(hidden_states))
```

### Note: .all() recursive properties

As mentioned, `.all()` is recursive and will work to append outputs called on children of the module that `.all` was called on. In this example, calling `.all()` on the model's layer modules will not recursively affect `model.lm_head.output` as it is not a child of layers.

```python
# A note on .all() recursive properties:
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on layers
    layers.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save, it's already initialized

print("Hidden state length: ",len(hidden_states)) # length is 1, meaning it only saved the first token generation
```

So, if you want to apply an intervention during multiple token generation while saving the state of a model component that isn't a child of that module, you can apply .`all()` to the full model.

```python
# Applying .all() to model fixes issue
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on model
    model.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save

print("Hidden state length: ",len(hidden_states)) # length is 3!
```

## Known Issues: `.all()`

* IteratorEnvoy contexts can produce undesired behavior for subsequent operations defined <u>below</u> it that are not dependent on InterventionProxys.

Example:
```
with lm.generate("Hello World!", max_new_tokens=10):
    hs_4 = nnsight.list().save()

    with lm.transformer.h[4].all():
        hs_4.append(lm.transformer.h[4].output)

    hs_4.append(433)

print(len(hs_4))
```
`>>> 20 # expected: 11`

# Syntax updates

With `nnsight 0.4`, we now support `if` statements and `for` loops applied to proxies with traditional Python syntax! We also remove the need to call `.value` on a proxy output.

## If Statements

Previously, we would need to use `.cond()` to create a conditional context that would only execute upon meeting the logical conditions inside the `.cond()`.

```python
import torch
# Old method
# model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace("The Eiffel Tower is in the city of") as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # To create the conditional context you need to put the
  # condition within tracer.cond()
  with tracer.cond(rand_int % 2 == 0):
    tracer.log("Random Integer ", rand_int, " is Even")

  with tracer.cond(rand_int % 2 == 1):
    tracer.log("Random Integer ", rand_int, " is Odd")
```

Now, we can use Python `if` statements within the tracing context to create a conditional context!

*Note: Colab may be a little strangely with this feature the first time you run it - expect some lagging and warnings.*

```python
with model.trace("The Eiffel Tower is in the city of") as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")

  if rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

Note: If the conditional statements are outside the tracing context, `if` operates as in base Python.

`elif` statements should also work as `if` statements within the tracing context:

```python
with model.trace("The Eiffel Tower is in the city of") as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")
  elif rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

## For Loops

With `nnsight 0.4`, you can now use `for` loops within a tracer context at scale. Previously, a `for` loop within a tracer context inside it resulted in creating intervention graphs over and over for each iteration - this is not scalable!

The `session.iter` context allows for scalable looping within sessions, but doesn't utilize traditional Python syntax:

```python
# Old Method
with model.session() as session:

  li = nnsight.list() # an NNsight built-in list object
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

Now, you can use simple `for` loops within a tracer context to run an intervention loop at scale.

*NOTE: inline for loops (i.e., `[x for x in <Proxy object>]`) are not currently supported.*

```python
# New: Using Python for loops for iterative interventions
with model.session() as session:

    li = nnsight.list()
    [li.append([num]) for num in range(0, 3)]
    li2 = nnsight.list().save()

    # Using regular for loops
    for item in li:
        for item_2 in item: # for loops can be nested!
            li2.append(item_2)

print("\nList: ", li2)
```

## `.value` injected into saved results

Previously, directly using non-traceable functions (i.e., tokenizers) on a proxy returned from a tracing context required calling `.value` to access the proxy's numerical value. Calling traceable functions (like `print()` or `.argmax()`) on such proxies automatically returned the `.value`, making it optional to call `.value` in certain cases.

```
input = "The Eiffel Tower is in the city of"
with model.trace(input):

    l2_input = model.transformer.h[2].input.save()

print(l2_input.value) # could optionally call .value
print(l2_input) # but not required for traceable functions
```

Now with `nnsight 0.4`, the proxy's value is automatically injected into the variable name, negating any needs to call `.value` on proxies. Proxy variables will automatically be populated with their value upon exiting the tracing context. This is a breaking change, and calling `.value` on a proxy will now throw an error.

```python
input = "The Eiffel Tower is in the city of"
with model.trace(input):

    l2_input = model.transformer.h[2].input.save()

print(l2_input) # no need to call .value
print(l2_input.value) # will throw an error
```

## Turning off syntactic changes

If you would like to turn off either the `if`/`for` functionality or the `.value` syntactic changes, you can apply the following changes to `nnsight.CONFIG`

```python
# Turn off if/for statements within tracing context:
import nnsight

nnsight.CONFIG.APP.CONTROL_FLOW_HANDLING = False
nnsight.CONFIG.save()
```

```python
# Turn off .value injection:
import nnsight

nnsight.CONFIG.APP.FRAME_INJECTION = False
nnsight.CONFIG.save()
```

## Known Issues: Syntax Update
* Colab behaves a little strangely with these features the first time you run it - expect some lagging and warnings.

* Inline Control Flow (for loops) are not supported.

Example:
```
with lm.trace("Hello World!"):
    foo = nnsight.list([0, 1, 2, 3]).save()
    [nnsight.log(item) for item in foo]

```
`>>> Error`

* Value Injection is not supported for proxies referenced within objects.

# vLLM Integration

Our new update includes support for vLLM models using `nnsight`. [vLLM](https://github.com/vllm-project/vllm) is a popular library used for fast inference. By leveraging PagedAttention, dynamic batching, and Hugging Face model integration, vLLM makes inference more efficient and scalable for real-world applications.

## Setup

You will need to install `nnsight 0.4`, `vllm`, and `triton 3.1.0` to use vLLM with NNsight.

```python
from IPython.display import clear_output
# install vllm
!pip install vllm==0.6.4.post1

# install triton 3.1.0
!pip install triton==3.1.0

clear_output()
```

**NOTE: you may need to restart your Colab session before the following step to properly load the `VLLM` model wrapper.**

 Next, let's load in our NNsight-supported vLLM model. You can find vLLM-supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html). For this exercise, we will use GPT-2.

```python
from nnsight.modeling.vllm import VLLM

# NNsight's VLLM wrapper currently supports "device = cuda" and device = "auto"
vllm = VLLM("gpt2", device = "auto", dispatch = True) # See supported models: https://docs.vllm.ai/en/latest/models/supported_models.html
print(vllm)
```

## Interventions on vLLM models
We now have a vLLM model that runs with `nnsight`. Let's try applying some interventions on it.

Note that vLLM takes in sampling parameters including `temperature` and `top_p`. These parameters can be included in the `.trace()` or `.invoke()` contexts. For default model behavior, set `temperature = 0` and `top_p = 1`. For more information about parameters, reference the [vLLM documentation](https://docs.vllm.ai/en/latest/dev/sampling_params.html).

```python
with vllm.trace(temperature=0.0, top_p=1.0, max_tokens=1) as tracer:
  with tracer.invoke("The Eiffel Tower is located in the city of"):
    clean_logits = vllm.logits.output.save()

  with tracer.invoke("The Eiffel Tower is located in the city of"):
    vllm.transformer.h[-2].mlp.output[:] = 0
    corrupted_logits = vllm.logits.output.save()
```

```python
print("CLEAN - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(clean_logits.argmax(dim=-1)))
print("CORRUPTED - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(corrupted_logits.argmax(dim=-1)))
```

We've successfully performed an intervention on our vLLM model!

## Sampled Token Traceability
vLLM provides functionality to configure how each sequence samples its next token. Here's an example of how you can trace token sampling operations with the nnsight VLLM wrapper.

```python
import nnsight
with vllm.trace("Madison Square Garden is located in the city of", temperature=0.8, top_p=0.95, max_tokens=3) as tracer:
    samples = nnsight.list().save()
    logits = nnsight.list().save()

    for ii in range(3):
        samples.append(vllm.samples.output)
        vllm.samples.next()
        logits.append(vllm.logits.output)
        vllm.logits.next()

print("Samples: ", samples)
print("Logits: ", logits) # different than samples with current sampling parameters
```

## Note: gradients are not supported with vLLM
vLLM speeds up inference through its paged attention mechanism. This means that accessing gradients and backward passes are not supported for vLLM models. As such, calling gradient operations when using `nnsight` vLLM wrappers will throw an error.

## Known Issues: vLLM Integration
* The vllm.LLM engine performs max_tokens + 1 forward passes which can lead to undesired behavior if you are running interventions on all iterations of multi-token generation.

Example:
```
with vllm_gpt2("Hello World!", max_tokens=10):
    logits = nnsight.list().save()
    with vllm_gpt2.logits.all():
        logits.append(vllm_gpt2.logits.output)

print(len(logits))

```
`>>> 11 # expected: 10`

# Streaming

Streaming enables users apply functions and datasets locally during remote model execution. This allows users to stream results for immediate consumption (i.e., seeing tokens as they are generated) or applying non-whitelisted functions such as model tokenizers, large local datasets, and more!

*   `nnsight.local()` context sends values immediately to user's local machine from server
*   Intervention graph is executed locally on downstream nodes
*   Exiting local context uploads data back to server
*   `@nnsight.trace` function decorator enables custom functions to be added to intervention graph when using `nnsight.local()`

## `nnsight.local()`

You may sometimes want to locally access and manipulate values during remote execution. Using `.local()` on a proxy, you can send remote content to your local machine and apply local functions. The intervention graph is then executed locally on downstream nodes until you exit the local context.

There are a few use cases for streaming with `.local()`, including live chat generation and applying large datasets or non-whitelisted local functions to the intervention graph.

Now let's explore how streaming works. We'll start by grabbing some hidden states of the model and printing their value using `tracer.log()`. Without calling `nnsight.local()`, these operations will all occur remotely.

```python
from nnsight import LanguageModel

llama = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
```

```python
# This will give you a remote LOG response because it's coming from the remote server
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

Now, let's try the same operation using the `nnsight.local()` context. This will send the operations to get and print the hidden states to your local machine, changing how the logging message is formatted (local formatting instead of remote).

```python
# This will print locally because it's already local
with llama.trace("hello", remote=True) as tracer:

    with nnsight.local():
        hs = llama.model.layers[-1].output[0]
        tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

## `@nnsight.trace` function decorator

We can also use function decorators to create custom functions to be used during `.local` calls. This is a handy way to enable live streaming of a chat or to train probing classifiers on model hidden states.

Let's try out `@nnsight.trace` and `nnsight.local()` to access a custom function during remote execution.

```python
# first, let's define our function
@nnsight.trace # decorator that enables this function to be added to the intervention graph
def my_local_fn(value):
    return value * 0

# We use a local function to ablate some hidden states
# This downloads the data for the .local context, and then uploads it back to set the value.
with llama.generate("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    with nnsight.local():

        hs = my_local_fn(hs)

    llama.model.layers[-1].output[0][:] = hs

    out =  llama.lm_head.output.save()
```

Note that without calling `.local`, the remote API does not know about `my_local_fn` and will throw a whitelist error. A whitelist error occurs because you are being allowed access to the function.

```python
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    hs = my_local_fn(hs) # no .local - will cause an error

    llama.model.layers[-1].output[0][:] = hs * 2

    out =  llama.lm_head.output.save()

print(out)
```

## Example: Live-streaming remote chat

Now that we can access data within the tracing context on our local computer, we can apply non-whitelisted functions, such as the model's tokenizer, within our tracing context.

Let's build a decoding function that will decode tokens into words and print the result.

```python
@nnsight.trace
def my_decoding_function(tokens, model, max_length=80, state=None):
    # Initialize state if not provided
    if state is None:
        state = {'current_line': '', 'current_line_length': 0}

    token = tokens[-1] # only use last token

    # Decode the token
    decoded_token = llama.tokenizer.decode(token).encode("unicode_escape").decode()

    if decoded_token == '\\n':  # Handle explicit newline tokens
        # Print the current line and reset state
        print('',flush=True)
        state['current_line'] = ''
        state['current_line_length'] = 0
    else:
        # Check if adding the token would exceed the max length
        if state['current_line_length'] + len(decoded_token) > max_length:
            print('',flush=True)
            state['current_line'] = decoded_token  # Start a new line with the current token
            state['current_line_length'] = len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line
        else:
            # Add a space if the line isn't empty and append the token
            if state['current_line']:
                state['current_line'] += decoded_token
            else:
                state['current_line'] = decoded_token
            state['current_line_length'] += len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line

    return state
```

Now we can decode and print our model outputs throughout token generation by accessing our decoding function through `nnsight.local()`.

```python
import torch

nnsight.CONFIG.APP.REMOTE_LOGGING = False

prompt = "A press release is an official statement delivered to members of the news media for the purpose of"
# prompt = "Your favorite board game is"

print("Prompt: ",prompt,'\n', end ="")

# Initialize the state for decoding
state = {'current_line': '', 'current_line_length': 0}

with llama.generate(prompt, remote=True, max_new_tokens = 50) as generator:
    # Call .all() to apply to each new token
    llama.all()

    all_tokens = nnsight.list().save()

    # Access model output
    out = llama.lm_head.output.save()

    # Apply softmax to obtain probabilities and save the result
    probs = torch.nn.functional.softmax(out, dim=-1)
    max_probs = torch.max(probs, dim=-1)
    tokens = max_probs.indices.cpu().tolist()
    all_tokens.append(tokens[0]).save()

    with nnsight.local():
        state = my_decoding_function(tokens[0], llama, max_length=20, state=state)
```

# General Considerations
* `Tracer.cond(…)` and `Tracer.iter(…)` are still supported.

* vLLM <U>does not</u> come as a pre-installed dependency of `nnsight`.

* `nnsight` supports `vllm==0.6.4.post1`

* vLLM support only includes `cuda` and `auto` devices at the moment.

* vLLM models <u>do not</u> support gradients.

* The `@nnsight.trace` decorator does not enable user-defined operations to be executed remotely. Something coming soon for that...

---

# activation_patching.ipynb

# Activation Patching

📗 You can find an interactive Colab version of this tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/activation_patching.ipynb).

**Activation patching** is a technique used to understand how different parts of a model contribute to its behavior. In an activation patching experiment, we modify or "patch" the activations of certain model components and observe the impact on model output.

**Activation patching experiments typically follow these steps:**

1.   **Baseline Run:** Run the model and record original activations.
2.   **Corrupted Run:** Run the model with with a counterfactual (i.e., corrupted) prompt and record the difference in activations.
3.   **Patching:** Replace activations at the model component of interest with alternate activations (or zeros, which is sometimes referred to as ablation).

By systematically testing different components this way, researchers can determine how information flows through the model. One common use case is **circuit identification**, where a circuit is a subgraph of a full model that is responsible for a specific and human-interpretable task (e.g., detecting whether an input is in English). Activation patching can help identify which model components are essential for model performance on a given task.

In this tutorial, we use `nnsight` to perform a simple activation patching experiment using an indirect object identification (IOI) task.

## Note: IOI Task

Activation patching was used to find the [Indirect Object Identification](https://openreview.net/forum?id=NpsVSN6o4ul) (IOI) circuit in GPT-2 small. IOI is a natural language task in which a model predicts the indirect object in a sentence. IOI tasks typically involve identifying the indirect object from two names introduced in an initial dependent clause. One name (e.g. "Mary") is the subject (S1), and the other name (e.g. "John") is the indirect object (IO). In the main clause, a second occurrence of the subject (S2) typically performs an action involving the exchange of an item. The sentence always ends with the preposition "to," and the task is to correctly complete it by identifying the non-repeated name (IO).

In this exercise, we will use the following 'clean' prompt:

```
"After John and Mary went to the store, Mary gave a bottle of milk to"
```
This prompt's correct answer (and thus its indirect object) is: `" John"`

We will also use a corrupted prompt to test how activation patching works. This corrupted prompt will switch the identity of the indirect object, so we can test how the model responds to this change.
```
"After John and Mary went to the store, John gave a bottle of milk to"
```
This prompt's correct answer (and thus its indirect object) is: `" Mary"`

![](images/Activation_patching-figure1.png)

# Setup

```python
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
```

```python
from IPython.display import clear_output
```

```python
import nnsight
from nnsight import CONFIG
```

Let's start with our imports:

```python
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "colab" if is_colab else "plotly_mimetype+notebook_connected+colab+notebook"
from nnsight import LanguageModel, util
```

```python
# Load gpt2
model = LanguageModel("openai-community/gpt2", device_map="auto")
clear_output()
```

```python
print(model)
```

Next up, we define our clean prompt and our corrupted prompt. As prompts may be associated with many different feature circuits (i.e., circuits responsible for IOI, deciding if the language is English, or prompt refusal), choosing a counterfactual prompt with only changes directly related your feature of interest is essential.

Here, we switch the name of the repeated subject, thus swapping out the indirect object for our IOI task:

```python
clean_prompt = "After John and Mary went to the store, Mary gave a bottle of milk to"
corrupted_prompt = (
    "After John and Mary went to the store, John gave a bottle of milk to"
)
```

We then use the tokenizer on the two words of interest (“John” and “Mary”) to find the token that represents them. That way we can grab the prediction for these two tokens and compare. Because our prompts don't end in a space, make sure to add a space before each word (i.e., the combined space + word token is what we're looking for).

```python
correct_index = model.tokenizer(" John")["input_ids"][0] # includes a space
incorrect_index = model.tokenizer(" Mary")["input_ids"][0] # includes a space

print(f"' John': {correct_index}")
print(f"' Mary': {incorrect_index}")
```

# Patching Experiment
Now we can run the actual patching intervention! What does this even mean?

We now have two prompts, a "clean" one and a "corrupted" one. Intuitively, the model output for each of these prompts should be different: we'd expect the model to answer "John" for the clean prompt and "Mary" for the corrupted prompt.

In this experiment, we run the model with the clean prompt as an input and then (1) get each layer's output value (i.e., residual stream) and (2) calculate the logit difference between the correct and incorrect answers for this run. Next, we calculate the logit difference between the correct and incorrect answers for the corrupted prompt.

## Step 1: Clean Run

First, we run the model with the **clean prompt**:

`"After John and Mary went to the store, Mary gave a bottle of milk to"`

During this clean run, we collect the final output of each layer. We also record the logit difference in the final model output between the correct answer token `" John"` and the incorrect token `" Mary"`.

```python
N_LAYERS = len(model.transformer.h)

# Clean run
with model.trace(clean_prompt) as tracer:
    clean_tokens = tracer.invoker.inputs[0][0]['input_ids'][0]

    # Get hidden states of all layers in the network.
    # We index the output at 0 because it's a tuple where the first index is the hidden state.

    clean_hs = [
        model.transformer.h[layer_idx].output[0].save()
        for layer_idx in range(N_LAYERS)
    ]

    # Get logits from the lm_head.
    clean_logits = model.lm_head.output

    # Calculate the difference between the correct answer and incorrect answer for the clean run and save it.
    clean_logit_diff = (
        clean_logits[0, -1, correct_index] - clean_logits[0, -1, incorrect_index]
    ).save()
```

## Step 2: Corrupted Run

Next, we run the model using the **corrupted** input prompt:

 `"After John and Mary went to the store, John gave a bottle of milk to"`

 During this corrupted run, we collect the logit difference in the final model output between the correct and incorrect answer tokens

 Note: because we are testing changes induced by the corrupted prompt, the target answers remain the same as in the clean run. That is, the correct token is still `" John"` and the incorrect token is still `" Mary"`.

```python
# Corrupted run
with model.trace(corrupted_prompt) as tracer:
    corrupted_logits = model.lm_head.output

    # Calculate the difference between the correct answer and incorrect answer for the corrupted run and save it.
    corrupted_logit_diff = (
        corrupted_logits[0, -1, correct_index]
        - corrupted_logits[0, -1, incorrect_index]
    ).save()
```

## Step 3: Activation Patching Intervention

Finally, we perform our **activation patching** procedure. For each token position in the clean prompt, we loop through all layers of the model. Within each layer, we run a forward pass using the corrupted prompt, and patch in the corresponding activation from our clean run at the given token position. We then collect the final output difference between the correct and incorrect answer tokens for each patched activation.

```python
# Activation Patching Intervention
ioi_patching_results = []

# Iterate through all the layers
for layer_idx in range(len(model.transformer.h)):
    _ioi_patching_results = []

    # Iterate through all tokens
    for token_idx in range(len(clean_tokens)):
        # Patching corrupted run at given layer and token
        with model.trace(corrupted_prompt) as tracer:
            # Apply the patch from the clean hidden states to the corrupted hidden states.
            model.transformer.h[layer_idx].output[0][:, token_idx, :] = clean_hs[layer_idx][:, token_idx, :]

            patched_logits = model.lm_head.output

            patched_logit_diff = (
                patched_logits[0, -1, correct_index]
                - patched_logits[0, -1, incorrect_index]
            )

            # Calculate the improvement in the correct token after patching.
            patched_result = (patched_logit_diff - corrupted_logit_diff) / (
                clean_logit_diff - corrupted_logit_diff
            )

            _ioi_patching_results.append(patched_result.item().save())

    ioi_patching_results.append(_ioi_patching_results)
```

### Note: Optimize workflow with NNsight batching

Although we broke up the workflow for ease of understanding, we can use `nnsight` to further speed up computation.

Thanks to `nnsight`, the whole experiment can happen in one forward pass by breaking up inputs into multiple invocation calls and batching them.

```python
N_LAYERS = len(model.transformer.h)

# Enter nnsight tracing context
with model.trace() as tracer:

    # Clean run
    with tracer.invoke(clean_prompt) as invoker:
        clean_tokens = invoker.inputs[0][0]['input_ids'][0]

        # No need to call .save() as we don't need the values after the run, just within the experiment run.
        clean_hs = [
            model.transformer.h[layer_idx].output[0]
            for layer_idx in range(N_LAYERS)
        ]

        # Get logits from the lm_head.
        clean_logits = model.lm_head.output

        # Calculate the difference between the correct answer and incorrect answer for the clean run and save it.
        clean_logit_diff = (
            clean_logits[0, -1, correct_index] - clean_logits[0, -1, incorrect_index]
        ).save()

    # Corrupted run
    with tracer.invoke(corrupted_prompt) as invoker:
        corrupted_logits = model.lm_head.output

        # Calculate the difference between the correct answer and incorrect answer for the corrupted run and save it.
        corrupted_logit_diff = (
            corrupted_logits[0, -1, correct_index]
            - corrupted_logits[0, -1, incorrect_index]
        ).save()

    ioi_patching_results = []

    # Iterate through all the layers
    for layer_idx in range(len(model.transformer.h)):
        _ioi_patching_results = []

        # Iterate through all tokens
        for token_idx in range(len(clean_tokens)):
            # Patching corrupted run at given layer and token
            with tracer.invoke(corrupted_prompt) as invoker:
                # Apply the patch from the clean hidden states to the corrupted hidden states.
                model.transformer.h[layer_idx].output[0][:, token_idx, :] = clean_hs[layer_idx][:, token_idx, :]

                patched_logits = model.lm_head.output

                patched_logit_diff = (
                    patched_logits[0, -1, correct_index]
                    - patched_logits[0, -1, incorrect_index]
                )

                # Calculate the improvement in the correct token after patching.
                patched_result = (patched_logit_diff - corrupted_logit_diff) / (
                    clean_logit_diff - corrupted_logit_diff
                )

                _ioi_patching_results.append(patched_result.item().save())

        ioi_patching_results.append(_ioi_patching_results)
```

## Visualize Results

Let's define a function to plot our activation patching results.

```python
from nnsight.tracing.graph import Proxy

def plot_ioi_patching_results(model,
                              ioi_patching_results,
                              x_labels,
                              plot_title="Normalized Logit Difference After Patching Residual Stream on the IOI Task"):

    ioi_patching_results = util.apply(ioi_patching_results, lambda x: x.value, Proxy)

    fig = px.imshow(
        ioi_patching_results,
        color_continuous_midpoint=0.0,
        color_continuous_scale="RdBu",
        labels={"x": "Position", "y": "Layer","color":"Norm. Logit Diff"},
        x=x_labels,
        title=plot_title,
    )

    return fig
```

Let's see how the patching intervention changes the logit difference! Let's use a heatmap to examine how the logit difference changes after patching each layer's output across token positions.

```python

print(f"Clean logit difference: {clean_logit_diff:.3f}")
print(f"Corrupted logit difference: {corrupted_logit_diff:.3f}")

clean_decoded_tokens = [model.tokenizer.decode(token) for token in clean_tokens]
token_labels = [f"{token}_{index}" for index, token in enumerate(clean_decoded_tokens)]

fig = plot_ioi_patching_results(model, ioi_patching_results,token_labels,"Patching GPT-2-small Residual Stream on IOI task")
fig.show()
```

In the above plot, we see that patching the clean residual stream into the corrupted model does not change much in the final token difference for input tokens 0-9. This is expected, as there is no difference in the clean vs. corrupted prompt for these tokens, so patching in the clean activations at this point shouldn't change the model prediction.

However, when we get to token #10, "Mary", where the subject is introduced for the second time, there is a sharp increase in output logit difference, indicating that the patch changes how the model predicts the outcome downstream, particularly for the earlier layers. There is also a transition in the middle layers of the network where the logit difference starts decreasing. We are thus seeing how the network is tracking information about the indirect object as the layers progress.

A similar but opposite effect is observed when the activations for the final prompt token are patched: the normalized logit difference increases after a transition period in the middle layers.

# Limitations

Although activation patching is an effective technique for circuit localization, it requires running a forward pass through the model for every patch, making it computationally expensive.

**Attribution patching** is an approximation of activation patching that helps scale the technique to larger experiments and models. See our attribution patching tutorial [here](https://nnsight.net/notebooks/tutorials/attribution_patching/) to try it out!

# Trying on a bigger model

Although the original IOI experiment was performed on GPT-2 small, NDIF allows researchers to explore similar problems on largescale models!

Let's see how the residual stream of Llama 3.1-8B contributes to the IOI task using activation patching with NDIF's remote infrastructure.

### NNsight Remote Setup
Make sure you have obtained your [NDIF API key](https://login.ndif.us/) and configured your workspace for [remote execution](https://nnsight.net/notebooks/features/remote_execution/).

```python
from nnsight import CONFIG

if is_colab:
    # include your HuggingFace Token and NNsight API key on Colab secrets
    from google.colab import userdata
    NDIF_API = userdata.get('NDIF_API')
    HF_TOKEN = userdata.get('HF_TOKEN')

    CONFIG.set_default_api_key(NDIF_API)
    !huggingface-cli login -token HF_TOKEN

clear_output()
```

```python
import torch
import nnsight
from nnsight import LanguageModel
```

```python
# Load model
llm = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
print(llm)
```

Define some IOI prompts. Each of these prompts can be used as a 'clean' and as a 'corrupted' prompt, as each prompt has a related corrupted version with the IO switched out.

```python
prompts = [
    "When Lisa and Sarah went to the cinema, Lisa gave the ticket to",
    "When Lisa and Sarah went to the cinema, Sarah gave the ticket to"
]
```

Define the answers to these prompts, formatted as `(correct, incorrect)`

```python
answers = [
    (" Sarah", " Lisa"),
    (" Lisa", " Sarah")
]
```

```python
# Tokenize clean and corrupted inputs:
clean_tokens = llm.tokenizer(prompts, return_tensors="pt")["input_ids"]
corrupted_tokens = clean_tokens[
    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]
]

# Tokenize answers for each prompt:
answer_token_indices = [
        [llm.tokenizer(answers[i][j])["input_ids"][1] for j in range(2)]
        for i in range(len(answers))
]

print("answer_tokens = " , answer_token_indices)
```

### Patching Attention Heads

The residual stream isn't the only model component you can apply activation patching on: let's try patching Llama's attention heads to see how they influence the IOI task! Here, we apply our patching intervention on the attention output, `o_proj.output` in Llama models.

Because the multihead attention of Llama models are stored in a projection matrix containing all attention heads, we will need to resize the tensor to reveal individual attention head contributions. The `einops` library is a handy way to resize tensors.

```python
import einops
```

Okay, now let's apply our three activation patching steps to our attention heads during an IOI task.

```python
# Enter nnsight tracing context
N_LAYERS = len(llm.model.layers)
batch = 1
seq = len(prompts[0]) #15 length of input tokens
N_HEADS = 32 #32 attention heads
D_MODEL = int(4096) #4096 size of model hidden
D_HEADS = int(D_MODEL/N_HEADS) #128 size of attention head

ioi_patching_results_all = []
prompt_id = 0
corrupt_id = (prompt_id + 1 if prompt_id % 2 == 0 else prompt_id - 1)

with llm.trace(remote = True) as tracer:
    # STEP 1: Clean run, grab clean activations for each attention head
    with tracer.invoke(prompts[prompt_id]) as invoker:
        clean_tokens = invoker.inputs[0][0]['input_ids'][0]

        # Get clean attention output for later patching
        z_hs = {}
        for layer_idx in range(N_LAYERS):
            # attention output for llama models needs to be reshaped to look at individual heads
            z = llm.model.layers[layer_idx].self_attn.o_proj.input # dimensions [1x15x4096] [batch x seq x D_MODEL]
            z_reshaped = einops.rearrange(z, 'b s (nh dh) -> b s nh dh',nh=32)
            for head_idx in range(N_HEADS):
                z_hs[layer_idx,head_idx] = z_reshaped[:,:,head_idx,:]

        # Get logits from the lm_head.
        clean_logits = llm.lm_head.output
        clean_logit_diff = (
            clean_logits[0, -1, answer_token_indices[prompt_id][0]] - clean_logits[0, -1, answer_token_indices[prompt_id][1]]
        ).save()

    # STEP 2: Corrupted run, grab corrupted logits for later comparison.
    with tracer.invoke(prompts[corrupt_id]) as invoker:
        corrupted_tokens = invoker.inputs[0][0]['input_ids'][0]
        corrupted_logits = llm.lm_head.output

        # Calculate the difference between the correct answer and incorrect answer for the corrupted run and save it.
        corrupted_logit_diff = (
            corrupted_logits[0, -1, answer_token_indices[prompt_id][0]] - corrupted_logits[0, -1, answer_token_indices[prompt_id][1]]
        ).save()

    # STEP 3: Patching runs, apply 'clean' model state at each layer and head,
    ioi_patching_results = []

    # Patching: Iterate through all the layers
    for layer_idx in range(len(llm.model.layers)):
        _ioi_patching_results = []
        # Iterate through all attention heads
        for head_idx in range(N_HEADS):
            # Patching corrupted run at given layer and token
            with tracer.invoke(prompts[corrupt_id]) as invoker:
                # Apply the patch from the clean hidden states to the corrupted hidden state for given layer and head.
                z_corrupt = llm.model.layers[layer_idx].self_attn.o_proj.input
                z_corrupt = einops.rearrange(z_corrupt, 'b s (nh dh) -> b s nh dh',nh=32)
                z_corrupt[:,:,head_idx,:] = z_hs[layer_idx,head_idx]
                z_corrupt = einops.rearrange(z_corrupt, 'b s nh dh -> b s (nh dh)', nh=32)
                llm.model.layers[layer_idx].self_attn.o_proj.input = z_corrupt

                patched_logits = llm.lm_head.output
                patched_logit_diff = (
                    patched_logits[0, -1, answer_token_indices[prompt_id][0]]
                    - patched_logits[0, -1, answer_token_indices[prompt_id][1]]
                )

                # Calculate the improvement in the correct token after patching.
                patched_result = (patched_logit_diff - corrupted_logit_diff) / (
                    clean_logit_diff - corrupted_logit_diff
                )

                _ioi_patching_results.append(patched_result.item().save())

        ioi_patching_results.append(_ioi_patching_results)
```

### Visualize Results

Let's use the same plotting function from earlier to visualize how patching the Llama-3.1-8B attention heads influenced model output during the IOI task.

```python
print(f"Clean logit difference: {clean_logit_diff.value:.3f}")
print(f"Corrupted logit difference: {corrupted_logit_diff.value:.3f}") # why do I still need .value?

print(ioi_patching_results)

x_labels = [f"Head {i}" for i in range(N_HEADS)]

fig2 = plot_ioi_patching_results(llm, ioi_patching_results,x_labels,"Patching Llama Attention Heads on IOI task")
fig2.show()
```

---

# attribution_patching.ipynb

# Attribution Patching

📗 This tutorial is adapted from Neel Nanda’s [blog post](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching).

Activation patching is a method to determine how model components influence model computations (see our activation patching tutorial for more information). Although activation patching is a useful tool for circuit identification, it requires a separate forward pass through the model for each patched activation, making it time- and resource-intensive.

**Attribution patching** uses gradients to take a linear approximation to activation patching and can be done simultaneously in two forward and one backward pass, making it much more scalable to large models.

You can find a colab version of the tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/attribution_patching.ipynb) or Neel’s version [here](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb).

Read more about an application of Attribution Patching in [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/abs/2310.10348). 📙

## Setup

If you are using Colab or haven't yet installed NNsight, install the package:
```
!pip install -U nnsight
```

```python
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
```

Import libraries

```python
from IPython.display import clear_output
import einops
import torch
import plotly.express as px
import plotly.io as pio
pio.renderers.default = "colab" if is_colab else "plotly_mimetype+notebook_connected+notebook"

from nnsight import LanguageModel
```

```python
import nnsight
print(nnsight.__version__)
```

## 1️⃣ Indirect Object Identification (IOI) Patching

Indirect object identification (IOI) is the ability to infer the correct indirect object in a sentence, allowing one to complete sentences like "John and Mary went to the shops, John gave a bag to" with the correct answer " Mary". Understanding how language models like GPT-2 perform linguistic tasks like IOI helps us gain insights into their internal mechanisms and decision-making processes.

Here, we apply the [IOI task](https://arxiv.org/abs/2211.00593) to explore how GPT-2 small is performing IOI with attribution patching.

*📚 Note: For more detail on the IOI task, check out the [ARENA walkthrough](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification).*

```python
model = LanguageModel("openai-community/gpt2", device_map="auto", dispatch=True)
clear_output()
print(model)
```

Looking at the model architecture, we can see there are 12 layers, each with 12 GPT-2 Blocks. We will use attribution patching to approximate the contribution of each layer and each attention head for the IOI task.

We next define 8 IOI prompts, with each prompt having one related corrupted prompt variation (i.e., the indirect object is swapped out).

```python
prompts = [
    "When John and Mary went to the shops, John gave the bag to",
    "When John and Mary went to the shops, Mary gave the bag to",
    "When Tom and James went to the park, James gave the ball to",
    "When Tom and James went to the park, Tom gave the ball to",
    "When Dan and Sid went to the shops, Sid gave an apple to",
    "When Dan and Sid went to the shops, Dan gave an apple to",
    "After Martin and Amy went to the park, Amy gave a drink to",
    "After Martin and Amy went to the park, Martin gave a drink to",
]

# Answers are each formatted as (correct, incorrect):
answers = [
    (" Mary", " John"),
    (" John", " Mary"),
    (" Tom", " James"),
    (" James", " Tom"),
    (" Dan", " Sid"),
    (" Sid", " Dan"),
    (" Martin", " Amy"),
    (" Amy", " Martin"),
]

# Tokenize clean and corrupted inputs:
clean_tokens = model.tokenizer(prompts, return_tensors="pt")["input_ids"]
# The associated corrupted input is the prompt after the current clean prompt
# for even indices, or the prompt prior to the current clean prompt for odd indices
corrupted_tokens = clean_tokens[
    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]
]

# Tokenize answers for each prompt:
answer_token_indices = torch.tensor(
    [
        [model.tokenizer(answers[i][j])["input_ids"][0] for j in range(2)]
        for i in range(len(answers))
    ]
)

```

Next, we create a function to calculate the mean logit difference for the correct vs incorrect answer tokens.

```python
def get_logit_diff(logits, answer_token_indices=answer_token_indices):
    logits = logits[:, -1, :]
    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))
    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))
    return (correct_logits - incorrect_logits).mean()
```

We then calculate the logit difference for both the clean and the corrupted baselines.

```python
clean_logits = model.trace(clean_tokens, trace=False).logits.cpu()
corrupted_logits = model.trace(corrupted_tokens, trace=False).logits.cpu()

CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()
print(f"Clean logit diff: {CLEAN_BASELINE:.4f}")

CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()
print(f"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}")
```

Now let's define an `ioi_metric` function to evaluate patched IOI changes normalized to our clean and corruped baselines.

```python
def ioi_metric(
    logits,
    answer_token_indices=answer_token_indices,
):
    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (
        CLEAN_BASELINE - CORRUPTED_BASELINE
    )

print(f"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}")
print(f"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}")
```

## 2️⃣ Attribution Patching Over Components

Attribution patching is a technique that uses gradients to take a linear approximation to activation patching. The key assumption is that the corrupted run is a locally linear function of its activations.

We thus take the gradient of the patch metric (`ioi_metric`) with respect to its activations, where we consider a patch of activations to be applying `corrupted_x` to `corrupted_x + (clean_x - corrupted_x)`. Then, we compute the patch metric's change: `(corrupted_grad_x * (clean_x - corrupted_x)).sum()`. All we need to do is take a backwards pass on the corrupted prompt with respect to the patch metric and cache all gradients with respect to the activations.

Let’s see how this breaks down in NNsight!

**A note on c_proj:** *Most HuggingFace models don’t have nice individual attention head representations to hook. Instead, we have the linear layer `c_proj` which implicitly combines the “projection per attention head” and the “sum over attention head” operations. See [this snippet](https://arena3-chapter1-transformer-interp.streamlit.app/~/+/[1.4.2]_Function_Vectors_&_Model_Steering) from ARENA for more information.*

TL;DR: We will use the input to `c_proj` for causal interventions on a particular attention head.

```python
clean_out = []
corrupted_out = []
corrupted_grads = []

with model.trace() as tracer:
# Using nnsight's tracer.invoke context, we can batch the clean and the
# corrupted runs into the same tracing context, allowing us to access
# information generated within each of these runs within one forward pass

    with tracer.invoke(clean_tokens) as invoker_clean:
        # Gather each layer's attention
        for layer in model.transformer.h:
            # Get clean attention output for this layer
            # across all attention heads
            attn_out = layer.attn.c_proj.input
            clean_out.append(attn_out.save())

    with tracer.invoke(corrupted_tokens) as invoker_corrupted:
        # Gather each layer's attention and gradients
        for layer in model.transformer.h:
            # Get corrupted attention output for this layer
            # across all attention heads
            attn_out = layer.attn.c_proj.input
            corrupted_out.append(attn_out.save())
            # save corrupted gradients for attribution patching
            corrupted_grads.append(attn_out.grad.save())

        # Let's get the logits for the model's output
        # for the corrupted run
        logits = model.lm_head.output.save()

        # Our metric uses tensors saved on cpu, so we
        # need to move the logits to cpu.
        value = ioi_metric(logits.cpu())

        # We also need to run a backwards pass to
        # update gradient values
        value.backward()
```

Next, for a given activation we compute `(corrupted_grad_act * (clean_act - corrupted_act)).sum()`. We use `einops.reduce` to rearrange and sum activations over the correct dimension. In this case, we want to estimate the effect of specific attention heads, so we sum over heads rather than token position.

```python
patching_results = []

for corrupted_grad, corrupted, clean, layer in zip(
    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))
):

    residual_attr = einops.reduce(
        corrupted_grad.value[:,-1,:] * (clean.value[:,-1,:] - corrupted.value[:,-1,:]),
        "batch (head dim) -> head",
        "sum",
        head = 12,
        dim = 64,
    )

    patching_results.append(
        residual_attr.detach().cpu().numpy()
    )
```

```python
fig = px.imshow(
    patching_results,
    color_continuous_scale="RdBu",
    color_continuous_midpoint=0.0,
    title="Attribution Patching Over Attention Heads",
    labels={"x": "Head", "y": "Layer","color":"Norm. Logit Diff"},

)

fig.show()
```

Here, we see that the early layer attention heads may not be important for IOI.

## 3️⃣ Attribution Patching Over Position

One benefit of attribution patching is efficiency. Activation patching requires a separate forward pass per activation patched while every attribution patch can be done simultaneously in two forward passes and one backward pass. Attribution patching makes patching much more scalable to large models and can serve as a useful heuristic to find the interesting activations to patch.

In practice, whie this approximation is decent when patching in “small” activations like head outputs, performance decreases significantly when patching in “big” activations like those found in the residual stream.

Using the same outputs we cached above, we can get the individual contributions at each token position simply by summing across token positions. Although this is messy, it's a quick approximation of the attention mechanism's contribution across token position.

*Note: in our specific case here, patching across positions does NOT reflect the entire residual stream, just the post-attention output (i.e., excludes MLPs).*

```python
patching_results = []

for corrupted_grad, corrupted, clean, layer in zip(
    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))
):

    residual_attr = einops.reduce(
        corrupted_grad.value * (clean.value - corrupted.value),
        "batch pos dim -> pos",
        "sum",
    )

    patching_results.append(
        residual_attr.detach().cpu().numpy()
    )
```

```python
fig = px.imshow(
    patching_results,
    color_continuous_scale="RdBu",
    color_continuous_midpoint=0.0,
    title="Attribution Patching Over Token Position",
    labels={"x": "Token Position", "y": "Layer","color":"Norm. Logit Diff"},

)

fig.show()
```

This result looks similar to our previous result using activation patching but is much less precise, as expected!

# Remote Attribution Patching

Now that we know how to run an attribution patching experiment in `nnsight`, let's go over how you can use NDIF's publicly-hosted models to further scale your research!

We're going to run the same experiment, but now using Llama 3.1 8B. Completing this section of the tutorial will require you to [configure NNsight for remote execution](https://nnsight.net/notebooks/features/remote_execution/) if you haven't already.

## Remote Setup

```python
from nnsight import CONFIG

if is_colab:
    # include your HuggingFace Token and NNsight API key on Colab secrets
    from google.colab import userdata
    NDIF_API = userdata.get('NDIF_API')
    HF_TOKEN = userdata.get('HF_TOKEN')

    CONFIG.set_default_api_key(NDIF_API)
    !huggingface-cli login -token HF_TOKEN

clear_output()
```

```python
import torch
import nnsight
from nnsight import LanguageModel
```

Next, let's load the Llama 3.1 8B model, once again using NNsight's `LanguageModel` wrapper. Because we'll be running the model on NDIF's remote servers, no need to specify a `device_map`!

```python
# Load model
llm = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
print(llm)
```

## IOI Task Setup

We've already defined some prompts in the above tutorial, but we'll have to re-tokenize them for Llama 8B.

```python
# Tokenize clean and corrupted inputs:
clean_tokens = llm.tokenizer(prompts, return_tensors="pt")["input_ids"]
# The associated corrupted input is the prompt after the current clean prompt
# for even indices, or the prompt prior to the current clean prompt for odd indices
corrupted_tokens = clean_tokens[
    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]
]

# Tokenize answers for each prompt:
answer_token_indices = torch.tensor(
    [
        [llm.tokenizer(answers[i][j])["input_ids"][1] for j in range(2)]
        for i in range(len(answers))
    ]
)
```

Next, we'll establish clean & corrupted baselines for our IOI metric, using the model's clean and corrupted logits and the `get_logit_diff` function defined earlier.

```python
clean_logits = llm.trace(clean_tokens, trace=False, remote=True)
corrupted_logits = llm.trace(corrupted_tokens, trace=False, remote=True)

clean_logits = clean_logits['logits']
corrupted_logits = corrupted_logits['logits']

CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()
print(f"\n\nClean logit diff: {CLEAN_BASELINE:.4f}")

CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()
print(f"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}")
```

We've also already defined our `ioi_metric` function. Let's plug in our logit values.

```python
print(f"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}")
print(f"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}")
```

## Remote Attribution Patching

Great! We have some baselines. Now, let's run the attribution patching pipeline on Llama 8B. We can't copy the code exactly, because Llama 8B has a different model structure than GPT-2, but we're following the same steps: a clean run and a corrupted run as invokes during one tracing context.

```python
clean_out = []
corrupted_out = []
corrupted_grads = []

with llm.trace(remote = True) as tracer:
# Using nnsight's tracer.invoke context, we can batch the clean and the
# corrupted runs into the same tracing context, allowing us to access
# information generated within each of these runs within one forward pass

    with tracer.invoke(clean_tokens) as invoker_clean:
      # need to set requires grad to true for remote
        llm.model.layers[0].self_attn.o_proj.input.requires_grad = True
        # Gather each layer's attention
        for layer in llm.model.layers:
            # Get clean attention output for this layer
            # across all attention heads
            attn_out = layer.self_attn.o_proj.input
            clean_out.append(attn_out.save())

    with tracer.invoke(corrupted_tokens) as invoker_corrupted:
        # Gather each layer's attention and gradients
        for layer in llm.model.layers:
            # Get corrupted attention output for this layer
            # across all attention heads
            attn_out = layer.self_attn.o_proj.input
            corrupted_out.append(attn_out.save())
            # save corrupted gradients for attribution patching
            corrupted_grads.append(attn_out.grad.save())

        # Let's get the logits for the model's output
        # for the corrupted run
        logits = llm.lm_head.output.save()

        # Our IOI metric uses tensors saved on cpu, so we
        # need to move the logits to cpu.
        value = ioi_metric(logits.cpu())

        # We also need to run a backwards pass to
        # update gradient values
        value.backward()
```

Awesome! Let's take a look at attention head contributions across layers.

```python
# format data for plotting across attention heads
patching_results = []

for corrupted_grad, corrupted, clean, layer in zip(
    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))
):

    residual_attr = einops.reduce(
        corrupted_grad.value[:,-1,:] * (clean.value[:,-1,:] - corrupted.value[:,-1,:]),
        "batch (head dim) -> head",
        "sum",
        head = 32,
        dim = 128,
    )

    patching_results.append(
        (residual_attr.float()).detach().numpy()
    )
```

```python
fig = px.imshow(
    patching_results,
    color_continuous_scale="RdBu",
    color_continuous_midpoint=0.0,
    title="Attribution Patching Over Attention Heads",
    labels={"x": "Head", "y": "Layer","color":"Norm. Logit Diff"},

)

fig.show()
```

Next, let's check out the contribution of the residual stream over token position across layers.

```python
# format data for plotting across input tokens
patching_results = []

for corrupted_grad, corrupted, clean, layer in zip(
    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))
):

    residual_attr = einops.reduce(
        corrupted_grad.value * (clean.value - corrupted.value),
        "batch pos dim -> pos",
        "sum",
    )

    patching_results.append(
        (residual_attr.detach().cpu().float()).numpy()
    )
```

```python
fig = px.imshow(
    patching_results,
    color_continuous_scale="RdBu",
    color_continuous_midpoint=0.0,
    title="Attribution Patching Over Token Position",
    labels={"x": "Token Position", "y": "Layer","color":"Norm. Logit Diff"},

)

fig.show()
```

Great! We've now successfully performed an attribution patching experiment on GPT-2 and Llama 8b.

---

# boundless_DAS.ipynb


---

# conditionals.ipynb

# Conditional Interventions

Interventions can also be made conditional.

Inside the tracing context, we can specify a conditional context:

```
with tracer.cond(Boolean):
  pass
```

This context will only execute its contained interventions if the specified condition is met. Let's try an example!

```python
import torch
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace("The Eiffel Tower is in the city of") as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  with tracer.cond(rand_int % 2 == 0):
    tracer.log("Random Integer ", rand_int, " is Even")

  with tracer.cond(rand_int % 2 == 1):
    tracer.log("Random Integer ", rand_int, " is Odd")
```

In the example above, we have two conditional contexts with mutually exclusive conditions, just like a usual `If`-`Else` statement.

Conditional contexts can also be nested, if we want our interventions to depend on more than one condition at a time.

```python
with model.trace("The Eiffel Tower is in the city of") as tracer:

  non_rand_int = 8

  with tracer.cond(non_rand_int > 0):
    with tracer.cond(non_rand_int % 2 == 0):
      tracer.log("Rand Int ", non_rand_int, " is Positive and Even")
```

`nnsight 0.4` introduces support for native Python `if` statements within the tracing context! Simply create an `if` statement within a trace, and it should perform as `tracer.cond()`.

```python
with model.trace("The Eiffel Tower is in the city of") as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")

  if rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

## Considerations
If you would like to turn off NNsight's support of native `if` statements, you can apply the following changes to `nnsight.CONFIG`

This will not affect any of NNsight's `tracer.cond()` functionality.

```python
# Turn off support if/for statements within tracing context.
import nnsight

nnsight.CONFIG.APP.CONTROL_FLOW_HANDLING = False
nnsight.CONFIG.save()
```

---

# cross_prompt.ipynb

# Cross-Prompt Intervention

Intervention operations work cross prompt! Use two invocations within the same generation block and operations can work between them.

In this case, we grab the token embeddings coming from the first prompt, "Madison square garden is located in the city of New" and replace the embeddings of the second prompt with them.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')
```

```python
with model.generate(max_new_tokens=3) as tracer:

    with tracer.invoke("Madison square garden is located in the city of New") as invoker:

        embeddings = model.transformer.wte.output
        original = model.generator.output.save()

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _") as invoker:

        model.transformer.wte.output = embeddings
        intervened = model.generator.output.save()

print(model.tokenizer.batch_decode(original))
print(model.tokenizer.batch_decode(intervened))
```

We also could have entered a pre-saved embedding tensor as shown here:

```python
with model.generate("Madison square garden is located in the city of New", max_new_tokens=3) as tracer:

    embeddings = model.transformer.wte.output.save()
    original = model.generator.output.save()

print(model.tokenizer.batch_decode(original))

with model.generate("_ _ _ _ _ _ _ _ _ _", max_new_tokens=3) as tracer:

    model.transformer.wte.output = embeddings
    intervened = model.generator.output.save()

print(model.tokenizer.batch_decode(intervened))
```

---

# custom_functions.ipynb

# Custom Functions

Everything within the tracing context operates on the intervention graph. Therefore for `nnsight` to trace a  function it must also be a part of the intervention graph.

Out-of-the-box `nnsight` supports Pytorch functions and methods, all operators, as well the `einops` library. We don't need to do anything special to use them.

For custom functions we can use `nnsight.apply()` to add them to the intervention graph:

```python
import nnsight
from nnsight import LanguageModel
import torch

model = LanguageModel('openai-community/gpt2', device_map='auto')

# We define a simple custom function that sums all the elements of a tensor
def tensor_sum(tensor):
    flat = tensor.flatten()
    total = 0
    for element in flat:
        total += element.item()

    return torch.tensor(total)

with model.trace("The Eiffel Tower is in the city of") as tracer:

    # Specify the function name and its arguments (in a coma-separated form) to add to the intervention graph
    custom_sum = nnsight.apply(tensor_sum, model.transformer.h[0].output[0]).save()
    sum = model.transformer.h[0].output[0].sum().save()

print("\nPyTorch sum: ", sum)
print("Our sum: ", custom_sum)
```

`nnsight.apply()` executes the function it wraps and returns its output as a Proxy object. We can then use this Proxy object as we would any other.

The applications of `nnsight.apply` are wide. It can be used to wrap any custom function or functions from libraries that `nnsight` does not support out-of-the-box.

---

# dict_learning.ipynb

# Dictionary Learning

📗 You can find an interactive Colab version of this tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/dict_learning.ipynb).

## Polysemanticity

The field of mechanistic interpretability focuses on understanding individual components of neural networks. However, many neurons in these networks respond to multiple (seemingly unrelated) inputs – a phenomenon called *polysemanticity*. That is, a single neuron might separately respond to images of car tires **and** rubber ducks.

Although polysemanticity may help networks fit as many features as possible into a given parameter space, it makes it more difficult for humans to interpret the network's actions. There are a few strategies for finding monosemantic features, but in this tutorial we will explore the use of sparse autoencoders. If you are interested in learning more, this idea is explored by Anthropic in [*Towards Monosemanticity*](https://transformer-circuits.pub/2023/monosemantic-features) and [*Scaling Monosemanticity*](https://transformer-circuits.pub/2024/scaling-monosemanticity/).

## Sparse Autoencoders

Sparse autoencoders (SAEs) are algorithms that can extract learned features from a trained model. SAEs are a form of dictionary learning algorithms, which find a sparse representation of input data in a high-dimensional space. These features can serve as a more focused and monosemantic unit of analysis than the model's individual neurons, helping address polysemanticity and enabling a clearer and more interpretable understanding of model behavior.

📚 This tutorial is adapted from work by Samuel Marks and Aaron Mueller (See their [GitHub Repository](https://github.com/saprmarks/dictionary_learning) and [Alignment Forum post](https://www.alignmentforum.org/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning)). They created this repository as a resource for dictionary learning via sparse autoencoders on neural network activations, using Anthropic's approach detailed [here](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder).

Here, we will use one of their pre-trained autoencoders to explore how it creates an easily-interpretable monosemantic relationship between tokens and feature activation.

# Setup

Install NNsight & Dictionary Learning libraries

```python
from IPython.display import clear_output
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
    !git clone https://github.com/saprmarks/dictionary_learning
    %cd dictionary_learning
    !pip install -r requirements.txt
clear_output()
```

```python
from nnsight import LanguageModel
from dictionary_learning.dictionary import AutoEncoder
import torch
```

```python
# Load pretrained autoencoder
!./pretrained_dictionary_downloader.sh
clear_output()

weights_path = "./dictionaries/pythia-70m-deduped/mlp_out_layer0/10_32768/ae.pt"
activation_dim = 512 # dimension of the NN's activations to be autoencoded
dictionary_size = 64 * activation_dim # number of features in the dictionary

ae = AutoEncoder(activation_dim, dictionary_size)
ae.load_state_dict(torch.load(weights_path,weights_only=True))
ae.cuda()
```

# Apply SAE

```python
model = LanguageModel("EleutherAI/pythia-70m-deduped", device_map="auto")
tokenizer = model.tokenizer

prompt = """
Call me Ishmael. Some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off--then, I account it high time to get to sea as soon as I can.
"""

# Extract layer 0 MLP output from base model
with model.trace(prompt) as tracer:
    mlp_0 = model.gpt_neox.layers[0].mlp.output.save()

# Use SAE to get features from activations
features = ae.encode(mlp_0)
```

```python
# Find top features using the autoencoder
summed_activations = features.abs().sum(dim=1) # Sort by max activations
top_activations_indices = summed_activations.topk(20).indices # Get indices of top 20

compounded = []
for i in top_activations_indices[0]:
    compounded.append(features[:,:,i.item()].cpu()[0])

compounded = torch.stack(compounded, dim=0)
```

## Visualization

### With Autoencoder

Now let's take a look at each of the top 20 most active features and what they respond to in our prompt. Note that each feature only responds to one token, making these features highly interpretable!

```python
from circuitsvis.tokens import colored_tokens_multi

tokens = tokenizer.encode(prompt)
str_tokens = [tokenizer.decode(t) for t in tokens]

# Visualize activations for top 20 most prominent features
colored_tokens_multi(str_tokens, compounded.T)
```

### Without Autoencoder (optional comparison)
Without the autoencoder, the top neurons are active (or negatively associated) for many tokens, demonstrating how individual neurons can be difficult to interpret.

```python
# Find top neurons using the MLP output
summed_activations_or = mlp_0.abs().sum(dim=1) # Sort by max activations
top_activations_indices_or = summed_activations_or.topk(20).indices # Get indices of top 20

compounded_orig = []
for i in top_activations_indices_or[0]:
    compounded_orig.append(mlp_0[:,:,i.item()].cpu()[0])

compounded_orig = torch.stack(compounded_orig, dim=0)
```

```python
from circuitsvis.tokens import colored_tokens_multi

tokens = tokenizer.encode(prompt)
str_tokens = [tokenizer.decode(t) for t in tokens]

# Visualize original activations for top 20 most prominent neurons
colored_tokens_multi(str_tokens, compounded_orig.T)
```

---

# diffusion_lens.ipynb

# Diffusion Lens
## Introduction

🔎 Diffusion Lens is a technique to observe the inner computations of Diffusion Models, developed by Michael Toker in his paper, *Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines* ([Project Website](https://tokeron.github.io/DiffusionLensWeb/), [ACL Paper](https://aclanthology.org/2024.acl-long.524/)).

> **Colab: [exercises](https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/docs/source/notebooks/tutorials/diffusion_lens.ipynb)**

Diffusion models produce images from text by first encoding text into numerical embeddings, which then guide image generation through a diffusion denoising process. Text encoder models can be trained along with the diffusion model, or models may use pretrained text encoders like CLIP.

Diffusion Lens works by generating images from the text encoder's intermediate representations during text embedding, allowing us to visualize how the model  encodes text as its computations move throughout its layers. The original Diffusion Lens paper revealed some fascinating insights into the text encoding process, finding differences in encoding between types of text encoders and the process of encoding different complexities of prompts. For instance, the authors observed that text encoders tend to embed common knowledge at earlier layers than uncommon knowledge. Another key finding was that different text encoders can encode the same prompt in different orders. For compound prompts with two nouns, they found that T5 and CLIP text encoders approached the encoding process differently.

![Compound Prompt Example](https://tokeron.github.io/DiffusionLensWeb/static/images/encoders.png)
**Text encoders differ in computation process (Toker et al.):** Diffusion models prompted with a compound prompt tend to represent concepts individually before combining them in the final embedding, and that the order of concepts introduced can vary between text encoding models (T5 vs CLIP). While processing a text prompt, T5  tended to represent the second noun first, while CLIP tended to represent the first noun first.

Let's test to see if this holds using NNsight and Diffusion Lens! We will use the prompt `"A few people are in the ocean splashing around"` to see if we can replicate the results from the paper. We will use Stable Diffusion 1.5 which uses a CLIP encoder and Deep Floyd which uses a T5 encoder. We will also explore the behavior of a few other models that weren't investigated in the paper.

## Setup

If you are running in Colab, install NNsight and ensure you are connected to GPU. NOTE: Colab built-in T4 GPUs will only have enough GPU RAM to run one model at a time. You will need to disconnect and restart the session to run multiple models. After restarting the session, you can rerun this setup section and then skip ahead to the model that you'd like to run.

```python
from IPython.display import clear_output
import torch
try:
    import google.colab
    is_colab = True
    if torch.cuda.is_available():
        print("GPU is connected")
    else:
        print("GPU is not connected: Please restart session with a GPU")

except ImportError:
    is_colab = False

if is_colab:
    !pip install --no-deps nnsight
    !pip install msgspec python-socketio[client]
    !pip install ftfy

clear_output()
```

Let's do our imports. We will be using the `DiffusionModel` class of NNsight for this exercise.

```python
from nnsight.modeling.diffusion import DiffusionModel
import matplotlib.pyplot as plt
from math import ceil, sqrt
import PIL
import torch
```

## Stable Diffusion 1.5 (CLIP encoder)

### Load Model

We will start with the Stable Diffusion 1.5 model, which uses the CLIP text encoder. We're going to apply the diffusion lens technique to visualize how CLIP is processing the prompt across its layers.

Let's instantiate the model and define some parameters for the experiment, including our prompt. We can use NNsight's `DiffusionModel` wrapper to load in the model from HuggingFace, which we will send to the GPU using `dispatch = True`.

```python
model = DiffusionModel(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    dispatch=True
).to("cuda")
```

```python
SEED = 17 # random seed for image generation: play around with it and see if it changes results!
STEP_SIZE = 1 # number of steps between layers for our experiment
```

```python
prompt = "A few people are in the ocean splashing around"
```

### Run Diffusion Lens

Now we have the model ready for our experiment.

Diffusion Lens works by processing each of the intermediate text encoder layer outputs through the final layer norm to visualize how the model is progressively refining the text embedding for the diffusion process.

Let's try implementing this in `nnsight`.

```python
layers = range(0, model.text_encoder.config.num_hidden_layers, STEP_SIZE)
images = []

for layer in layers:
    print(f"Generating Diffusion Lens for skip_layers {model.text_encoder.config.num_hidden_layers - layer - 1}")

    # We will use NNsight's .generate() method for image generation.
    # We're specifying our prompt and the random generation seed.
    with model.generate(
        prompt,
        seed=SEED
        ):

        # replace the final_layer_norm input with the text_encoder's output for the layer.
        hidden_state = model.text_encoder.text_model.encoder.layers[layer].output[0]
        model.text_encoder.text_model.final_layer_norm.input = hidden_state

        # Save the generated image and add it to our collection
        image = model.output.images[0].save()
        images.append(image)

if not isinstance(images[0], PIL.Image.Image):
    images = [image.value for image in images]
```

### Visualize Results

Great, now our Diffusion Lens experiment has finished running! Let's plot the images and see how the CLIP text encoder is processing the input across layers.

```python
# Calculate grid dimensions
num_images = len(images)
grid_size = ceil(sqrt(num_images))
fig, axes = plt.subplots(ceil(num_images / grid_size), grid_size, figsize=(15, 15))
axes = axes.flatten()

# Add a main title to the figure
fig.suptitle(f"SD1.5 Diffusion Lens - {prompt}", fontsize=16)

# Display images in a grid
for i, (layer, image) in enumerate(zip(layers, images)):
    if i < len(axes):
        axes[i].imshow(image.resize((256, 256)))
        axes[i].set_title(f"Layer {layer}")
        axes[i].axis('off')

# Hide any unused subplots
for i in range(num_images, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.show()
```

Cool! As reported in the diffusion lens paper, the CLIP encoder started by representing people (the first noun in the prompt) and then added water/ocean (the second noun in the prompt). Let's next try with the T5  text encoder to see if things change.

## Deep Floyd (T5 encoder)

*NOTE: If running on Colab T4 GPUs, you will need to disconnect and restart the session to load in this model. There isn't enough GPU RAM to load both models.*

Once you restart the session, rerun the "Setup" section and then you can skip ahead to this section to get the Deep Floyd results.

### Load Model

```python
model = DiffusionModel(
    "DeepFloyd/IF-I-L-v1.0",
    torch_dtype=torch.float16,
    variant="fp16",
    dispatch=True
).to("cuda")

print(model)
```

```python
prompt = "A few people are in the ocean splashing around"
```

```python
SEED = 128998123
STEP_SIZE = 2
```

### Run Diffusion Lens

Now that we have Deep Floyd loaded, let's set up the diffusion lens experiment again. This code is pretty similar

```python
import ftfy
layers = range(0, model.text_encoder.config.num_hidden_layers - 1, STEP_SIZE)
images = []

for layer in layers:
    print(f"Generating Diffusion Lens for skip_layers {model.text_encoder.config.num_hidden_layers - layer}")
    with torch.no_grad():
        with model.generate(
            prompt,
            seed=SEED
        ):
            hidden_states = model.text_encoder.encoder.block[layer].output[0]
            model.text_encoder.encoder.final_layer_norm.input = hidden_states

            image = model.output.images[0].save()
            images.append(image)

if not isinstance(images[0], PIL.Image.Image):
    images = [image.value for image in images]
```

### Visualize Results

```python
# Calculate grid dimensions
num_images = len(images)
grid_size = ceil(sqrt(num_images))
fig, axes = plt.subplots(ceil(num_images / grid_size), grid_size, figsize=(15, 15))
axes = axes.flatten()

# Add a main title to the figure
fig.suptitle(f"Deep Floyd Diffusion Lens - {prompt}", fontsize=16)

# Display images in a grid
for i, (layer, image) in enumerate(zip(layers, images)):
    if i < len(axes):
        axes[i].imshow(image.resize((256, 256)))
        axes[i].set_title(f"Layer {layer}")
        axes[i].axis('off')

# Hide any unused subplots
for i in range(num_images, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.show()
```

Interesting! The T5 encoder first started with ocean (the second noun) and then added people (the first noun), exhibiting how CLIP and T5 differ in their encoding processes.

## BONUS: Stable Diffusion XL (Two CLIP encoders)

### Load Model

We will start with the Stable Diffusion XL model, which uses two CLIP encoders. Let's define some parameters for the experiment and instantiate the model.

```python
model = DiffusionModel(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16",
    dispatch=True
).to("cuda")
```

```python
prompt = "A few people are in the ocean splashing around"
```

```python
ADD_LAYER_NORM = True # SDXL doesn't automatically use the layer norm, so we have some logic to add it in here
SEED = 17
NUM_INFERENCE_STEPS = 100
STEP_SIZE = 2
```

### Run Diffusion Lens

Great, we have the SDXL model ready for our experiment. SDXL is a little weird, so the diffusion lens code is a little more complicated. We need to mask the first text encoder to isolate the effects of the second text encoder. We also need to manually add in the layer norm, because SDXL doesn't include it automatically. Try setting `LAYER_NORM = False` to see the effects of this!

```python
# Defines the hidden states to embed, we skip the last layer because SDXL ignores it
layers = range(0, model.text_encoder_2.config.num_hidden_layers - 1, STEP_SIZE)
images = []

# Create an empty prompt input for the first text encoder
# This will be used to mask out the original text input, allowing us to isolate
# the effect of injecting hidden states from the second text encoder
mask_input = model.tokenizer(
    '',  # Empty string as we want to mask out the original text
    padding="max_length",
    max_length=model.tokenizer.model_max_length,
    truncation=True,
    return_overflowing_tokens=False,
    return_length=False,
    return_tensors="pt"
).to(model.device)

for layer in layers:
    print(f"Generating Diffusion Lens for skip_layers {model.text_encoder_2.config.num_hidden_layers - layer}")
    with model.generate(
        prompt,
        num_inference_steps=40,
        seed=SEED
        ):

        # Replace the input to the first text encoder with our empty mask
        # This effectively nullifies the contribution of the first text encoder
        model.text_encoder.input = mask_input['input_ids']

        if ADD_LAYER_NORM:

            hidden_state = model.text_encoder_2.text_model.encoder.layers[layer].output[0]

            # SDXL grabs the penultimate hidden state from the text encoder
            model.text_encoder_2.text_model.encoder.layers[-2].output[0][:] = model.text_encoder_2.text_model.final_layer_norm(hidden_state)[0][:]
        else:
            # SDXL grabs the penultimate hidden state from the text encoder
            model.text_encoder_2.text_model.encoder.layers[-2].output[0][:] = model.text_encoder_2.text_model.encoder.layers[layer].output[0][:]

        # Save the generated image and add it to our collection
        image = model.output.images[0].save()
        images.append(image)

if not isinstance(images[0], PIL.Image.Image):
    images = [image.value for image in images]

```

### Visualize Results

Great, now let's visualize this by plotting the image created from the processed intermediate layers.

```python
# Calculate grid dimensions
num_images = len(images)
grid_size = ceil(sqrt(num_images))
fig, axes = plt.subplots(ceil(num_images / grid_size), grid_size, figsize=(15, 15))
axes = axes.flatten()

# Add a main title to the figure
fig.suptitle(f"SDXL Diffusion Lens - {prompt}", fontsize=16)

# Display images in a grid
for i, (layer, image) in enumerate(zip(layers, images)):
    if i < len(axes):
        axes[i].imshow(image.resize((512, 512)))
        axes[i].set_title(f"Layer {layer}")
        axes[i].axis('off')

# Hide any unused subplots
for i in range(num_images, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.show()
```

Interesting! SDXL starts by making the ocean, then the people. This is in contrast to the other CLIP encoder that represented the first noun in the prompt first. Try playing around with the settings to see if you can change how the encoder is operating.

## BONUS: FLUX Schnell (CLIP and T5 XXL encoders)

### Load Model

Let's try implementing diffusion lens on the FLUX Schnell Model, which uses both CLIP and T5 XXL encoders. We'll once again define some parameters and load in the model.

*NOTE: FLUX is too large for Google Colab T4s, so you will need to run this locally on your own GPU or use a paid Colab plan to run this section.*

```python
SEED = 17
NUM_INFERENCE_STEPS = 1
STEP_SIZE = 2
GUIDANCE_SCALE = 0.0
HEIGHT = 512
WIDTH = 512
```

```python
model = DiffusionModel(
    "black-forest-labs/FLUX.1-schnell", torch_dtype=torch.bfloat16, dispatch=True
).to('cuda')
```

Let's use the example prompt from FLUX to see how it is encoded with diffusion lens.

```python
prompt = "Penguin playing chess at a wooden table in a snowy landscape."
```

### Run Diffusion Lens

Let's run the Diffusion Lens experiment again. CLIP is the first text encoder, while T5 is the second. We're going to mask out the effects of the CLIP encoder to isolate the T5 encoder. Let's see if the pattern that T5 represents the prompt's nouns in reverse order holds.

```python
layers = range(0, model.text_encoder_2.config.num_hidden_layers - 1, STEP_SIZE)
images = []

mask_input = model.tokenizer(
    '',
    padding="max_length",
    max_length=model.tokenizer.model_max_length,
    truncation=True,
    return_overflowing_tokens=False,
    return_length=False,
    return_tensors="pt"
).to(model.device)

for layer in layers:
    print(f"Generating Diffusion Lens for skip_layers {model.text_encoder_2.config.num_hidden_layers - layer}")
    with torch.no_grad():
        with model.generate(
            prompt,
            guidance_scale=0.0,
            height=512,
            width=512,
            num_inference_steps=1,
            seed=17
        ):
            model.text_encoder.input = mask_input['input_ids']

            model.text_encoder_2.encoder.final_layer_norm.input = model.text_encoder_2.encoder.block[layer].output[0]

            image = model.output.images[0].save()
            images.append(image)

if not isinstance(images[0], PIL.Image.Image):
    images = [image.value for image in images]
```

### Visualize Results
Let's see how FLUX Schnell processed the penguin prompt.

```python
# Calculate grid dimensions
num_images = len(images)
grid_size = ceil(sqrt(num_images))
fig, axes = plt.subplots(ceil(num_images / grid_size), grid_size, figsize=(15, 15))
axes = axes.flatten()

# Add a main title to the figure
fig.suptitle(f"Flux Schnell Diffusion Lens - {prompt}", fontsize=16)

# Display images in a grid
for i, (layer, image) in enumerate(zip(layers, images)):
    if i < len(axes):
        axes[i].imshow(image)
        axes[i].set_title(f"Layer {layer}")
        axes[i].axis('off')

# Hide any unused subplots
for i in range(num_images, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
# Adjust layout to make room for the title
# plt.subplots_adjust(top=0.9)
plt.show()
```

Fascinating! FLUX Schnell creates the snowy landscape first, then the table, chess board, and finally the penguins. This supports our hypothesis that T5 models represent nouns in compound prompts in reverse order.

---

# early_stopping.ipynb

# Early Stopping

If we are only interested in a model's intermediate computations, we can halt a forward pass run at any module level, reducing runtime and conserving compute resources. One examples where this could be particularly useful would if we are working with SAEs - we can train an SAE on one layer and then stop the execution.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace("The Eiffel Tower is in the city of"):
   l1_out = model.transformer.h[0].output.save()
   model.transformer.h[0].output.stop()

# get the output of the first layer and stop tracing
print("L1 - Output: ", l1_out)
```

Interventions within the tracing context do not necessarily execute in the order they are defined. Instead, their execution is tied to the module they are associated with.

As a result, if the forward pass is terminated early any interventions linked to modules beyond that point will be skipped, even if they were defined earlier in the context.

In the example below, the output of layer 2 _**cannot**_ be accessed since the model's execution was stopped at layer 1.

```python
with model.trace("The Eiffel Tower is in the city of"):
   l2_out = model.transformer.h[1].output.save()
   model.transformer.h[0].output.stop()

print("L2 - Output: ", l2_out)
```

---

# editing_tutorial.ipynb

# Editing

The edit module sets default nodes on the intervention graph to be executed on every future trace. Let's start by loading and dispatching a LanguageModel.

```python
from nnsight import LanguageModel

model = LanguageModel("openai-community/gpt2", device_map="auto", dispatch=True)
```

Editing is useful for attaching default modules to the graph such as LoRAs or SAEs. We declare a toy, passthrough SAE class below.

```python
import torch

# Create a simple torch module
class SAE(torch.nn.Module):
    def __init__(self):
        super(SAE, self).__init__()

    def forward(self, x):
        return x
```

To attach a module to a model's tree, simply set it as an attribute on a desired module. Note that edits must be of type `torch.nn.Module` in order to be attached to the tree.

To set a default edit on a model's intervention graph, create an `edit` context and declare operations as usual.

```python
# Create a reference to the l0 Envoy
submodule = model.transformer.h[0]
# Set the SAE as a property on .sae
submodule.sae = SAE()

# Declare an edit context like you would a trace
with model.edit(""):
    acts = submodule.output[0]
    submodule.sae(acts)
```

Calling the `.sae` attribute in future `trace` contexts will return the `l0` output as expected.

```python
with model.trace("Hello, world!"):
    acts = submodule.sae.output.save()

print(acts.shape)
```

You can also hook into submodules of attached modules. Let's edit the `SAE` class to include a passthrough `encoder` and `decoder`.

```python
class Coder(torch.nn.Module):
    def __init__(self):
        super(Coder, self).__init__()

    def forward(self, x):
        return x

class SAE(torch.nn.Module):
    def __init__(self):
        super(SAE, self).__init__()
        self.encoder = Coder()
        self.decoder = Coder()

    def forward(self, x):
        return self.decoder(
            self.encoder(x)
        )
```

We make the edit just as before, this time setting the `hook` kwarg to `True`. This tells NNsight that we'd like to call the `forward` method on the `SAE` module, passing inputs through all subhooks.

```python
# Create a reference to the l0 Envoy
submodule = model.transformer.h[0]
# Set the SAE as a property on .sae
submodule.sae = SAE()

# Declare an edit context like you would a trace
with model.edit(""):
    acts = submodule.output[0]
    submodule.sae(acts, hook=True)

# Now we can call .encoder and other submodules!
with model.trace("Hello, world!"):
    acts = submodule.sae.encoder.output.save()

print(acts.shape)
```

---

# function_vectors.ipynb

# Function Vectors
**ARENA Function Vectors & Model Steering Tutorial**

This tutorial is adapted from the ARENA program material and serves as a fantastic introduction to running experiments in NNsight and working with function vectors and model steering. Thanks to Callum McDougall for writing this comprehensive tutorial and for allowing us to adapt the tutorial for NNsight users, and thanks to Eric Todd for writing the original function vector paper!

> **ARENA: [Streamlit Page](https://arena-chapter1-transformer-interp.streamlit.app/22_📚_[1.4.2]_Function_Vectors_&_Model_Steering)**
>
> **Colab: [exercises](https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/docs/source/notebooks/tutorials/function_vectors.ipynb) | [solutions](https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/function_vectors_solutions.ipynb)**

You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-2.png" width="350">

# Introduction

These exercises serve as an exploration of the following question: ***can we steer a model to produce different outputs / have a different behaviour, by intervening on the model's forward pass using vectors found by non gradient descent-based methods?***

The majority of the exercises focus on [function vectors](https://functions.baulab.info/): vectors which are extracted from forward passes on in-context learning (ICL) tasks, and added to the residual stream in order to trigger the execution of this task from a zero-shot prompt. The diagram below illustrates this.

<img src="https://functions.baulab.info/images/Paper/fv-demonstrations.png" width="650">

The exercises also take you through use of the `nnsight` library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.

The final set of exercises look at Alex Turner et al's work on [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector), which is conceptually related but has different aims and methodologies.

## Content & Learning Objectives

### 1️⃣ Introduction to `nnsight`

In this section, you'll learn the basics of how to use the `nnsight` library: running forward passes on your model, and saving the internal states. You'll also learn some basics of HuggingFace models which translate over into `nnsight` models (e.g. tokenization, and how to work with model output).

> ##### Learning Objectives
>
> * Learn the basics of the `nnsight` library, and what it can be useful for
> * Learn some basics of HuggingFace models (e.g. tokenization, model output)
> * Use it to extract & visualise GPT-J-6B's internal activations

### 2️⃣ Task-encoding hidden states

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

(Note - this section structurally follows section 2.1 of the function vectors paper).

> ##### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts

### 3️⃣ Function Vectors

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).

> ##### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

### 4️⃣ Steering Vectors in GPT2-XL

Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of "interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour", but it has a different setup, objectives, and approach.

> ##### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

### ☆ Bonus

Lastly, we discuss some possible extensions of function vectors & steering vectors work, which is currently an exciting area of development (e.g. with a paper on steering Llama 2-13b coming out as recently as December 2023).

## Setup code

```python
import os
import sys
from pathlib import Path

IN_COLAB = "google.colab" in sys.modules

chapter = "chapter1_transformer_interp"
repo = "ARENA_3.0"
branch = "main"

# Install dependencies
try:
    import nnsight
except:
    %pip install openai>=1.56.2 nnsight einops jaxtyping plotly transformer_lens==2.11.0 git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python gradio typing-extensions
    %pip install --upgrade pydantic

# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
root = (
    "/content"
    if IN_COLAB
    else "/root"
    if repo not in os.getcwd()
    else str(next(p for p in Path.cwd().parents if p.name == repo))
)

if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
    if not IN_COLAB:
        !sudo apt-get install unzip
        %pip install jupyter ipython --upgrade

    if not os.path.exists(f"{root}/{chapter}"):
        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
        !rm {root}/{branch}.zip
        !rmdir {root}/{repo}-{branch}

if f"{root}/{chapter}/exercises" not in sys.path:
    sys.path.append(f"{root}/{chapter}/exercises")

os.chdir(f"{root}/{chapter}/exercises")
```

```python
! pip install circuitsvis
! pip install plotly
! pip install jaxtyping
! pip install nnsight
```

```python
import logging
import os
import sys
import time
from collections import defaultdict
from pathlib import Path

import circuitsvis as cv
import einops
import numpy as np
import torch as t
from IPython.display import display
from jaxtyping import Float
from nnsight import CONFIG, LanguageModel
from openai import OpenAI
from rich import print as rprint
from rich.table import Table
from torch import Tensor

# Hide some info logging messages from nnsight
logging.disable(sys.maxsize)

t.set_grad_enabled(False)
device = t.device("mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu")

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part42_function_vectors_and_model_steering"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section

import part42_function_vectors_and_model_steering.solutions as solutions
import part42_function_vectors_and_model_steering.tests as tests
from plotly_utils import imshow

MAIN = __name__ == "__main__"
```

# 1️⃣ Introduction to `nnsight`

> ##### Learning Objectives
>
> * Learn the basics of the `nnsight` library, and what it can be useful for
> * Learn some basics of HuggingFace models (e.g. tokenization, model output)
> * Use it to extract & visualise GPT-J-6B's internal activations

## Remote execution

We'll start by discussing [remote execution]((https://nnsight.net/notebooks/features/remote_execution/)) - the ability `nnsight` has to run models on an external server, which is one of the major benefits of the library as a research tool. This helps you bypass the memory & computational limits you might be faced with on your own machine. For remote execution to work, you need 2 things:

1. An API key from the NDIF login page, which you can request [here](https://login.ndif.us/)
2. The model you're working with being live - you can see all live models in the status page [here](https://nnsight.net/status/)

Note that the status page sometimes takes ~5 minutes to load all live models - click the dropdown below to see an example of what the status page should look like once the models have loaded. If you can't see the model you're looking for in this list, then you should set `REMOTE=False` for these exercises, or else make a request on the NDIF Discord to get the model live.

<details>
<summary>Example status page</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ndif-status.png" width="650">

</details>

## Important syntax

Here, we'll discuss some important syntax for interacting with `nnsight` models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as `nnsight` models, and some of it (e.g. forward passes) is specific to `nnsight`, i.e. it would work differently if you just had a standard HuggingFace model. Make sure to keep this distinction in mind, otherwise syntax can get confusing!

### Model config

Each model comes with a `model.config`, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with `model.config`. Run the code below to see this in action, and to define some useful variables for later.

```python
model = LanguageModel("EleutherAI/gpt-j-6b", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = model.tokenizer

N_HEADS = model.config.n_head
N_LAYERS = model.config.n_layer
D_MODEL = model.config.n_embd
D_HEAD = D_MODEL // N_HEADS

print(f"Number of heads: {N_HEADS}")
print(f"Number of layers: {N_LAYERS}")
print(f"Model dimension: {D_MODEL}")
print(f"Head dimension: {D_HEAD}\n")

print("Entire config: ", model.config)
```

### Tokenizers

A model comes with a tokenizer, accessable with `model.tokenizer` (just like TransformerLens). Unlike TransformerLens, we won't be using utility functions like `model.to_str_tokens`, instead we'll be using the tokenizer directly. Some important functions for today's exercises are:

* `tokenizer` (i.e. just calling it on some input)
    * This takes in a string (or list of strings) and returns the tokenized version.
    * It will return a dictionary, always containing `input_ids` (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. `attention_mask` - see dropdown).
    * Other useful arguments for this function:
        * `return_tensors` - if this is `"pt"`, you'll get results returned as PyTorch tensors, rather than lists (which is the default).
        * `padding` - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).
* `tokenizer.decode`
    * This takes in tokens, and returns the decoded string.
    * If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).
* `tokenizer.batch_decode`
    * Equivalent to `tokenizer.decode`, but it doesn't concatenate.
    * If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.
* `tokenizer.tokenize`
    * Takes in a string, and returns a list of strings.

Run the code below to see some examples of these functions in action.

```python
# Calling tokenizer returns a dictionary, containing input ids & other data.
# If returned as a tensor, then by default it will have a batch dimension.
print(tokenizer("This must be Thursday", return_tensors="pt"))

# Decoding a list of integers, into a concatenated string.
print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))

# Using batch decode, on both 1D and 2D input.
print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))
print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))

# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).
print(tokenizer.tokenize("This sentence will be tokenized"))
```

<details>
<summary>Note on <code>attention_mask</code> (optional)</summary>

`attention_mask`, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don't allow these tokens to be attended to). This is useful when you have to do padding. For example:

```python
model.tokenizer(["Hello world", "Hello"], return_tensors="pt", padding=True)
```

will return:

```python
{
    'attention_mask': tensor([[1, 1], [0, 1]]),
    'input_ids': tensor([[15496,   995], [50256, 15496]])
}
```

We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.

</details>

### Model outputs

At a high level, there are 2 ways to run our model: using the `trace` method (a single forward pass) and the `generate` method (generating multiple tokens). We'll focus on `trace` for now, and we'll discuss `generate` when it comes to multi-token generation later.

The default behaviour of forward passes in normal HuggingFace models is to return an object containing logits (and optionally a bunch of other things). The default behaviour of `trace` in `nnsight` is to not return anything, because anything that we choose to return is explicitly returned inside the context manager.

Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below. Remember to obtain and set an API key first if you're using remote execution!

```python
REMOTE = True

if IN_COLAB:
    # include your HuggingFace Token and NNsight API key on Colab secrets
    from google.colab import userdata
    NDI  F_API = userdata.get('NDIF_API')
    CONFIG.set_default_api_key(NDIF_API)

prompt = "The Eiffel Tower is in the city of"

with model.trace(prompt, remote=REMOTE):
    # Save the model's hidden states
    hidden_states = model.transformer.h[-1].output[0].save()

    # Save the model's logit output
    logits = model.lm_head.output[0, -1].save()

# Get the model's logit output, and it's next token prediction
print(f"logits.shape = {logits.shape} = (vocab_size,)")
print("Predicted token ID =", predicted_token_id := logits.argmax().item())
print(f"Predicted token = {tokenizer.decode(predicted_token_id)!r}")

# Print the shape of the model's residual stream
print(f"\nresid.shape = {hidden_states.shape} = (batch_size, seq_len, d_model)")
```

Lets go over this piece by piece.

**First, we create a context block** by calling `.trace(...)` on the model object. This denotes that we wish to generate tokens given some prompts.

```python
with model.trace(prompt, remote=REMOTE):
```

By default, running this will cause your model to be loaded & run locally, but by passing `remote=REMOTE`, it causes the model to be run on the server instead. This is very useful when working with models too large to fit on your machine (or even models which can fit on your machine, but run slowly due to their size, however if you're running this material on a sufficiently large GPU, you may prefer to set `REMOTE=False`).  The input argument can take a variety of formats: strings, lists of tokens, tensors of tokens, etc. Here, we've just used a string `prompt`.

The most interesting part of `nnsight` is the ability to access the model's internal states (like you might already have done with TransformerLens). Let's now see how this works!

```python
hidden_states = model.transformer.h[-1].output[0]
```

On this line we're saying: within our forward pass, access the last layer of the transformer `model.transformer.h[-1]`, access this layer's output `.output` (which is a tuple of tensors), index the first tensor in this tuple `.output[0]`.

Let's break down this line in a bit more detail:

* `model.transformer.h[-1]` is a module in our transformer.
    * If you `print(model)`, you'll see that it consists of `transformer` and `lm_head` (for "language modelling head"). The `transformer` module is made up of embeddings & dropout, a series of layers (called `.h`, for "hidden states"), and a final layernorm. So indexing `.h[-1]` gives you the final layer.
    * Note - it's often useful to visit the documentation page for whatever model you're working on, e.g. you can find GPT-J [here](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html). Not all models will have a nice uniform standardized architecture like you might be used to in TransformerLens!
* `.output[0]` gives you this module's output, as a **proxy**.
    * The output of a module is often a tuple (again, you can see on the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) what the output of each module is). In this case, it's a tuple of 2 tensors, the first of which is the actual layer output (the thing we want).
    * Doing operations on a proxy still returns a proxy - this is why we can index into the `output` proxy tuple and get a proxy tensor!

<details>
<summary>Optional exercise - we mentioned that <code>.output</code> returns a tuple of 2 tensors. Can you use the <a href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> what the second tensor in this tuple is?</summary>

The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called `present`. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we're only generating one new token, these are just the full keys and values.

</details>

<br>

The next command:

```python
logits = model.lm_head.output[0, -1]
```

can be understood in a very similar way. The only difference is that we're accessing the output of `lm_head`, the language modelling head (i.e. the unembedding at the very end), and the output is just a tensor of shape `(batch, seq, d_vocab)` rather than a tuple of tensors. Again, see the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) for this.

If you've worked with Hugging Face models then you might be used to getting logits directly from the model output, but here we generally extract logits from the model internals just like any other activation because this allows us to **control exactly what we return.** If we return lots of very large tensors, this can take quite a while to download from the server (remember that `d_vocab` is often very large for transformers, i.e. around 50k). See the "which objects to save" section below for more discussion on this.

### Output vs input

You can also extract a module's input using `.input` or `.inputs`. If a module's forward method is called as `module.forward(*args, **kwargs)` then `.inputs` returns a tuple of `(tuple_of_args, dict_of_kwargs)`. Alternatively, `.input` is an alias for `.inputs[0][0]`, in other words it returns the first arg from the module's forward method (which is usually the tensor we want).

Remember that if you're not sure then you can debug with `print(module.input.shape)` - even if `.inputs` is a tuple of inputs, this will work to recursively print the shape of all the tensors in the tuple, rather than causing an error.

### Which objects to save

Note that we saved `logits` above, which is a vector of length 50k. In general, it's best to save as small an object as possible, because this reduces the size of object you'll have to download from the server. For example, if you only want the next token completions, just argmax the logits and then save the result! All basic tensor operations can be performed within your context manager.

## Scan & Validate

A really cool feature in nnsight is the scan & validate mode, which allows you to efficiently debug without getting long uninterpretable error messages. For example, consider the code below, which tries to zero ablate one of the model's output tensors. Can you figure out what's wrong with it before running it?

```python
seq_len = len(model.tokenizer.encode(prompt))

try:
    with model.trace(prompt, remote=REMOTE):
        original_output = model.transformer.h[-1].output[0].clone()
        model.transformer.h[-1].output[0][:, seq_len] = 0
        modified_output = model.transformer.h[-1].output[0].save()

except Exception as e:
    print(f"Uninformative error message:\n  {e.__class__.__name__}: {e}")
```

If you guessed "we're indexing a tensor along a dimension of size `seq_len` with index `seq_len` which is an indexing error, you'd be correct! But the error message we get is pretty opaque. This is because of the way the objects in nnsight work: they're not tensors, they're tensor proxies, and can behave in funny ways sometimes.

If we want to debug, we should instead pass `scan=True` and `validate=True` into our `model.trace` call. `scan=True` means we run "fake inputs" through the model which incur no memory costs, and so can be done very quickly and cheaply to detect errors. `validate=True` will run tests during our forward pass that make our error messages more informative. When we use both, we get fast no-memory-cost operations with interpretable error messages!

```python
try:
    with model.trace(prompt, remote=REMOTE, scan=True, validate=True):
        original_output = model.transformer.h[-1].output[0].clone()
        print(f"{model.transformer.h[-1].output.shape=}\n")
        model.transformer.h[-1].output[0][:, seq_len] = 0
        modified_output = model.transformer.h[-1].output[0].save()

except Exception as e:
    print(f"Informative error message:\n  {e.__class__.__name__}: {e}")
```

It's possible to use `validate` without using `scan` (e.g. if you have any `assert proxy.shape == ...` then you must use `validate=True`), although we generally recommend using both when debugging, and then neither when you're finished debugging.

Also note that (as the example above shows) it's useful to use `scan=True, validate=True` when printing tensor shapes, at the initial exploration phase, if you're not exactly sure what the shape of a particular input / output will be. Even if your proxy objects are tuples of tensors, you can still call `.shape`, and it will return a tuple of the shapes of each tensor in the proxy!

## Putting this into practice

### Exercise - visualize attention heads

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-20 minutes on this exercise.
> ```

We just covered a lot of content, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:

```python
cv.attention.attention_patterns(
    tokens=tokens,
    attention=attention,
)
```

where `tokens` is a list of strings, and `attention` is a tensor of shape `(num_heads, num_tokens, num_tokens)`.

If you're stuck, [here's a link](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) to the source code for GPT-J. Look for how the attention patterns are calculated, within the `GPTJAttention` block.

*Note - this model uses dropout on the attention probabilities, as you'll probably notice from looking at the source code in the link above. This won't affect the model's behaviour because dropout is disabled in inference mode (and using the `generate` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.*

<details>
<summary>Aside - inference mode</summary>

Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).

If you want to run the model without inference mode, you can wrap your code in `with model.trace(inference=False):`. However, you don't need to worry about this for the purposes of these exercises.

</details>

If you're stuck on how to reference the right module, see the following hint:

<details>
<summary>Hint - what module you should get attention from</summary>

You want to extract attention from `model.transformer.h[0].attn.attn_dropout.input`. If you used `.output`, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.

</details>

<details>
<summary>Aside - GPT2 tokenizer uses special characters to represent space </summary>

GPT2 tokenizer uses "Ġ" to represent prepended space. So ["My", " name", " is", " James"] will be tokenized as ["My", "Ġname", "Ġis", "ĠJames"]. Make sure you replace "Ġ" with an actual space.

</details>

```python
# YOUR CODE HERE - extract and visualize attention
```

<details>
<summary>Solution (and explanation)</summary>

```python
with model.trace(prompt, remote=REMOTE):
    attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()

# Get string tokens (replacing special character for spaces)
str_tokens = model.tokenizer.tokenize(prompt)
str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]

# Attention patterns (squeeze out the batch dimension)
attn_patterns_value = attn_patterns.squeeze(0)

print("Layer 0 Head Attention Patterns:")
display(cv.attention.attention_patterns(
    tokens=str_tokens,
    attention=attn_patterns_value,
))
```

Explanation:

* Within the context managers:
    * We access the attention patterns by taking the input to the `attn_dropout`.
        * From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed `nn.Softmax` module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.
        * Because of the previously discussed point about dropout not working in inference mode, we could also use the output of `attn_dropout`, and get the same values.
* Outside of the context managers:
    * We use the `tokenize` method to tokenize the prompt.

</details>

As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using `model.transformer.h[0].attn.q_proj.output` will give you the query vectors, and `k_proj` for the key vectors. However, one thing to be wary of is that GPT-J uses **rotary embeddings**, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See [here](https://blog.eleuther.ai/rotary-embeddings/) for an in-depth discussion of rotary embeddings, and [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bef36Bf9k7FYsCt1DpzCw6eV) for some rough intuitions.

# 2️⃣ Task-encoding hidden states

> ##### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

Note - this section structurally follows section 2.1 of the function vectors paper.

## ICL Task

### Exercise (optional) - generate your own antonym pairs

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
>
> If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.
> ```

We've provided you two options for the antonym dataset you'll use in these exercises.

1. Firstly, we've provided you a list of word pairs, in the file `data/antonym_pairs.txt`.
2. Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).

If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function `generate_dataset` below, which should query GPT-4 and get a list of antonym pairs.

See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it. Use the two dropdowns below (in order) for some guidance.

<details>
<summary>Getting started #1</summary>

Here is a recommended template:

```python
client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": antonym_task},
        {"role": "assistant", "content": start_of_response},
    ]
)
```

where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. "Sure, here are some antonyms: ..."), to guide its subsequent behaviour.

</details>

<details>
<summary>Getting started #2</summary>

Here is an template you might want to use for the actual request:

```python
example_antonyms = "old: young, top: bottom, awake: asleep, future: past, "

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym."},
        {"role": "assistant", "content": f"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}"},
    ]
)
```

where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).

</details>

Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it.

```python
def generate_antonym_dataset(N: int):
    """
    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.
    """
    assert os.environ.get("OPENAI_API_KEY", None) is not None, "Please set your API key before running this function!"

    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": f"Generate {N} pairs of antonyms in the form of a list of 2-tuples. For example, [['old', 'young'], ['top', bottom'], ['awake', 'asleep']...].",
            },
            {"role": "assistant", "content": "Sure, here is a list of 100 antonyms: "},
        ],
    )
    return response

if os.environ.get("OPENAI_API_KEY", None) is not None:
    ANTONYM_PAIRS = generate_antonym_dataset(100)
    # Save the word pairs in a text file
    with open(section_dir / "data" / "my_antonym_pairs.txt", "w") as f:
        for word_pair in ANTONYM_PAIRS:
            f.write(f"{word_pair[0]} {word_pair[1]}\n")

# Load the word pairs from the text file
with open(section_dir / "data" / "antonym_pairs.txt", "r") as f:
    ANTONYM_PAIRS = [line.split() for line in f.readlines()]

print(ANTONYM_PAIRS[:10])
```

## ICL Dataset

To handle this list of word pairs, we've given you some helpful classes.

Firstly, there's the `ICLSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.

```python
class ICLSequence:
    """
    Class to store a single antonym sequence.

    Uses the default template "Q: {x}\nA: {y}" (with separate pairs split by "\n\n").
    """

    def __init__(self, word_pairs: list[list[str]]):
        self.word_pairs = word_pairs
        self.x, self.y = zip(*word_pairs)

    def __len__(self):
        return len(self.word_pairs)

    def __getitem__(self, idx: int):
        return self.word_pairs[idx]

    def prompt(self):
        """Returns the prompt, which contains all but the second element in the last word pair."""
        p = "\n\n".join([f"Q: {x}\nA: {y}" for x, y in self.word_pairs])
        return p[: -len(self.completion())]

    def completion(self):
        """Returns the second element in the last word pair (with padded space)."""
        return " " + self.y[-1]

    def __str__(self):
        """Prints a readable string representation of the prompt & completion (indep of template)."""
        return f"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->".strip(", ")

word_list = [["hot", "cold"], ["yes", "no"], ["in", "out"], ["up", "down"]]
seq = ICLSequence(word_list)

print("Tuple-representation of the sequence:")
print(seq)
print("\nActual prompt, which will be fed into the model:")
print(seq.prompt())
```

Secondly, we have the `ICLDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).

```python
class ICLDataset:
    """
    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency
    between the corrupted and clean datasets.

    Inputs:
        word_pairs:
            list of ICL task, e.g. [["old", "young"], ["top", "bottom"], ...] for the antonym task
        size:
            number of prompts to generate
        n_prepended:
            number of antonym pairs before the single-word ICL task
        bidirectional:
            if True, then we also consider the reversed antonym pairs
        corrupted:
            if True, then the second word in each pair is replaced with a random word
        seed:
            random seed, for consistency & reproducibility
    """

    def __init__(
        self,
        word_pairs: list[list[str]],
        size: int,
        n_prepended: int,
        bidirectional: bool = True,
        seed: int = 0,
        corrupted: bool = False,
    ):
        assert n_prepended + 1 <= len(word_pairs), "Not enough antonym pairs in dataset to create prompt."

        self.word_pairs = word_pairs
        self.word_list = [word for word_pair in word_pairs for word in word_pair]
        self.size = size
        self.n_prepended = n_prepended
        self.bidirectional = bidirectional
        self.corrupted = corrupted
        self.seed = seed

        self.seqs = []
        self.prompts = []
        self.completions = []

        # Generate the dataset (by choosing random word pairs, and constructing `ICLSequence` objects)
        for n in range(size):
            np.random.seed(seed + n)
            random_pairs = np.random.choice(len(self.word_pairs), n_prepended + 1, replace=False)
            # Randomize the order of each word pair (x, y). If not bidirectional, we always have x -> y not y -> x
            random_orders = np.random.choice([1, -1], n_prepended + 1)
            if not (bidirectional):
                random_orders[:] = 1
            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]
            # If corrupted, then replace y with a random word in all (x, y) pairs except the last one
            if corrupted:
                for i in range(len(word_pairs) - 1):
                    word_pairs[i][1] = np.random.choice(self.word_list)
            seq = ICLSequence(word_pairs)

            self.seqs.append(seq)
            self.prompts.append(seq.prompt())
            self.completions.append(seq.completion())

    def create_corrupted_dataset(self):
        """Creates a corrupted version of the dataset (with same random seed)."""
        return ICLDataset(
            self.word_pairs,
            self.size,
            self.n_prepended,
            self.bidirectional,
            corrupted=True,
            seed=self.seed,
        )

    def __len__(self):
        return self.size

    def __getitem__(self, idx: int):
        return self.seqs[idx]
```

You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `"A: answer" -> ["A", ":", " answer"]`. Forgetting prepended spaces is a classic mistake when working with transformers!

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)

table = Table("Prompt", "Correct completion")
for seq, completion in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completion))

rprint(table)
```

Compare this output to what it looks like when `corrupted=True`. Each of the pairs before the last one has their second element replaced with a random one (but the last pair is unchanged).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)

table = Table("Prompt", "Correct completion")
for seq, completions in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completions))

rprint(table)
```

<details>
<summary>Aside - the <code>rich</code> library</summary>

The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.

The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:

```python
rprint("[green]This is green text[/], this is default color")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-1.png" width="350">

and for making text bold / underlined:

```python
rprint("[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-2.png" width="350">

It can also print tables:

```python
from rich.table import Table

table = Table("Col1", "Col2", title="Title") # title is optional
table.add_row("A", "a")
table.add_row("B", "b")

rprint(table)
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-3.png" width="150">

The text formatting (bold, underlined, colors, etc) is also supported within table cells.

</details>

## Task-encoding vector

### Exercise - forward pass on antonym dataset

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the `calculate_h` function below. It should:

* Run a forward pass on the model with the dataset prompts (i.e. the `.prompts` attribute), using the `nnsight` syntax we've demonstrated previously,
* Return a tuple of the model's output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-1.png" width="900">

You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `-1` token (where the model makes the antonym prediction), and same for the completions.

<details>
<summary> Help - I'm not sure how to run (and index into) a batch of inputs.</summary>

If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.

The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.

</details>

```python
def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> tuple[list[str], Tensor]:
    """
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: list[str]
            list of the model's next-token predictions (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    """
    raise NotImplementedError()

tests.test_calculate_h(calculate_h, model)
```

<details><summary>Solution</summary>

```python
def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> tuple[list[str], Tensor]:
    """
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: list[str]
            list of the model's next-token predictions (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    """
    with model.trace(dataset.prompts, remote=REMOTE):
        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0).save()
        logits = model.lm_head.output[:, -1]
        next_tok_id = logits.argmax(dim=-1).save()

    completions = model.tokenizer.batch_decode(next_tok_id)
    return completions, h
```
</details>

We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct). Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!

If the antonyms dataset was constructed well, you should find that the model's completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting `wet -> wet` rather than `wet -> dry`) or understandable completions which shouldn't really be considered mistakes (e.g. predicting `right -> left` rather than `right -> wrong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but for these exercises, we won't worry about this.

```python
def display_model_completions_on_antonyms(
    model: LanguageModel,
    dataset: ICLDataset,
    completions: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt (tuple representation)",
        "Model's completion\n(green=correct)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        # Get model's completion, and correct completion
        completion = completions[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion == correct_completion_first_token
        completion = f"[b green]{repr(completion)}[/]" if is_correct else repr(completion)

        table.add_row(str(seq), completion, repr(correct_completion))

    rprint(table)

# Get uncorrupted dataset
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)

# Getting it from layer 12, as in the description in section 2.1 of paper
model_completions, h = calculate_h(model, dataset, layer=12)

# Displaying the output
display_model_completions_on_antonyms(model, dataset, model_completions)
```

### Using multiple invokes

Another cool feature of `nnsight` is the ability to run multiple different batches through the model at once (or the same batch multiple times) in a way which leads to very clean syntax for doing causal interventions. Rather than doing something like this:

```python
with model.trace(inputs, remote=REMOTE):
    # some causal interventions
```

we can write a double-nested context manager:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(inputs):
        # some causal interventions

    with tracer.invoke(other_inputs):
        # some other causal interventions
```

Both inputs will be run together in parallel, and proxies defined within one `tracer.invoke` block can be used in another. A common use-case is to have clean and corrupted inputs, so we can patch from one to the other and get both outputs all in a single forward pass:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(clean_inputs):
        # extract clean activations
        clean_activations = model.transformer.h[10].output[0]

    with tracer.invoke(corrupted_inputs):
        # patch clean into corrupted
        model.transformer.h[10].output[0][:] = clean_activations
```

You'll do something like this in a later exercise. However for your first exercise (immediately below), you'll only be intervening with vectors that are defined outside of your context manager.

**One important thing to watch out for** - make sure you're not using your proxy before it's being defined! For example, if you were extracting `clean_activations` from `model.transformer.h[10]` but then intervening with it on `model.transformer.h[9]`, this couldn't be done in parallel (you'd need to first extract the clean activations, *then* run the patched forward pass). Doing this should result in a warning message, but may pass silently in some cases - so you need to be extra vigilant!

### Exercise - intervene with $h$

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the function `intervene_with_h` below. This will involve:

* Run two forward passes (within the same context manager) on a zero-shot dataset:
    * One with no intervention (i.e. the residual stream is unchanged),
    * One with an intervention using `h` (i.e. `h` is added to the residual stream at the layer it was taken from).
* Return the completions for no intervention and intervention cases respectively (see docstring).

The diagram below shows how all of this should work, when combined with the `calculate_h` function.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-2.png" width="950">

Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.

<details>
<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>

You can use `with tracer.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.

</details>

<details>
<summary>Help - I'm not sure how to intervene on the hidden state.</summary>

First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).

Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.

</details>

```python
def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
    remote: bool = REMOTE,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: the model we're using to generate completions
        zero_shot_dataset: the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: the `h`-vector we'll be adding to the residual stream
        layer: the layer we'll be extracting the `h`-vector from
        remote: whether to run the forward pass on the remote server (used for running test code)

    Returns:
        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention
    """
    raise NotImplementedError()

tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS, REMOTE)
```

<details><summary>Solution</summary>

```python
def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
    remote: bool = REMOTE,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: the model we're using to generate completions
        zero_shot_dataset: the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: the `h`-vector we'll be adding to the residual stream
        layer: the layer we'll be extracting the `h`-vector from
        remote: whether to run the forward pass on the remote server (used for running test code)

    Returns:
        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=remote) as tracer:
        # First, run a forward pass where we don't intervene, just save token id completions
        with tracer.invoke(zero_shot_dataset.prompts):
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with tracer.invoke(zero_shot_dataset.prompts):
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention)

    return completions_zero_shot, completions_intervention
```
</details>

Run the code below to calculate completions for the function.

**Note, it's very important that we set a different random seed for the zero shot dataset, otherwise we'll be intervening on examples which were actually in the dataset we used to compute $h$!**

```python
layer = 12
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

# Run previous function to get h-vector
h = calculate_h(model, dataset, layer=layer)[1]

# Run new function to intervene with h-vector
completions_zero_shot, completions_intervention = intervene_with_h(model, zero_shot_dataset, h, layer=layer)

print("Zero-shot completions: ", completions_zero_shot)
print("Completions with intervention: ", completions_intervention)
```

Next, run the code below to visualise the completions in a table. You should see:

* ~0% correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt
* ~25% correct completions on the zero-shot prompt with intervention

```python
def display_model_completions_on_h_intervention(
    dataset: ICLDataset,
    completions: list[str],
    completions_intervention: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt",
        "Model's completion\n(no intervention)",
        "Model's completion\n(intervention)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        completion_ni = completions[i]
        completion_i = completions_intervention[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion_i == correct_completion_first_token
        completion_i = f"[b green]{repr(completion_i)}[/]" if is_correct else repr(completion_i)

        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))

    rprint(table)

display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

### Exercise - combine the last two functions

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.

In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.trace` context manager. In other words, **we'll be using `with tracer.invoke...` three times** in this context manager.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-3.png" width="1000">

You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).

Your output should be exactly the same as before (since the `ICLDataset` class is deterministic), hence we've not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you're batching the operations of "compute $h$" and "intervene with $h$" together into a single forward pass.

<details>
<summary>Help - I'm not sure how to use the <code>h</code> vector inside the context manager.</summary>

You extract `h` the same way as before, but you don't need to save it. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.

You shouldn't have to `.save()` anything inside your context manager, other than the token completions.

</details>
<details>
<summary>Help - If I want to add <code>x</code> vector to a slice of my hidden state tensor <code>h</code>, is <code>h[slice]+=x</code> the same as <code>h2 = h[slice], h2 += x</code>?</summary>

No, only `h[slice]+=x` does what you want. This is because when doing <code>h2 = h[slice], h2 += x</code>, the modification line <code>h2 += x</code> is no longer modifying the original tensor `h`, but a different tensor`h2`. In contrast, `h[slice]+=x` keeps the original tensor `h` in the modification line.

A good rule to keep in mind is: If you're trying to modify a tensor some in-place operation, make sure that tensor is in the actual modification line!

</details>

```python
def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    raise NotImplementedError()

dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

completions_zero_shot, completions_intervention = calculate_h_and_intervene(
    model, dataset, zero_shot_dataset, layer=layer
)

display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

<details><summary>Solution</summary>

```python
def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

    completions_zero_shot = tokenizer.batch_decode(clean_tokens)
    completions_intervention = tokenizer.batch_decode(intervene_tokens)
    return completions_zero_shot, completions_intervention
```
</details>

### Exercise - compute change in accuracy

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-20 minutes on this exercise.
> ```

So far, all we've done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn't just give us token completions, it gives us logits too!

You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning two lists of string completions, it returns two lists of floats containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.

<details>
<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>

First, apply log softmax to the logits, to get logprobs.

Second, you can use `tokenizer(dataset.completions)["input_ids"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)

Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).

</details>

```python
def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[float], list[float]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: list[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: list[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    """
    raise NotImplementedError()
```

<details><summary>Solution</summary>

```python
def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[float], list[float]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: list[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: list[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    """
    correct_completion_ids = [toks[0] for toks in tokenizer(zero_shot_dataset.completions)["input_ids"]]

    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

    return clean_logprobs, intervene_logprobs
```
</details>

When you run the code below, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**

```python
def display_model_logprobs_on_h_intervention(
    dataset: ICLDataset,
    correct_logprobs_zero_shot: list[float],
    correct_logprobs_intervention: list[float],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Zero-shot prompt",
        "Model's logprob\n(no intervention)",
        "Model's logprob\n(intervention)",
        "Change in logprob",
        title="Model's antonym logprobs, with zero-shot h-intervention\n(green = intervention improves accuracy)",
    )

    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):
        logprob_ni = correct_logprobs_zero_shot[i]
        logprob_i = correct_logprobs_intervention[i]
        delta_logprob = logprob_i - logprob_ni
        zero_shot_prompt = f"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}"

        # Color code the logprob based on whether it's increased with this intervention
        is_improvement = delta_logprob >= 0
        delta_logprob = f"[b green]{delta_logprob:+.2f}[/]" if is_improvement else f"{delta_logprob:+.2f}"

        table.add_row(zero_shot_prompt, f"{logprob_ni:.2f}", f"{logprob_i:.2f}", delta_logprob)

    rprint(table)

dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(
    model, dataset, zero_shot_dataset, layer=layer
)

display_model_logprobs_on_h_intervention(
    zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention
)
```

# 3️⃣ Function Vectors

> ##### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper.

Here, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**

## Extracting & using FVs

### A note on `out_proj`

First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations. What we have is the linear layer `out_proj` which implicitly combines the "projection per attention head" and the "sum over attention head" operations (if you can't see how this is possible, see the section "Attention Heads are Independent and Additive" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-2.png" width="950">

This presents some question for us, when it comes to causal interventions on attention heads. Use the dropdowns below to read them answer these questions (they'll be important for the coming exercises).

<br>

<details>
<summary>If we want to do a causal intervention on a particular head, should we intervene on <code>z</code> (the input of <code>out_proj</code>) or on <code>attn_output</code> (the output of <code>out_proj</code>) ?</summary>

We should intervene on `z`, because we can just rearrange the `z` tensor of shape `(batch, seq, d_model)` into `(batch, seq, n_heads, d_head)`, in other words separating out all the heads. On the other hand, we can't do this with the `attn_output` because it's *already* summed over heads and we can't separate them out.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we had the ability to access model weights within our context managers?</summary>

We can take a slice of the `z` tensor corresponding to a single attention head:

```python
z.reshape(batch, seq, n_heads, d_head)[:, :, head_idx]
```

and we can take a slice of the `out_proj` weight matrix corresponding to a single attention head (remember that PyTorch stores linear layers in the shape `(out_feats, in_feats)`):

```python
out_proj.weight.rearrange(d_model, n_heads, d_head)[:, head_idx]
```

then finally we can multiply these together.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we </b>didn't have</b> the ability to access model weights within our context managers? (This is currently the case for <code>nnsight</code>, since having access to the weights could allow users to change them!).</summary>

We can be a bit clever, and ablate certain heads in the `z` vector before passing it through the output projection:

```python
# ablate all heads except #2 (using a cloned activation)
heads_to_ablate = [0, 1, 3, 4, ...]
z_ablated = z.reshape(batch, seq, n_heads, d_head).clone()
z_ablated[:, :, heads_to_ablate] = 0

# save the output
attn_head_output = out_proj(z_ablated)
```

Illustration:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-ablated-2.png" width="950">

Note - this would actually fail if `out_proj` had a bias, because we want to just get an attention head's output, not the bias term as well. But if you look at the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) you'll see that `out_proj` doesn't have a bias term, so we're all good!

</details>

### Exercise - implement `calculate_fn_vectors_and_intervene`

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵🔵
>
> You should spend up to 30-60 minutes on this exercise.
> ```

This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:

* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.
    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.
* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).
    * You can use the method `create_corrupted_dataset` method of the `ICLDataset` class for this.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/cie-intervention.png" width="1200">

Before you actually start writing the code, it might be helpful to answer the following:

<details>
<summary>How many different <code>invoke</code> calls will you need in total?</summary>

You'll need `(N_LAYERS * N_HEADS) + 2`. To explain:

- One for the clean prompts, which you'll extract internal activations from and patch them into corrupted prompts,
- One for the corrupted prompts, which you don't intervene on,
- One for the corrupted prompts **for every attention head**, which you'll patch into using the clean run activations.

</details>

<details>
<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>

You don't need to `.save()` the function vectors you're extracting from the model's internals, because these will only be used for causal interventions within the context manager.

The only thing you need to save is the correct token logprobs for (1) the corrupted forward pass where we don't intervene, and (2) each corrupted forward pass where we do intervene on one of the heads. In other words, you'll need to save `(N_LAYERS * N_HEADS) + 1` tensors in total.

</details>

A few other notes:

* We've added a `layers` argument, so you can iterate through different layers of the model (i.e. running the model with `layers = [3, 4, 5]` will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you're getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so even with few prompts per head this adds up fast!).
    * We've included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.
* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).
* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.
* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - this is a wonderful tool, it's like using code annotations within your actual code!

One last note - **if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don't rely on this function working.** However, you should definitely at least read & understand the solution.

```python
def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: list[int] | None = None,
) -> Float[Tensor, "layers heads"]:
    """
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a corrupted
            version of this dataset for interventions)
        layers: list[int] | None
            the layers which this function will calculate the score for (if None, we assume all layers)
    """
    raise NotImplementedError()
```

<details><summary>Solution</summary>

```python
def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: list[int] | None = None,
) -> Float[Tensor, "layers heads"]:
    """
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a corrupted
            version of this dataset for interventions)
        layers: list[int] | None
            the layers which this function will calculate the score for (if None, we assume all layers)
    """
    layers = range(model.config.n_layer) if (layers is None) else layers
    heads = range(model.config.n_head)

    # Get corrupted dataset
    corrupted_dataset = dataset.create_corrupted_dataset()
    N = len(dataset)

    # Get correct token ids, so we can get correct token logprobs
    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)["input_ids"]]

    with model.trace(remote=REMOTE) as tracer:
        # Run a forward pass on clean prompts, where we store attention head outputs
        z_dict = {}
        with tracer.invoke(dataset.prompts):
            for layer in layers:
                # Get hidden states, reshape to get head dimension, store the mean tensor
                z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)
                for head in heads:
                    z_dict[(layer, head)] = z_reshaped[head]

        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can get the
        # correct-token logprobs to compare with our intervention)
        with tracer.invoke(corrupted_dataset.prompts):
            logits = model.lm_head.output[:, -1]
            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[t.arange(N), correct_completion_ids].save()

        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes, since
        # we're doing different interventions each time)
        correct_logprobs_dict = {}
        for layer in layers:
            for head in heads:
                with tracer.invoke(corrupted_dataset.prompts):
                    # Get hidden states, reshape to get head dimension, then set it to the a-vector
                    z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]
                    # Get logprobs at the end, which we'll compare with our corrupted logprobs
                    logits = model.lm_head.output[:, -1]
                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[
                        t.arange(N), correct_completion_ids
                    ].save()

    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim
    all_correct_logprobs_intervention = einops.rearrange(
        t.stack([v for v in correct_logprobs_dict.values()]),
        "(layers heads) batch -> layers heads batch",
        layers=len(layers),
    )
    logprobs_diff = all_correct_logprobs_intervention - correct_logprobs_corrupted  # shape [layers heads batch]

    # Return mean effect of intervention, over the batch dimension
    return logprobs_diff.mean(dim=-1)
```
</details>

As mentioned, the code below calls the function multiple times separately and combines the results.

When you run this code & plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper's figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=8, n_prepended=2)

def batch_process_layers(n_layers, batch_size):
    for i in range(0, n_layers, batch_size):
        yield range(n_layers)[i : i + batch_size]

results = t.empty((0, N_HEADS), device=device)

# If this fails to run, reduce the batch size so the fwd passes are split up more, or reduce dataset size
for layers in batch_process_layers(N_LAYERS, batch_size=4):
    print(f"Computing layers in {layers} ...")
    t0 = time.time()
    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])
    print(f"... finished in {time.time()-t0:.2f} seconds.\n")
```

```python
imshow(
    results.T,
    title="Average indirect effect of function-vector intervention on antonym task",
    width=1000,
    height=600,
    labels={"x": "Layer", "y": "Head"},
    aspect="equal",
)
```

### Exercise - calculate the function vector

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 25-50 minutes on this exercise.
> ```

Your next task is to actually calculate and return the function vector, so we can do a few experiments with it. The function vector is the sum of the outputs of all the attention heads we found using the previous function (i.e. the sum of all of the vectors these heads write to the residual stream), averaged over the prompts in our dataset.

There's a difficulty here - rather than just getting the `z` vectors, we're actually trying to get the `attn_out` vectors, but *before* they're summed over heads. As we discussed previously, this is a bit tricky to do for the model we're working with, because the `out_proj` linear map actually does the "project up" and "sum over heads" operations simultaneously. It would be nice to just take a slice of the `out_proj` matrix and multiply it with a slice of the `z` vector, but the `nnsight` library doesn't yet allow users to access weights directly (for security reasons). To understand how we can extract the `attn_out` vector for a head separately without accessing the underlying weights, you should go back to read the subsection **A note on `out_proj`** at the start of this section.

```python
def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "d_model"]:
    """
    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream
    by the attention heads in `head_list`, averaged over all inputs in `dataset`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: list[tuple[int, int]]
            list of attention heads we're calculating the function vector from
    """
    raise NotImplementedError()

tests.test_calculate_fn_vector(calculate_fn_vector, model)
```

<details><summary>Solution</summary>

```python
def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "d_model"]:
    """
    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream
    by the attention heads in `head_list`, averaged over all inputs in `dataset`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: list[tuple[int, int]]
            list of attention heads we're calculating the function vector from
    """
    # Turn head_list into a dict of {layer: heads we need in this layer}
    head_dict = defaultdict(set)
    for layer, head in head_list:
        head_dict[layer].add(head)

    fn_vector_list = []

    with model.trace(dataset.prompts, remote=REMOTE):
        for layer, head_list in head_dict.items():
            # Get the output projection layer
            out_proj = model.transformer.h[layer].attn.out_proj

            # Get the mean output projection input (note, setting values of this tensor will not have
            # downstream effects on other tensors)
            hidden_states = out_proj.input[:, -1].mean(dim=0)

            # Zero-ablate all heads which aren't in our list, then get the output (which
            # will be the sum over the heads we actually do want!)
            heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]
            for head in heads_to_ablate:
                hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0

            # Now that we've zeroed all unimportant heads, get the output & add it to the list
            # (we need a single batch dimension so we can use `out_proj`)
            out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze().save()
            fn_vector_list.append(out_proj_output)

    # We sum all attention head outputs to get our function vector
    fn_vector = sum([v for v in fn_vector_list])

    assert fn_vector.shape == (D_MODEL,)
    return fn_vector
```
</details>

## Multi-token generation

We're now going to replicate some of the results in Table 3, in the paper:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/tab3.png" width="700">

This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.

Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.

The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces "next-token antonym behaviour" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).

### Using `nnsight` for multi-token generation

Previously, our context managers have looked like:

```python
# Single invoke
with model.trace(prompt, remote=REMOTE):
    ... # Intervene on fwd pass

# Multiple invokes
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(prompt):
        ... # Intervene on fwd pass
```

But for multi-token generation, we'll be using the `generate` method rather than `trace`. Our context managers will look like:

```python
# Single invoke
with model.generate(prompt, remote=REMOTE, max_new_tokens=max_new_tokens):
    with model.all(): # signals to NNsight that you want to run interventions performed on all generated tokens
        ... # Intervene on fwd pass for n-th token to be generated

# Multiple invokes
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with model.all():
        with generator.invoke(prompt):
            ... # Intervene on fwd pass for n-th token to be generated
        with generator.invoke(prompt2):
            ... # Intervene on fwd pass for n-th token to be generated
```

The line `with model.all():` denotes that the following interventions should be applied to the forward pass for all generated tokens.

Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).

Note that `model.generate` takes the same arguments as the normal [HuggingFace generate method](https://huggingface.co/docs/transformers/en/main_classes/text_generation). This means we can use arguments like `top_k`, `top_p`, or `repetition_penalty` to control generation behaviour. In the exercises below we use a repetition penalty (we choose a value of 1.2, in line with the [paper](https://arxiv.org/pdf/1909.05858) that suggested it) - this can avoid the model falling into loops of repeating the same sequence, which is especially common in steering when we're pushing the model OOD.

<!-- #### Optional questions - multi-token generation with NNsight

Here are a few quick optional questions to test your understanding of how multi-generation works with NNsight. These are non-essential, and only mentioned here as potentially helpful pointers.

<details>
<summary>How do I add vector <code>h</code> to all the tokens in the original prompt but not to the generated tokens? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):
        # Add vectors to the model's internals on the first forward pass
        model.transformer.h[layer].output[0][:, :seq_len] += h

```
You don't have to call `model.next()` because you're only adding the vector once to tokens in the original prompt. This will be cached when the model is subsequently generating tokens.

</details>

<details>
<summary>How do I intervene with vector <code>h</code> during the generation of the first k generated tokens? </summary>

To intervene during the generation of the first `k` generated tokens:
```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            # Add vector to the model's internals, on the k-th forward pass
            model.transformer.h[layer].output[0] += h
            model.next()
```
When `n=0`, you are adding to tokens in the original prompt before a new token is a generated. After calling `model.next()`, you are accessing the hidden state of the last token that was generated (with seq_len=1).

</details>

</details>

<details>
<summary>How do I intervene with vector <code>h</code> only during the generation of the first k tokens, but not to tokens in the original prompt before the first generated token? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            model.next()
            # Add vector AFTER calling model.next() to add to the token that just got generated
            model.transformer.h[layer].output[0] += h

```
By not adding things before `model.next()`, we never add to the original prompt but always after a new token has been generated.

</details>

</details>

<details>
<summary>What is the difference between adding vector <code>h</code> before and after vector <code>model.next()</code>? </summary>

As explained in Q3, adding vector before `model.next()` means the operation is always done to the current sequence **before** a new generated token is appended. Adding vector after `model.next()` means the operation is always done to the newly generated token.

</details> -->

### Key-Value Caching

TLDR - caching can make causal interventions inside `model.generate` more complicated, but if you only intervene on sequence positions other than the very last one. In our exercises, we'll only be intervening on the last seqpos so you don't need to worry about it, but it's still a useful topic to understand.

<details>
<summary>See this dropdown if you're curious for more details.</summary>

To speed up inference, transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) for more on transformer inference arithmetic.

When caching takes place, and we're doing causal interventions, we have to be careful that the caching won't override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention "add the function vector to *only* the final sequence position of the prompt for each token we generate" then we'd have to disable caching (since previous forward passes would contain cached values where we intervened on a sequence position which is no longer the final sequence position). However, here we're performing the intervention "add the function vector to the final token of the original prompt, and to *all subsequent sequence positions*", meaning enabling caching (the default behaviour) will give us the right causal intervention.

</details>

### Generator Output

The object `generator.output` is by default a tensor which contains the model's token ID completions (not the logits).

By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour. But in future exercises we'll experiment with different sampling methods than greedy sampling (which generate uses by default), so `generator.output` and argmaxing over logits will not be identical!

### Exercise - intervene with function vector, in multi-token generation

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 15-30 minutes on this exercise.
> ```

You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template.

We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.

```python
def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template='The word "{x}" means',
    n_tokens: int = 5,
) -> tuple[str, str]:
    """
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    """
    raise NotImplementedError()

```

<details><summary>Solution</summary>

```python
def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template='The word "{x}" means',
    n_tokens: int = 5,
) -> tuple[str, str]:
    """
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    """
    prompt = prompt_template.format(x=word)

    with model.generate(remote=REMOTE, max_new_tokens=n_tokens, repetition_penalty=1.2) as generator:
        with model.all():

            with generator.invoke(prompt):
                tokens = model.generator.output.save()

            with generator.invoke(prompt):
                model.transformer.h[layer].output[0][0, -1] += fn_vector
                tokens_intervention = model.generator.output.save()

    completion, completion_intervention = tokenizer.batch_decode(
        [tokens.squeeze().tolist(), tokens_intervention.squeeze().tolist()]
    )
    return completion, completion_intervention
```
</details>

To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym (you might have to play around a bit with the scale factor of `fn_vector`, to balance between effectiveness and coherence of output). If this works, congratulations - **you've just successfully induced an OOD behavioural change in a 6b-parameter model!**

```python
# Remove word from our pairs, so it can be a holdout
word = "light"
_ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]

# Define our dataset, and the attention heads we'll use
dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

# Extract the function vector
fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model,
    word=word,
    layer=9,
    fn_vector=0.1 * fn_vector,
    prompt_template='The word "{x}" means',
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

### Exercise - generalize results to another task (optional)

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 15-30 minutes on this exercise.
> ```

In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).

We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.

<details>
<summary>Guidance for exercise</summary>

Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.

When you define the `ICLDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.

You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.

</details>

```python
with open(section_dir / "data/country_capital_pairs.txt", "r", encoding="utf-8") as f:
    COUNTRY_CAPITAL_PAIRS = [line.split() for line in f.readlines()]

country = "Netherlands"
_COUNTRY_CAPITAL_PAIRS = [pair for pair in COUNTRY_CAPITAL_PAIRS if pair[0] != country]

dataset = ICLDataset(_COUNTRY_CAPITAL_PAIRS, size=20, n_prepended=5, bidirectional=False)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model=model,
    word=country,
    layer=9,
    fn_vector=0.05 * fn_vector,
    prompt_template="When you think of {x},",
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

# 4️⃣ Steering Vectors in GPT2-XL

> ##### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

**Note**: GPT2-XL is not hosted remotely by NNsight at the moment. If you use GPT2-XL, we recommend setting `REMOTE = False`. Otherwise, you can use one of the remotely hosted models (see [here](https://nnsight.net/status/)) and set `REMOTE = True`.

## Steering model behaviour

In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model's completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we'll explore this kind of work in more detail. We'll be primarily using Turner et al's work on [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector).

Summary of the way in which this work differs from the function vector work we've done so far:

* Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).
* Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We'll be focusing on GPT2-XL.
* The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we'll go back to the simpler method of adding directly into the residual stream.

Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of *"finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes & change the model's output"*. This description would also include the following:

* [Inference-time intervention](https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from), which focuses on inducing the behavioural change of "making the model tell the truth". It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.
* [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681), which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

We'll discuss some of this work more in the bonus section, but for now, let's get on with the exercises!

First, we'll load in GPT2-XL, then we'll replicate some of the examples in the main post.

```python
gpt2_xl = LanguageModel("gpt2-xl", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = gpt2_xl.tokenizer

REMOTE = False
# If you are using gpt2_xl, set REMOTE = False as gpt2_xl is not hosted remotely by nnsight. You can
# set REMOTE = True for a remotely hosted model here (https://nnsight.net/status/)
```

### Exercise - replicate the steering vector results

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 30-50 minutes on this exercise.
> ```

Replicate the results in the LessWrong post [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu); specifically the "demonstrations of additions that work well" section.

Read the "How activation additions work" section of [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#How_activation_additions_work) to understand how vectors are extracted and added. We've provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function `calculate_and_intervene_with_h`), although there will be a few methodological differences.

This is the last exercise in this set, and hopefully it'll provide an opportunity to draw together all the threads of what you've learned so far!

### Caching

This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we're adding a slice of vectors to the first sequence positions of the original prompt (see tables like in [this section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) for an illustration). How do you think this will affect our function? Should we still cache? Should we be using `.generate()` or `.trace()`? If using `.generate()`, do we need to call `model.next()` ?

<details>
<summary>Click this dropdown for answers to the questions above.</summary>

Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:

- We can still use caching (because the values we cache shouldn't be different in subsequent token generations),
- We should be using `.generate()` (because we're doing multi-token generation),
- We don't need to call `model.next()` (because we only intervene once, and our intervention will be cached & applied to all subsequent tokens which are generated).

Again, if any of this is confusing then please ask a TA or message in the Slack channel.

</details>

### Padding

The [tables](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:

1. Right-pad the input sequences manually, i.e. use something like `len(tokenizer.tokenize(prompt))` to see how long each of the prompts is, and add copies of `tokenizer.pad_token` to the end of each sequence.
2. Don't manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.

The solutions use (2), but you can use either of these methods.

### Sampling

Following the post, we'll use top-p sampling with probability 0.3 to generate our sequences. We'll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you've done earlier exercises in this section then you might have implemented `freq_penalty` during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar `repetition_penalty` (default value is 1.0 indicating no penalty, values higher than 1.0 apply a penalty to repeated tokens).

We apply these sampling methods by passing keyword arguments into the `generate` method:

```python
{
    "do_sample": True, # necessary whenever we're sampling rather than doing greedy decoding
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}
```

Note that the sequences are generated stochastically rather than greedily - this means we'll get different results if we input multiple different copies of the same sequence. We've given you the `n_comparisons` argument in the function below, i.e. you should generate this many steered *and* this many unsteered completions.

### Other tips / notes

We recommend starting with example #9 (the "talking about weddings" one). It seems quite robust to the exact conditions of the forward pass, unlike the `Love - Hate` example. You can use any of the template cells we've given you below.

We've given you a `use_bos` argument; if this is True then you should append `tokenizer.bos_token` to the start of all the prompts. This is just to be true to the LessWrong post's implementation; it won't change behaviour much and you can probably ignore it and still get good results.

```python
SAMPLING_KWARGS = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.2,
}

def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: list[tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        model: LanguageModel
            the transformer you're doing this computation with
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: list[tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: list[str]
            List of length `n_comparisons`, containing all the unsteered completions.

        steered_completions: list[str]
            List of length `n_comparisons`, containing all the steered completions.
    """
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    raise NotImplementedError()

```

<details><summary>Solution</summary>

```python
SAMPLING_KWARGS = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.2,
}

def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: list[tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        model: LanguageModel
            the transformer you're doing this computation with
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: list[tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: list[str]
            List of length `n_comparisons`, containing all the unsteered completions.

        steered_completions: list[str]
            List of length `n_comparisons`, containing all the steered completions.
    """
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same
    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)
    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]
    assert len(set(act_add_seq_lens)) == 1, "All activation addition prompts must be the same length."
    assert act_add_seq_lens[0] <= len(
        tokenizer.tokenize(prompt)
    ), "All act_add prompts should be shorter than original prompt."

    # Get the prompts we'll intervene on (unsteered and steered)
    steered_prompts = [prompt for _ in range(n_comparisons)]
    unsteered_prompts = [prompt for _ in range(n_comparisons)]

    with model.generate(max_new_tokens=n_tokens, remote=REMOTE, **SAMPLING_KWARGS) as generator:
        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations
        with generator.invoke(act_add_prompts):
            # Get all the prompts from the activation additions, and put them in a list
            # (note, we slice from the end of the sequence because of left-padding)
            act_add_vectors = [
                model.transformer.h[layer].output[0][i, -seq_len:]
                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))
            ]

        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)
        with generator.invoke(unsteered_prompts):
            unsteered_out = model.generator.output

        # Forward pass on steered prompts (we add in the results from the act_add prompts)
        with generator.invoke(steered_prompts):
            # For each act_add prompt, add the vector to residual stream, at the start of the sequence
            for i, (layer, coeff, seq_len) in enumerate(zip(act_add_layers, act_add_coeffs, act_add_seq_lens)):
                model.transformer.h[layer].output[0][:, :seq_len] += coeff * act_add_vectors[i]
            steered_out = model.generator.output

    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results
    unsteered_completions = tokenizer.batch_decode(unsteered_out[-n_comparisons:])
    steered_completions = tokenizer.batch_decode(steered_out[-n_comparisons:])

    return unsteered_completions, steered_completions
```
</details>

To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust).

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I hate you because",
    activation_additions=[(6, +5.0, "Love "), (6, -5.0, "Hate")],
    n_tokens=50,
    n_comparisons=3,
    use_bos=True,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I went up to my friend and said",
    activation_additions=[
        (20, +4.0, "I talk about weddings constantly  "),
        (20, -4.0, "I do not talk about weddings constantly"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="To see the eiffel tower, people flock to",
    activation_additions=[
        (24, +10.0, "The Eiffel Tower is in Rome"),
        (24, -10.0, "The Eiffel Tower is in France"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

# ☆ Bonus

## Extensions of the Function Vectors Paper

There are two other interesting results from the paper, although neither of them are as important as the ones we've covered so far. If you have time, you can try to reproduce these results yourself.

### The Decoded Vocabulary of Function Vectors (3.2)

In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:

* For the antonyms task, the top words evoke the idea of antonyms, e.g. `" negate"`, `" counterpart"`, `" lesser"`.
* For the country-capitals task, the top words are actually the names of capitals, e.g. `" Moscow"`, `" Paris"`, `" Madrid"`.

Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?

An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?

```python
# YOUR CODE HERE - find the decoded vocabulary
```

<details>
<summary>My results for this (spoiler!)</summary>

In the Country-Capitals task, I found:

* The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.
* The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Top logits:
' London'
' Moscow'
' Madrid'
' Budapest'
' Athens'
' Paris'
' Berlin'
' Bangkok'
' Istanbul'
' Montreal'
' Barcelona'
' Jerusalem'
' Seoul'
' Miami'
' Dublin'
' Atlanta'
' Copenhagen'
' Mumbai'
' Minneapolis'
' Beijing'</pre>

</details>

<details><summary>Solution</summary>

```python
# Code to calculate decoded vocabulary:
logits = model._model.lm_head(fn_vector)
max_logits = logits.topk(20).indices.tolist()
tokens = model.tokenizer.batch_decode(max_logits)
print("Top logits:\n" + "\n".join(map(repr, tokens)))
```
</details>

### Vector Algebra on Function Vectors (3.3)

In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?

The authors test this on a variety of different tasks. They find that it's effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn't as effective as function vectors. Do you get these same results?

## Extensions of the Steering Vectors Post

We only implemented one small subset of the results from the steering vectors post (and did it in a fairly slap-dash way). But there are many others you can play around with. For example:

* The authors note that they were unsuccessful in finding a "speak in French" vector. One of the top comments on the LessWrong post describes a process they used to create a French vector which happened to work (link to comment [here](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector?commentId=sqsS9QaDy2bG83XKP)). Can you replicate their results? (They also linked a Colab in this comment, which can help if you're stuck.)
* In a [later section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Perplexity_on_lots_of_sentences_about_weddings_or_about_shipping) of the paper, the authors extensively discuss perplexity (a measure which is related to entropy). They find that the "weddings" vector reduces perplexity on wedding-related sentences, and maintains perplexity on unrelated sentences. Can you replicate their results - in particular, their graph of perplexity ratios against injection layers for wedding and non-wedding-related sentences?
* The authors wrote up the post into a full paper, which you can find [here](https://arxiv.org/abs/2308.10248). Can you replicate some of the extra results in this paper?

## Suggested paper replications

### [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)

In this paper, the authors focus on inducing the behavioural change of "making the model tell the truth". They also look at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.

This might be a good replication for you if:

* You enjoyed the exercises in this section, but are also interested in experimenting with techniques which weren't covered in this section (e.g. linear probing),
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in studying [model truthfulness](https://arxiv.org/abs/2109.07958).

### [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681)

This paper can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in [evaluating models](https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written) on traits like myopia, power seeking, etc,
* You're comfortable doing prompt-engineering, and working with large datasets (like the ones linked above).

*Update* - there is now a [LessWrong post](https://www.lesswrong.com/posts/v7f8ayBxLhmMFRzpa/steering-llama-2-with-contrastive-activation-additions) associated with this paper, which also briefly discusses related areas. We strongly recommend reading this post if you're interested in this replication, or any of the other suggested replications in this section.

### [Red-teaming language models via activation engineering](https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering)

This work, done by Nina Rimsky, extends the results from much of the work we've seen previously, but applied to the domain of **refusal** - what determines whether the LLM will refuse to answer your request, and how can you affect this behaviour? From her post:

> *Validating if finetuning and RLHF have robustly achieved the intended outcome is challenging ... We can try to trigger unwanted behaviors in models more efficiently by manipulating their internal states during inference rather than searching through many inputs. The idea is that if a behavior can be easily triggered through techniques such as activation engineering, it may also occur in deployment. The inability to elicit behaviors via small internal perturbations could serve as a stronger guarantee of safety.*

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context,
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in RLHF, adversarial attacks and jailbreaking,
* You're comfortable doing prompt-engineering (although some of the data you'd need for this replication is available on Nina's [GitHub repo](https://github.com/nrimsky/LM-exp/tree/main)).

<br>

---

<br>

Note - for a week of work, we weakly suggest participants don't try one of these paper replications, because they're quite compute-heavy (even considering the fact that participants have the `nnsight` library at their disposal). There are many possible replications and extensions that can be done from the function vectors or GPT2-XL work, and this might be a better option for you if you enjoyed the exercises in this section and want to do more things like them.

However, if you do feel comfortable working with large models (e.g. you have some past experience of this) and you're interested in this work, then you're certainly welcome to try one of these replications!

---

# function_vectors_solutions.ipynb

# Function Vectors
**ARENA Function Vectors & Model Steering Tutorial**

This tutorial is adapted from the ARENA program material and serves as a fantastic introduction to running experiments in NNsight and working with function vectors and model steering. Thanks to Callum McDougall for writing this comprehensive tutorial and for allowing us to adapt the tutorial for NNsight users, and thanks to Eric Todd for writing the original function vector paper!

> **ARENA: [Streamlit Page](https://arena-chapter1-transformer-interp.streamlit.app/22_📚_[1.4.2]_Function_Vectors_&_Model_Steering)**
>
> **Colab: [exercises](https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/docs/source/notebooks/tutorials/function_vectors_.ipynb) | [solutions](https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/function_vectors_solutions.ipynb)**

You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-2.png" width="350">

# Introduction

These exercises serve as an exploration of the following question: ***can we steer a model to produce different outputs / have a different behaviour, by intervening on the model's forward pass using vectors found by non gradient descent-based methods?***

The majority of the exercises focus on [function vectors](https://functions.baulab.info/): vectors which are extracted from forward passes on in-context learning (ICL) tasks, and added to the residual stream in order to trigger the execution of this task from a zero-shot prompt. The diagram below illustrates this.

<img src="https://functions.baulab.info/images/Paper/fv-demonstrations.png" width="650">

The exercises also take you through use of the `nnsight` library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.

The final set of exercises look at Alex Turner et al's work on [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector), which is conceptually related but has different aims and methodologies.

## Content & Learning Objectives

### 1️⃣ Introduction to `nnsight`

In this section, you'll learn the basics of how to use the `nnsight` library: running forward passes on your model, and saving the internal states. You'll also learn some basics of HuggingFace models which translate over into `nnsight` models (e.g. tokenization, and how to work with model output).

> ##### Learning Objectives
>
> * Learn the basics of the `nnsight` library, and what it can be useful for
> * Learn some basics of HuggingFace models (e.g. tokenization, model output)
> * Use it to extract & visualise GPT-J-6B's internal activations

### 2️⃣ Task-encoding hidden states

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

(Note - this section structurally follows section 2.1 of the function vectors paper).

> ##### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts

### 3️⃣ Function Vectors

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).

> ##### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

### 4️⃣ Steering Vectors in GPT2-XL

Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of "interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour", but it has a different setup, objectives, and approach.

> ##### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

### ☆ Bonus

Lastly, we discuss some possible extensions of function vectors & steering vectors work, which is currently an exciting area of development (e.g. with a paper on steering Llama 2-13b coming out as recently as December 2023).

## Setup code

```python
import os
import sys
from pathlib import Path

IN_COLAB = "google.colab" in sys.modules

chapter = "chapter1_transformer_interp"
repo = "ARENA_3.0"
branch = "main"

# Install dependencies
try:
    import nnsight
except:
    %pip install openai>=1.56.2 nnsight einops jaxtyping plotly transformer_lens==2.11.0 git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python gradio typing-extensions
    %pip install --upgrade pydantic

# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo
root = (
    "/content"
    if IN_COLAB
    else "/root"
    if repo not in os.getcwd()
    else str(next(p for p in Path.cwd().parents if p.name == repo))
)

if Path(root).exists() and not Path(f"{root}/{chapter}").exists():
    if not IN_COLAB:
        !sudo apt-get install unzip
        %pip install jupyter ipython --upgrade

    if not os.path.exists(f"{root}/{chapter}"):
        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip
        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}
        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}
        !rm {root}/{branch}.zip
        !rmdir {root}/{repo}-{branch}

if f"{root}/{chapter}/exercises" not in sys.path:
    sys.path.append(f"{root}/{chapter}/exercises")

os.chdir(f"{root}/{chapter}/exercises")
```

```python
! pip install circuitsvis
! pip install plotly
! pip install jaxtyping
! pip install nnsight
```

```python
import logging
import os
import sys
import time
from collections import defaultdict
from pathlib import Path

import circuitsvis as cv
import einops
import numpy as np
import torch as t
from IPython.display import display
from jaxtyping import Float
from nnsight import CONFIG, LanguageModel
from openai import OpenAI
from rich import print as rprint
from rich.table import Table
from torch import Tensor

# Hide some info logging messages from nnsight
logging.disable(sys.maxsize)

t.set_grad_enabled(False)
device = t.device("mps" if t.backends.mps.is_available() else "cuda" if t.cuda.is_available() else "cpu")

# Make sure exercises are in the path
chapter = "chapter1_transformer_interp"
section = "part42_function_vectors_and_model_steering"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section

import part42_function_vectors_and_model_steering.solutions as solutions
import part42_function_vectors_and_model_steering.tests as tests
from plotly_utils import imshow

MAIN = __name__ == "__main__"
```

# 1️⃣ Introduction to `nnsight`

> ##### Learning Objectives
>
> * Learn the basics of the `nnsight` library, and what it can be useful for
> * Learn some basics of HuggingFace models (e.g. tokenization, model output)
> * Use it to extract & visualise GPT-J-6B's internal activations

## Remote execution

We'll start by discussing [remote execution]((https://nnsight.net/notebooks/features/remote_execution/)) - the ability `nnsight` has to run models on an external server, which is one of the major benefits of the library as a research tool. This helps you bypass the memory & computational limits you might be faced with on your own machine. For remote execution to work, you need 2 things:

1. An API key from the NDIF login page, which you can request [here](https://login.ndif.us/)
2. The model you're working with being live - you can see all live models in the status page [here](https://nnsight.net/status/)

Note that the status page sometimes takes ~5 minutes to load all live models - click the dropdown below to see an example of what the status page should look like once the models have loaded. If you can't see the model you're looking for in this list, then you should set `REMOTE=False` for these exercises, or else make a request on the NDIF Discord to get the model live.

<details>
<summary>Example status page</summary>

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ndif-status.png" width="650">

</details>

## Important syntax

Here, we'll discuss some important syntax for interacting with `nnsight` models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as `nnsight` models, and some of it (e.g. forward passes) is specific to `nnsight`, i.e. it would work differently if you just had a standard HuggingFace model. Make sure to keep this distinction in mind, otherwise syntax can get confusing!

### Model config

Each model comes with a `model.config`, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with `model.config`. Run the code below to see this in action, and to define some useful variables for later.

```python
model = LanguageModel("EleutherAI/gpt-j-6b", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = model.tokenizer

N_HEADS = model.config.n_head
N_LAYERS = model.config.n_layer
D_MODEL = model.config.n_embd
D_HEAD = D_MODEL // N_HEADS

print(f"Number of heads: {N_HEADS}")
print(f"Number of layers: {N_LAYERS}")
print(f"Model dimension: {D_MODEL}")
print(f"Head dimension: {D_HEAD}\n")

print("Entire config: ", model.config)
```

### Tokenizers

A model comes with a tokenizer, accessable with `model.tokenizer` (just like TransformerLens). Unlike TransformerLens, we won't be using utility functions like `model.to_str_tokens`, instead we'll be using the tokenizer directly. Some important functions for today's exercises are:

* `tokenizer` (i.e. just calling it on some input)
    * This takes in a string (or list of strings) and returns the tokenized version.
    * It will return a dictionary, always containing `input_ids` (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. `attention_mask` - see dropdown).
    * Other useful arguments for this function:
        * `return_tensors` - if this is `"pt"`, you'll get results returned as PyTorch tensors, rather than lists (which is the default).
        * `padding` - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).
* `tokenizer.decode`
    * This takes in tokens, and returns the decoded string.
    * If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).
* `tokenizer.batch_decode`
    * Equivalent to `tokenizer.decode`, but it doesn't concatenate.
    * If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.
* `tokenizer.tokenize`
    * Takes in a string, and returns a list of strings.

Run the code below to see some examples of these functions in action.

```python
# Calling tokenizer returns a dictionary, containing input ids & other data.
# If returned as a tensor, then by default it will have a batch dimension.
print(tokenizer("This must be Thursday", return_tensors="pt"))

# Decoding a list of integers, into a concatenated string.
print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))

# Using batch decode, on both 1D and 2D input.
print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))
print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))

# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).
print(tokenizer.tokenize("This sentence will be tokenized"))
```

<details>
<summary>Note on <code>attention_mask</code> (optional)</summary>

`attention_mask`, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don't allow these tokens to be attended to). This is useful when you have to do padding. For example:

```python
model.tokenizer(["Hello world", "Hello"], return_tensors="pt", padding=True)
```

will return:

```python
{
    'attention_mask': tensor([[1, 1], [0, 1]]),
    'input_ids': tensor([[15496,   995], [50256, 15496]])
}
```

We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.

</details>

### Model outputs

At a high level, there are 2 ways to run our model: using the `trace` method (a single forward pass) and the `generate` method (generating multiple tokens). We'll focus on `trace` for now, and we'll discuss `generate` when it comes to multi-token generation later.

The default behaviour of forward passes in normal HuggingFace models is to return an object containing logits (and optionally a bunch of other things). The default behaviour of `trace` in `nnsight` is to not return anything, because anything that we choose to return is explicitly returned inside the context manager.

Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below. Remember to obtain and set an API key first if you're using remote execution!

```python
REMOTE = True

if IN_COLAB:
    # include your HuggingFace Token and NNsight API key on Colab secrets
    from google.colab import userdata
    NDIF_API = userdata.get('NDIF_API')
    CONFIG.set_default_api_key(NDIF_API)

prompt = "The Eiffel Tower is in the city of"

with model.trace(prompt, remote=REMOTE):
    # Save the model's hidden states
    hidden_states = model.transformer.h[-1].output[0].save()

    # Save the model's logit output
    logits = model.lm_head.output[0, -1].save()

# Get the model's logit output, and it's next token prediction
print(f"logits.shape = {logits.shape} = (vocab_size,)")
print("Predicted token ID =", predicted_token_id := logits.argmax().item())
print(f"Predicted token = {tokenizer.decode(predicted_token_id)!r}")

# Print the shape of the model's residual stream
print(f"\nresid.shape = {hidden_states.shape} = (batch_size, seq_len, d_model)")
```

Lets go over this piece by piece.

**First, we create a context block** by calling `.trace(...)` on the model object. This denotes that we wish to generate tokens given some prompts.

```python
with model.trace(prompt, remote=REMOTE):
```

By default, running this will cause your model to be loaded & run locally, but by passing `remote=REMOTE`, it causes the model to be run on the server instead. This is very useful when working with models too large to fit on your machine (or even models which can fit on your machine, but run slowly due to their size, however if you're running this material on a sufficiently large GPU, you may prefer to set `REMOTE=False`).  The input argument can take a variety of formats: strings, lists of tokens, tensors of tokens, etc. Here, we've just used a string `prompt`.

The most interesting part of `nnsight` is the ability to access the model's internal states (like you might already have done with TransformerLens). Let's now see how this works!

```python
hidden_states = model.transformer.h[-1].output[0]
```

On this line we're saying: within our forward pass, access the last layer of the transformer `model.transformer.h[-1]`, access this layer's output `.output` (which is a tuple of tensors), index the first tensor in this tuple `.output[0]`.

Let's break down this line in a bit more detail:

* `model.transformer.h[-1]` is a module in our transformer.
    * If you `print(model)`, you'll see that it consists of `transformer` and `lm_head` (for "language modelling head"). The `transformer` module is made up of embeddings & dropout, a series of layers (called `.h`, for "hidden states"), and a final layernorm. So indexing `.h[-1]` gives you the final layer.
    * Note - it's often useful to visit the documentation page for whatever model you're working on, e.g. you can find GPT-J [here](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html). Not all models will have a nice uniform standardized architecture like you might be used to in TransformerLens!
* `.output[0]` gives you this module's output, as a **proxy**.
    * The output of a module is often a tuple (again, you can see on the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) what the output of each module is). In this case, it's a tuple of 2 tensors, the first of which is the actual layer output (the thing we want).
    * Doing operations on a proxy still returns a proxy - this is why we can index into the `output` proxy tuple and get a proxy tensor!

<details>
<summary>Optional exercise - we mentioned that <code>.output</code> returns a tuple of 2 tensors. Can you use the <a href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> what the second tensor in this tuple is?</summary>

The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called `present`. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we're only generating one new token, these are just the full keys and values.

</details>

<br>

The next command:

```python
logits = model.lm_head.output[0, -1]
```

can be understood in a very similar way. The only difference is that we're accessing the output of `lm_head`, the language modelling head (i.e. the unembedding at the very end), and the output is just a tensor of shape `(batch, seq, d_vocab)` rather than a tuple of tensors. Again, see the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) for this.

If you've worked with Hugging Face models then you might be used to getting logits directly from the model output, but here we generally extract logits from the model internals just like any other activation because this allows us to **control exactly what we return.** If we return lots of very large tensors, this can take quite a while to download from the server (remember that `d_vocab` is often very large for transformers, i.e. around 50k). See the "which objects to save" section below for more discussion on this.

### Output vs input

You can also extract a module's input using `.input` or `.inputs`. If a module's forward method is called as `module.forward(*args, **kwargs)` then `.inputs` returns a tuple of `(tuple_of_args, dict_of_kwargs)`. Alternatively, `.input` is an alias for `.inputs[0][0]`, in other words it returns the first arg from the module's forward method (which is usually the tensor we want).

Remember that if you're not sure then you can debug with `print(module.input.shape)` - even if `.inputs` is a tuple of inputs, this will work to recursively print the shape of all the tensors in the tuple, rather than causing an error.

### Which objects to save

Note that we saved `logits` above, which is a vector of length 50k. In general, it's best to save as small an object as possible, because this reduces the size of object you'll have to download from the server. For example, if you only want the next token completions, just argmax the logits and then save the result! All basic tensor operations can be performed within your context manager.

## Scan & Validate

A really cool feature in nnsight is the scan & validate mode, which allows you to efficiently debug without getting long uninterpretable error messages. For example, consider the code below, which tries to zero ablate one of the model's output tensors. Can you figure out what's wrong with it before running it?

```python
seq_len = len(model.tokenizer.encode(prompt))

try:
    with model.trace(prompt, remote=REMOTE):
        original_output = model.transformer.h[-1].output[0].clone()
        model.transformer.h[-1].output[0][:, seq_len] = 0
        modified_output = model.transformer.h[-1].output[0].save()

except Exception as e:
    print(f"Uninformative error message:\n  {e.__class__.__name__}: {e}")
```

If you guessed "we're indexing a tensor along a dimension of size `seq_len` with index `seq_len` which is an indexing error, you'd be correct! But the error message we get is pretty opaque. This is because of the way the objects in nnsight work: they're not tensors, they're tensor proxies, and can behave in funny ways sometimes.

If we want to debug, we should instead pass `scan=True` and `validate=True` into our `model.trace` call. `scan=True` means we run "fake inputs" through the model which incur no memory costs, and so can be done very quickly and cheaply to detect errors. `validate=True` will run tests during our forward pass that make our error messages more informative. When we use both, we get fast no-memory-cost operations with interpretable error messages!

```python
try:
    with model.trace(prompt, remote=REMOTE, scan=True, validate=True):
        original_output = model.transformer.h[-1].output[0].clone()
        print(f"{model.transformer.h[-1].output.shape=}\n")
        model.transformer.h[-1].output[0][:, seq_len] = 0
        modified_output = model.transformer.h[-1].output[0].save()

except Exception as e:
    print(f"Informative error message:\n  {e.__class__.__name__}: {e}")
```

It's possible to use `validate` without using `scan` (e.g. if you have any `assert proxy.shape == ...` then you must use `validate=True`), although we generally recommend using both when debugging, and then neither when you're finished debugging.

Also note that (as the example above shows) it's useful to use `scan=True, validate=True` when printing tensor shapes, at the initial exploration phase, if you're not exactly sure what the shape of a particular input / output will be. Even if your proxy objects are tuples of tensors, you can still call `.shape`, and it will return a tuple of the shapes of each tensor in the proxy!

## Putting this into practice

### Exercise - visualize attention heads

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-20 minutes on this exercise.
> ```

We just covered a lot of content, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:

```python
cv.attention.attention_patterns(
    tokens=tokens,
    attention=attention,
)
```

where `tokens` is a list of strings, and `attention` is a tensor of shape `(num_heads, num_tokens, num_tokens)`.

If you're stuck, [here's a link](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) to the source code for GPT-J. Look for how the attention patterns are calculated, within the `GPTJAttention` block.

*Note - this model uses dropout on the attention probabilities, as you'll probably notice from looking at the source code in the link above. This won't affect the model's behaviour because dropout is disabled in inference mode (and using the `generate` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.*

<details>
<summary>Aside - inference mode</summary>

Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).

If you want to run the model without inference mode, you can wrap your code in `with model.trace(inference=False):`. However, you don't need to worry about this for the purposes of these exercises.

</details>

If you're stuck on how to reference the right module, see the following hint:

<details>
<summary>Hint - what module you should get attention from</summary>

You want to extract attention from `model.transformer.h[0].attn.attn_dropout.input`. If you used `.output`, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.

</details>

<details>
<summary>Aside - GPT2 tokenizer uses special characters to represent space </summary>

GPT2 tokenizer uses "Ġ" to represent prepended space. So ["My", " name", " is", " James"] will be tokenized as ["My", "Ġname", "Ġis", "ĠJames"]. Make sure you replace "Ġ" with an actual space.

</details>

```python
with model.trace(prompt, remote=REMOTE):
    attn_patterns = model.transformer.h[0].attn.attn_dropout.input.save()

# Get string tokens (replacing special character for spaces)
str_tokens = model.tokenizer.tokenize(prompt)
str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]

# Attention patterns (squeeze out the batch dimension)
attn_patterns_value = attn_patterns.squeeze(0)

print("Layer 0 Head Attention Patterns:")
display(cv.attention.attention_patterns(
    tokens=str_tokens,
    attention=attn_patterns_value,
))
```

<details>
<summary>Explanation</summary>

Explanation:

* Within the context managers:
    * We access the attention patterns by taking the input to the `attn_dropout`.
        * From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed `nn.Softmax` module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.
        * Because of the previously discussed point about dropout not working in inference mode, we could also use the output of `attn_dropout`, and get the same values.
* Outside of the context managers:
    * We use the `tokenize` method to tokenize the prompt.

</details>

As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using `model.transformer.h[0].attn.q_proj.output` will give you the query vectors, and `k_proj` for the key vectors. However, one thing to be wary of is that GPT-J uses **rotary embeddings**, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See [here](https://blog.eleuther.ai/rotary-embeddings/) for an in-depth discussion of rotary embeddings, and [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bef36Bf9k7FYsCt1DpzCw6eV) for some rough intuitions.

# 2️⃣ Task-encoding hidden states

> ##### Learning Objectives
>
> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself
> * Reproduce the "h-vector results" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts

We'll begin with the following question, posed by the Function Vectors paper:

> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*

We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.

This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.

Note - this section structurally follows section 2.1 of the function vectors paper.

## ICL Task

### Exercise (optional) - generate your own antonym pairs

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵⚪⚪⚪
>
> If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.
> ```

We've provided you two options for the antonym dataset you'll use in these exercises.

1. Firstly, we've provided you a list of word pairs, in the file `data/antonym_pairs.txt`.
2. Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).

If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function `generate_dataset` below, which should query GPT-4 and get a list of antonym pairs.

See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it. Use the two dropdowns below (in order) for some guidance.

<details>
<summary>Getting started #1</summary>

Here is a recommended template:

```python
client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": antonym_task},
        {"role": "assistant", "content": start_of_response},
    ]
)
```

where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. "Sure, here are some antonyms: ..."), to guide its subsequent behaviour.

</details>

<details>
<summary>Getting started #2</summary>

Here is an template you might want to use for the actual request:

```python
example_antonyms = "old: young, top: bottom, awake: asleep, future: past, "

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym."},
        {"role": "assistant", "content": f"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}"},
    ]
)
```

where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).

</details>

Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it.

```python
def generate_antonym_dataset(N: int):
    """
    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.
    """
    assert os.environ.get("OPENAI_API_KEY", None) is not None, "Please set your API key before running this function!"

    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {
                "role": "user",
                "content": f"Generate {N} pairs of antonyms in the form of a list of 2-tuples. For example, [['old', 'young'], ['top', bottom'], ['awake', 'asleep']...].",
            },
            {"role": "assistant", "content": "Sure, here is a list of 100 antonyms: "},
        ],
    )
    return response

if os.environ.get("OPENAI_API_KEY", None) is not None:
    ANTONYM_PAIRS = generate_antonym_dataset(100)
    # Save the word pairs in a text file
    with open(section_dir / "data" / "my_antonym_pairs.txt", "w") as f:
        for word_pair in ANTONYM_PAIRS:
            f.write(f"{word_pair[0]} {word_pair[1]}\n")

# Load the word pairs from the text file
with open(section_dir / "data" / "antonym_pairs.txt", "r") as f:
    ANTONYM_PAIRS = [line.split() for line in f.readlines()]

print(ANTONYM_PAIRS[:10])
```

## ICL Dataset

To handle this list of word pairs, we've given you some helpful classes.

Firstly, there's the `ICLSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.

```python
class ICLSequence:
    """
    Class to store a single antonym sequence.

    Uses the default template "Q: {x}\nA: {y}" (with separate pairs split by "\n\n").
    """

    def __init__(self, word_pairs: list[list[str]]):
        self.word_pairs = word_pairs
        self.x, self.y = zip(*word_pairs)

    def __len__(self):
        return len(self.word_pairs)

    def __getitem__(self, idx: int):
        return self.word_pairs[idx]

    def prompt(self):
        """Returns the prompt, which contains all but the second element in the last word pair."""
        p = "\n\n".join([f"Q: {x}\nA: {y}" for x, y in self.word_pairs])
        return p[: -len(self.completion())]

    def completion(self):
        """Returns the second element in the last word pair (with padded space)."""
        return " " + self.y[-1]

    def __str__(self):
        """Prints a readable string representation of the prompt & completion (indep of template)."""
        return f"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->".strip(", ")

word_list = [["hot", "cold"], ["yes", "no"], ["in", "out"], ["up", "down"]]
seq = ICLSequence(word_list)

print("Tuple-representation of the sequence:")
print(seq)
print("\nActual prompt, which will be fed into the model:")
print(seq.prompt())
```

Secondly, we have the `ICLDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).

```python
class ICLDataset:
    """
    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency
    between the corrupted and clean datasets.

    Inputs:
        word_pairs:
            list of ICL task, e.g. [["old", "young"], ["top", "bottom"], ...] for the antonym task
        size:
            number of prompts to generate
        n_prepended:
            number of antonym pairs before the single-word ICL task
        bidirectional:
            if True, then we also consider the reversed antonym pairs
        corrupted:
            if True, then the second word in each pair is replaced with a random word
        seed:
            random seed, for consistency & reproducibility
    """

    def __init__(
        self,
        word_pairs: list[list[str]],
        size: int,
        n_prepended: int,
        bidirectional: bool = True,
        seed: int = 0,
        corrupted: bool = False,
    ):
        assert n_prepended + 1 <= len(word_pairs), "Not enough antonym pairs in dataset to create prompt."

        self.word_pairs = word_pairs
        self.word_list = [word for word_pair in word_pairs for word in word_pair]
        self.size = size
        self.n_prepended = n_prepended
        self.bidirectional = bidirectional
        self.corrupted = corrupted
        self.seed = seed

        self.seqs = []
        self.prompts = []
        self.completions = []

        # Generate the dataset (by choosing random word pairs, and constructing `ICLSequence` objects)
        for n in range(size):
            np.random.seed(seed + n)
            random_pairs = np.random.choice(len(self.word_pairs), n_prepended + 1, replace=False)
            # Randomize the order of each word pair (x, y). If not bidirectional, we always have x -> y not y -> x
            random_orders = np.random.choice([1, -1], n_prepended + 1)
            if not (bidirectional):
                random_orders[:] = 1
            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]
            # If corrupted, then replace y with a random word in all (x, y) pairs except the last one
            if corrupted:
                for i in range(len(word_pairs) - 1):
                    word_pairs[i][1] = np.random.choice(self.word_list)
            seq = ICLSequence(word_pairs)

            self.seqs.append(seq)
            self.prompts.append(seq.prompt())
            self.completions.append(seq.completion())

    def create_corrupted_dataset(self):
        """Creates a corrupted version of the dataset (with same random seed)."""
        return ICLDataset(
            self.word_pairs,
            self.size,
            self.n_prepended,
            self.bidirectional,
            corrupted=True,
            seed=self.seed,
        )

    def __len__(self):
        return self.size

    def __getitem__(self, idx: int):
        return self.seqs[idx]
```

You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `"A: answer" -> ["A", ":", " answer"]`. Forgetting prepended spaces is a classic mistake when working with transformers!

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)

table = Table("Prompt", "Correct completion")
for seq, completion in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completion))

rprint(table)
```

Compare this output to what it looks like when `corrupted=True`. Each of the pairs before the last one has their second element replaced with a random one (but the last pair is unchanged).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)

table = Table("Prompt", "Correct completion")
for seq, completions in zip(dataset.seqs, dataset.completions):
    table.add_row(str(seq), repr(completions))

rprint(table)
```

<details>
<summary>Aside - the <code>rich</code> library</summary>

The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.

The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:

```python
rprint("[green]This is green text[/], this is default color")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-1.png" width="350">

and for making text bold / underlined:

```python
rprint("[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].")
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-2.png" width="350">

It can also print tables:

```python
from rich.table import Table

table = Table("Col1", "Col2", title="Title") # title is optional
table.add_row("A", "a")
table.add_row("B", "b")

rprint(table)
```

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-3.png" width="150">

The text formatting (bold, underlined, colors, etc) is also supported within table cells.

</details>

## Task-encoding vector

### Exercise - forward pass on antonym dataset

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the `calculate_h` function below. It should:

* Run a forward pass on the model with the dataset prompts (i.e. the `.prompts` attribute), using the `nnsight` syntax we've demonstrated previously,
* Return a tuple of the model's output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-1.png" width="900">

You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `-1` token (where the model makes the antonym prediction), and same for the completions.

<details>
<summary> Help - I'm not sure how to run (and index into) a batch of inputs.</summary>

If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.

The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.

</details>

```python
def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> tuple[list[str], Tensor]:
    """
    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce
    a single vector `h`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)
        layer: int
            the layer you're extracting activations from

    Returns:
        completions: list[str]
            list of the model's next-token predictions (i.e. the strings the model predicts to follow the last token)
        h: Tensor
            average hidden state tensor at final sequence position, of shape (d_model,)
    """
    with model.trace(dataset.prompts, remote=REMOTE):
        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0).save()
        logits = model.lm_head.output[:, -1]
        next_tok_id = logits.argmax(dim=-1).save()

    completions = model.tokenizer.batch_decode(next_tok_id)
    return completions, h

tests.test_calculate_h(calculate_h, model)
```

We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct). Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!

If the antonyms dataset was constructed well, you should find that the model's completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting `wet -> wet` rather than `wet -> dry`) or understandable completions which shouldn't really be considered mistakes (e.g. predicting `right -> left` rather than `right -> wrong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but for these exercises, we won't worry about this.

```python
def display_model_completions_on_antonyms(
    model: LanguageModel,
    dataset: ICLDataset,
    completions: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt (tuple representation)",
        "Model's completion\n(green=correct)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        # Get model's completion, and correct completion
        completion = completions[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion == correct_completion_first_token
        completion = f"[b green]{repr(completion)}[/]" if is_correct else repr(completion)

        table.add_row(str(seq), completion, repr(correct_completion))

    rprint(table)

# Get uncorrupted dataset
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)

# Getting it from layer 12, as in the description in section 2.1 of paper
model_completions, h = calculate_h(model, dataset, layer=12)

# Displaying the output
display_model_completions_on_antonyms(model, dataset, model_completions)
```

### Using multiple invokes

Another cool feature of `nnsight` is the ability to run multiple different batches through the model at once (or the same batch multiple times) in a way which leads to very clean syntax for doing causal interventions. Rather than doing something like this:

```python
with model.trace(inputs, remote=REMOTE):
    # some causal interventions
```

we can write a double-nested context manager:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(inputs):
        # some causal interventions

    with tracer.invoke(other_inputs):
        # some other causal interventions
```

Both inputs will be run together in parallel, and proxies defined within one `tracer.invoke` block can be used in another. A common use-case is to have clean and corrupted inputs, so we can patch from one to the other and get both outputs all in a single forward pass:

```python
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(clean_inputs):
        # extract clean activations
        clean_activations = model.transformer.h[10].output[0]

    with tracer.invoke(corrupted_inputs):
        # patch clean into corrupted
        model.transformer.h[10].output[0][:] = clean_activations
```

You'll do something like this in a later exercise. However for your first exercise (immediately below), you'll only be intervening with vectors that are defined outside of your context manager.

**One important thing to watch out for** - make sure you're not using your proxy before it's being defined! For example, if you were extracting `clean_activations` from `model.transformer.h[10]` but then intervening with it on `model.transformer.h[9]`, this couldn't be done in parallel (you'd need to first extract the clean activations, *then* run the patched forward pass). Doing this should result in a warning message, but may pass silently in some cases - so you need to be extra vigilant!

### Exercise - intervene with $h$

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

You should fill in the function `intervene_with_h` below. This will involve:

* Run two forward passes (within the same context manager) on a zero-shot dataset:
    * One with no intervention (i.e. the residual stream is unchanged),
    * One with an intervention using `h` (i.e. `h` is added to the residual stream at the layer it was taken from).
* Return the completions for no intervention and intervention cases respectively (see docstring).

The diagram below shows how all of this should work, when combined with the `calculate_h` function.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-2.png" width="950">

Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.

<details>
<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>

You can use `with tracer.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.

</details>

<details>
<summary>Help - I'm not sure how to intervene on the hidden state.</summary>

First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).

Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.

</details>

```python
def intervene_with_h(
    model: LanguageModel,
    zero_shot_dataset: ICLDataset,
    h: Tensor,
    layer: int,
    remote: bool = REMOTE,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the
    residual stream of a set of generated zero-shot prompts.

    Inputs:
        model: the model we're using to generate completions
        zero_shot_dataset: the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        h: the `h`-vector we'll be adding to the residual stream
        layer: the layer we'll be extracting the `h`-vector from
        remote: whether to run the forward pass on the remote server (used for running test code)

    Returns:
        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=remote) as tracer:
        # First, run a forward pass where we don't intervene, just save token id completions
        with tracer.invoke(zero_shot_dataset.prompts):
            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()

        # Next, run a forward pass on the zero-shot prompts where we do intervene
        with tracer.invoke(zero_shot_dataset.prompts):
            # Add the h-vector to the residual stream, at the last sequence position
            hidden_states = model.transformer.h[layer].output[0]
            hidden_states[:, -1] += h
            # Also save completions
            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()

    # Decode to get the string tokens
    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot)
    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention)

    return completions_zero_shot, completions_intervention

tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS, REMOTE)
```

Run the code below to calculate completions for the function.

**Note, it's very important that we set a different random seed for the zero shot dataset, otherwise we'll be intervening on examples which were actually in the dataset we used to compute $h$!**

```python
layer = 12
dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

# Run previous function to get h-vector
h = calculate_h(model, dataset, layer=layer)[1]

# Run new function to intervene with h-vector
completions_zero_shot, completions_intervention = intervene_with_h(model, zero_shot_dataset, h, layer=layer)

print("Zero-shot completions: ", completions_zero_shot)
print("Completions with intervention: ", completions_intervention)
```

Next, run the code below to visualise the completions in a table. You should see:

* ~0% correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt
* ~25% correct completions on the zero-shot prompt with intervention

```python
def display_model_completions_on_h_intervention(
    dataset: ICLDataset,
    completions: list[str],
    completions_intervention: list[str],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Prompt",
        "Model's completion\n(no intervention)",
        "Model's completion\n(intervention)",
        "Correct completion",
        title="Model's antonym completions",
    )

    for i in range(min(len(completions), num_to_display)):
        completion_ni = completions[i]
        completion_i = completions_intervention[i]
        correct_completion = dataset.completions[i]
        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace("Ġ", " ")
        seq = dataset.seqs[i]

        # Color code the completion based on whether it's correct
        is_correct = completion_i == correct_completion_first_token
        completion_i = f"[b green]{repr(completion_i)}[/]" if is_correct else repr(completion_i)

        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))

    rprint(table)

display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

### Exercise - combine the last two functions

> ```yaml
> Difficulty: 🔴🔴🔴⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-15 minutes on this exercise.
> ```

One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.

In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.trace` context manager. In other words, **we'll be using `with tracer.invoke...` three times** in this context manager.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-3.png" width="1000">

You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).

Your output should be exactly the same as before (since the `ICLDataset` class is deterministic), hence we've not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you're batching the operations of "compute $h$" and "intervene with $h$" together into a single forward pass.

<details>
<summary>Help - I'm not sure how to use the <code>h</code> vector inside the context manager.</summary>

You extract `h` the same way as before, but you don't need to save it. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.

You shouldn't have to `.save()` anything inside your context manager, other than the token completions.

</details>
<details>
<summary>Help - If I want to add <code>x</code> vector to a slice of my hidden state tensor <code>h</code>, is <code>h[slice]+=x</code> the same as <code>h2 = h[slice], h2 += x</code>?</summary>

No, only `h[slice]+=x` does what you want. This is because when doing <code>h2 = h[slice], h2 += x</code>, the modification line <code>h2 += x</code> is no longer modifying the original tensor `h`, but a different tensor`h2`. In contrast, `h[slice]+=x` keeps the original tensor `h` in the modification line.

A good rule to keep in mind is: If you're trying to modify a tensor some in-place operation, make sure that tensor is in the actual modification line!

</details>

```python
def calculate_h_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[str], list[str]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the completions from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        completions_zero_shot: list[str]
            list of string completions for the zero-shot prompts, without intervention
        completions_intervention: list[str]
            list of string completions for the zero-shot prompts, with h-intervention
    """
    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()

    completions_zero_shot = tokenizer.batch_decode(clean_tokens)
    completions_intervention = tokenizer.batch_decode(intervene_tokens)
    return completions_zero_shot, completions_intervention

dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

completions_zero_shot, completions_intervention = calculate_h_and_intervene(
    model, dataset, zero_shot_dataset, layer=layer
)

display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)
```

### Exercise - compute change in accuracy

> ```yaml
> Difficulty: 🔴🔴⚪⚪⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 10-20 minutes on this exercise.
> ```

So far, all we've done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn't just give us token completions, it gives us logits too!

You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning two lists of string completions, it returns two lists of floats containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.

<details>
<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>

First, apply log softmax to the logits, to get logprobs.

Second, you can use `tokenizer(dataset.completions)["input_ids"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)

Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).

</details>

```python
def calculate_h_and_intervene_logprobs(
    model: LanguageModel,
    dataset: ICLDataset,
    zero_shot_dataset: ICLDataset,
    layer: int,
) -> tuple[list[float], list[float]]:
    """
    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,
    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.

    Inputs:
        model: LanguageModel
            the model we're using to generate completions
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the `h`-vector
        zero_shot_dataset: ICLDataset
            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector
        layer: int
            the layer we'll be extracting the `h`-vector from

    Returns:
        correct_logprobs: list[float]
            list of correct-token logprobs for the zero-shot prompts, without intervention
        correct_logprobs_intervention: list[float]
            list of correct-token logprobs for the zero-shot prompts, with h-intervention
    """
    correct_completion_ids = [toks[0] for toks in tokenizer(zero_shot_dataset.completions)["input_ids"]]

    with model.trace(remote=REMOTE) as tracer:
        with tracer.invoke(dataset.prompts):
            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)

        with tracer.invoke(zero_shot_dataset.prompts):
            clean_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

        with tracer.invoke(zero_shot_dataset.prompts):
            hidden = model.transformer.h[layer].output[0]
            hidden[:, -1] += h
            intervene_logprobs = model.lm_head.output.log_softmax(dim=-1)[
                range(len(zero_shot_dataset)), -1, correct_completion_ids
            ].save()

    return clean_logprobs, intervene_logprobs
```

When you run the code below, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**

```python
def display_model_logprobs_on_h_intervention(
    dataset: ICLDataset,
    correct_logprobs_zero_shot: list[float],
    correct_logprobs_intervention: list[float],
    num_to_display: int = 20,
) -> None:
    table = Table(
        "Zero-shot prompt",
        "Model's logprob\n(no intervention)",
        "Model's logprob\n(intervention)",
        "Change in logprob",
        title="Model's antonym logprobs, with zero-shot h-intervention\n(green = intervention improves accuracy)",
    )

    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):
        logprob_ni = correct_logprobs_zero_shot[i]
        logprob_i = correct_logprobs_intervention[i]
        delta_logprob = logprob_i - logprob_ni
        zero_shot_prompt = f"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}"

        # Color code the logprob based on whether it's increased with this intervention
        is_improvement = delta_logprob >= 0
        delta_logprob = f"[b green]{delta_logprob:+.2f}[/]" if is_improvement else f"{delta_logprob:+.2f}"

        table.add_row(zero_shot_prompt, f"{logprob_ni:.2f}", f"{logprob_i:.2f}", delta_logprob)

    rprint(table)

dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)
zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)

correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(
    model, dataset, zero_shot_dataset, layer=layer
)

display_model_logprobs_on_h_intervention(
    zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention
)
```

# 3️⃣ Function Vectors

> ##### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper.

Here, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**

## Extracting & using FVs

### A note on `out_proj`

First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations. What we have is the linear layer `out_proj` which implicitly combines the "projection per attention head" and the "sum over attention head" operations (if you can't see how this is possible, see the section "Attention Heads are Independent and Additive" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-2.png" width="950">

This presents some question for us, when it comes to causal interventions on attention heads. Use the dropdowns below to read them answer these questions (they'll be important for the coming exercises).

<br>

<details>
<summary>If we want to do a causal intervention on a particular head, should we intervene on <code>z</code> (the input of <code>out_proj</code>) or on <code>attn_output</code> (the output of <code>out_proj</code>) ?</summary>

We should intervene on `z`, because we can just rearrange the `z` tensor of shape `(batch, seq, d_model)` into `(batch, seq, n_heads, d_head)`, in other words separating out all the heads. On the other hand, we can't do this with the `attn_output` because it's *already* summed over heads and we can't separate them out.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we had the ability to access model weights within our context managers?</summary>

We can take a slice of the `z` tensor corresponding to a single attention head:

```python
z.reshape(batch, seq, n_heads, d_head)[:, :, head_idx]
```

and we can take a slice of the `out_proj` weight matrix corresponding to a single attention head (remember that PyTorch stores linear layers in the shape `(out_feats, in_feats)`):

```python
out_proj.weight.rearrange(d_model, n_heads, d_head)[:, head_idx]
```

then finally we can multiply these together.

</details>

<br>

<details>
<summary>How could we get the <code>attn_output</code> vector for a single head, if we </b>didn't have</b> the ability to access model weights within our context managers? (This is currently the case for <code>nnsight</code>, since having access to the weights could allow users to change them!).</summary>

We can be a bit clever, and ablate certain heads in the `z` vector before passing it through the output projection:

```python
# ablate all heads except #2 (using a cloned activation)
heads_to_ablate = [0, 1, 3, 4, ...]
z_ablated = z.reshape(batch, seq, n_heads, d_head).clone()
z_ablated[:, :, heads_to_ablate] = 0

# save the output
attn_head_output = out_proj(z_ablated)
```

Illustration:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-ablated-2.png" width="950">

Note - this would actually fail if `out_proj` had a bias, because we want to just get an attention head's output, not the bias term as well. But if you look at the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) you'll see that `out_proj` doesn't have a bias term, so we're all good!

</details>

### Exercise - implement `calculate_fn_vectors_and_intervene`

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵🔵
>
> You should spend up to 30-60 minutes on this exercise.
> ```

This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:

* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.
    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.
* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).
    * You can use the method `create_corrupted_dataset` method of the `ICLDataset` class for this.

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/cie-intervention.png" width="1200">

Before you actually start writing the code, it might be helpful to answer the following:

<details>
<summary>How many different <code>invoke</code> calls will you need in total?</summary>

You'll need `(N_LAYERS * N_HEADS) + 2`. To explain:

- One for the clean prompts, which you'll extract internal activations from and patch them into corrupted prompts,
- One for the corrupted prompts, which you don't intervene on,
- One for the corrupted prompts **for every attention head**, which you'll patch into using the clean run activations.

</details>

<details>
<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>

You don't need to `.save()` the function vectors you're extracting from the model's internals, because these will only be used for causal interventions within the context manager.

The only thing you need to save is the correct token logprobs for (1) the corrupted forward pass where we don't intervene, and (2) each corrupted forward pass where we do intervene on one of the heads. In other words, you'll need to save `(N_LAYERS * N_HEADS) + 1` tensors in total.

</details>

A few other notes:

* We've added a `layers` argument, so you can iterate through different layers of the model (i.e. running the model with `layers = [3, 4, 5]` will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you're getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so even with few prompts per head this adds up fast!).
    * We've included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.
* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).
* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.
* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - this is a wonderful tool, it's like using code annotations within your actual code!

One last note - **if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don't rely on this function working.** However, you should definitely at least read & understand the solution.

```python
def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: list[int] | None = None,
) -> Float[Tensor, "layers heads"]:
    """
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a corrupted
            version of this dataset for interventions)
        layers: list[int] | None
            the layers which this function will calculate the score for (if None, we assume all layers)
    """
    layers = range(model.config.n_layer) if (layers is None) else layers
    heads = range(model.config.n_head)

    # Get corrupted dataset
    corrupted_dataset = dataset.create_corrupted_dataset()
    N = len(dataset)

    # Get correct token ids, so we can get correct token logprobs
    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)["input_ids"]]

    with model.trace(remote=REMOTE) as tracer:
        # Run a forward pass on clean prompts, where we store attention head outputs
        z_dict = {}
        with tracer.invoke(dataset.prompts):
            for layer in layers:
                # Get hidden states, reshape to get head dimension, store the mean tensor
                z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)
                for head in heads:
                    z_dict[(layer, head)] = z_reshaped[head]

        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can get the
        # correct-token logprobs to compare with our intervention)
        with tracer.invoke(corrupted_dataset.prompts):
            logits = model.lm_head.output[:, -1]
            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[t.arange(N), correct_completion_ids].save()

        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes, since
        # we're doing different interventions each time)
        correct_logprobs_dict = {}
        for layer in layers:
            for head in heads:
                with tracer.invoke(corrupted_dataset.prompts):
                    # Get hidden states, reshape to get head dimension, then set it to the a-vector
                    z = model.transformer.h[layer].attn.out_proj.input[:, -1]
                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]
                    # Get logprobs at the end, which we'll compare with our corrupted logprobs
                    logits = model.lm_head.output[:, -1]
                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[
                        t.arange(N), correct_completion_ids
                    ].save()

    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim
    all_correct_logprobs_intervention = einops.rearrange(
        t.stack([v for v in correct_logprobs_dict.values()]),
        "(layers heads) batch -> layers heads batch",
        layers=len(layers),
    )
    logprobs_diff = all_correct_logprobs_intervention - correct_logprobs_corrupted  # shape [layers heads batch]

    # Return mean effect of intervention, over the batch dimension
    return logprobs_diff.mean(dim=-1)
```

As mentioned, the code below calls the function multiple times separately and combines the results.

When you run this code & plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper's figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=8, n_prepended=2)

def batch_process_layers(n_layers, batch_size):
    for i in range(0, n_layers, batch_size):
        yield range(n_layers)[i : i + batch_size]

results = t.empty((0, N_HEADS), device=device)

# If this fails to run, reduce the batch size so the fwd passes are split up more, or reduce dataset size
for layers in batch_process_layers(N_LAYERS, batch_size=4):
    print(f"Computing layers in {layers} ...")
    t0 = time.time()
    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])
    print(f"... finished in {time.time()-t0:.2f} seconds.\n")
```

```python
imshow(
    results.T,
    title="Average indirect effect of function-vector intervention on antonym task",
    width=1000,
    height=600,
    labels={"x": "Layer", "y": "Head"},
    aspect="equal",
)
```

### Exercise - calculate the function vector

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 25-50 minutes on this exercise.
> ```

Your next task is to actually calculate and return the function vector, so we can do a few experiments with it. The function vector is the sum of the outputs of all the attention heads we found using the previous function (i.e. the sum of all of the vectors these heads write to the residual stream), averaged over the prompts in our dataset.

There's a difficulty here - rather than just getting the `z` vectors, we're actually trying to get the `attn_out` vectors, but *before* they're summed over heads. As we discussed previously, this is a bit tricky to do for the model we're working with, because the `out_proj` linear map actually does the "project up" and "sum over heads" operations simultaneously. It would be nice to just take a slice of the `out_proj` matrix and multiply it with a slice of the `z` vector, but the `nnsight` library doesn't yet allow users to access weights directly (for security reasons). To understand how we can extract the `attn_out` vector for a head separately without accessing the underlying weights, you should go back to read the subsection **A note on `out_proj`** at the start of this section.

```python
def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: list[tuple[int, int]],
) -> Float[Tensor, "d_model"]:
    """
    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream
    by the attention heads in `head_list`, averaged over all inputs in `dataset`.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: list[tuple[int, int]]
            list of attention heads we're calculating the function vector from
    """
    # Turn head_list into a dict of {layer: heads we need in this layer}
    head_dict = defaultdict(set)
    for layer, head in head_list:
        head_dict[layer].add(head)

    fn_vector_list = []

    with model.trace(dataset.prompts, remote=REMOTE):
        for layer, head_list in head_dict.items():
            # Get the output projection layer
            out_proj = model.transformer.h[layer].attn.out_proj

            # Get the mean output projection input (note, setting values of this tensor will not have
            # downstream effects on other tensors)
            hidden_states = out_proj.input[:, -1].mean(dim=0)

            # Zero-ablate all heads which aren't in our list, then get the output (which
            # will be the sum over the heads we actually do want!)
            heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]
            for head in heads_to_ablate:
                hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0

            # Now that we've zeroed all unimportant heads, get the output & add it to the list
            # (we need a single batch dimension so we can use `out_proj`)
            out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze().save()
            fn_vector_list.append(out_proj_output)

    # We sum all attention head outputs to get our function vector
    fn_vector = sum([v for v in fn_vector_list])

    assert fn_vector.shape == (D_MODEL,)
    return fn_vector

tests.test_calculate_fn_vector(calculate_fn_vector, model)
```

## Multi-token generation

We're now going to replicate some of the results in Table 3, in the paper:

<img src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/tab3.png" width="700">

This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.

Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.

The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces "next-token antonym behaviour" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).

### Using `nnsight` for multi-token generation

Previously, our context managers have looked like:

```python
# Single invoke
with model.trace(prompt, remote=REMOTE):
    ... # Intervene on fwd pass

# Multiple invokes
with model.trace(remote=REMOTE) as tracer:
    with tracer.invoke(prompt):
        ... # Intervene on fwd pass
```

But for multi-token generation, we'll be using the `generate` method rather than `trace`. Our context managers will look like:

```python
# Single invoke
with model.generate(prompt, remote=REMOTE, max_new_tokens=max_new_tokens):
    with model.all(): # signals to NNsight that you want to run interventions performed on all generated tokens
        ... # Intervene on fwd pass for n-th token to be generated

# Multiple invokes
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with model.all():
        with generator.invoke(prompt):
            ... # Intervene on fwd pass for n-th token to be generated
        with generator.invoke(prompt2):
            ... # Intervene on fwd pass for n-th token to be generated
```

The line `with model.all():` denotes that the following interventions should be applied to the forward pass for all generated tokens.

Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).

Note that `model.generate` takes the same arguments as the normal [HuggingFace generate method](https://huggingface.co/docs/transformers/en/main_classes/text_generation). This means we can use arguments like `top_k`, `top_p`, or `repetition_penalty` to control generation behaviour. In the exercises below we use a repetition penalty (we choose a value of 1.2, in line with the [paper](https://arxiv.org/pdf/1909.05858) that suggested it) - this can avoid the model falling into loops of repeating the same sequence, which is especially common in steering when we're pushing the model OOD.

<!-- #### Optional questions - multi-token generation with NNsight

Here are a few quick optional questions to test your understanding of how multi-generation works with NNsight. These are non-essential, and only mentioned here as potentially helpful pointers.

<details>
<summary>How do I add vector <code>h</code> to all the tokens in the original prompt but not to the generated tokens? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):
        # Add vectors to the model's internals on the first forward pass
        model.transformer.h[layer].output[0][:, :seq_len] += h

```
You don't have to call `model.next()` because you're only adding the vector once to tokens in the original prompt. This will be cached when the model is subsequently generating tokens.

</details>

<details>
<summary>How do I intervene with vector <code>h</code> during the generation of the first k generated tokens? </summary>

To intervene during the generation of the first `k` generated tokens:
```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            # Add vector to the model's internals, on the k-th forward pass
            model.transformer.h[layer].output[0] += h
            model.next()
```
When `n=0`, you are adding to tokens in the original prompt before a new token is a generated. After calling `model.next()`, you are accessing the hidden state of the last token that was generated (with seq_len=1).

</details>

</details>

<details>
<summary>How do I intervene with vector <code>h</code> only during the generation of the first k tokens, but not to tokens in the original prompt before the first generated token? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            model.next()
            # Add vector AFTER calling model.next() to add to the token that just got generated
            model.transformer.h[layer].output[0] += h

```
By not adding things before `model.next()`, we never add to the original prompt but always after a new token has been generated.

</details>

</details>

<details>
<summary>What is the difference between adding vector <code>h</code> before and after vector <code>model.next()</code>? </summary>

As explained in Q3, adding vector before `model.next()` means the operation is always done to the current sequence **before** a new generated token is appended. Adding vector after `model.next()` means the operation is always done to the newly generated token.

</details> -->

### Key-Value Caching

TLDR - caching can make causal interventions inside `model.generate` more complicated, but if you only intervene on sequence positions other than the very last one. In our exercises, we'll only be intervening on the last seqpos so you don't need to worry about it, but it's still a useful topic to understand.

<details>
<summary>See this dropdown if you're curious for more details.</summary>

To speed up inference, transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) for more on transformer inference arithmetic.

When caching takes place, and we're doing causal interventions, we have to be careful that the caching won't override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention "add the function vector to *only* the final sequence position of the prompt for each token we generate" then we'd have to disable caching (since previous forward passes would contain cached values where we intervened on a sequence position which is no longer the final sequence position). However, here we're performing the intervention "add the function vector to the final token of the original prompt, and to *all subsequent sequence positions*", meaning enabling caching (the default behaviour) will give us the right causal intervention.

</details>

### Generator Output

The object `generator.output` is by default a tensor which contains the model's token ID completions (not the logits).

By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour. But in future exercises we'll experiment with different sampling methods than greedy sampling (which generate uses by default), so `generator.output` and argmaxing over logits will not be identical!

### Exercise - intervene with function vector, in multi-token generation

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 15-30 minutes on this exercise.
> ```

You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template.

We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.

```python
def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template='The word "{x}" means',
    n_tokens: int = 5,
) -> tuple[str, str]:
    """
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        model: LanguageModel
            the transformer you're doing this computation with
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    """
    prompt = prompt_template.format(x=word)

    with model.generate(remote=REMOTE, max_new_tokens=n_tokens, repetition_penalty=1.2) as generator:
        with model.all():

            with generator.invoke(prompt):
                tokens = model.generator.output.save()

            with generator.invoke(prompt):
                model.transformer.h[layer].output[0][0, -1] += fn_vector
                tokens_intervention = model.generator.output.save()

    completion, completion_intervention = tokenizer.batch_decode(
        [tokens.squeeze().tolist(), tokens_intervention.squeeze().tolist()]
    )
    return completion, completion_intervention

```

To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym (you might have to play around a bit with the scale factor of `fn_vector`, to balance between effectiveness and coherence of output). If this works, congratulations - **you've just successfully induced an OOD behavioural change in a 6b-parameter model!**

```python
# Remove word from our pairs, so it can be a holdout
word = "light"
_ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]

# Define our dataset, and the attention heads we'll use
dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

# Extract the function vector
fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model,
    word=word,
    layer=9,
    fn_vector=0.1 * fn_vector,
    prompt_template='The word "{x}" means',
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

### Exercise - generalize results to another task (optional)

> ```yaml
> Difficulty: 🔴🔴🔴🔴⚪
> Importance: 🔵🔵🔵⚪⚪
>
> You should spend up to 15-30 minutes on this exercise.
> ```

In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).

We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.

<details>
<summary>Guidance for exercise</summary>

Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.

When you define the `ICLDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.

You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.

</details>

```python
with open(section_dir / "data/country_capital_pairs.txt", "r", encoding="utf-8") as f:
    COUNTRY_CAPITAL_PAIRS = [line.split() for line in f.readlines()]

country = "Netherlands"
_COUNTRY_CAPITAL_PAIRS = [pair for pair in COUNTRY_CAPITAL_PAIRS if pair[0] != country]

dataset = ICLDataset(_COUNTRY_CAPITAL_PAIRS, size=20, n_prepended=5, bidirectional=False)
head_list = [
    (8, 0),
    (8, 1),
    (9, 14),
    (11, 0),
    (12, 10),
    (13, 12),
    (13, 13),
    (14, 9),
    (15, 5),
    (16, 14),
]

fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model=model,
    word=country,
    layer=9,
    fn_vector=0.05 * fn_vector,
    prompt_template="When you think of {x},",
    n_tokens=40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

# 4️⃣ Steering Vectors in GPT2-XL

> ##### Learning Objectives
>
> * Understand the goals & main results from Alex Turner et al's work on steering vectors
> * Reproduce the changes in behaviour described in their initial post

**Note**: GPT2-XL is not hosted remotely by NNsight at the moment. If you use GPT2-XL, we recommend setting `REMOTE = False`. Otherwise, you can use one of the remotely hosted models (see [here](https://nnsight.net/status/)) and set `REMOTE = True`.

## Steering model behaviour

In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model's completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we'll explore this kind of work in more detail. We'll be primarily using Turner et al's work on [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector).

Summary of the way in which this work differs from the function vector work we've done so far:

* Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).
* Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We'll be focusing on GPT2-XL.
* The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we'll go back to the simpler method of adding directly into the residual stream.

Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of *"finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes & change the model's output"*. This description would also include the following:

* [Inference-time intervention](https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from), which focuses on inducing the behavioural change of "making the model tell the truth". It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.
* [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681), which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

We'll discuss some of this work more in the bonus section, but for now, let's get on with the exercises!

First, we'll load in GPT2-XL, then we'll replicate some of the examples in the main post.

```python
gpt2_xl = LanguageModel("gpt2-xl", device_map="auto", torch_dtype=t.bfloat16)
tokenizer = gpt2_xl.tokenizer

REMOTE = False
# If you are using gpt2_xl, set REMOTE = False as gpt2_xl is not hosted remotely by nnsight. You can
# set REMOTE = True for a remotely hosted model here (https://nnsight.net/status/)
```

### Exercise - replicate the steering vector results

> ```yaml
> Difficulty: 🔴🔴🔴🔴🔴
> Importance: 🔵🔵🔵🔵⚪
>
> You should spend up to 30-50 minutes on this exercise.
> ```

Replicate the results in the LessWrong post [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu); specifically the "demonstrations of additions that work well" section.

Read the "How activation additions work" section of [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#How_activation_additions_work) to understand how vectors are extracted and added. We've provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function `calculate_and_intervene_with_h`), although there will be a few methodological differences.

This is the last exercise in this set, and hopefully it'll provide an opportunity to draw together all the threads of what you've learned so far!

### Caching

This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we're adding a slice of vectors to the first sequence positions of the original prompt (see tables like in [this section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) for an illustration). How do you think this will affect our function? Should we still cache? Should we be using `.generate()` or `.trace()`? If using `.generate()`, do we need to call `model.next()` ?

<details>
<summary>Click this dropdown for answers to the questions above.</summary>

Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:

- We can still use caching (because the values we cache shouldn't be different in subsequent token generations),
- We should be using `.generate()` (because we're doing multi-token generation),
- We don't need to call `model.next()` (because we only intervene once, and our intervention will be cached & applied to all subsequent tokens which are generated).

Again, if any of this is confusing then please ask a TA or message in the Slack channel.

</details>

### Padding

The [tables](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:

1. Right-pad the input sequences manually, i.e. use something like `len(tokenizer.tokenize(prompt))` to see how long each of the prompts is, and add copies of `tokenizer.pad_token` to the end of each sequence.
2. Don't manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.

The solutions use (2), but you can use either of these methods.

### Sampling

Following the post, we'll use top-p sampling with probability 0.3 to generate our sequences. We'll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you've done earlier exercises in this section then you might have implemented `freq_penalty` during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar `repetition_penalty` (default value is 1.0 indicating no penalty, values higher than 1.0 apply a penalty to repeated tokens).

We apply these sampling methods by passing keyword arguments into the `generate` method:

```python
{
    "do_sample": True, # necessary whenever we're sampling rather than doing greedy decoding
    "top_p": 0.3,
    "repetition_penalty": 1.1,
}
```

Note that the sequences are generated stochastically rather than greedily - this means we'll get different results if we input multiple different copies of the same sequence. We've given you the `n_comparisons` argument in the function below, i.e. you should generate this many steered *and* this many unsteered completions.

### Other tips / notes

We recommend starting with example #9 (the "talking about weddings" one). It seems quite robust to the exact conditions of the forward pass, unlike the `Love - Hate` example. You can use any of the template cells we've given you below.

We've given you a `use_bos` argument; if this is True then you should append `tokenizer.bos_token` to the start of all the prompts. This is just to be true to the LessWrong post's implementation; it won't change behaviour much and you can probably ignore it and still get good results.

```python
SAMPLING_KWARGS = {
    "do_sample": True,
    "top_p": 0.3,
    "repetition_penalty": 1.2,
}

def calculate_and_apply_steering_vector(
    model: LanguageModel,
    prompt: str,
    activation_additions: list[tuple[int, float, str]],
    n_tokens: int,
    n_comparisons: int = 1,
    use_bos: bool = True,
) -> tuple[list[str], list[str]]:
    """
    Performs the steering vector experiments described in the LessWrong post.

    Args:
        model: LanguageModel
            the transformer you're doing this computation with
        prompt: str
            The original prompt, which we'll be doing activation steering on.

        activation_additions: list[tuple[int, float, str]], each tuple contains:
            layer - the layer we're applying these steering vectors to
            coefficient - the value we're multiplying it by
            prompt - the prompt we're inputting
            e.g. activation_additions[0] = [6, 5.0, " Love"] means we add the " Love" vector at layer 6, scaled by 5x

        n_tokens: int
            Number of tokens which will be generated for each completion

        n_comparisons: int
            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and
            the same number which are steered).

    Returns:
        unsteered_completions: list[str]
            List of length `n_comparisons`, containing all the unsteered completions.

        steered_completions: list[str]
            List of length `n_comparisons`, containing all the steered completions.
    """
    # Add the BOS token manually, if we're including it
    if use_bos:
        bos = model.tokenizer.bos_token
        prompt = bos + prompt
        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]

    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same
    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)
    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]
    assert len(set(act_add_seq_lens)) == 1, "All activation addition prompts must be the same length."
    assert act_add_seq_lens[0] <= len(
        tokenizer.tokenize(prompt)
    ), "All act_add prompts should be shorter than original prompt."

    # Get the prompts we'll intervene on (unsteered and steered)
    steered_prompts = [prompt for _ in range(n_comparisons)]
    unsteered_prompts = [prompt for _ in range(n_comparisons)]

    with model.generate(max_new_tokens=n_tokens, remote=REMOTE, **SAMPLING_KWARGS) as generator:
        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations
        with generator.invoke(act_add_prompts):
            # Get all the prompts from the activation additions, and put them in a list
            # (note, we slice from the end of the sequence because of left-padding)
            act_add_vectors = [
                model.transformer.h[layer].output[0][i, -seq_len:]
                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))
            ]

        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)
        with generator.invoke(unsteered_prompts):
            unsteered_out = model.generator.output

        # Forward pass on steered prompts (we add in the results from the act_add prompts)
        with generator.invoke(steered_prompts):
            # For each act_add prompt, add the vector to residual stream, at the start of the sequence
            for i, (layer, coeff, seq_len) in enumerate(zip(act_add_layers, act_add_coeffs, act_add_seq_lens)):
                model.transformer.h[layer].output[0][:, :seq_len] += coeff * act_add_vectors[i]
            steered_out = model.generator.output

    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results
    unsteered_completions = tokenizer.batch_decode(unsteered_out[-n_comparisons:])
    steered_completions = tokenizer.batch_decode(steered_out[-n_comparisons:])

    return unsteered_completions, steered_completions

```

To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust).

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I hate you because",
    activation_additions=[(6, +5.0, "Love "), (6, -5.0, "Hate")],
    n_tokens=50,
    n_comparisons=3,
    use_bos=True,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="I went up to my friend and said",
    activation_additions=[
        (20, +4.0, "I talk about weddings constantly  "),
        (20, -4.0, "I do not talk about weddings constantly"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

```python
unsteered_completions, steered_completions = calculate_and_apply_steering_vector(
    gpt2_xl,
    prompt="To see the eiffel tower, people flock to",
    activation_additions=[
        (24, +10.0, "The Eiffel Tower is in Rome"),
        (24, -10.0, "The Eiffel Tower is in France"),
    ],
    n_tokens=50,
    n_comparisons=3,
    use_bos=False,
)

table = Table("Unsteered", "Steered", title="Completions", show_lines=True)
for usc, sc in zip(unsteered_completions, steered_completions):
    table.add_row(usc, sc)
rprint(table)
```

# ☆ Bonus

## Extensions of the Function Vectors Paper

There are two other interesting results from the paper, although neither of them are as important as the ones we've covered so far. If you have time, you can try to reproduce these results yourself.

### The Decoded Vocabulary of Function Vectors (3.2)

In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:

* For the antonyms task, the top words evoke the idea of antonyms, e.g. `" negate"`, `" counterpart"`, `" lesser"`.
* For the country-capitals task, the top words are actually the names of capitals, e.g. `" Moscow"`, `" Paris"`, `" Madrid"`.

Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?

An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?

```python
# YOUR CODE HERE - find the decoded vocabulary
```

<details>
<summary>My results for this (spoiler!)</summary>

In the Country-Capitals task, I found:

* The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.
* The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Top logits:
' London'
' Moscow'
' Madrid'
' Budapest'
' Athens'
' Paris'
' Berlin'
' Bangkok'
' Istanbul'
' Montreal'
' Barcelona'
' Jerusalem'
' Seoul'
' Miami'
' Dublin'
' Atlanta'
' Copenhagen'
' Mumbai'
' Minneapolis'
' Beijing'</pre>

</details>

<details><summary>Solution</summary>

```python
# Code to calculate decoded vocabulary:
logits = model._model.lm_head(fn_vector)
max_logits = logits.topk(20).indices.tolist()
tokens = model.tokenizer.batch_decode(max_logits)
print("Top logits:\n" + "\n".join(map(repr, tokens)))
```
</details>

### Vector Algebra on Function Vectors (3.3)

In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?

The authors test this on a variety of different tasks. They find that it's effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn't as effective as function vectors. Do you get these same results?

## Extensions of the Steering Vectors Post

We only implemented one small subset of the results from the steering vectors post (and did it in a fairly slap-dash way). But there are many others you can play around with. For example:

* The authors note that they were unsuccessful in finding a "speak in French" vector. One of the top comments on the LessWrong post describes a process they used to create a French vector which happened to work (link to comment [here](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector?commentId=sqsS9QaDy2bG83XKP)). Can you replicate their results? (They also linked a Colab in this comment, which can help if you're stuck.)
* In a [later section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Perplexity_on_lots_of_sentences_about_weddings_or_about_shipping) of the paper, the authors extensively discuss perplexity (a measure which is related to entropy). They find that the "weddings" vector reduces perplexity on wedding-related sentences, and maintains perplexity on unrelated sentences. Can you replicate their results - in particular, their graph of perplexity ratios against injection layers for wedding and non-wedding-related sentences?
* The authors wrote up the post into a full paper, which you can find [here](https://arxiv.org/abs/2308.10248). Can you replicate some of the extra results in this paper?

## Suggested paper replications

### [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)

In this paper, the authors focus on inducing the behavioural change of "making the model tell the truth". They also look at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.

This might be a good replication for you if:

* You enjoyed the exercises in this section, but are also interested in experimenting with techniques which weren't covered in this section (e.g. linear probing),
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in studying [model truthfulness](https://arxiv.org/abs/2109.07958).

### [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681)

This paper can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in [evaluating models](https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written) on traits like myopia, power seeking, etc,
* You're comfortable doing prompt-engineering, and working with large datasets (like the ones linked above).

*Update* - there is now a [LessWrong post](https://www.lesswrong.com/posts/v7f8ayBxLhmMFRzpa/steering-llama-2-with-contrastive-activation-additions) associated with this paper, which also briefly discusses related areas. We strongly recommend reading this post if you're interested in this replication, or any of the other suggested replications in this section.

### [Red-teaming language models via activation engineering](https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering)

This work, done by Nina Rimsky, extends the results from much of the work we've seen previously, but applied to the domain of **refusal** - what determines whether the LLM will refuse to answer your request, and how can you affect this behaviour? From her post:

> *Validating if finetuning and RLHF have robustly achieved the intended outcome is challenging ... We can try to trigger unwanted behaviors in models more efficiently by manipulating their internal states during inference rather than searching through many inputs. The idea is that if a behavior can be easily triggered through techniques such as activation engineering, it may also occur in deployment. The inability to elicit behaviors via small internal perturbations could serve as a stronger guarantee of safety.*

This might be a good replication for you if:

* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context,
* You're comfortable working with very large models, possibly via the `nnsight` library,
* You're interested in RLHF, adversarial attacks and jailbreaking,
* You're comfortable doing prompt-engineering (although some of the data you'd need for this replication is available on Nina's [GitHub repo](https://github.com/nrimsky/LM-exp/tree/main)).

<br>

---

<br>

---

# getting.ipynb

# Getting Values

Hidden states are exposed by accessing the desired module and calling its `.input` or `.output` attributes.

Once accessed, you call `.save()` on it so it's value is populated and not deleted after.

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace("The Eiffel Tower is in the city of") as tracer:

    hidden_states = model.transformer.h[-1].output[0].save()
```

After exiting the tracing context, the `.value` attribute of the `hidden_states` object will be populated.

```python
print(hidden_states)
```

---

# gradients.ipynb

# Gradients

There are a couple of ways we can interact with the gradients during and after a backward pass.

In the following example, we save the hidden states of the last layer and do a backward pass on the sum of the logits.

Note two things:

1. `requires_grad=True` by default.
2. We can all `.backward()` on a value within the tracing context just like you normally would.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel("openai-community/gpt2", device_map="auto")
```

```python
with model.trace("Hello World") as tracer:

    hidden_states = model.transformer.h[-1].output[0].save()

    logits = model.output.logits

    logits.sum().backward()

print(hidden_states)
```

If we wanted to see the gradients for the hidden_states, we can call `.retain_grad()` on it and access the `.grad` attribute after execution.

```python
with model.trace("Hello World") as tracer:

    hidden_states = model.transformer.h[-1].output[0].save()
    hidden_states_grad = model.transformer.h[-1].output[0].grad.save()

    model.output.logits.sum().backward()

print(hidden_states)
print(hidden_states_grad)
```

Even better, `nnsight` also provides proxy access into the backward process via the `.grad` attribute on proxies. This works just like  `.input` and `.output` where operations , including getting and setting, are traced and performed on the model at runtime. (assuming it's a proxy of a Tensor, as this calls `.register_hook(...)` on it!)

The following examples demonstrate ablating (setting to zero) the gradients for a hidden state in GPT-2. The first example is an in-place operation and the second swaps the gradient out for a new tensor of zeroes.

```python
with model.trace("Hello World") as tracer:
    hidden_states = model.transformer.h[-1].output[0].save()

    hidden_states_grad_before = hidden_states.grad.clone().save()
    hidden_states.grad[:] = 0
    hidden_states_grad_after = hidden_states.grad.save()

    logits = model.output.logits

    logits.sum().backward()

print("Before", hidden_states_grad_before)
print("After", hidden_states_grad_after)
```

```python
with model.trace("Hello World") as tracer:
    hidden_states = model.transformer.h[-1].output[0].save()

    hidden_states_grad_before = hidden_states.grad.clone().save()
    hidden_states.grad = torch.zeros(hidden_states.grad.shape).to(hidden_states.grad.device)
    hidden_states_grad_after = hidden_states.grad.save()

    logits = model.output.logits

    logits.sum().backward()

print("Before", hidden_states_grad_before)
print("After", hidden_states_grad_after)
```

---

# iterator.ipynb

# Iterative Interventions

NNsight's <b> iterator context </b> allows us to run an intervention loop at scale. It iteratively executes and updates a single intervention graph.

Use a `session` to define the Iterator context and pass in a sequence of items that you want to loop over at each iteration:

```python
import nnsight
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.session() as session:

  with session.iter([0, 1, 2]) as item:
    # define intervention body here ...

    with model.trace("_"):
      # define interventions here ...
      pass

    with model.trace("_"):
      # define interventions here ...
      pass
```

The Iterator context extends all the nnsight graph-based functionalities, but also closely mimics the conventional `for` loop statement in Python, which allows it to support all kind of iterative operations with a use of `as item` syntax.

Beyond specifying iteration indices, you can also loop across an NNsight list object (`nnsight.list()`).

```python
with model.session() as session:

  li = nnsight.list() # an NNsight built-in list object
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

`nnsight 0.4` introduces support for native Python for loops within a tracer context at scale!

*NOTE: inline for loops (i.e., `[x for x in <Proxy object>]`) are not currently supported.*

```python
# New: Using Python for loops for iterative interventions
with model.session() as session:

    li = nnsight.list()
    [li.append([num]) for num in range(0, 3)]
    li2 = nnsight.list().save()

    # Using regular for loops
    for item in li:
        for item_2 in item: # for loops can be nested!
            li2.append(item_2)

print("\nList: ", li2)
```

## Considerations

If you would like to turn off NNsight's support of native `for` loops, you can apply the following changes to `nnsight.CONFIG`

This will not affect any of NNsight's `.iter()` functionality.

```python
# Turn off support if/for statements within tracing context.
import nnsight

nnsight.CONFIG.APP.CONTROL_FLOW_HANDLING = False
nnsight.CONFIG.save()
```

---

# logit_lens.ipynb

# Logit Lens

## Introduction

🔍 Logit Lens is a powerful tool that grants us a simplified (yet insightful) understanding of the inner workings of transformer models.

We can estimate the model's guess for the output after each computational step by applying a softmax function to each layer's output. Unlike traditional approaches focusing on *how* beliefs are updated within a step, with Logit Lens we gain a glimpse into *what* output the model is predicting at each processing step.

📗 Read more about Logit Lens from nostalgebraist’s blog post on LessWrong, [here](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)

💻 You can find a Colab version of our tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/logit_lens.ipynb), or nostalgebraist’s original code [here](https://colab.research.google.com/drive/1-nOE-Qyia3ElM17qrdoHAtGmLCPUZijg?usp=sharing)

## Setup

If using Colab, install NNsight:
```
!pip install -U nnsight
```

```python
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
```

Import libraries and load GPT-2 model.

```python
# Import libraries
from IPython.display import clear_output
from nnsight import LanguageModel
from typing import List, Callable
import torch
import numpy as np
from IPython.display import clear_output

clear_output()
```

```python
# Load gpt2
model = LanguageModel("openai-community/gpt2", device_map="auto", dispatch=True)
```

## GPT-2 Model Architecture

Let's take a look at GPT-2's architecture. GPT-2 has 12 layers, accessed as `model.transformer.h`.

```python
print(model)
```

## Apply Logit Lens

To apply logit lens, we collect activations at each layer's output, apply layer normalization (`model.transformer.ln_f`), and then process through the model's head (`model.lm_head`) to get the logits. Next, we apply the softmax to the logits to obtain output token probabilities.

By observing different layers' output token probabilities, logit lens provides insights into the model's confidence throughout processing steps.

```python
prompt= "The Eiffel Tower is in the city of"
layers = model.transformer.h
probs_layers = []

with model.trace() as tracer:
    with tracer.invoke(prompt) as invoker:
        for layer_idx, layer in enumerate(layers):
            # Process layer output through the model's head and layer normalization
            layer_output = model.lm_head(model.transformer.ln_f(layer.output[0]))

            # Apply softmax to obtain probabilities and save the result
            probs = torch.nn.functional.softmax(layer_output, dim=-1).save()
            probs_layers.append(probs)

probs = torch.cat([probs.value for probs in probs_layers])

# Find the maximum probability and corresponding tokens for each position
max_probs, tokens = probs.max(dim=-1)

# Decode token IDs to words for each layer
words = [[model.tokenizer.decode(t.cpu()).encode("unicode_escape").decode() for t in layer_tokens]
    for layer_tokens in tokens]

# Access the 'input_ids' attribute of the invoker object to get the input words
input_words = [model.tokenizer.decode(t) for t in invoker.inputs[0][0]["input_ids"][0]]
```

## Visualizing GPT-2 Layer Interpretations

Now we will visualize the prediction of the GPT-2 model while processing the string *`'The Eiffel Tower is in the city of'`* and we’ll explore the interpretations of each layer within the GPT2Block, gaining insights into what each layer believes could be the next word for every input word.

```python
import plotly.express as px
import plotly.io as pio

if is_colab:
    pio.renderers.default = "colab"
else:
    pio.renderers.default = "plotly_mimetype+notebook_connected+notebook"

fig = px.imshow(
    max_probs.detach().cpu().numpy(),
    x=input_words,
    y=list(range(len(words))),
    color_continuous_scale=px.colors.diverging.RdYlBu_r,
    color_continuous_midpoint=0.50,
    text_auto=True,
    labels=dict(x="Input Tokens", y="Layers", color="Probability")
)

fig.update_layout(
    title='Logit Lens Visualization',
    xaxis_tickangle=0
)

fig.update_traces(text=words, texttemplate="%{text}")
fig.show()
```

The vertical axis indexes the layers, zero-indexed from 0 to 11. The top guess for each token, according to the model’s activations at a given layer, is printed in each cell. The colors show the probability associated with the top guess.

---

# lora_training.ipynb


---

# model_editing.ipynb

# Model Editing

NNsight's model editing feature allows you to create persistently modified versions of a model with a use of `.edit()`. Unlike interventions in a tracing context, which are temporary, the **Editor** context enables you to make lasting changes to a model instance.

This feature is useful for:
* Creating modified model variants without altering the original
* Applying changes that persist across multiple forward passes
* Comparing interventions between original and edited models

Let's explore how to use the **Editor** context to make a simple persistent change to a model:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

# we take the hidden states with the expected output "Paris"
with model.trace("The Eiffel Tower is located in the city of") as tracer:
    hs11 = model.transformer.h[11].output[0][:, -1, :].save()

# the edited model will now always predict "Paris" as the next token
with model.edit() as model_edited:
    model.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with model.trace("Vatican is located in the city of") as tracer:
    original_tokens = model.lm_head.output.argmax(dim=-1).save()

# ...with the output of the edited model
with model_edited.trace("Vatican is located in the city of") as tracer:
    modified_tokens = model.lm_head.output.argmax(dim=-1).save()

print("\nOriginal Prediction: ", model.tokenizer.decode(original_tokens[0][-1]))
print("Modified Prediction: ", model.tokenizer.decode(modified_tokens[0][-1]))
```

Edits defined within an **Editor** context create a new, modified version of the model by default, preserving the original. This allows for safe experimentation with model changes. If you wish to modify the original model directly, you can set `inplace=True` when calling `.edit()`.

Use this option cautiously, as in-place edits alter the base model for all the consequent model calls.

```python
# we use the hidden state we saved above (hs11)
with model.edit(inplace=True) as model_edited:
    model.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with model.trace("Vatican is located in the city of") as tracer:
    modified_tokens = model.lm_head.output.argmax(dim=-1).save()

print("Modified In-place: ", model.tokenizer.decode(modified_tokens[0][-1]))
```

If you've made in-place edits to your model and need to revert these changes, `.clear_edits()` can help. This method removes all edits applied to the model, effectively restoring it to its original state.

```python
model.clear_edits()

with model.trace("Vatican is located in the city of"):
    modified_tokens = model.lm_head.output.argmax(dim=-1).save()

print("Edits cleared: ", model.tokenizer.decode(modified_tokens[0][-1]))
```

---

# modules.ipynb

# Modules

We can also apply modules in the model's module tree at any point during computation, even if they are out of order.

Here, we get the hidden states of the last layer like usual. We also chain apply `model.transformer.ln_f` and `model.lm_head` in order to "decode" the hidden states into the vocabulary space. Applying softmax and then argmax then transformz the vocabulary space hidden states into tokens that we can decode with the tokenizer.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel("openai-community/gpt2", device_map='auto')

with model.trace('The Eiffel Tower is in the city of') as tracer:

    hidden_states = model.transformer.h[-1].output[0]

    hidden_states = model.lm_head(model.transformer.ln_f(hidden_states)).save()

    tokens = torch.softmax(hidden_states, dim=2).argmax(dim=2).save()
```

The output looks like:

```python
print(hidden_states)
print(tokens)
print(model.tokenizer.decode(tokens[0]))
```

---

# multiple_token.ipynb

# Multiple Token Generation

When generating more than one token, use `<module>.next()` to denote following interventions should be applied to the subsequent generations for that module.

Here we generate three tokens and save the hidden states of the last layer for each one:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')
```

## `.generate()`

NNsight's `LanguageModel` class supports multiple token generation with `.generate()`. You can control the number of new tokens generated by setting `max_new_tokens = N` within your call to `.generate()`.

```python
prompt = 'The Eiffel Tower is in the city of'
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    out = model.generator.output.save()

decoded_prompt = model.tokenizer.decode(out[0][0:-n_new_tokens].cpu())
decoded_answer = model.tokenizer.decode(out[0][-n_new_tokens:].cpu())

print("Prompt: ", decoded_prompt)
print("Generated Answer: ", decoded_answer)
```

## `.next()`

When generating more than one token, use `<module>.next()` to denote following interventions should be applied to the subsequent generations for that module.

Here we generate three tokens and save the hidden states of the last layer for each one:

```python
n_new_tokens = 3
with model.generate('The Eiffel Tower is in the city of', max_new_tokens=n_new_tokens) as tracer:

    hidden_states1 = model.transformer.h[-1].output[0].save()

    hidden_states2 = model.transformer.h[-1].next().output[0].save()

    hidden_states3 = model.transformer.h[-1].next().output[0].save()

    out = model.generator.output.save()
```

Note how calling save before `tracer.next()` returns the hidden state across the initial prompt while calling save after returns the hidden state of each subsequent generated token.

```python
print(hidden_states1.shape) # hidden states across prompt & first generated token
print(hidden_states2.shape) # only hidden states across next token
print(hidden_states3.shape) # only hidden states across next token
print(out) # model output tokens, including prompt
```

Great, we've now successfully stored hidden states across multiple different rounds of token generation! However, if you're generating many tokens while applying interventions, using `.next()` requires you to set a loop within the tracing context, which can be clunky:

```python
# Old approach:
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 50
hidden_states = []
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    for i in range(n_new_tokens):
        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output.save())

        # Move to next generated token
        layers[0].next()

print("Hidden state length: ",len(hidden_states))
```

## `.all()` streamlines interventions on many generated tokens

With `nnsight 0.4` you can use `.all()` to recursively apply interventions to a model. Calling `.all()` on a module within a model will recursively apply its `.input` and `.output` across all iterations. Previously, we'd need to loop across each new generated token, saving the intervention for every generated token and calling `.next()` to move forward, as demonstrated in the previous section.

Let's try using `.all()` to streamline the multiple token generation process. We simply call `.all()` on the module where we are applying the intervention (in this case GPT-2's layers), apply our intervention, and append our hidden states (stored in an `nnsight.list()` object).

```python
import nnsight
# using .all():
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 50
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() to apply intervention to each new token
    with layers.all():

        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output) # no need to call .save
        # Don't need to loop or call .next()!

print("Hidden state length: ",len(hidden_states))
```

Easy! Note that because `.all()` is recursive, it will only work to append outputs called on children of the module that `.all()` was called on. See example below for more information. TL;DR: apply `.all()` on the highest-level accessed module if interventions and outputs have different hierarchies within model structure.

<details>
<summary>Recursive properties of .all()</summary>

`.all()` recursively acts on model components. In the below code example, only the first token generation is saved, because `.all()` applied to `layers`, while the saved variable `hidden_states` is produced from `model.lm_head`, which is not a child of `layers`.

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on layers
    with layers.all():

        # Apply same intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(model.lm_head.output) # no need to call .save, it's already initialized

print("Hidden state length: ",len(hidden_states)) # length is 1, meaning it only saved the first token generation
```

If you want to apply an intervention during multiple token generation while saving the state of a model component that isn't a child of that module, you can instead apply `.all()` to the full model:

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on model
    with model.all():

        # Apply same intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(model.lm_head.output) # no need to call .save

print("Hidden state length: ",len(hidden_states)) # length is 3, as expected!
```

</details>

---

# operations.ipynb

# Operations

Most basic operations and torch operations work on proxies and are added to the computation graph.

In this example we get the sum of the hidden states and add them to the hidden_states themselves (for whatever reason). By saving the various steps, we can see how the values change.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace('The Eiffel Tower is in the city of') as tracer:

    hidden_states_pre = model.transformer.h[-1].output[0].save()

    hs_sum = torch.sum(hidden_states_pre).save()

    hs_edited = hidden_states_pre + hs_sum

    hs_edited = hs_edited.save()
```

```python
print(hidden_states_pre)
print(hs_sum)
print(hs_edited)
```

---

# remote_execution.ipynb

# Remote Execution

To access remote models, `NDIF` requires you to receive an API key. To get one, simply
go to https://login.ndif.us and sign up.

With a valid API key, you then can configure `nnsight` by doing the following:

```python
from nnsight import CONFIG

CONFIG.set_default_api_key("YOUR_API_KEY")
```

This only needs to be run once as it will save this api key as the default in a
config file along with the `nnsight` installation.

Let's demonstrate using `nnsight`'s tracing
context with one of the larger open source language models, `Llama-3.1-70b`!

```python
import os

# llama3.1 70b is a gated model and you need access via your huggingface token
os.environ['HF_TOKEN'] = "YOUR_HUGGING_FACE_TOKEN"
```

```python
from nnsight import LanguageModel
# We'll never actually load the parameters so no need to specify a device_map.
llama = LanguageModel("meta-llama/Meta-Llama-3.1-70B")

# All we need to specify using NDIF vs executing locally is remote=True.
with llama.trace("The Eiffel Tower is in the city of", remote=True) as runner:

    hidden_states = llama.model.layers[-1].output.save()

    output = llama.output.save()

print(hidden_states)

print(output["logits"])
```

It really is as simple as `remote=True`! All of the techniques available in NNsight locally work just the same when running remotely.

# Remote Model Considerations & System Limits
To view currently hosted models, please visit https://nnsight.net/status/. All models except for `meta-llama/Meta-Llama-3.1-405B` and `meta-llama/Meta-Llama-3.1-405B-Instruct` are currently available for public access. If you are interested in running an experiment on Llama 405b, please reach out to us at [info@ndif.us](mailto:info@ndif.us)
.

Our system is currently actively in development, so please be prepared for system outages, updates, and wait times. NDIF is running on [DeltaAI](https://delta.ncsa.illinois.edu/deltaai-allocations/), so our services will be down during any of their planned and unplanned outages.

We currently have some rate-limiting and timeouts in place to ensure equitable model access between users.

- Maximum Request Rate: 2 requests/minute
- Maximum Job Run Time: 1 hour

Jobs violating these parameters will be automatically denied or aborted. Please plan your experiments accordingly. You can also reach out to our team at [info@ndif.us](mailto:info@ndif.us) if you have a special research case and would like to request any changes!

---

# scan_validate.ipynb

# Scan and Validate

Have you encountered a situation where you are changing the tensor values in the intervention code and getting an error message that is not very helpful?

This is where "Scanning" and "Validating" can help. As the name suggests, these features help you scan the shapes of the tensors throughout the model and validate that the current tensor shapes are compatible with the model.

We can enable these helpful tools by setting the `scan=True` and `validate=True` flags in the `trace` method.

Here is an example that demonstrates how **Scan** and **Validate** can help us debug the model:

```python
from nnsight import LanguageModel

model = LanguageModel('openai-community/gpt2', device_map='auto')

input = "The Eiffel Tower is in the city of"
number_of_tokens = len(model.tokenizer.encode(input))

# turn on scan and validate
with model.trace(input, scan=True, validate=True):

    original_output = model.transformer.h[11].output[0].clone().save()

    # we want to modify the hidden states for the last token
    model.transformer.h[11].output[0][:, number_of_tokens, :] = 0

    modified_output = model.transformer.h[11].output[0].save()

print("\nOriginal Output: ", original_output[0][-1])
print("Modified Output: ", modified_output[0][-1])
```

Ah of course, we needed to index at `number_of_tokens - 1` not `number_of_tokens`.

How was `nnsight` able to catch this error?

If `scan` and `validate` are enabled, our input is run though the model, but under its own "fake" context. This means the input makes its way through all of the model operations, allowing `nnsight` to record the shapes and data types of module inputs and outputs! The operations are never executed using tensors with real values so it doesn't incur any memory costs. Then, when creating proxy requests like the setting one above, `nnsight` also attempts to execute the request on the "fake" values we recorded.

"Scanning" is what we call running "fake" inputs throught the model to collect
information like shapes and types. "Validating" is what we call trying to
execute the intervention proxies with "fake" inputs to see if they work.
"Validating" is dependent on "Scanning" to work correctly, so we need to run the scan of the model at least once to debug with validate.

<details>
<summary>A word of caution</summary>

---

Some pytorch operations and related libraries don't work well with fake tensors

If you are doing anything in a loop where efficiency is important, you should keep scanning and validating off. It's best to use them only when debugging or when you are unsure if your intervention will work.

---

</details>

Let's try again with the correct indexing, and view the shape of the output
before leaving the tracing context:

```python
with model.trace(input, scan=True, validate=True):

    original_output = model.transformer.h[11].output[0].clone().save()

    # we want to modify the hidden states for the last token
    model.transformer.h[11].output[0][:, number_of_tokens-1, :] = 0

    modified_output = model.transformer.h[11].output[0].save()

print("\nOriginal Output: ", original_output[0][-1])
print("Modified Output: ", modified_output[0][-1])
```

We can also just replace proxy inputs and outputs with tensors of the same shape
and type. Let's use the shape information we have at our disposal to add noise
to the output, and replace it with this new noised tensor:

```python
with model.scan(input):

    dim = model.transformer.h[11].output[0].shape[-1]

print(dim)
```

---

# sessions.ipynb

# Sessions

NDIF uses a queue to handle concurrent requests from multiple users. To optimize the execution of our experiments we can use the `session` context to efficiently package multiple interventions together as one single request to the server.

This offers the following benefits:
1) All interventions within a session will be executed one after another without additional wait in the queue
2) All intermediate outputs of each intervention are stored on the server and can be accessed by other interventions in the same session without moving the data back and forth between NDIF and the local machine.

Let's take a look:

```python
from nnsight import CONFIG
import os

# we are using Llama model remotely hosted on NDIF servers
CONFIG.set_default_api_key("YOUR_API_KEY")
os.environ['HF_TOKEN'] = "YOUR_HUGGING_FACE_TOKEN"
```

```python
from nnsight import LanguageModel
model = LanguageModel("meta-llama/Meta-Llama-3.1-70B")
```

```python
with model.session(remote=True) as session:

  with model.trace("The Eiffel Tower is in the city of") as t1:
    # capture the hidden state from layer 11 at the last token
    hs_79 = model.model.layers[79].output[0][:, -1, :] # no .save()
    t1_tokens_out = model.lm_head.output.argmax(dim=-1).save()

  with model.trace("Buckingham Palace is in the city of") as t2:
    model.model.layers[1].output[0][:, -1, :] = hs_79[:]
    t2_tokens_out = model.lm_head.output.argmax(dim=-1).save()

print("\nT1 - Original Prediction: ", model.tokenizer.decode(t1_tokens_out[0][-1]))
print("T2 - Modified Prediction: ", model.tokenizer.decode(t2_tokens_out[0][-1]))
```

In the example above, we are interested in replacing the hidden state of a later layer with an earlier one. Since we are using a `session`, we don't have to save the hidden state from Tracer 1 to reference it in Tracer 2.

It is important to note that all the traces defined within the `session` context are executed sequentially, strictly following the order of definition (i.e. `t2` being executed after `t1` and `t3` after `t2` etc.).

The `session` context object has its own methods to log values and be terminated early.

```python
import nnsight
with model.session(remote=True) as session:

  nnsight.log("-- Early Stop --")
  nnsight.stop

```

In addition to the benefits mentioned above, the `session` context also enables interesting experiments not possible with other `nnsight` tools - since every trace is run on its own model, it means that within one session we can run interventions between different models – for example, we can swap activations between vanilla and instruct versions of the Llama model and compare the outputs. And `session` can also be used to run experiments entirely locally!

---

# setting.ipynb

# Setting Values

We often not only want to see whats happening during computation, but intervene and edit the flow of information.

In this example, we create a tensor of noise to add to the hidden states. We then add it, use the assigment `=` operator to update the tensors of `.output[0][:]` with these new noised values.

```python
from nnsight import LanguageModel
import torch

model = LanguageModel('openai-community/gpt2', device_map='auto')

with model.trace('The Eiffel Tower is in the city of') as tracer:

    hidden_states_pre = model.transformer.h[-1].output[0].clone().save()

    noise = (0.001**0.5)*torch.randn(hidden_states_pre.shape)

    # model.transformer.h[-1].output = (hidden_states_pre + noise, model.transformer.h[-1].output[1])
    model.transformer.h[-1].output[0][:] = hidden_states_pre + noise

    hidden_states_post = model.transformer.h[-1].output[0].save()
```

We can see the change in the results:

```python
print(hidden_states_pre)
print(hidden_states_post)
```

---

# start_remote_access.ipynb

# Access LLMs with NDIF and NNsight

* [NDIF](https://ndif.us/) is an inference service hosting large open-weight LLMs for use by researchers.
* [NNsight](https://nnsight.net/) is a package for interpreting and manipulating internals of deep learning models.

Together, NDIF and NNsight work hand in hand to let researchers run complex experiments on huge open models easily with full transparent access.

[Run an interactive version of this walkthrough in Google Colab](https://colab.research.google.com/github/ndif-team/ndif-website/blob/onboarding-fixes/public/notebooks/NDIFGetStarted.ipynb)

# Install NNsight

To start using NNsight, you can install it via `pip`.

```python
!pip install nnsight

from IPython.display import clear_output
clear_output()
```

# Sign up for NDIF remote model access

In order to remotely access LLMs through NDIF, users must sign up for an NDIF API key.

## **[Register here](https://login.ndif.us/) for a free API key!**

Once you have a valid NDIF API key, you then can configure `nnsight` by doing the following:

```python
from nnsight import CONFIG

CONFIG.API.APIKEY = input("Enter your API key: ")
clear_output()
```

<details>
<summary>
More about API key configuration
</summary>

The above code saves your API key as the default in a config file along with the `nnsight` installation. If you're running this walkthrough using a local Python installation, this only needs to be run once. If you're using Colab, we recommend saving your API key as a Colab Secret, and configuring it as follows in your notebooks:

```
from nnsight import CONFIG

if is_colab:
    # include your NNsight API key on Colab secrets
    from google.colab import userdata
    NDIF_API = userdata.get('NDIF_API')
    CONFIG.set_default_api_key(NDIF_API)
```

</details>

# Choose a Model

NDIF hosts multiple LLMs, including various sizes of the Llama 3.1 models and DeepSeek-R1 models. **You can view the full list of hosted models on [our status page](https://nnsight.net/status/).** All of our models are open for public use, except you need to apply for access to the Llama-3.1-405B models.

<details>
<summary>
Apply for 405B access
</summary>

If you have a clear research need for Llama-3.1-405B and would like more details about applying for access, please refer to [this page](https://ndif.us/405b.html)!

</details>

For these exercises, we will explore how we can access and modify the Llama-3.1-70B model's internal states. This 70-billion-parameter model is about the maximum size that you could run on a single A100 GPU with 80GB of VRAM, but we are going to access it remotely on NDIF resources, so you can run it on Colab or your laptop computer!

<details>
<summary>
Note: Llama models are gated on HuggingFace
</summary>

Llama models are gated and require you to register for access via HuggingFace. [Check out their website for more information about registration with Meta](https://huggingface.co/meta-llama/Llama-3.1-70B).

If you are using a local Python installation, you can activate your HuggingFace token using the terminal:

`huggingface-cli login -token YOUR_HF_TOKEN`

If you are using Colab, you can add your HuggingFace token to your Secrets.

</details>

We will be using the `LanguageModel` subclass of NNsight to load in the Llama-3.1-70B model and access its internal states.

<details>
<summary>
About NNsight LanguageModel
</summary>

The `LanguageModel` subclass of NNsight is a wrapper that includes special support for HuggingFace language models, including automatically loading models from a HuggingFace ID together with the appropriate tokenizer.

This way there's no need to pretokenize your input, and instead you can just pass a string as an input!

*Note: `LanguageModel` models also accept tokenized inputs, including [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating).*
</details>

```python
# instantiate the model using the LanguageModel class
from nnsight import LanguageModel

# don't worry, this won't load locally!
llm = LanguageModel("meta-llama/Meta-Llama-3.1-70B", device_map="auto")

print(llm)
```

# Access model internals

Now that we've installed `nnsight`, configured our API key, and instantiated a model, we can run an experiment.

For this experiment, let's try grabbing some of the LLM's hidden states using `nnsight`'s tracing context, `.trace()`.

Entering the tracing context allows us to customize how a neural network runs. By calling `.trace()`, we are telling the model to run with a given input and to collect and/or modify the internal model states based on user-defined code within the tracing context. We can also specify that we want to use an NDIF-hosted model instead of executing locally by setting `remote=True`.

To get started, let's ask NNsight to collect the layer output (known as "logits") at the final layer, along with the overall model output. NNsight needs to know what specific parts of the model we're interested in accessing later, so we need to specify which elements we'd like to save after exiting the tracing context using `.save()`.

*Note: You will not be able to access any values defined within a `.trace()` that aren't saved with `.save()` after exiting the tracing context!*

```python
# remote = True means the model will execute on NDIF's shared resources
with llm.trace("The Eiffel Tower is in the city of", remote=True):

    # user-defined code to access internal model components
    hidden_states = llm.model.layers[-1].output[0].save()
    output = llm.output.save()

# after exiting the tracing context, we can access any values that were saved
print("Hidden State Logits: ",hidden_states[0])

output_logits = output["logits"]
print("Model Output Logits: ",output_logits[0])

# decode the final model output from output logits
max_probs, tokens = output_logits[0].max(dim=-1)
word = [llm.tokenizer.decode(tokens.cpu()[-1])]
print("Model Output: ", word[0])
```

What are we seeing here? NNsight tells you if your job is recieved, approved, running, or completed via logs.

<details>
<summary>
Disabling remote logging notifications
</summary>
If you prefer, you can disable NNsight remote logging notifications with the following code, although they can help troubleshoot any network issues.

```
from nnsight import CONFIG
CONFIG.APP.REMOTE_LOGGING = False
```

If you'd like to turn them back on, just set `REMOTE_LOGGING = True`:
```
from nnsight import CONFIG
CONFIG.APP.REMOTE_LOGGING = True
```
</details>

We are also seeing our printed results. After exiting the tracing context, NNsight downloads the saved results, which we can perform operations on using Python code. Pretty simple!

# Alter model internals

Now that we've accessed the internal layers of the model, let's try modifying them and see how it affects the output!

We can do this using in-place operations in NNsight, which alter the model's state during execution. Let's try changing the output of layer 8 to be equal to 4.

```python
# remote = True means the model will execute on NDIF's shared resources
with llm.trace("The Eiffel Tower is in the city of", remote=True):

    # user-defined code to access internal model components
    llm.model.layers[7].output[0][:] = 4 # in-place operation to change a single layer's output values
    output = llm.output.save()

# after exiting the tracing context, we can access any values that were saved

output_logits = output["logits"]
print("Model Output Logits: ",output_logits[0])

# decode the final model output from output logits
max_probs, tokens = output_logits[0].max(dim=-1)
word = [llm.tokenizer.decode(tokens.cpu()[-1])]
print("Model Output: ", word[0])
```

Okay! The output for "The Eiffel Tower is in the city of" is now "Bounty". Looks like our intervention on the hidden 8th layer worked to change the model output!

Are you ready for something a little more complicated? Let's take the model's state when answering the city that the London Bridge is in, and swap that into the model's final layer when answering the Eiffel Tower question! We can do this using NNsight's invoking contexts, which batch different inputs into the same run through the model.

We can access values defined in invoking contexts throughout the other invoke context, allowing us to do something like swapping model states for different inputs. Let's try it out!

```python
import nnsight
# remote = True means the model will execute on NDIF's shared resources
with llm.trace(remote=True) as tracer:

    with tracer.invoke("The London Bridge is in the city of"):
        hidden_states = llm.model.layers[-1].output[0] # no .save()

    with tracer.invoke("The Eiffel Tower is in the city of"):
        # user-defined code to access internal model components
        llm.model.layers[-1].output[0][:] = hidden_states # can be accessed without .save()!
        output = llm.output.save()

output_logits = output["logits"]
print("Model Output Logits: ",output_logits[0])

# decode the final model output from output logits
max_probs, tokens = output_logits[0].max(dim=-1)
word = [llm.tokenizer.decode(tokens.cpu()[-1])]
print("Model Output: ", word[0])
```

Awesome, looks like it worked! The model output London instead of Paris when asked about the location of the Eiffel Tower.

# Next steps: Run your own experiment with NDIF and NNsight

This is just a quick overview of some of NNsight's functionality when working with remote models, so to learn more we recommend taking a deeper dive into these resources:

*   📚 Get a comprehensive overview of the library with the [NNsight Walkthrough](https://nnsight.net/notebooks/tutorials/walkthrough/)
*   🔎 Check out some NNsight implementations of common [LLM interpretability techniques](https://nnsight.net/tutorials/)
*   💬 Join the conversation with the NDIF [Discord](https://discord.com/invite/6uFJmCSwW7) community
*   💟 Follow us on [GitHub](https://github.com/ndif-team/nnsight), [Bluesky](https://bsky.app/profile/ndif-team.bsky.social), [X](https://x.com/ndif_team), and [LinkedIn](https://www.linkedin.com/company/national-deep-inference-fabric/)

**Want to scale up your research? [Apply for access to Llama-3.1-405B](https://ndif.us/405b.html)!**

<br>

<img src="https://ndif.us/images//NDIF_Acr_color.png" alt="drawing" width="400"/>

---

# streaming.ipynb


---

# vllm_support.ipynb

# vLLM Support

[vLLM](https://github.com/vllm-project/vllm) is a popular library used for fast inference. By leveraging PagedAttention, dynamic batching, and Hugging Face model integration, vLLM makes inference more efficient and scalable for real-world applications.

Starting with `NNsight 0.4`, NNsight includes support for internal investigations of vLLM models.

## Setup

You will need to install `nnsight 0.4`, `vllm==0.6.4.post1`, and `triton 3.1.0` to use vLLM with NNsight.

Please note that the current version of `vllm` isn't supported with NNsight, so you will need to specifically install the supported version: `vllm==0.6.4.post1`.

```python
from IPython.display import clear_output
try:
    import google.colab
    is_colab = True
except ImportError:
    is_colab = False

if is_colab:
    !pip install -U nnsight
clear_output()
```

```python
# install vllm
!pip install vllm==0.6.4.post1

# install triton 3.1.0
!pip install triton==3.1.0

clear_output()
```

 Next, let's load in our NNsight-supported vLLM model. You can find vLLM-supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html). For this exercise, we will use GPT-2.

 Please note that vLLM models require a GPU to run.

```python
from IPython.display import clear_output
from nnsight.modeling.vllm import VLLM

# NNsight's VLLM wrapper currently supports "device = cuda" and device = "auto"
vllm = VLLM("gpt2", device = "auto", dispatch = True) # See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html

clear_output()
print(vllm)
```

## Interventions on vLLM models
We now have a vLLM model that runs with `nnsight`. Let's try applying some interventions on it.

Note that vLLM takes in sampling parameters including `temperature` and `top_p`. These parameters can be included in the `.trace()` or `.invoke()` contexts. For default model behavior, set `temperature = 0` and `top_p = 1`. For more information about parameters, reference the [vLLM documentation](https://docs.vllm.ai/en/latest/dev/sampling_params.html).

```python
with vllm.trace(temperature=0.0, top_p=1.0, max_tokens=1) as tracer:
  with tracer.invoke("The Eiffel Tower is located in the city of"):
    clean_logits = vllm.logits.output.save()

  with tracer.invoke("The Eiffel Tower is located in the city of"):
    vllm.transformer.h[-2].mlp.output[:] = 0
    corrupted_logits = vllm.logits.output.save()
```

```python
print("\nCLEAN - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(clean_logits.argmax(dim=-1)))
print("\nCORRUPTED - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(corrupted_logits.argmax(dim=-1)))
```

We've successfully performed an intervention on our vLLM model!

## Sampled Token Traceability
vLLM provides functionality to configure how each sequence samples its next token. Here's an example of how you can trace token sampling operations with the nnsight VLLM wrapper.

```python
import nnsight
with vllm.trace("Madison Square Garden is located in the city of", temperature=0.8, top_p=0.95, max_tokens=3) as tracer:
    samples = nnsight.list().save()
    logits = nnsight.list().save()

    for ii in range(3):
        samples.append(vllm.samples.output)
        vllm.samples.next()
        logits.append(vllm.logits.output)
        vllm.logits.next()

print("Samples: ", samples)
print("Logits: ", logits) # different than samples with current sampling parameters
```

<details>
<summary>
Note: gradients are not supported with vLLM
</summary>

vLLM speeds up inference through its paged attention mechanism. This means that accessing gradients and backward passes are not supported for vLLM models. As such, calling gradient operations when using `nnsight` vLLM wrappers will throw an error.
</details>

## Known Issues
* The vllm.LLM engine performs max_tokens + 1 forward passes which can lead to undesired behavior if you are running interventions on all iterations of multi-token generation.

Example:
```
with vllm_gpt2("Hello World!", max_tokens=10):
    logits = nnsight.list().save()
    with vllm_gpt2.logits.all():
        logits.append(vllm_gpt2.logits.output)

print(len(logits))

```
`>>> 11 # expected: 10`

```python
with vllm.trace(temperature=0.0, top_p=1.0, max_tokens=1) as tracer:
  with tracer.invoke("The Eiffel Tower is located in the city of"):
    clean_logits = vllm.logits.output.save()

  with tracer.invoke("The Eiffel Tower is located in the city of"):
    vllm.language_model.model.layers[-2].mlp.output[:] = 0
    corrupted_logits = vllm.logits.output.save()
```

```python
print("\nCLEAN - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(clean_logits.argmax(dim=-1)))
print("\nCORRUPTED - The Eiffel Tower is located in the city of", vllm.tokenizer.decode(corrupted_logits.argmax(dim=-1)))
```

---

# walkthrough.ipynb

# Walkthrough

## The API for a transparent science on black-box AI

In this era of large-scale deep learning, the most interesting AI models are
massive black boxes that are hard to run. Ordinary commercial inference service
APIs let us interact with huge models, but they do not let us access model
internals.

The `nnsight` library is different: it provides full access to all neural
network internals. When using `nnsight` together with a remote service like the
[National Deep Inference Fabric](https://www.ndif.us)
(NDIF), it is possible to run complex experiments on huge open models easily
with fully transparent access.

Through NDIF and NNsight, our team wants to enable entire labs and independent researchers alike, as we
believe a large, passionate, and collaborative community will produce the next
big insights on this profoundly important field.

# 1 First, let's start small

[Run an interactive version of this walkthrough in Google Colab](https://colab.research.google.com/github/ndif-team/nnsight/blob/new-new-tutorials/docs/source/notebooks/tutorials/walkthrough.ipynb)

## Setup

Install NNsight:
```
pip install nnsight
```

## Tracing Context

To demonstrate the core functionality and syntax of nnsight, we'll define and
use a tiny two layer neural network.

Our little model here is composed of two submodules – linear layers `layer1` and `layer2`. We specify the sizes of each of these modules and create
some complementary example input.

```python
from collections import OrderedDict
import torch

input_size = 5
hidden_dims = 10
output_size = 2

net = torch.nn.Sequential(
    OrderedDict(
        [
            ("layer1", torch.nn.Linear(input_size, hidden_dims)),
            ("layer2", torch.nn.Linear(hidden_dims, output_size)),
        ]
    )
).requires_grad_(False)
```

The core object of the NNsight package is `NNsight`. This wraps around a given
PyTorch model to enable investigation of its internal parameters.

```python
import nnsight
from nnsight import NNsight

tiny_model = NNsight(net)
```

Printing a PyTorch model shows a named hierarchy of modules which is very useful
when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed.

```python
print(tiny_model)
```

Before we actually get to using the model we just created, let's talk about
Python contexts.

Python contexts define a scope using the `with` statement and are often used to
create some object, or initiate some logic, that you later want to destroy or
conclude.

The most common application is opening files as in the following example:

```python
with open('myfile.txt', 'r') as file:
  text = file.read()
```

Python uses the `with` keyword to enter a context-like object. This object
defines logic to be run at the start of the `with` block, as well as logic to be
run when exiting. When using `with` for a file, entering the context opens the
file and exiting the context closes it. Being within the context means we can
read from the file.

Simple enough! Now we can discuss how `nnsight` uses
contexts to enable intuitive access into the internals of a neural network.

The main tool with `nnsight` is a context for tracing.

We enter the tracing context by calling `model.trace(<input>)` on an `NNsight`
model, which defines how we want to run the model. Inside the context, we will
be able to customize how the neural network runs. The model is actually run upon
exiting the tracing context.

```python
# random input
input = torch.rand((1, input_size))

with tiny_model.trace(input) as tracer:
    pass
```

But where's the output? To get that, we'll have to learn how to request it from
within the tracing context.

## Getting

Earlier, we wrapped our little neural net with the `NNsight` class. This
added a couple properties to each module in the model (including the root model
itself). The two most important ones are `.input` and `.output`.

```python
model.input
model.output
```

The names are self explanatory. They correspond to the inputs and outputs of
their respective modules during a forward pass of the model. We can use these
attributes inside the `with` block.

However, it is important to understand that the model is not executed until the
end of the tracing context. How can we access inputs and outputs before the
model is run? The trick is deferred execution.

`.input` and `.output` are Proxies for the eventual inputs and outputs of a
module. In other words, when we access `model.output` what we are
communicating to `nnsight` is, "When you compute the output of `model`, please
grab it for me and put the value into its corresponding Proxy object. Let's try it:

```python
with tiny_model.trace(input) as tracer:

    output = tiny_model.output

print(output)
```

Oh no an error! "Accessing value before it's been set."

Why doesn't our `output` have a `value`?

Proxy objects will only have their value at the end of a context if we call
`.save()` on them. This helps to reduce memory costs. Adding `.save()` fixes the
error:

```python
with tiny_model.trace(input) as tracer:

    output = tiny_model.output.save()

print(output)
```

Success! We now have the model output. We just completed out first
intervention using `nnsight`.

Each time we access a module's input or output, we create an _intervention_ in
the neural network's forward pass. Collectively these requests form the
_intervention graph_. We call the process of executing it alongside the model's
normal computation graph, _interleaving_.

<details>
<summary>On Model output</summary>

---

If we don't need to access anything other than the model's final output (i.e., the model's predicted next token), we can
call the tracing context with `trace=False` and not use it as a context. This could be useful for simple inference using NNsight.

```python
  output = model.trace(<inputs>, trace=False)
```

---

</details>

Just like we saved the output of the model as a whole, we can save the output of
any of its submodules. We use normal Python attribute syntax. We can discover
how to access them by name by printing out the model:

```python
print(tiny_model)
```

Let's access the output of the first layer (which we've named `layer1`):

```python
with tiny_model.trace(input) as tracer:

    l1_output = tiny_model.layer1.output.save()

print(l1_output)
```

Let's do the same for the input of `layer2`.

Because we aren't accessing the `tracer` object within these tracing contexts, we can also drop `as tracer`.

```python
with tiny_model.trace(input):

    l2_input = tiny_model.layer2.input.save()

print(l2_input)
```

<details>
  <summary>On module inputs</summary>

---

Notice how the value for `l2_input` is just a single tensor. By default, the `.input` attribute of a module will return the **first** tensor input to the module.

We can also access the full input to a module by using the `.inputs` attribute, which will return the values in the form of:

      tuple(tuple(args), dictionary(kwargs))

Where the first index of the tuple is itself a tuple of all positional
arguments, and the second index is a dictionary of the keyword arguments.

---

</details>

Until now we were saving the output of the model and its submodules within the `Trace` context to then print it after exiting the context. We will continuing doing this in the rest of the tutorial since it's a good practice to save the computation results for later analysis.

However, we can also log the outputs of the model and its submodules within the `Trace` context. This is useful for debugging and understanding the model's behavior while saving memory.

Let's see how to do this:

```python
with tiny_model.trace(input) as tracer:
  tracer.log("Layer 1 - out: ", tiny_model.layer1.output)
```

## Functions, Methods, and Operations

Now that we can access activations, we also want to do some post-processing on
it. Let's find out which dimension of layer1's output has the highest value.

We could do this by calling `torch.argmax(...)` after the tracing context or we
can just leverage the fact that `nnsight` handles Pytorch functions and methods within
the tracing context, by creating a Proxy request for it:

```python
with tiny_model.trace(input):

    # Note we don't need to call .save() on the output,
    # as we're only using its value within the tracing context.
    l1_output = tiny_model.layer1.output

    # We do need to save the argmax tensor however,
    # as we're using it outside the tracing context.
    l1_amax = torch.argmax(l1_output, dim=1).save()

print(l1_amax[0])
```

Nice! That worked seamlessly, but hold on, how come we didn't need to call
`.value[0]` on the result? In previous sections, we were just being explicit to
get an understanding of Proxies and their value. In practice, however, `nnsight`
knows that when outside of the tracing context we only care about the actual
value, and so printing, indexing, and applying functions all immediately return
and reflect the data in `.value`. So for the rest of the tutorial we won't use
it.

The same principles work for Pytorch methods and all operators as well:

```python
with tiny_model.trace(input):

    value = (tiny_model.layer1.output.sum() + tiny_model.layer2.output.sum()).save()

print(value)
```

The code block above is saying to `nnsight`, "Run the model with
the given `input`. When the output of `tiny_model.layer1` is computed, take its sum. Then do
the same for `tiny_model.layer2`. Now that both of those are computed, add them and make sure
not to delete this value as I wish to use it outside of the tracing context."

## Custom Functions

Everything within the tracing context operates on the intervention graph. Therefore, for `nnsight` to trace a  function it must also be a part of the intervention graph.

Out-of-the-box `nnsight` supports PyTorch functions and methods, all operators, as well the `einops` library. We don't need to do anything special to use them. But what do we do if we want to use custom functions? How do we add them to the intervention graph?

Enter `nnsight.apply()`. It allows us to add new functions to the intervention graph. Let's see how it works:

```python
# Take a tensor and return the sum of its elements
def tensor_sum(tensor):
    flat = tensor.flatten()
    total = 0
    for element in flat:
        total += element.item()

    return torch.tensor(total)

with tiny_model.trace(input) as tracer:

    # Specify the function name and its arguments (in a comma-separated form) to add to the intervention graph
    custom_sum = nnsight.apply(tensor_sum, tiny_model.layer1.output).save()
    sum = tiny_model.layer1.output.sum()
    sum.save()

print(custom_sum, sum)
```

`nnsight.apply()` executes the function it wraps and returns its output as a Proxy object. We can then use this Proxy object as we would any other.

The applications of `nnsight.apply` are wide: it can be used to wrap any custom function or functions from libraries that `nnsight` does not support out-of-the-box.

## Setting

Getting and analyzing the activations from various points in a model can be
really insightful, and a number of ML techniques do exactly that. However, often we not only want to view the computation of a model, but also to influence it.

To demonstrate the effect of editing the flow of information through the model,
let's set the first dimension of the first layer's output to 0. `NNsight` makes
this really easy using the '=' operator:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    # Access the 0th index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, 0] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

Seems our change was reflected. Now let's do the same for the last dimension:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    # Access the last index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, hidden_dims] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

Oh no, we are getting an error! Ah of course, we needed to index at `hidden_dims - 1` not `hidden_dims`.

If you've been using `nnsight`, you are probably familiar with error messages that can be quite difficult to troubleshoot. In `nnsight 0.4` we've now improved error messaging to be descriptive and line-specific, as you should see in the above example!

<details>

<summary>
Old NNsight error messaging
</summary>

If you've been using NNsight prior to the NNsight 0.4 release, you will be familiar with the following non-descriptive error messaging. If you choose to turn off NNsight 0.4's new error messaging feature, this is how errors within the tracing context will appear.

```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in execute(self)
    379                 # Call the target to get value.
--> 380                 output = self.target(*args, **kwargs)
    381

IndexError: index 10 is out of bounds for dimension 1 with size 10

The above exception was the direct cause of the following exception:

IndexError                                Traceback (most recent call last)
20 frames
<ipython-input-16-5c81de91fb1f> in <cell line: 0>()
----> 1 with tiny_model.trace(input):
      2
      3     # Save the output before the edit to compare.
      4     # Notice we apply .clone() before saving as the setting operation is in-place.
      5     l1_output_before = tiny_model.layer1.output.clone().save()

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/Tracer.py in __exit__(self, exc_type, exc_val, exc_tb)
    100
    101
--> 102         super().__exit__(exc_type, exc_val, exc_tb)
    103
    104     def invoke(self, *inputs: Any, **kwargs) -> Invoker:

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/GraphBasedContext.py in __exit__(self, exc_type, exc_val, exc_tb)
    215             raise exc_val
    216
--> 217         self.backend(self)
    218
    219     ### BACKENDS ########

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/backends/LocalBackend.py in __call__(self, obj)
     25     def __call__(self, obj: LocalMixin):
     26
---> 27         obj.local_backend_execute()

/usr/local/lib/python3.11/dist-packages/nnsight/contexts/Tracer.py in local_backend_execute(self)
    144         self.graph.execute()
    145
--> 146         self.model.interleave(
    147             self.model._execute,
    148             self.graph,

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in interleave(self, fn, intervention_graph, *inputs, **kwargs)
    467         module_paths = InterventionProtocol.get_interventions(intervention_graph).keys()
    468
--> 469         with HookHandler(
    470             self._model,
    471             list(module_paths),

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in __exit__(self, exc_type, exc_val, exc_tb)
    579
    580         if isinstance(exc_val, Exception):
--> 581             raise exc_val
    582
    583

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in interleave(self, fn, intervention_graph, *inputs, **kwargs)
    478         ):
    479             try:
--> 480                 fn(*inputs, **kwargs)
    481             except protocols.EarlyStopProtocol.EarlyStopException:
    482                 # TODO: Log.

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in _execute(self, *prepared_inputs, **kwargs)
    585             pass
    586
--> 587         return self._model(
    588             *prepared_inputs,
    589             **kwargs,

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735         else:
-> 1736             return self._call_impl(*args, **kwargs)
   1737
   1738     # torchrec tests the code consistency with the following code

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1842
   1843         try:
-> 1844             return inner()
   1845         except Exception:
   1846             # run always called hooks if they have not already been run

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in inner()
   1788                 args = bw_hook.setup_input_hook(args)
   1789
-> 1790             result = forward_call(*args, **kwargs)
   1791             if _global_forward_hooks or self._forward_hooks:
   1792                 for hook_id, hook in (

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py in forward(self, input)
    248     def forward(self, input):
    249         for module in self:
--> 250             input = module(input)
    251         return input
    252

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1735         else:
-> 1736             return self._call_impl(*args, **kwargs)
   1737
   1738     # torchrec tests the code consistency with the following code

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1842
   1843         try:
-> 1844             return inner()
   1845         except Exception:
   1846             # run always called hooks if they have not already been run

/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py in inner()
   1801                         hook_result = hook(self, args, kwargs, result)
   1802                     else:
-> 1803                         hook_result = hook(self, args, result)
   1804
   1805                     if hook_result is not None:

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in output_hook(module, input, output, module_path)
    564
    565                 def output_hook(module, input, output, module_path=module_path):
--> 566                     return self.output_hook(output, module_path)
    567
    568                 self.handles.append(

/usr/local/lib/python3.11/dist-packages/nnsight/models/NNsightModel.py in <lambda>(activations, module_path)
    473                 activations, module_path, "input", intervention_handler
    474             ),
--> 475             output_hook=lambda activations, module_path: InterventionProtocol.intervene(
    476                 activations, module_path, "output", intervention_handler
    477             ),

/usr/local/lib/python3.11/dist-packages/nnsight/intervention.py in intervene(cls, activations, module_path, key, intervention_handler)
    454
    455                 # Value injection.
--> 456                 node.set_value(value)
    457
    458                 # Check if through the previous value injection, there was a 'swap' intervention.

/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in set_value(self, value)
    408
    409             if listener.fulfilled() and not self.graph.sequential:
--> 410                 listener.execute()
    411
    412         for dependency in self.arg_dependencies:

/usr/local/lib/python3.11/dist-packages/nnsight/tracing/Node.py in execute(self)
    385         except Exception as e:
    386
--> 387             raise type(e)(
    388                 f"Above exception when execution Node: '{self.name}' in Graph: '{self.graph.id}'"
    389             ) from e

IndexError: Above exception when execution Node: 'setitem_0' in Graph: '132147685816016'

```

</details>

The error messaging feature can be toggled using `nnsight.CONFIG.APP.DEBUG` which defaults to true.

<details>

<summary>
Toggle Error Messaging
</summary>

Turn off debugging:
```
import nnsight

nnsight.CONFIG.APP.DEBUG = False
nnsight.CONFIG.save()
```

Turn on debugging:
```
import nnsight

nnsight.CONFIG.APP.DEBUG = True
nnsight.CONFIG.save()
```
</details>

Now that we know more about NNsight's error messaging, let's try our setting operation again with the correct indexing and view the shape of the output
before leaving the tracing context:

```python
with tiny_model.trace(input):

    # Save the output before the edit to compare.
    # Notice we apply .clone() before saving as the setting operation is in-place.
    l1_output_before = tiny_model.layer1.output.clone().save()

    print(f"Layer 1 output shape: {tiny_model.layer1.output.shape}")

    # Access the last index of the hidden state dimension and set it to 0.
    tiny_model.layer1.output[:, hidden_dims - 1] = 0

    # Save the output after to see our edit.
    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

## Scan and Validate
Error codes are helpful, but sometimes you may want to quickly troubleshoot your code without actually running it.

Enter "Scanning" and "Validating"! We can enable this features by setting the `scan=True` and `validate=True` flag in the `trace` method.

"Scanning" runs "fake" inputs throught the model to collect information like shapes and types (i.e., scanning will populate all called `.inputs` and `.outputs`).

"Validating" attempts to execute the intervention proxies with "fake" inputs to check if they work (i.e., executes all interventions in your code with fake tensors).

"Validating" is dependent on "Scanning" to work correctly, so we need to run the scan of the model at least once to debug with validate. Let's try it out on our example above.

```python
# turn on scan and validate
with tiny_model.trace(input, scan=True, validate=True):

    l1_output_before = tiny_model.layer1.output.clone().save()

    # the error is happening here
    tiny_model.layer1.output[:, hidden_dims] = 0

    l1_output_after = tiny_model.layer1.output.save()

print("Before:", l1_output_before)
print("After:", l1_output_after)
```

The operations are never executed using tensors with real values so it doesn't incur any memory costs. Then, when creating proxy requests like the setting one above, `nnsight` also attempts to execute the request on the "fake" values we recorded. Hence, it lets us know if our request is feasible before even running the model. [Here](https://nnsight.net/notebooks/features/scan_validate/) is a more detailed example of scan and validate in action!

<details>
<summary>A word of caution</summary>

---

Some pytorch operations and related libraries don't work well with fake tensors

If you are doing anything in a loop where efficiency is important, you should keep scanning and validating off. It's best to use them only when debugging or when you are unsure if your intervention will work.

---

</details>

We can also use the `.scan()` method to get the shape of a module without having to fully run the model. If scan  is enabled, our input is run though the model under its own "fake" context. This means the input makes its way through all of the model operations, allowing `nnsight` to record the shapes and data types of module inputs and outputs!

```python
with tiny_model.scan(input):

    dim = tiny_model.layer1.output.shape[-1]

print(dim)
```

## Gradients

`NNsight` also lets us apply backpropagation and access gradients with respect to a
loss. Like `.input` and `.output` on modules, `nnsight` exposes `.grad` on
Proxies themselves (assuming they are proxies of tensors):

```python
with tiny_model.trace(input):

    # We need to explicitly have the tensor require grad
    # as the model we defined earlier turned off requiring grad.
    tiny_model.layer1.output.requires_grad = True

    # We call .grad on a tensor Proxy to communicate we want to store its gradient.
    # We need to call .save() since .grad is its own Proxy.
    layer1_output_grad = tiny_model.layer1.output.grad.save()
    layer2_output_grad = tiny_model.layer2.output.grad.save()

    # Need a loss to propagate through the later modules in order to have a grad.
    loss = tiny_model.output.sum()
    loss.backward()

print("Layer 1 output gradient:", layer1_output_grad)
print("Layer 2 output gradient:", layer2_output_grad)
```

All of the features we learned previously, also apply to `.grad`. In other
words, we can apply operations to and edit the gradients. Let's zero the grad of
`layer1` and double the grad of `layer2`.

```python
with tiny_model.trace(input):

    # We need to explicitly have the tensor require grad
    # as the model we defined earlier turned off requiring grad.
    tiny_model.layer1.output.requires_grad = True

    tiny_model.layer1.output.grad[:] = 0
    tiny_model.layer2.output.grad = tiny_model.layer2.output.grad * 2

    layer1_output_grad = tiny_model.layer1.output.grad.save()
    layer2_output_grad = tiny_model.layer2.output.grad.save()

    # Need a loss to propagate through the later modules in order to have a grad.
    loss = tiny_model.output.sum()
    loss.backward()

print("Layer 1 output gradient:", layer1_output_grad)
print("Layer 2 output gradient:", layer2_output_grad)
```

## Early Stopping

If we are only interested in a model's intermediate computations, we can halt a forward pass run at any module level, reducing runtime and conserving compute resources. One examples where this could be particularly useful would if we are working with SAEs - we can train an SAE on one layer and then stop the execution.

```python
with tiny_model.trace(input):
   l1_out = tiny_model.layer1.output.save()
   tiny_model.layer1.output.stop()

# get the output of the first layer and stop tracing
print("L1 - Output: ", l1_out)
```

Interventions within the tracing context do not necessarily execute in the order they are defined. Instead, their execution is tied to the module they are associated with.

As a result, if the forward pass is terminated early any interventions linked to modules beyond that point will be skipped, even if they were defined earlier in the context.

In the example below, the output of layer 2 _**cannot**_ be accessed since the model's execution was stopped at layer 1.

```python
with tiny_model.trace(input):
   l2_out = tiny_model.layer2.output.save()
   tiny_model.layer1.output.stop()

print("L2 - Output: ", l2_out)
```

## Conditional Interventions

Interventions can also be made conditional.

Inside the tracing context we can specify a new - conditional - context. This context will only execute the interventions within it if the condition is met.

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  with tracer.cond(rand_int % 2 == 0):
    tracer.log("Random Integer ", rand_int, " is Even")

  with tracer.cond(rand_int % 2 == 1):
    tracer.log("Random Integer ", rand_int, " is Odd")
```

Conditional contexts can also be nested, if we want our interventions to depend on more than one condition at a time.

```python
with tiny_model.trace(input) as tracer:

  non_rand_int = 8

  with tracer.cond(non_rand_int > 0):
    with tracer.cond(non_rand_int % 2 == 0):
      tracer.log("Rand Int ", non_rand_int, " is Positive and Even")
```

With `nnsight 0.4` we can now also use Python `if` statements within the tracing context to create a conditional context!

*Note: Colab behaves a little strangely with this feature the first time you run it - expect some lagging and warnings*

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")

  if rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

`elif` statements should also work as `if` statements within the tracing context:

```python
with tiny_model.trace(input) as tracer:

  rand_int = torch.randint(low=-10, high=10, size=(1,))

  # Since this if statement is inside the tracing context the if will
  # create a conditional context and will only execute the intervention
  # if this condition is met
  if rand_int % 2 == 0:
    tracer.log("Random Integer ", rand_int, " is Even")
  elif rand_int % 2 == 1:
    tracer.log("Random Integer ", rand_int, " is Odd")
```

## Iterative Interventions

With the iterator context, you can now run an intervention loop at scale. It iteratively executes and updates a single intervention graph. Use a `.session()` to define the Iterator context and pass in a sequence of items that you want to loop over at each iteration

```python
with tiny_model.session() as session:

  li = nnsight.list() # an NNsight built-in list object
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

With `nnsight 0.4` we can now also use Python `for` loops within a tracer context at scale.

*NOTE: inline for loops (i.e., `[x for x in <Proxy object>`]) are not currently supported.*

```python
# New: Using Python for loops for iterative interventions
with tiny_model.session() as session:

    li = nnsight.list()
    [li.append([num]) for num in range(0, 3)]
    li2 = nnsight.list().save()

    # Using regular for loops
    for item in li:
        for item_2 in item: # for loops can be nested!
            li2.append(item_2)

print("\nList: ", li2)
```

# 2️ Bigger

Now that we have the basics of `nnsight` under our belt, we can scale our model
up and combine the techniques we've learned into more interesting experiments.

The `NNsight` class is very bare bones. It wraps a pre-defined model and does no
pre-processing on the inputs we enter. It's designed to be extended with more
complex and powerful types of models, and we're excited to see what can be done
to leverage its features!

However, if you'd like to load a Language Model from HuggingFace with its tokenizer, the`LanguageModel` subclass greatly simplifies this process.

## LanguageModel

`LanguageModel` is a subclass of `NNsight`. While we could define and create a
model to pass in directly, `LanguageModel` includes special support for
Huggingface language models, including automatically loading models from a
Huggingface ID, and loading the model together with the appropriate tokenizer.

Here is how we can use `LanguageModel` to load `GPT-2`:

```python
from nnsight import LanguageModel

llm = LanguageModel("openai-community/gpt2", device_map="auto")

print(llm)
```

When we initialize `LanguageModel`, we aren't yet loading the parameters of the
model into memory. We are actually loading a 'meta' version of the model which
doesn't take up any memory, but still allows us to view and trace actions on it.
After exiting the first tracing context, the model is then fully loaded into
memory. To load into memory on initialization, you can pass `dispatch=True` into
`LanguageModel` like
`LanguageModel('openai-community/gpt2', device_map="auto", dispatch=True)`.

<details>
<summary>On Model Initialization</summary>

---

A few important things to note:

Keyword arguments passed to the initialization of `LanguageModel` is forwarded
to HuggingFace specific loading logic. In this case, `device_map` specifies
which devices to use and its value `auto` indicates to evenly distribute it to
all available GPUs (and CPU if no GPUs available). Other arguments can be found
here:
https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM

---

</details>

Let's now apply some of the features that we used on the small model to `GPT-2`. Unlike `NNsight`, `LanguageModel` does define logic to pre-process
inputs upon entering the tracing context. This makes interacting with the model
simpler (i.e., you can send prompts to the model without having to directly access the tokenizer).

In the following example, we ablate the value coming from the last layer's MLP
module and decode the logits to see what token the model predicts without
influence from that particular module:

```python
with llm.trace("The Eiffel Tower is in the city of"):

    # Access the last layer using h[-1] as it's a ModuleList
    # Access the first index of .output as that's where the hidden states are.
    llm.transformer.h[-1].mlp.output[0][:] = 0

    # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.
    token_ids = llm.lm_head.output.argmax(dim=-1).save()

print("\nToken IDs:", token_ids)

# Apply the tokenizer to decode the ids into words after the tracing context.
print("Prediction:", llm.tokenizer.decode(token_ids[0][-1]))
```

We just ran a little intervention on a much more complex model with many more
parameters! However, we're missing an important piece of information: what the
prediction would have looked like without our ablation.

We could just run two tracing contexts and compare the outputs. However, this would require two forward passes through the model. `NNsight` can do
better than that with batching.

## Batching

Batching is a way to process multiple inputs in one forward pass. To better understand how batching works, we're going to bring back the `Tracer` object that we dropped before.

When we call `.trace(...)`, it's actually creating two different contexts behind the scenes. The first one is the tracing context that we've discussed previously, and the second one is the invoker context. The invoker context defines the values of the `.input` and `.output` Proxies.

If we call `.trace(...)` with some input, the input is passed on to the invoker. As there is only one input, only one invoker context is created.

If we call `.trace()` without an input, then we can call `tracer.invoke(input1)` to manually create the invoker context with an input, `input1`. We can also repeatedly call `tracer.invoke(...)` to create the invoker context for additional inputs. Every subsequent time we call
`.invoke(...)`, interventions within its context will only refer to the input in that particular invoke statement.

When exiting the tracing context, the inputs from all of the invokers will be batched together, and they will be executed in one forward pass! To test this out, let's do the same ablation experiment, but also add a 'control' output for comparison:

<details>
<summary>More on the invoker context</summary>

---

Note that when injecting data to only the relevant invoker interventions, `nnsight` tries, but can't guarantee, to narrow the data into the right
batch indices. Thus, there are cases
where all invokes will get all of the data. Specifically, if the input or output data is stored
as an object that is not an arbitrary collection of tensors, it will be broadcasted to all invokes.

Just like `.trace(...)` created a `Tracer` object, `.invoke(...)` creates an `Invoker` object. For `LanguageModel` models, the `Invoker` prepares the input by running a tokenizer on it.
`Invoker` stores pre-processed inputs at `invoker.inputs`, which can be accessed to see information about our inputs.
In a case where we pass a single input to `.trace(...)` directly, we can still access the invoker
object at `tracer.invoker` without having to call `tracer.invoke(...)`.

Keyword arguments given to `.invoke(..)` make their way to the input pre-processing.
`LanguageModel` has keyword arguments `max_length` and `truncation` used for tokenization which can be
passed to the invoker. If we want to pass keyword arguments to the invoker for a single-input `.trace(...)`, we can pass `invoker_args` as a dictionary of invoker keyword arguments.

Here is an example to demonstrate everything we've described:

**This snippet**

```
with llm.trace("hello", invoker_args={"max_length":10}) as tracer:
  invoker = tracer.invoker

```
  **does the same as**

```
with llm.trace() as tracer:
  with tracer.invoke("hello", max_length=10) as invoker:
    invoker = invoker
```

---

</details>

```python
with llm.trace() as tracer:

    with tracer.invoke("The Eiffel Tower is in the city of"):

        # Ablate the last MLP for only this batch.
        llm.transformer.h[-1].mlp.output[0][:] = 0

        # Get the output for only the intervened on batch.
        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()

    with tracer.invoke("The Eiffel Tower is in the city of"):

        # Get the output for only the original batch.
        token_ids_original = llm.lm_head.output.argmax(dim=-1).save()

print("Original token IDs:", token_ids_original)
print("Modified token IDs:", token_ids_intervention)

print("Original prediction:", llm.tokenizer.decode(token_ids_original[0][-1]))
print("Modified prediction:", llm.tokenizer.decode(token_ids_intervention[0][-1]))
```

Based on our control results, our ablation did end up affecting what the model predicted. That's pretty neat!

Another cool thing with multiple invokes is that Proxies can interact between them.

Here, we transfer the token embeddings from a real prompt into another placeholder prompt. Therefore the latter prompt produces the output of the former prompt:

```python
with llm.trace() as tracer:

    with tracer.invoke("The Eiffel Tower is in the city of"):
        embeddings = llm.transformer.wte.output

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):
        llm.transformer.wte.output = embeddings
        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()

    with tracer.invoke("_ _ _ _ _ _ _ _ _ _"):
      token_ids_original = llm.lm_head.output.argmax(dim=-1).save()

print("original prediction shape", token_ids_original[0][-1].shape)
print("Original prediction:", llm.tokenizer.decode(token_ids_original[0][-1]))

print("modified prediction shape", token_ids_intervention[0][-1].shape)
print("Modified prediction:", llm.tokenizer.decode(token_ids_intervention[0][-1]))
```

For larger batch sizes, you can also iteratate across multiple invoke contexts.

## Multiple Token Generation

### .next()

Some HuggingFace models define methods to generate multiple outputs at a time.
`LanguageModel` wraps that functionality to provide the same tracing features by
using `.generate(...)` instead of `.trace(...)`. This calls the underlying
model's `.generate` method. It passes the output through a `.generator`
module that we've added onto the model, allowing us to get the generate output
at `.generator.output`.

In a case like this, the underlying model is called more than once; the modules
of said model produce more than one output. Which iteration should a given
`module.output` refer to? That's where `Module.next()` comes in!

Each module has a call index associated with it and `.next()` simply increments
that attribute. At the time of execution, data is injected into the intervention
graph only at the iteration that matches the call index.

```python
with llm.generate('The Eiffel Tower is in the city of', max_new_tokens=3) as tracer:

    hidden_states1 = llm.transformer.h[-1].output[0].save()

    # use module.next() to access the next intervention
    hidden_states2 = llm.transformer.h[-1].next().output[0].save()

    # saving the output allows you to save the hidden state across the initial prompt
    out = llm.generator.output.save()

print(hidden_states1.shape)
print(hidden_states2.shape)
print(out)
```

### using .all()

With `nnsight 0.4` you can now use `.all()` to recursively apply interventions to a model. Calling `.all()` on a module within a model will recursively apply its `.input` and `.output` across all iterations. Previously, we'd need to loop across each new generated token, saving the intervention for every generated token and calling `.next()` to move forward.

```python
# Old approach:
prompt = 'The Eiffel Tower is in the city of'
layers = llm.transformer.h
n_new_tokens = 3
hidden_states = []
with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    for i in range(n_new_tokens):
        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output.save())

        # Move to next generated token
        layers[0].next()

print("Hidden state length: ",len(hidden_states))
```

We can use also `.all()` to streamline the multiple token generation process. We simply call `.all` on the module where we are applying the intervention (in this case GPT-2's layers), apply our intervention, and append our hidden states (stored in an `nnsight.list()` object).
<br> <br>

Let's test this out for the multiple token generation case:

```python
# using .all():
prompt = 'The Eiffel Tower is in the city of'
layers = llm.transformer.h
n_new_tokens = 3
with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() to apply intervention to each new token
    with layers.all():

        # Apply intervention - set first layer output to zero
        layers[0].output[0][:] = 0

        # Append desired hidden state post-intervention
        hidden_states.append(layers[-1].output) # no need to call .save
        # Don't need to loop or call .next()!

print("Hidden state length: ",len(hidden_states))
```

Easy! Note that because `.all()` is recursive, it will only work to append outputs called on children of the module that `.all()` was called on. See example below for more information. TL;DR: apply `.all()` on the highest-level accessed module if interventions and outputs have different hierarchies within model structure.

<details>
<summary>Recursive properties of .all()</summary>

`.all()` recursively acts on model components. In the below code example, only the first token generation is saved, because `.all()` applied to `layers`, while the saved variable `hidden_states` is produced from `model.lm_head`, which is not a child of `layers`.

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on layers
    layers.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save, it's already initialized

print("Hidden state length: ",len(hidden_states)) # length is 1, meaning it only saved the first token generation
```

If you want to apply an intervention during multiple token generation while saving the state of a model component that isn't a child of that module, you can instead apply `.all()` to the full model:

```
prompt = 'The Eiffel Tower is in the city of'
layers = model.transformer.h
n_new_tokens = 3
with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:
    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list

    # Call .all() on model
    model.all()

    # Apply same intervention - set first layer output to zero
    layers[0].output[0][:] = 0

    # Append desired hidden state post-intervention
    hidden_states.append(model.lm_head.output) # no need to call .save

print("Hidden state length: ",len(hidden_states)) # length is 3, as expected!
```

</details>

## Model Editing

NNsight's model editing feature allows you to create persistently modified versions of a model with a use of `.edit()`. Unlike interventions in a tracing context, which are temporary, the **Editor** context enables you to make lasting changes to a model instance.

This feature is useful for:
* Creating modified model variants without altering the original
* Applying changes that persist across multiple forward passes
* Comparing interventions between original and edited models

Let's explore how to use the **Editor** context to make a simple persistent change to a model:

```python
# we take the hidden states with the expected output "Paris"
with llm.trace("The Eiffel Tower is located in the city of") as tracer:
    hs11 = llm.transformer.h[11].output[0][:, -1, :].save()

# the edited model will now always predict "Paris" as the next token
with llm.edit() as llm_edited:
    llm.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with llm.trace("Vatican is located in the city of") as tracer:
    original_tokens = llm.lm_head.output.argmax(dim=-1).save()

# ...with the output of the edited model
with llm_edited.trace("Vatican is located in the city of") as tracer:
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("\nOriginal Prediction: ", llm.tokenizer.decode(original_tokens[0][-1]))
print("Modified Prediction: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

Edits defined within an **Editor** context create a new, modified version of the model by default, preserving the original. This allows for safe experimentation with model changes. If you wish to modify the original model directly, you can set `inplace=True` when calling `.edit()`.

Use this option cautiously, as in-place edits alter the base model for all the consequent model calls.

```python
# we use the hidden state we saved above (hs11)
with llm.edit(inplace=True) as llm_edited:
    llm.transformer.h[11].output[0][:, -1, :] = hs11

# we demonstrate this by comparing the output of an unmodified model...
with llm.trace("Vatican is located in the city of") as tracer:
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("Modified In-place: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

If you've made in-place edits to your model and need to revert these changes, you can apply `.clear_edits()`. This method removes all edits applied to the model, effectively restoring it to its original state.

```python
llm.clear_edits()

with llm.trace("Vatican is located in the city of"):
    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()

print("Edits cleared: ", llm.tokenizer.decode(modified_tokens[0][-1]))
```

# 3 I thought you said huge models?

`NNsight` is only one part of our project to democratize access to AI internals. The other half is the National Deep Inference Fabric, or `NDIF`. `NDIF` hosts large models for shared access using `NNsight`, so you don't have to worry about any of the headaches of hosting large models yourself!

The interaction between `NDIF` and `NNsight` is fairly straightforward. The
**intervention graph** we create via the tracing context can be encoded into a
custom json format and sent via an http request to the `NDIF` servers. `NDIF`
then decodes the **intervention graph** and **interleaves** it alongside the
specified model.

To see which models are currently being hosted, check out the following status
page: https://nnsight.net/status/

## Remote execution

In its current state, `NDIF` requires you to receive an API key. Therefore, to
run the rest of this walkthrough, you need one of your own. To get one, simply
register at https://login.ndif.us.

With a valid API key, you then can configure `nnsight` as follows:

```python
from nnsight import CONFIG

CONFIG.set_default_api_key("YOUR_API_KEY")
```

If you're running in a local IDE, this only needs to be run once as it will save the API key as the default in a
.config file along with your `nnsight` installation. You can also add your API key to Google Colab secrets.

To amp things up a few levels, let's demonstrate using `nnsight`'s tracing
context with `Llama-3.1-8b`!

```python
import os

# Llama 3.1 8b is a gated model, so you need to apply for access on HuggingFace and include your token.
os.environ['HF_TOKEN'] = "YOUR_HUGGING_FACE_TOKEN"
```

```python
from nnsight import LanguageModel

# We'll never actually load the parameters locally, so no need to specify a device_map.
llama = LanguageModel("meta-llama/Meta-Llama-3.1-8B")
# All we need to specify using NDIF vs executing locally is remote=True.
with llama.trace("The Eiffel Tower is in the city of", remote=True) as runner:

    hidden_states = llama.model.layers[-1].output.save()

    output = llama.output.save()

print(hidden_states)

print(output["logits"])
```

It really is as simple as `remote=True`. All of the techniques we went through
in earlier sections work just the same when running locally or remotely.

## Sessions

NDIF uses a queue to handle concurrent requests from multiple users. To optimize the execution of our experiments we can use the `session` context to efficiently package multiple interventions together as one single request to the server.

This offers the following benefits:
1.   All interventions within a session will be executed one after another without additional wait in the NDIF queue
2.   All intermediate outputs for each intervention are stored on the server and can be accessed by other interventions in the same session without moving the data back and forth between NDIF and the local machine

Let's take a look:

```python
with llama.session(remote=True) as session:

  with llama.trace("The Eiffel Tower is in the city of") as t1:
    # capture the hidden state from layer 32 at the last token
    hs_31 = llama.model.layers[31].output[0][:, -1, :] # no .save()
    t1_tokens_out = llama.lm_head.output.argmax(dim=-1).save()

  with llama.trace("Buckingham Palace is in the city of") as t2:
    llama.model.layers[1].output[0][:, -1, :] = hs_31[:]
    t2_tokens_out = llama.lm_head.output.argmax(dim=-1).save()

print("\nT1 - Original Prediction: ", llama.tokenizer.decode(t1_tokens_out[0][-1]))
print("T2 - Modified Prediction: ", llama.tokenizer.decode(t2_tokens_out[0][-1]))
```

In the example above, we are interested in replacing the hidden state of a later layer with an earlier one. Since we are using a `session`, we don't have to save the hidden state from Tracer 1 to reference it in Tracer 2.

It is important to note that all the traces defined within the `session` context are executed sequentially, strictly following the order of definition (i.e. `t2` being executed after `t1` and `t3` after `t2` etc.).

The `session` context object has its own methods to log values and be terminated early.

```python
with llama.session(remote=True) as session:
  session.log("-- Early Stop --")
  nnsight.stop
```

In addition to the benefits mentioned above, the `session` context also enables interesting experiments not possible with other `nnsight` tools — since every trace is run on its own model, it means that within one session we can run interventions between different models — for example, we could swap activations between base and instruct versions of the Llama model and compare their outputs. And `session` can also be used to run similar experiments entirely locally!

## Streaming

Streaming enables users apply functions and datasets locally during remote model execution. This allows users to stream results for immediate consumption (i.e., seeing tokens as they are generated) or applying non-whitelisted functions such as model tokenizers, large local datasets, and more!

*   `nnsight.local()` context sends values immediately to user's local machine from server
*   Intervention graph is executed locally on downstream nodes
*   Exiting local context uploads data back to server
*   `@nnsight.trace` function decorator enables custom functions to be added to intervention graph when using `nnsight.local()`

## `nnsight.local()`

You may sometimes want to locally access and manipulate values during remote execution. Using `.local()` on a proxy, you can send remote content to your local machine and apply local functions. The intervention graph is then executed locally on downstream nodes (until you send execution back to the remote server by exiting the `.local()` context).

There are a few use cases for streaming with `.local()`, including live chat generation and applying large datasets or non-whitelisted local functions to the intervention graph.

Now let's explore how streaming works. We'll start by grabbing some hidden states of the model and printing their value using `tracer.log()`. Without calling `nnsight.local()`, these operations will all occur remotely.

```python
# This will give you a remote LOG response because it's coming from the remote server
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

Now, let's try the same operation using the `nnsight.local()` context. This will send the operations to get and print the hidden states to your local machine, changing how the logging message is formatted (local formatting instead of remote).

```python
# This will print locally because it's already local
with llama.trace("hello", remote=True) as tracer:

    with nnsight.local():
        hs = llama.model.layers[-1].output[0]
        tracer.log(hs[0,0,0])

    out =  llama.lm_head.output.save()

print(out)
```

## `@nnsight.trace` function decorator

We can also use function decorators to create custom functions to be used during `.local` calls. This is a handy way to enable live streaming of a chat or to train probing classifiers on model hidden states.

Let's try out `@nnsight.trace` and `nnsight.local()` to access a custom function during remote execution.

```python
# first, let's define our function
@nnsight.trace # decorator that enables this function to be added to the intervention graph
def my_local_fn(value):
    return value * 0

# We use a local function to ablate some hidden states
# This downloads the data for the .local context, and then uploads it back to set the value.
with llama.generate("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    with nnsight.local():

        hs = my_local_fn(hs)

    llama.model.layers[-1].output[0][:] = hs

    out =  llama.lm_head.output.save()
```

Note that without calling `.local`, the remote API does not know about `my_local_fn` and will throw a whitelist error. A whitelist error occurs because you are being allowed access to the function.

```python
with llama.trace("hello", remote=True) as tracer:

    hs = llama.model.layers[-1].output[0]

    hs = my_local_fn(hs) # no .local - will cause an error

    llama.model.layers[-1].output[0][:] = hs * 2

    out =  llama.lm_head.output.save()

print(out)
```

## Example: Live-streaming remote chat

Now that we can access data within the tracing context on our local computer, we can apply non-whitelisted functions, such as the model's tokenizer, within our tracing context.

Let's build a decoding function that will decode tokens into words and print the result.

```python
@nnsight.trace
def my_decoding_function(tokens, model, max_length=80, state=None):
    # Initialize state if not provided
    if state is None:
        state = {'current_line': '', 'current_line_length': 0}

    token = tokens[-1] # only use last token

    # Decode the token
    decoded_token = llama.tokenizer.decode(token).encode("unicode_escape").decode()

    if decoded_token == '\\n':  # Handle explicit newline tokens
        # Print the current line and reset state
        print('',flush=True)
        state['current_line'] = ''
        state['current_line_length'] = 0
    else:
        # Check if adding the token would exceed the max length
        if state['current_line_length'] + len(decoded_token) > max_length:
            print('',flush=True)
            state['current_line'] = decoded_token  # Start a new line with the current token
            state['current_line_length'] = len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line
        else:
            # Add a space if the line isn't empty and append the token
            if state['current_line']:
                state['current_line'] += decoded_token
            else:
                state['current_line'] = decoded_token
            state['current_line_length'] += len(decoded_token)
            print(state['current_line'], flush=True, end="")  # Print the current line

    return state
```

Now we can decode and print our model outputs throughout token generation by accessing our decoding function through `nnsight.local()`.

```python
import torch

nnsight.CONFIG.APP.REMOTE_LOGGING = False

prompt = "A press release is an official statement delivered to members of the news media for the purpose of"
# prompt = "Your favorite board game is"

print("Prompt: ",prompt,'\n', end ="")

# Initialize the state for decoding
state = {'current_line': '', 'current_line_length': 0}

with llama.generate(prompt, remote=True, max_new_tokens = 50) as generator:
    # Call .all() to apply to each new token
    llama.all()

    all_tokens = nnsight.list().save()

    # Access model output
    out = llama.lm_head.output.save()

    # Apply softmax to obtain probabilities and save the result
    probs = torch.nn.functional.softmax(out, dim=-1)
    max_probs = torch.max(probs, dim=-1)
    tokens = max_probs.indices.cpu().tolist()
    all_tokens.append(tokens[0]).save()

    with nnsight.local():
        state = my_decoding_function(tokens[0], llama, max_length=20, state=state)
```

## Looping across sessions

We mention earlier that the `session` context enables multi-tracing execution. But how can we optimize a process that would require running an intervention graph in a loop? If we create a simple `for` loop with a **Tracer context** inside, this will result in creating a new intervention graph at each iteration, which is not scalable.

We solve this problem the `nnsight` way via the **Iterator context**: an intervention loop that iteratively executes and updates a single intervention graph.

Use a `session` to define the **Iterator context** and pass in a sequence of items that you want to loop over at each iteration:

```python
with llama.session(remote=True) as session:

  with session.iter([0, 1, 2]) as item:
    # define intervention body here ...

    with llama.trace("_"):
      # define interventions here ...
      pass

    with llama.trace("_"):
      # define interventions here ...
      pass
```

The `Iterator` context extends all the `nnsight` graph-based functionalities, but also closely mimics the conventional `for` loop statement in Python, which allows it to support all kind of iterative operations with a use of `as item` syntax:

```python
with llama.session(remote=True) as session:

  li = nnsight.list()
  [li.append([num]) for num in range(0, 3)] # adding [0], [1], [2] to the list
  li2 = nnsight.list().save()

  # You can create nested Iterator contexts
  with session.iter(li) as item:
    with session.iter(item) as item_2:
      li2.append(item_2)

print("\nList: ", li2)
```

Notice how we used the `nnsight.list()` method to create a list of lists to loop over. This type of method is what we call an **NNsight Built-in**. It is a special type of methods that serve as a wrapper around `nnsight.apply()` to provide a more user-friendly interface for adding common datatypes to the Intervention Graph.

<details>
<summary>A full list of NNsight Built-ins</summary>

`nnsight.bool()` creates a traceable Boolean

`nnsight.bytes()` creates a traceable Bytes

`nnsight.int()` creates a traceable Integer

`nnsight.float()` creates a traceable Float

`nnsight.str()` creates a traceable String

`nnsight.comples()` creates a traceable Complex number

`nnsight.bytearray()` creates a traceable Bytearray

`nnsight.tuple()` creates a traceable Tuple

`nnsight.list()` creates a traceable List

`nnsight.set()` creates a traceable Set

`nnsight.dict()` creates a traceable Dictionary

</details>

We can also expose the `iterator` context object via a `return_context` flag. You can then use it to `exit` out of the Iteration loop early and log the intermediate outputs within the loop:

```python
with llama.session(remote=True) as session:

  # with session.iter([0, 1, 2, 3], return_context=True) as (item, iterator):
  with session.iter([0, 1, 2, 3]) as item:

      nnsight.log(item)

      with nnsight.cond(item == 2):
        nnsight.stop()
```

The **Iterator** context is a niece piece of functionality that allows you to define a bunch of basic code operations that can now be "traceable" by `nnsight`.

But in what kind of experimental scenario would someone need iterators?

In the next section, we delve into a powerful use case of the `Iterator` context and see how it enables it!

## Training a LoRA

Here is an example of a task that uses everything we have covered in the last section - remote execution, **Session** context and iterative interventions. Using session and iterator contexts, we're going apply a very simple fine-tuning approach called low-rank adaptation (LoRA).

Let's try training a LoRA that, when applied, makes our model always predict "Paris" no matter what.

```python
import torch
import torch.nn as nn
import nnsight
# from nnsight.envoy import Envoy # this moved in 0.4
from nnsight import Envoy

# We will define a LORA class.
# The LORA class call method operations are simply traced like you would normally do in a .trace.
class LORA(nn.Module):
    def __init__(self, module: Envoy, dim: int, r: int) -> None:
        """Init.

        Args:
            module (Envoy): Which model Module we are adding the LORA to.
            dim (int): Dimension of the layer we are adding to (This could potentially be auto populated if the user scanned first so we know the shape)
            r (int): Inner dimension of the LORA
        """
        super(LORA, self).__init__()
        self.r = r
        self.module = module
        self.WA = torch.nn.Parameter(torch.randn(dim, self.r), requires_grad=True).save()
        self.WB = torch.nn.Parameter(torch.zeros(self.r, dim), requires_grad=True).save()

    # The Call method defines how to actually apply the LORA.
    def __call__(self, alpha: float = 1.0):
        """Call.

        Args:
            alpha (float, optional): How much to apply the LORA. Can be altered after training for inference. Defaults to 1.0.
        """

        # We apply WA to the first positional arg (the hidden states)
        A_x = torch.matmul(self.module.input[0][0], self.WA)
        BA_x = torch.matmul(A_x, self.WB)

        # LORA is additive
        h = BA_x + self.module.output

        # Replace the output with our new one * alpha
        # Could also have been self.module.output[:] = h * alpha, for in-place
        self.module.output = h * alpha

    def parameters(self):
        # Some way to get all the parameters.
        return [self.WA, self.WB]
```

Let's define all the variables to use in LoRA training.

```python
# We need the token id of the correct answer.
answer = " Paris"
answer_token = llama.tokenizer.encode(answer)[1]
# Inner LORA dimension
lora_dim = 4
# Module to train LORA on
module = llama.model.layers[-1].mlp
```

We can use the `.scan()` method to get the shape of the module without having to fully run the model.

```python
with llama.scan(" "):
    dim = module.output.shape[-1]

print(dim)
```

It's time to run the LORA training loop! We using the **Session** and the **Iterator** contexts to achieve this.

```python
from torch.utils.data import DataLoader

# The LORA object itself isn't transmitted to the server. Only the forward / call method.
# The parameters are created remotely and never sent only retrieved
with llama.session(remote=True) as session:

    # Create dataset of 100 pairs of a blank prompt and the " Paris " id
    dataset = [["_", answer_token]] * 100

    # Create a dataloader from it.
    dataloader = DataLoader(dataset, batch_size=10)

    # Create our LORA on the last mlp
    lora = LORA(module, dim, lora_dim)

    # Create an optimizer. Use the parameters from LORA
    optimizer = torch.optim.AdamW(lora.parameters(), lr=3)

    # Iterate over dataloader using .iter.
    with session.iter(dataloader) as batch:

        prompt = batch[0]
        correct_token = batch[1]

        # Run .trace with prompt
        with llama.trace(prompt) as tracer:

            # Apply LORA to intervention graph just by calling it with .trace
            lora()

            # Get logits
            logits = llama.lm_head.output

            # Do cross entropy on last predicted token and correct_token
            loss = torch.nn.functional.cross_entropy(logits[:, -1], batch[1])
            # Call backward
            loss.backward()

        # Call methods on optimizer. Graphs that arent from .trace (so in this case session and iterator both have their own graph) are executed sequentially.
        # The Graph of Iterator here will be:
        # 1.) Index batch at 0 for prompt
        # 2.) Index batch at 1 for correct_token
        # 3.) Execute the .trace using the prompt
        # 4.) Call .step() on optimizer
        optimizer.step()
        # 5.) Call .zero_grad() in optimizer
        optimizer.zero_grad()
        # 6.) Print out the lora WA weights to show they are indeed changing
        nnsight.log(lora.WA)

```

Now `WA` and `WB` are optimized! So we generate with the LoRA just by calling `lora()` in the `.generate` and save the output to then de-tokenize it.

```python
# With lora. Should produce "Hello Paris"
with llama.generate("Hello", remote=True) as generator:

    lora()

    out = llama.generator.output.save()

print(llama.tokenizer.batch_decode(out.value))

# Then without. Should produce "Hello,"
with llama.generate("Hello", remote=True) as generator:

    out = llama.generator.output.save()

print(llama.tokenizer.batch_decode(out.value))

```

# Next Steps
Check out [nnsight.net/tutorials](https://nnsight.net/tutorials) for more walkthroughs implementating classic interpretability techniques using `nnsight`.

## Getting Involved!

Note that both `nnsight` and `NDIF` are in active development, so changes may be made and errors may arise during use. If you’re interested in following updates to `nnsight`, contributing, giving feedback, or finding collaborators, please join the [NDIF discord](https://discord.gg/6uFJmCSwW7). We’d love to hear about your work using nnsight!

You can also follow us on [LinkedIn](https://www.linkedin.com/company/national-deep-inference-fabric/), Bluesky: [@ndif-team.bsky.social](https://bsky.app/profile/ndif-team.bsky.social), and X: [@ndif_team](https://x.com/ndif_team).

💟


---

