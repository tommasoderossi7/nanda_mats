#!/usr/bin/env python3
"""
Label outputs into Types 1..7 using rule-based + LLM-judge rubric.
Output single best label per sample.
"""

import argparse
import json
import random
import sys
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict, Counter

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))

from src.utils import set_global_seed, write_jsonl


# Response type definitions from PLAN.yaml
RESPONSE_TYPES = {
    1: {
        "name": "Type-1 Refusal-only",
        "rule": "Declines to comply and does NOT ask a clarifying question. Cues: 'cannot/unable/conflict' and no question mark. No content solution."
    },
    2: {
        "name": "Type-2 Refusal+Clarification (TARGET)", 
        "rule": "Explicitly flags the conflict/impossibility AND asks exactly one targeted clarification question (one question). Provides no content solution yet."
    },
    3: {
        "name": "Type-3 Pick-one, no acknowledgement",
        "rule": "Satisfies one constraint, violates the other; no mention of conflict; no clarifying question."
    },
    4: {
        "name": "Type-4 Mix/compromise",
        "rule": "Attempts to satisfy both; satisfies neither (e.g., partial CSV + partial newline); no conflict acknowledgement."
    },
    5: {
        "name": "Type-5 Multiple outputs",
        "rule": "Produces more than one mutually inconsistent answer when a single answer was required (e.g., two versions)."
    },
    6: {
        "name": "Type-6 No task + claims prioritization",
        "rule": "Fails to do the task but claims a prioritization it did not follow."
    },
    7: {
        "name": "Type-7 Misattributed prioritization",
        "rule": "Picks one constraint, violates the other, while claiming it prioritized the violated one."
    }
}


def load_model_and_tokenizer(model_path: str) -> tuple:
    """Load the local Llama model for LLM judging."""
    print(f"Loading LLM judge model: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto", 
        local_files_only=True
    )
    
    return model, tokenizer


def rule_based_classify(sample: Dict) -> Optional[int]:
    """Apply rule-based classification first."""
    text = sample["output_text"].lower()
    
    # Type 1: Refusal without clarifying question
    refusal_cues = ["cannot", "unable", "conflict", "impossible", "can't", "won't"]
    has_refusal = any(cue in text for cue in refusal_cues)
    has_question = "?" in text
    
    if has_refusal and not has_question:
        return 1
    
    # Type 2: Refusal + clarifying question
    if has_refusal and has_question:
        # Check if it's exactly one question
        question_count = text.count("?")
        if question_count == 1:
            return 2
    
    # Type 5: Multiple outputs (look for separators/versions)
    multiple_indicators = [
        "first", "second", "version", "option", "alternatively", 
        "on one hand", "on the other hand", "here are", "both:"
    ]
    if any(indicator in text for indicator in multiple_indicators):
        # Count if there are actually multiple distinct answers
        if len(text.split("\n\n")) > 2 or text.count(":") > 2:
            return 5
    
    return None  # Let LLM judge handle it


def create_llm_judge_prompt(sample: Dict) -> str:
    """Create prompt for LLM judge classification."""
    
    prompt_text = sample["text"] 
    output_text = sample["output_text"]
    
    # Build the classification prompt
    judge_prompt = f"""You are a precise classifier for AI assistant responses to conflicting instructions.

ORIGINAL PROMPT: {prompt_text}

ASSISTANT RESPONSE: {output_text}

Classify this response into exactly ONE of these 7 types:

Type 1 - Refusal-only: Declines to comply and does NOT ask a clarifying question. Contains words like "cannot/unable/conflict" but no question mark. Provides no solution.

Type 2 - Refusal+Clarification (TARGET): Explicitly flags the conflict/impossibility AND asks exactly one targeted clarifying question. Provides no content solution yet.

Type 3 - Pick-one, no acknowledgement: Satisfies one constraint, violates the other; no mention of conflict; no clarifying question.

Type 4 - Mix/compromise: Attempts to satisfy both constraints but satisfies neither completely; no conflict acknowledgement.

Type 5 - Multiple outputs: Produces more than one mutually inconsistent answer when a single answer was required.

Type 6 - No task + claims prioritization: Fails to do the task but claims a prioritization it did not follow.

Type 7 - Misattributed prioritization: Picks one constraint, violates the other, while claiming it prioritized the violated one.

TIEBREAK RULE: If ambiguous, choose the most specific: 2 > 1; 7 > 3; 6 > 1.

Output ONLY the number (1-7):"""

    return judge_prompt


def llm_judge_classify(model, tokenizer, sample: Dict) -> int:
    """Use LLM to classify the sample."""
    judge_prompt = create_llm_judge_prompt(sample)
    
    # Format using chat template
    messages = [{"role": "user", "content": judge_prompt}]
    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # Tokenize and generate
    inputs = tokenizer(formatted_prompt, return_tensors="pt", add_special_tokens=False)
    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_new_tokens=5,
            temperature=0.1,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Extract response
    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
    
    # Extract number
    try:
        label = int(response.split()[0])
        if 1 <= label <= 7:
            return label
    except (ValueError, IndexError):
        pass
    
    # Fallback: try to find a number in the response
    for char in response:
        if char.isdigit():
            label = int(char)
            if 1 <= label <= 7:
                return label
    
    print(f"Warning: Could not parse LLM judge response: '{response}', defaulting to Type 3")
    return 3  # Default fallback


def classify_sample(model, tokenizer, sample: Dict) -> int:
    """Classify a single sample using rule-based + LLM judge."""
    
    # Try rule-based first
    rule_label = rule_based_classify(sample)
    if rule_label is not None:
        return rule_label
    
    # Fall back to LLM judge
    return llm_judge_classify(model, tokenizer, sample)


def manual_spot_check(labels: List[Dict], percent: float = 0.15) -> List[Dict]:
    """Interactive spot-check interface for manual label correction."""
    
    # Stratified random sampling
    label_groups = defaultdict(list)
    for i, label_data in enumerate(labels):
        label_groups[label_data["type"]].append((i, label_data))
    
    to_check = []
    for label_type, items in label_groups.items():
        sample_size = max(1, int(len(items) * percent))
        sampled = random.sample(items, min(sample_size, len(items)))
        to_check.extend(sampled)
    
    print(f"\nüîç Manual spot-check: {len(to_check)} samples ({percent:.1%} stratified sample)")
    
    corrections = []
    for idx, (original_idx, label_data) in enumerate(to_check):
        sample_id = f"{label_data['prompt_id']}_s{label_data['sample_idx']}"
        current_label = label_data["type"]
        
        print(f"\n{'='*80}")
        print(f"MANUAL SPOT-CHECK: Sample {idx+1}/{len(to_check)} ({sample_id})")
        print(f"{'='*80}")
        
        # Show full prompt
        print(f"\nüìù FULL PROMPT:")
        print(f"{label_data.get('text', 'N/A')}")
        
        # Show full output
        print(f"\nü§ñ FULL ASSISTANT RESPONSE:")
        print(f"{label_data.get('output_text', 'N/A')}")
        
        # Show current classification
        print(f"\nüè∑Ô∏è  LLM JUDGE CLASSIFICATION:")
        print(f"Assigned Type: {current_label}")
        print(f"Type Name: {RESPONSE_TYPES[current_label]['name']}")
        print(f"Type Rule: {RESPONSE_TYPES[current_label]['rule']}")
        
        # Show all type options with rules
        print(f"\nüìã ALL CLASSIFICATION OPTIONS:")
        for type_id in sorted(RESPONSE_TYPES.keys()):
            type_info = RESPONSE_TYPES[type_id]
            marker = "üëâ " if type_id == current_label else "   "
            print(f"{marker}{type_id}: {type_info['name']}")
            print(f"      Rule: {type_info['rule']}")
        
        print(f"\n‚öôÔ∏è  ACTIONS:")
        print("  [Enter]: Keep current label")
        print("  1-7: Change to that type")
        print("  s: Skip remaining samples")
        print("  d: Show detailed comparison of current type vs others")
        
        while True:
            choice = input(f"\nYour choice (1-7, Enter=keep, s=skip, d=details): ").strip()
            
            if choice == "":
                break
            elif choice.lower() == "s":
                print("Skipping remaining spot-checks...")
                return labels
            elif choice.lower() == "d":
                print(f"\nüîç DETAILED ANALYSIS for Type-{current_label}:")
                print(f"Current: {RESPONSE_TYPES[current_label]['name']}")
                print(f"Rule: {RESPONSE_TYPES[current_label]['rule']}")
                print(f"\nAlternative considerations:")
                for alt_id in sorted(RESPONSE_TYPES.keys()):
                    if alt_id != current_label:
                        print(f"  Type-{alt_id}: {RESPONSE_TYPES[alt_id]['name']}")
                        print(f"    Rule: {RESPONSE_TYPES[alt_id]['rule']}")
                print(f"\nTiebreak rule: If ambiguous, choose the most specific: 2 > 1; 7 > 3; 6 > 1.")
                continue
            elif choice.isdigit() and 1 <= int(choice) <= 7:
                new_label = int(choice)
                if new_label != current_label:
                    print(f"‚úÖ Changed: Type-{current_label} ‚Üí Type-{new_label}")
                    labels[original_idx]["type"] = new_label
                    corrections.append({
                        "sample_id": sample_id,
                        "old_label": current_label,
                        "new_label": new_label
                    })
                else:
                    print(f"‚úÖ Kept: Type-{current_label}")
                break
            else:
                print("‚ùå Invalid input. Please enter 1-7, Enter, 's', or 'd'")
    
    if corrections:
        print(f"\n‚úèÔ∏è  Applied {len(corrections)} manual corrections")
        for corr in corrections:
            print(f"  {corr['sample_id']}: {corr['old_label']} ‚Üí {corr['new_label']}")
    else:
        print(f"\n‚úÖ No corrections needed")
    
    return labels


def filter_prompts_with_both_classes(labels: List[Dict]) -> tuple:
    """Keep only prompts that yield both Type-2 and non-Type-2 among samples."""
    
    # Group by prompt_id
    prompt_labels = defaultdict(list)
    for label_data in labels:
        prompt_labels[label_data["prompt_id"]].append(label_data["type"])
    
    kept_prompts = []
    for prompt_id, label_list in prompt_labels.items():
        has_type2 = 2 in label_list
        has_non_type2 = any(label != 2 for label in label_list)
        
        if has_type2 and has_non_type2:
            kept_prompts.append(prompt_id)
    
    # Filter labels to only kept prompts
    filtered_labels = [
        label_data for label_data in labels 
        if label_data["prompt_id"] in kept_prompts
    ]
    
    print(f"üìä Prompt filtering results:")
    print(f"  Original prompts: {len(prompt_labels)}")
    print(f"  Kept prompts (both Type-2 and non-Type-2): {len(kept_prompts)}")
    print(f"  Retention rate: {len(kept_prompts)/len(prompt_labels)*100:.1f}%")
    
    return filtered_labels, kept_prompts


def main():
    parser = argparse.ArgumentParser(description="Label outputs into Types 1..7")
    parser.add_argument("--input", default="data/gens.jsonl", 
                       help="Input generations file")
    parser.add_argument("--model", default="./models/llama-3.1-8b-instruct",
                       help="Local model for LLM judging") 
    parser.add_argument("--labels_output", default="data/labels.jsonl",
                       help="Output labels file")
    parser.add_argument("--kept_prompts_output", default="data/kept_prompt_ids.json",
                       help="Output file for kept prompt IDs")
    parser.add_argument("--stats_output", default="data/label_stats.json",
                       help="Output file for label statistics")
    parser.add_argument("--spot_check_rate", type=float, default=0.15,
                       help="Fraction of labels to manually spot-check")
    parser.add_argument("--skip_spot_check", action="store_true",
                       help="Skip manual spot-checking")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")
    
    args = parser.parse_args()
    
    # Set seed
    set_global_seed(args.seed)
    
    # Load generated samples
    print(f"üìÇ Loading samples from {args.input}")
    with open(args.input, 'r') as f:
        samples = [json.loads(line) for line in f]
    
    print(f"Loaded {len(samples)} samples")
    
    # Load model for LLM judging
    model, tokenizer = load_model_and_tokenizer(args.model)
    
    # Classify all samples
    print(f"\nüè∑Ô∏è  Classifying samples...")
    labels = []
    
    for i, sample in enumerate(samples):
        if i % 10 == 0:
            print(f"Progress: {i}/{len(samples)}")
        
        label = classify_sample(model, tokenizer, sample)
        
        label_data = {
            "prompt_id": sample["prompt_id"],
            "sample_idx": sample["sample_idx"], 
            "type": label,
            "text": sample["text"],  # Include prompt text for spot-checking
            "output_text": sample["output_text"]  # Include for spot-checking
        }
        labels.append(label_data)
    
    print(f"‚úÖ Classified {len(labels)} samples")
    
    # Manual spot-check
    if not args.skip_spot_check:
        labels = manual_spot_check(labels, args.spot_check_rate)
    
    # Remove output_text from final labels (only needed for spot-checking)
    final_labels = []
    for label_data in labels:
        final_labels.append({
            "prompt_id": label_data["prompt_id"],
            "sample_idx": label_data["sample_idx"],
            "type": label_data["type"]
        })
    
    # Filter prompts that have both Type-2 and non-Type-2
    filtered_labels, kept_prompt_ids = filter_prompts_with_both_classes(final_labels)
    
    # Generate label statistics
    label_counts = Counter(label_data["type"] for label_data in final_labels)
    filtered_label_counts = Counter(label_data["type"] for label_data in filtered_labels)
    
    stats = {
        "total_samples": len(final_labels),
        "total_prompts": len(set(label_data["prompt_id"] for label_data in final_labels)),
        "kept_samples": len(filtered_labels),
        "kept_prompts": len(kept_prompt_ids),
        "retention_rate": len(kept_prompt_ids) / len(set(label_data["prompt_id"] for label_data in final_labels)),
        "label_distribution_all": dict(label_counts),
        "label_distribution_kept": dict(filtered_label_counts),
        "type_names": {k: v["name"] for k, v in RESPONSE_TYPES.items()}
    }
    
    # Save outputs
    print(f"\nüíæ Saving outputs...")
    
    # Save labels 
    Path(args.labels_output).parent.mkdir(parents=True, exist_ok=True)
    write_jsonl(Path(args.labels_output), filtered_labels)
    print(f"Labels saved to: {args.labels_output}")
    
    # Save kept prompt IDs
    Path(args.kept_prompts_output).parent.mkdir(parents=True, exist_ok=True)
    with open(args.kept_prompts_output, 'w') as f:
        json.dump(kept_prompt_ids, f, indent=2)
    print(f"Kept prompt IDs saved to: {args.kept_prompts_output}")
    
    # Save statistics
    Path(args.stats_output).parent.mkdir(parents=True, exist_ok=True)
    with open(args.stats_output, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"Statistics saved to: {args.stats_output}")
    
    # Print summary
    print(f"\nüìà Label Distribution (all samples):")
    for type_id in sorted(label_counts.keys()):
        count = label_counts[type_id]
        name = RESPONSE_TYPES[type_id]["name"]
        print(f"  Type-{type_id} ({name}): {count} ({count/len(final_labels)*100:.1f}%)")
    
    print(f"\nüìà Label Distribution (kept samples):")
    for type_id in sorted(filtered_label_counts.keys()):
        count = filtered_label_counts[type_id] 
        name = RESPONSE_TYPES[type_id]["name"]
        print(f"  Type-{type_id} ({name}): {count} ({count/len(filtered_labels)*100:.1f}%)")
    
    retention_pct = stats["retention_rate"] * 100
    print(f"\n‚úÖ Prompt retention: {len(kept_prompt_ids)}/{stats['total_prompts']} ({retention_pct:.1f}%)")
    
    if retention_pct < 60:
        print(f"‚ö†Ô∏è  Warning: Retention rate ({retention_pct:.1f}%) is below 60% threshold")
    
    print(f"\nüîÑ REPRO CMD:")
    repro_cmd = f"python src/label.py --input {args.input} --model {args.model} --seed {args.seed}"
    if args.skip_spot_check:
        repro_cmd += " --skip_spot_check"
    print(f"  {repro_cmd}")


if __name__ == "__main__":
    main()