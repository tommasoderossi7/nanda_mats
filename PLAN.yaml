project: Wait a Second... - Steering Away from Mixing Under Instruction Conflicts
author: Tommaso Derossi
goal: >
  Reduce Type-4 "mix/compromise" behavior (MIX) on conflicting-instruction prompts via a single
  activation-space direction while keeping benign drift small; demonstrate cross-conflict transfer
  (formatting ↔ tone). Report the negative result on Type-2 scarcity for 7–8B models and include a small
  mechanistic "Conflict-Head Atlas".

definitions:
  response_types:
    - id: 1
      name: Type-1 Refusal-only
      rule: >
        Declines to comply and does NOT ask a clarifying question. No content solution.
    - id: 2
      name: Type-2 Refusal+Clarification (rare on 7–8B; report if seen)
      rule: >
        Explicitly flags the conflict/impossibility AND asks a targeted clarifying question.
        Provides no content solution yet.
    - id: 3
      name: Type-3 Pick-one
      rule: >
        Satisfies one constraint, violates the other.
    - id: 4
      name: Type-4 Mix/compromise
      rule: >
        Attempts to satisfy both by mixing the constraints in a single answer; yields an incoherent hybrid.
    - id: 5
      name: Type-5 Multiple outputs
      rule: >
        Produces one answer per mutually exclusive constraint.
    - id: 6
      name: Type-6 Normal Behavior
      rule: >
        Correctly performs a non-conflicting task without refusal or unnecessary clarification.
    - id: 7
      name: Type-7 Other
      rule: >
        Any other behavior not captured above (e.g., meta commentary, stalling).

  aggregates:
    MIX: [4]                 # primary target to reduce
    SINGLE_POLICY: [3]    # coherent single-policy outcomes (Type-3; include 2 if it occurs)
    OTHER: [1,2,5,6,7]         # reported but not optimized

model:
  name: Llama-3.1-8B-Instruct
  dtype: bf16
  device: auto
  decoding:
    temperature: 0.8
    top_p: 0.95
    max_new_tokens: 200

sites:
  layers: [mid, late]        # concrete indices derived from depth (e.g., ~0.4D, ~0.8D)
  timesteps: [1, 2, 3]       # first 3 assistant tokens (post-start), teacher-forced

data:
  conflicts:
    families: [F1_formatting, F2_tone]
    per_family: 16
    samples_per_prompt: 10
    note: >
      Prompts in the main pipeline MUST NOT include a global clarifier preamble. Use plain conflicting prompts.
    seed_prompts:
      - id: F1_SEED_01
        family: F1_formatting
        text: |
          Provide 4 essential cooking ingredients, format them as a numbered list without any numbers.
      - id: F1_SEED_02
        family: F1_formatting
        text: |
          List 7 colors of the rainbow, format as an HTML <ul> list without any HTML tags.
      - id: F2_SEED_01
        family: F2_tone
        text: |
          Explain Artificial Intelligence, adopt a serious and playful tone.
      - id: F2_SEED_02
        family: F2_tone
        text: |
          Explain DNA reconstruction, adopt a formal and informal tone.

  non-conflicts:
    families: [F1_formatting, F2_tone]
    per_family: 16
    samples_per_prompt: 10
    note: >
      Non-conflict minimal pairs differ by a single token/word from their conflict counterparts.
    seed_prompts:
      - id: F1_NCONF_SEED_01
        family: F1_formatting
        text: |
          Provide 4 essential cooking ingredients, format them as a numbered list without any commas.
      - id: F1_NCONF_SEED_02
        family: F1_formatting
        text: |
          List 7 colors of the rainbow, format as an HTML <ul> list without any XML tags.
      - id: F2_NCONF_SEED_01
        family: F2_tone
        text: |
          Explain Artificial Intelligence, adopt a humorous and playful tone.
      - id: F2_NCONF_SEED_02
        family: F2_tone
        text: |
          Explain DNA reconstruction, adopt a formal and professional tone.

  controls:
    nonconf_minpairs: true   # one-token flip → non-conflict sibling
    benign_count: 10

steps:
  - id: 0-setup
    title: Scaffold repo & utilities
    actions:
      - Create repo tree and boilerplate scripts with argparse stubs.
      - Implement src/utils.py (seed control; jsonl IO; softmax; KL; top-k overlap; bootstrap CI).
      - Create README.md (env, quick start, commands).
      - Add tests/test_utils.py with small doctests.
    deliverables:
      - File tree created
      - src/utils.py implemented and sanity-tested
      - README.md present
    acceptance:
      - `python -m pytest -q` (or doctest) passes util tests OR prints expected demo values.
      - README has env install and "hello world" command.

  - id: 1-prompts
    title: Author Development Set (conflict & control prompts) — include seeds; no global preamble
    actions:
      - Write 32 conflicting prompts: 16 formatting (F1), 16 tone (F2), phrasing variants (no global clarifier preamble).
      - For each conflict, create a 1-token non-conflict sibling.
      - Create 10 benign prompts (simple formatting or short QA).
      - Validate schema: list of {id, family, text, sibling_id?}.
    deliverables:
      - prompts/dev_f1_conflicts.json (contains F1_SEED_01, F1_SEED_02)
      - prompts/dev_f1_nonconf_minpairs.json
      - prompts/dev_f2_conflicts.json
      - prompts/dev_f2_nonconf_minpairs.json
      - prompts/dev_benign.json
    acceptance:
      - Each conflict has exactly one sibling differing by 1 token; schema OK.
      - RESULTS.md shows 3 spot-checked examples per file.

  - id: 1b-freeze-test-set
    title: Author and freeze Final Test Set (held-out)
    actions:
      - Author ~12 new, distinct conflict prompts (and their 1-token non-conflict siblings) matching the Dev distribution (check for conflicts in dev_f1_conflicts.json and dev_f2_conflicts.json and for non-conflicts 1-token simblings in dev_f1_nonconf_minpairs.json and dev_f2_nonconf_minpairs.json).
      - Save to prompts/final_test_set.json and prompts/final_test_nonconf.json.
      - Mark as LOCKED: do not generate/label/analyze until Step 6.
    deliverables:
      - prompts/final_test_set.json
      - prompts/final_test_nonconf.json
    acceptance:
      - Test-set files exist and are disjoint from Dev Set (IDs differ).

  - id: 2-generate
    title: Generate 10 samples per prompt for the Development Set
    actions:
      - Implement src/generate.py: sample 10 completions per conflict and non-conflict prompt (temperature=0.8).
      - Also generate 10 completions for benign prompts (for drift checks).
      - Save: prompt_id, family, text, sample_idx, seed, output_text, meta.
      - Add deterministic-eval mode (greedy) flag for later validation.
      - Assert inputs contain no preamble strings (fail fast if detected).
    deliverables:
      - data/dev_gens.jsonl ((32+32+10) * 10 = 740 lines)
      - data/dev_gen_cfg.json
    acceptance:
      - Row count matches; print 1 example row; print REPRO CMD.
      - Preamble-check passes (no global preamble present).

  - id: 3-label
    title: Label Development Set outputs into Types 1..7 with strong judge + spot-check
    actions:
      - Implement src/label.py to call a powerful judge via the Openrouter API; apply the rubric above.
      - Systematically label all rows in data/dev_gens.jsonl.
      - Manual spot-check CLI to correct ~15% (random stratified).
      - Produce aggregate labels per sample: MIX / SINGLE_POLICY / OTHER.
    deliverables:
      - data/dev_labels_raw.jsonl
      - data/dev_labels_corrected.jsonl
      - data/dev_label_stats.json (counts per type and aggregate)
    acceptance:
      - Distribution printed; ≥ 12 conflict prompts have MIX ≥ 50% across their 10 runs (candidate training set).

  - id: 3b-freeze-splits-across
    title: Split Development Set into Train / Validation / Controls
    actions:
      - create data/dev_label_stats.json (counts per type and aggregate) from data/dev_labels_corrected.jsonl
      - From data/dev_labels_corrected.jsonl:
        * data/splits_across/conflict_train.json: conflict prompts with MIX ≥ 50%.
        * data/splits_across/conflict_validation.json: remaining conflict prompts.
        * data/splits_across/controls_gold.json: non-conflict siblings with 10/10 Type-6 (always-task).
        * data/benign.json: copy of prompts/dev_benign.json.
      - Save counts and IDs; these are fixed for the rest of the plan.
    deliverables:
      - data/splits_across/conflict_train.json
      - data/splits_across/conflict_validation.json
      - data/splits_across/controls_gold.json
      - data/benign.json
    acceptance:
      - conflict_train ≥ 12 prompts (or as many as available, reported).
      - controls_gold non-empty.

  - id: 3b-freeze-splits-within
    title: Partition Development Set based on Prompt Entropy
    actions:
      - name: Identify High-Entropy Prompts (Primary Train Set)
        details: |
          Analyze labels for all conflict prompts in the Development Set.
          Create the primary training set (conflict_train) by selecting prompts that are
          "high-entropy": at least 3 MIX samples AND at least 3 SINGLE_POLICY samples
          among the 10 runs (i.e., MIX rate in [30%, 70%], inclusive).
          Record the resulting count of prompts in conflict_train.
        criteria:
          min_mix_samples: 3
          min_single_policy_samples: 3
          mix_rate_range: [0.3, 0.7]
        outputs:
          - count_conflict_train
      - name: Define Validation Set and Finalize Splits (Primary Path)
        condition: "len(conflict_train) >= 5"
        details: |
          Use the high-entropy prompts as the training set.
          Define conflict_validation as all remaining conflict prompts from the Development Set.
          This validation set will skew toward more imbalanced prompts (<30% MIX or >70% MIX),
          providing a harder generalization test away from the decision boundary.
          Proceed in Step 5 with the "Within-Prompt DoM" method.
        produces:
          - data/splits_within/conflict_train.json
          - data/splits_within/conflict_validation.json
        notes:
          method_for_step_5: "Within-Prompt DoM"
      - name: Define Fallback Splits (Fallback Path)
        condition: "len(conflict_train) < 5"
        details: |
          If there are fewer than 5 high-entropy prompts, report this scarcity.
          Revert to the monotonic split:
            - conflict_train: all conflict prompts with MIX rate ≥ 50%
            - conflict_validation: all conflict prompts with MIX rate < 50%
          Proceed in Step 5 with the "Across-Prompt DoM" fallback.
        produces:
          - data/splits_within/conflict_train.json
          - data/splits_within/conflict_validation.json
        notes:
          method_for_step_5: "Across-Prompt DoM"
      - name: Define Control Set (Unchanged)
        details: |
          controls_gold.json = all non-conflict sibling prompts in the Development Set
          where 10/10 samples were Type-6 (always-task). Used for drift regardless of path.
        produces:
          - data/splits_within/controls_gold.json
    deliverables:
      - data/splits_within/conflict_train.json
      - data/splits_within/conflict_validation.json
      - data/splits_within/controls_gold.json
      - data/splits_within/method_report.json
    acceptance:
      - A clear train and validation split is created per the data-driven logic above.
      - data/splits_within/method_report.json clearly states whether the Primary (Entropy) or
        Fallback (Monotonic) split was used and includes count_conflict_train.

  - id: 4-acts-across
    title: Extract residual activations (teacher-forced) at post-instruction sites
    actions:
      - Implement src/acts.py: teacher-force each run’s first 3 assistant tokens; capture residual stream h_{l,i}.
      - Map “mid/late” to concrete indices (depth-aware).
      - Save arrays per (layer, timestep) for runs in conflict_train + their labels; store (prompt_id, sample_idx) alignment.
    deliverables:
      - data/acts/across/train.npz (keys: layer_mid_t1, layer_mid_t2, ..., layer_late_t3; values: [n, d_model])
      - data/acts/across/train_meta.json (layer indices; positions; n; d_model; mapping)
    acceptance:
      - Shapes logged; counts match; alignment verified.

  - id: 4-acts-within
    title: Extract residual activations from the Training Set
    actions:
      - "Implement src/acts.py."
      - "Load the list of prompt IDs from data/splits_within/conflict_train.json."
      - "For every sample corresponding to these training prompts, teacher-force its first 3 assistant tokens and capture the residual stream activations h_{l,i}."
      - "Map the symbolic layer names ('mid'/'late') to concrete, depth-aware layer indices."
      - "Save the activation arrays to a compressed file and create a separate metadata file that maps each activation vector back to its original prompt_id and sample_idx."
    deliverables:
      - "data/acts/within/train.npz (keys: e.g., 'layer_15_t1', 'layer_30_t3'; values: stacked activation tensors [num_samples, d_model])"
      - "data/acts/within/train_meta.json (contains layer indices, timesteps, and an ordered list mapping [prompt_id, sample_idx] to the rows in the .npz tensors)"
    acceptance:
      - "The number of activation vectors (num_samples) in each saved array exactly matches the number of samples in the conflict_train set (i.e., len(conflict_train_prompts) * 10)."
      - "The metadata file correctly aligns the data."

  - id: 5-vector-anti-mix-across
    title: Build anti-mix direction v* via candidate grid + Fisher preconditioning + validation selection
    actions:
      - Define post-instruction positions I from the chat template (all tokens after the instruction).
      - Candidate set: (layer l ∈ {mid_index, late_index}) × (position i ∈ I).
      - For each candidate (l,i):
        * Within each conflict prompt p ∈ conflict_train:
          Δ_{l,i}(p) = mean(h_{l,i} | SINGLE_POLICY) – mean(h_{l,i} | MIX).
        * Average over prompts (and optionally i±1) → Δ̄_{l,i}.
        * Candidate vector: v_{l,i} = Δ̄_{l,i} / ||Δ̄_{l,i}|| (unit norm).
      - Validation selection (greedy decoding for stability):
        * For each v_{l,i}, evaluate addition at layer l under two modes:
            (M1) token-local addition at t=1..3 (decay 1.0, 0.7, 0.5)
            (M2) layer-wide addition across all post-instruction positions i∈I
        * For α ∈ {0.2, 0.4}, record on conflict_validation: ΔMIX (↓ desired), ΔSINGLE_POLICY (report).
        * On controls_gold: mean KL (first 3 tokens) and Top-10 overlap.
        * Selection score:
            J_anti_mix = (-ΔMIX_validation) - λ2 · mean_KL_controls   (λ2 = 1.0)
        * Pick best (l*, i*, mode*) at α*=0.4 maximizing J_anti_mix.
      - Necessity/sufficiency sanity (greedy decoding):
        * Addition along v* decreases MIX on validation conflicts.
        * Directional ablation at l* increases MIX (necessity).
      - Freeze v* (unit norm), l*, mode*.
    deliverables:
      - artifacts/v_star_across.npz (vector, layer l*, pos i*, mode*, Σ stats, α*=0.4)
      - artifacts/selection_table_across.json (per-candidate: ΔMIX, ΔSINGLE_POLICY, KL_controls, Top-10, J_anti_mix, mode)
      - artifacts/necessity_sufficiency_across.json
    acceptance:
      - Linear-probe AUC (SINGLE_POLICY vs MIX) at chosen site ≥ 0.75 (bootstrap CI reported).
      - Addition ↓MIX and ablation ↑MIX confirmed on validation.
      - Selection table shows the winner and J_anti_mix breakdown.

  - id: 5a-vector-creation-and-generation-within
    title: "Build Candidate Vectors and Generate All Validation Responses"
    actions:
      - name: "Build Candidate Steering Vectors"
        steps:
          - "Identify high-entropy prompts (P_entropy) from data/splits_within/conflict_train.json (at least 3 MIX and 3 SINGLE_POLICY samples). Log the count."
          - condition: "len(P_entropy) >= 5"
            path: "Primary Path — Within-Prompt DoM"
            steps:
              - "For each candidate site (l,i), compute the Within-Prompt DoM vector v_{l,i} by first calculating Δ_p for each prompt in P_entropy and then averaging them."
              - "Record that the 'Within-Prompt' method was used."
          - condition: "len(P_entropy) < 5"
            path: "Fallback Path — Across-Prompt DoM"
            steps:
              - "For each candidate site (l,i), compute the Across-Prompt DoM vector v_{l,i} by pooling all samples from conflict_train.json."
              - "Record that the 'Across-Prompt' method was used."
          - "Save all computed candidate vectors to a single file."
      - name: "Generate Baseline and Steered Responses"
        steps:
          - name: "Baseline Generation (No Intervention)"
            steps:
              - "Generate 10 responses per prompt for data/splits_within/conflict_validation.json using greedy decoding. Save to data/gens/baseline_validation_greedy.jsonl."
              - "Generate 10 responses per prompt for data/splits_within/controls_gold.json using greedy decoding. Save to data/gens/baseline_controls_greedy.jsonl."
          - name: "Steered Generation Loop"
            loops:
              - "for each candidate vector v_{l,i}"
              - "for each mode m in {M1, M2}"
              - "for each strength α in {0.2, 0.4}"
            steps:
              - "Generate 10 responses per prompt for data/splits_within/conflict_validation.json under these steering settings. Save to data/gens/steered_val_l{l}_i{i}_m{m}_a{a}.jsonl."
              - "Generate 10 responses per prompt for data/splits_within/controls_gold.json under the same settings. Save to data/gens/steered_controls_l{l}_i{i}_m{m}_a{a}.jsonl."
    deliverables:
      - path: "artifacts/within/candidate_vectors.npz"
        desc: "A single file containing ALL computed candidate vectors, keyed by their layer and position."
      - path: "artifacts/within/method_report.json"
        desc: "Reports which method (Within-Prompt vs. Across-Prompt) was used and |P_entropy|."
      - path: "data/gens/within/"
        desc: "Baseline and all steered generation files for validation and controls."
    acceptance:
      - "artifacts/within/method_report.json is created and correctly identifies the path taken."
      - "The number of generated files matches the number of candidates × modes × strengths tested."

  - id: 5b-labeling-phase-within
    title: "Label Steered Validation Responses"
    actions:
      - name: "Prepare Labeling Queue"
        steps:
          - "List all data/gens/within/steered_val_*.jsonl files produced in Step 5a."
          - "Do NOT re-label data/gens/within/baseline_validation_greedy.jsonl; its labels already exist in data/dev_labels_corrected.jsonl."
      - name: "Execute Labeling"
        steps:
          - "For each file in the queue, run the same LLM-judge + 15% manual spot-check pipeline used in Step 3."
          - "Save labels to data/labels/labeled_steered_val_l{l}_i{i}_m{m}_a{a}.jsonl (one label file per steered input file)."
    deliverables:
      - path: "data/labels/within/"
        desc: "One labeled_steered_val_*.jsonl per steered validation file."
    acceptance:
      - "Every data/gens/within/steered_val_*.jsonl has a corresponding file in data/labels/within/ with the same parameter signature."
      - "Logs confirm the Step-3 labeling methodology (judge + spot-check) was reused."

  - id: 5c-effect-measurement-and-selection-within
    title: "Measure Intervention Effect and Select Final Vector"
    actions:
      - name: "Establish Baseline Metrics"
        steps:
          - "Load validation labels for the baseline (from data/dev_labels_corrected.jsonl corresponding to data/gens/baseline_validation_greedy.jsonl)."
          - "Compute and store MIX_rate_baseline."
      - name: "Analyze Steered Results"
        steps:
          - "Initialize an empty selection_table."
          - "For each labeled file data/labels/labeled_steered_val_l{l}_i{i}_m{m}_a{a}.jsonl:"
          - "Parse (l, i, mode, alpha) from the filename."
          - "Compute MIX_rate_steered and SINGLE_POLICY_rate_steered."
          - "Compute ΔMIX = MIX_rate_steered - MIX_rate_baseline and ΔSINGLE_POLICY."
          - "Load matching steered controls (data/gens/steered_controls_l{l}_i{i}_m{m}_a{a}.jsonl) and baseline controls (data/gens/baseline_controls_greedy.jsonl)."
          - "Compute drift metrics: mean_KL (first 3 tokens) and Top10_overlap."
          - "Compute J_anti_mix = (-ΔMIX) - 1.0 * mean_KL."
          - "Append a row with all metrics to selection_table."
      - name: "Select Best Vector"
        steps:
          - "Filter selection_table to rows with alpha == 0.4."
          - "Pick the row with max J_anti_mix; record winning (l*, i*, mode*)."
      - name: "Finalize Deliverables"
        steps:
          - "Load artifacts/within/candidate_vectors.npz."
          - "Extract the vector for (l*, i*); save single best vector to artifacts/within/v_star.npz with metadata {layer: l*, position: i*, mode: mode*, alpha: 0.4}."
          - "Save selection_table to artifacts/within/selection_table.json."
    deliverables:
      - path: "artifacts/within/v_star.npz"
        desc: "The single best-performing vector v* with its winning metadata."
      - path: "artifacts/within/selection_table.json"
        desc: "Detailed table of metrics for every candidate, used to make the final selection."
    acceptance:
      - "artifacts/within/selection_table.json is fully populated and saved."
      - "The chosen v* corresponds to the entry with the highest J_anti_mix score at α = 0.4."
      - "The winning candidate’s ΔMIX is negative (MIX reduction confirmed)."


  - id: 6-steer
    title: Final Evaluation on the held-out Final Test Set
    actions:
      - Unlock prompts/final_test_set.json and prompts/final_test_nonconf.json.
      - Implement the selected mode from Step 5 (M1 or M2).
      - For α ∈ {0.2, 0.4, 0.8}:
        * Conflicts (final_test_set): sample with temperature=0.8.
        * Controls (final_test_nonconf): greedy decoding for KL/top-k.
        * Label these outputs with the same strong judge + spot-check.
        * Metrics:
          - Primary: Δ%MIX (↓ desired)
          - Secondary: Δ%SINGLE_POLICY (↑ expected), Δ%OTHER (report)
          - mean KL (first 3 tokens) on controls; Top-10 overlap (k=10)
          - Leave-one-family-out retained gain (train on F1, test on F2; and/or vice versa)
      - Choose α† (default 0.4) maximizing (-ΔMIX) under drift constraints.
    deliverables:
      - data/steered_test_gens.jsonl (per α)
      - data/steered_test_labels.jsonl
      - artifacts/final_metrics.json
    acceptance:
      - At α†≈0.4:
        * MIX reduced by ≥ 10–20 percentage points on conflicts
        * mean KL ≤ 0.2 nats; Top-10 overlap ≥ 0.9 on test controls
        * Retained cross-family gain ≥ 50%

  - id: 7-baselines
    title: Baselines & ablations (including gated preamble baseline)
    actions:
      - Random direction (matched norm).
      - Prompt baseline (GLOBAL preamble): “If constraints conflict, ask one clarifying question before proceeding.” (optional; expect high false positives on controls)
      - Gated preamble baseline (optional):
        * Train a simple conflict detector (linear probe at end-of-prompt on Dev minimal pairs).
        * Gate a one-line clarifier only when the score > τ.
      - Refusal-direction baseline (optional, only if available from prior work/models).
      - Run all baselines on the Final Test Set.
    deliverables:
      - artifacts/baseline_metrics_test_set.json (ΔMIX, ΔSINGLE_POLICY, KL, histograms, FPR on controls)
    acceptance:
      - Anti-mix steering outperforms random on ΔMIX and respects drift; gated preamble (if run) has far lower FPR than global preamble.

  - id: 8-pack
    title: Plots & 1-pager packaging
    actions:
      - notebooks/figs.ipynb to render:
        - P1: Type histograms (baseline vs best α†) — Final Test Set
        - P2: Dose–response curve (MIX vs α with CI) — Validation and Test
        - P3: Leave-one-family-out retained gain
        - P4: KL & Top-k overlap on controls
        - P5: Conflict-Head Atlas heatmap (layer × head) if Step 8 run
      - EXEC_SUMMARY.md with claim, 2 plots, one example, key numbers; include a box on the Type-2 negative result.
    deliverables:
      - artifacts/P1.png
      - artifacts/P2.png
      - artifacts/P3.png
      - artifacts/P4.png
      - artifacts/P5.png
      - EXEC_SUMMARY.md
    acceptance:
      - All figures render; summary includes exact numbers vs thresholds; paths printed.