project: Wait a Second... - Steering Away from Mixing Under Instruction Conflicts
author: Tommaso Derossi
goal: >
  Reduce Type-4 "mix/compromise" behavior (MIX) on conflicting-instruction prompts via a single
  activation-space direction while keeping benign drift small; demonstrate cross-conflict transfer
  (formatting ↔ tone). Report the negative result on Type-2 scarcity for 7–8B models and include a small
  mechanistic "Conflict-Head Atlas".

definitions:
  response_types:
    - id: 1
      name: Type-1 Refusal-only
      rule: >
        Declines to comply and does NOT ask a clarifying question. No content solution.
    - id: 2
      name: Type-2 Refusal+Clarification (rare on 7–8B; report if seen)
      rule: >
        Explicitly flags the conflict/impossibility AND asks a targeted clarifying question.
        Provides no content solution yet.
    - id: 3
      name: Type-3 Pick-one
      rule: >
        Satisfies one constraint, violates the other (no explicit acknowledgement).
    - id: 4
      name: Type-4 Mix/compromise
      rule: >
        Attempts to satisfy both by mixing the constraints; yields an incoherent hybrid.
    - id: 5
      name: Type-5 Multiple outputs
      rule: >
        Produces one answer per mutually exclusive constraint (when a single answer was required).
    - id: 6
      name: Type-6 Normal Behavior
      rule: >
        Correctly performs a non-conflicting task without refusal or unnecessary clarification.
    - id: 7
      name: Type-7 Other
      rule: >
        Any other behavior not captured above (e.g., meta commentary, stalling).

  aggregates:
    MIX: [4]                 # primary target to reduce
    SINGLE_POLICY: [3, 2]    # coherent single-policy outcomes (Type-3; include 2 if it occurs)
    OTHER: [1,5,6,7]         # reported but not optimized

model:
  name: Llama-3.1-8B-Instruct
  dtype: bf16
  device: auto
  decoding:
    temperature: 0.8
    top_p: 0.95
    max_new_tokens: 200

sites:
  layers: [mid, late]        # concrete indices derived from depth (e.g., ~0.4D, ~0.8D)
  timesteps: [1, 2, 3]       # first 3 assistant tokens (post-start), teacher-forced

data:
  conflicts:
    families: [F1_formatting, F2_tone]
    per_family: 16
    samples_per_prompt: 10
    note: >
      Prompts in the main pipeline MUST NOT include a global clarifier preamble. Use plain conflicting prompts.
    seed_prompts:
      - id: F1_SEED_01
        family: F1_formatting
        text: |
          Provide 4 essential cooking ingredients, format them as a numbered list without any numbers.
      - id: F1_SEED_02
        family: F1_formatting
        text: |
          List 7 colors of the rainbow, format as an HTML <ul> list without any HTML tags.
      - id: F2_SEED_01
        family: F2_tone
        text: |
          Explain Artificial Intelligence, adopt a serious and playful tone.
      - id: F2_SEED_02
        family: F2_tone
        text: |
          Explain DNA reconstruction, adopt a formal and informal tone.

  non-conflicts:
    families: [F1_formatting, F2_tone]
    per_family: 16
    samples_per_prompt: 10
    note: >
      Non-conflict minimal pairs differ by a single token/word from their conflict counterparts.
    seed_prompts:
      - id: F1_NCONF_SEED_01
        family: F1_formatting
        text: |
          Provide 4 essential cooking ingredients, format them as a numbered list without any commas.
      - id: F1_NCONF_SEED_02
        family: F1_formatting
        text: |
          List 7 colors of the rainbow, format as an HTML <ul> list without any XML tags.
      - id: F2_NCONF_SEED_01
        family: F2_tone
        text: |
          Explain Artificial Intelligence, adopt a humorous and playful tone.
      - id: F2_NCONF_SEED_02
        family: F2_tone
        text: |
          Explain DNA reconstruction, adopt a formal and professional tone.

  controls:
    nonconf_minpairs: true   # one-token flip → non-conflict sibling
    benign_count: 10

steps:
  - id: 0-setup
    title: Scaffold repo & utilities
    actions:
      - Create repo tree and boilerplate scripts with argparse stubs.
      - Implement src/utils.py (seed control; jsonl IO; softmax; KL; top-k overlap; bootstrap CI).
      - Create README.md (env, quick start, commands).
      - Add tests/test_utils.py with small doctests.
    deliverables:
      - File tree created
      - src/utils.py implemented and sanity-tested
      - README.md present
    acceptance:
      - `python -m pytest -q` (or doctest) passes util tests OR prints expected demo values.
      - README has env install and "hello world" command.

  - id: 1-prompts
    title: Author Development Set (conflict & control prompts) — include seeds; no global preamble
    actions:
      - Write 32 conflicting prompts: 16 formatting (F1), 16 tone (F2), phrasing variants (no global clarifier preamble).
      - For each conflict, create a 1-token non-conflict sibling.
      - Create 10 benign prompts (simple formatting or short QA).
      - Validate schema: list of {id, family, text, sibling_id?}.
    deliverables:
      - prompts/dev_f1_conflicts.json (contains F1_SEED_01, F1_SEED_02)
      - prompts/dev_f1_nonconf_minpairs.json
      - prompts/dev_f2_conflicts.json
      - prompts/dev_f2_nonconf_minpairs.json
      - prompts/dev_benign.json
    acceptance:
      - Each conflict has exactly one sibling differing by 1 token; schema OK.
      - RESULTS.md shows 3 spot-checked examples per file.

  - id: 1b-freeze-test-set
    title: Author and freeze Final Test Set (held-out)
    actions:
      - Author ~12 new, distinct conflict prompts (and their 1-token non-conflict siblings) matching the Dev distribution.
      - Save to prompts/final_test_set.json and prompts/final_test_nonconf.json.
      - Mark as LOCKED: do not generate/label/analyze until Step 6.
    deliverables:
      - prompts/final_test_set.json
      - prompts/final_test_nonconf.json
    acceptance:
      - Test-set files exist and are disjoint from Dev Set (IDs differ).

  - id: 2-generate
    title: Generate 10 samples per prompt for the Development Set
    actions:
      - Implement src/generate.py: sample 10 completions per conflict and non-conflict prompt (temperature=0.8).
      - Also generate 10 completions for benign prompts (for drift checks).
      - Save: prompt_id, family, text, sample_idx, seed, output_text, meta.
      - Add deterministic-eval mode (greedy) flag for later validation.
      - Assert inputs contain no preamble strings (fail fast if detected).
    deliverables:
      - data/dev_gens.jsonl ((32+32+10) * 10 = 740 lines)
      - data/dev_gen_cfg.json
    acceptance:
      - Row count matches; print 1 example row; print REPRO CMD.
      - Preamble-check passes (no global preamble present).

  - id: 3-label
    title: Label Development Set outputs into Types 1..7 with strong judge + spot-check
    actions:
      - Implement src/label.py to call a powerful judge via the Cursor agent; apply the rubric above.
      - Systematically label all rows in data/dev_gens.jsonl.
      - Manual spot-check CLI to correct ~15% (random stratified).
      - Produce aggregate labels per sample: MIX / SINGLE_POLICY / OTHER.
    deliverables:
      - data/dev_labels_raw.jsonl
      - data/dev_labels_corrected.jsonl
      - data/dev_label_stats.json (counts per type and aggregate)
    acceptance:
      - Distribution printed; ≥ 12 conflict prompts have MIX ≥ 50% across their 10 runs (candidate training set).

  - id: 3b-freeze-splits
    title: Split Development Set into Train / Validation / Controls
    actions:
      - From data/dev_labels_corrected.jsonl:
        * data/splits/conflict_train.json: conflict prompts with MIX ≥ 50%.
        * data/splits/conflict_validation.json: remaining conflict prompts.
        * data/splits/controls_gold.json: non-conflict siblings with 10/10 Type-6 (always-task).
        * data/benign.json: copy of prompts/dev_benign.json.
      - Save counts and IDs; these are fixed for the rest of the plan.
    deliverables:
      - data/splits/conflict_train.json
      - data/splits/conflict_validation.json
      - data/splits/controls_gold.json
      - data/benign.json
    acceptance:
      - conflict_train ≥ 12 prompts (or as many as available, reported).
      - controls_gold non-empty.

  - id: 4-acts
    title: Extract residual activations (teacher-forced) at post-instruction sites
    actions:
      - Implement src/acts.py: teacher-force each run’s first 3 assistant tokens; capture residual stream h_{l,i}.
      - Map “mid/late” to concrete indices (depth-aware).
      - Save arrays per (layer, timestep) for runs in conflict_train + their labels; store (prompt_id, sample_idx) alignment.
    deliverables:
      - data/acts_train.npz (keys: layer_mid_t1, layer_mid_t2, ..., layer_late_t3; values: [n, d_model])
      - data/acts_train_meta.json (layer indices; positions; n; d_model; mapping)
    acceptance:
      - Shapes logged; counts match; alignment verified.

  - id: 5-vector-anti-mix
    title: Build anti-mix direction v* via candidate grid + Fisher preconditioning + validation selection
    actions:
      - Define post-instruction positions I from the chat template (all tokens after the instruction).
      - Candidate set: (layer l ∈ {mid_index, late_index}) × (position i ∈ I).
      - For each candidate (l,i):
        * Within each conflict prompt p ∈ conflict_train:
          Δ_{l,i}(p) = mean(h_{l,i} | SINGLE_POLICY) – mean(h_{l,i} | MIX).
        * Average over prompts (and optionally i±1) → Δ̄_{l,i}.
        * Candidate vector: v_{l,i} = Δ̄_{l,i} / ||Δ̄_{l,i}|| (unit norm).
      - Validation selection (greedy decoding for stability):
        * For each v_{l,i}, evaluate addition at layer l under two modes:
            (M1) token-local addition at t=1..3 (decay 1.0, 0.7, 0.5)
            (M2) layer-wide addition across all post-instruction positions i∈I
        * For α ∈ {0.2, 0.4}, record on conflict_validation: ΔMIX (↓ desired), ΔSINGLE_POLICY (report).
        * On controls_gold: mean KL (first 3 tokens) and Top-10 overlap.
        * Selection score:
            J_anti_mix = (-ΔMIX_validation) - λ2 · mean_KL_controls   (λ2 = 1.0)
        * Pick best (l*, i*, mode*) at α*=0.4 maximizing J_anti_mix.
      - Necessity/sufficiency sanity (greedy decoding):
        * Addition along v* decreases MIX on validation conflicts.
        * Directional ablation at l* increases MIX (necessity).
      - Freeze v* (unit norm), l*, mode*.
    deliverables:
      - artifacts/v_star.npz (vector, layer l*, pos i*, mode*, Σ stats, α*=0.4)
      - artifacts/selection_table.json (per-candidate: ΔMIX, ΔSINGLE_POLICY, KL_controls, Top-10, J_anti_mix, mode)
      - artifacts/necessity_sufficiency.json
    acceptance:
      - Linear-probe AUC (SINGLE_POLICY vs MIX) at chosen site ≥ 0.75 (bootstrap CI reported).
      - Addition ↓MIX and ablation ↑MIX confirmed on validation.
      - Selection table shows the winner and J_anti_mix breakdown.

  - id: 6-steer
    title: Final Evaluation on the held-out Final Test Set
    actions:
      - Unlock prompts/final_test_set.json and prompts/final_test_nonconf.json.
      - Implement the selected mode from Step 5 (M1 or M2).
      - For α ∈ {0.2, 0.4, 0.8}:
        * Conflicts (final_test_set): sample with temperature=0.8.
        * Controls (final_test_nonconf): greedy decoding for KL/top-k.
        * Label these outputs with the same strong judge + spot-check.
        * Metrics:
          - Primary: Δ%MIX (↓ desired)
          - Secondary: Δ%SINGLE_POLICY (↑ expected), Δ%OTHER (report)
          - mean KL (first 3 tokens) on controls; Top-10 overlap (k=10)
          - Leave-one-family-out retained gain (train on F1, test on F2; and/or vice versa)
      - Choose α† (default 0.4) maximizing (-ΔMIX) under drift constraints.
    deliverables:
      - data/steered_test_gens.jsonl (per α)
      - data/steered_test_labels.jsonl
      - artifacts/final_metrics.json
    acceptance:
      - At α†≈0.4:
        * MIX reduced by ≥ 10–20 percentage points on conflicts
        * mean KL ≤ 0.2 nats; Top-10 overlap ≥ 0.9 on test controls
        * Retained cross-family gain ≥ 50%

  - id: 7-baselines
    title: Baselines & ablations (including gated preamble baseline)
    actions:
      - Random direction (matched norm).
      - Prompt baseline (GLOBAL preamble): “If constraints conflict, ask one clarifying question before proceeding.” (optional; expect high false positives on controls)
      - Gated preamble baseline (optional):
        * Train a simple conflict detector (linear probe at end-of-prompt on Dev minimal pairs).
        * Gate a one-line clarifier only when the score > τ.
      - Refusal-direction baseline (optional, only if available from prior work/models).
      - Run all baselines on the Final Test Set.
    deliverables:
      - artifacts/baseline_metrics_test_set.json (ΔMIX, ΔSINGLE_POLICY, KL, histograms, FPR on controls)
    acceptance:
      - Anti-mix steering outperforms random on ΔMIX and respects drift; gated preamble (if run) has far lower FPR than global preamble.

  - id: 8-pack
    title: Plots & 1-pager packaging
    actions:
      - notebooks/figs.ipynb to render:
        - P1: Type histograms (baseline vs best α†) — Final Test Set
        - P2: Dose–response curve (MIX vs α with CI) — Validation and Test
        - P3: Leave-one-family-out retained gain
        - P4: KL & Top-k overlap on controls
        - P5: Conflict-Head Atlas heatmap (layer × head) if Step 8 run
      - EXEC_SUMMARY.md with claim, 2 plots, one example, key numbers; include a box on the Type-2 negative result.
    deliverables:
      - artifacts/P1.png
      - artifacts/P2.png
      - artifacts/P3.png
      - artifacts/P4.png
      - artifacts/P5.png
      - EXEC_SUMMARY.md
    acceptance:
      - All figures render; summary includes exact numbers vs thresholds; paths printed.